<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Caffe2官方教程(一)-Intro Tutorial]]></title>
    <url>%2Fz_post%2FCaffe2-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-IntroTutorial%2F</url>
    <content type="text"><![CDATA[本篇教程主要介绍一些 Caffe2 里面基础概念, 以便帮助理解 Caffe2 模型的设计思路和基本原理. 在 Caffe2 中, 对于任意一个 operator, 我们不仅仅需要提供 input, 同时还需要提供 weights 和 bias.(这是与 Caffe 的不同点之一) Blobs and Workspace, Tensors在 Caffe2 中, 数组的组织形式是 blobs. 一个 blobs 可以看做是内存当前一块带有名字的数据. 绝大多数 blobs 都包含一个 tensor, 在 Python 中, 会被转换成 numpy 数组来表示. 而 workspace 会存储所有的 blobs, 下面的代码展示了如何将 blobs 添加到 workspace 中, 以及如何再次获取它们. workspace 会在你开始使用的时候对自身进行初始化. 1234567891011from caffe2.python import workspace, model_helperimport numpy as np# Create random tensor of three dimensionsx = np.random.rand(4, 3, 2)print(x)print(x.shape)workspace.FeedBlob("my_x", x)x2 = workspace.FetchBlob("my_x")print(x2) Nets and Operators在 Caffe2 中, 基本模型的抽象形式为 net. 一个 net 可以看成是一个图, 其中节点为各种 operators, 这些 operators 会接受一系列 blobs 作为输入, 然后会输出一个或多个 blobs.在下面的代码块中, 我们将会创建一个简单的模型, 它包含以下三个部分: 一个全连接层(FC) 一个使用了 Softmax 的 Sigmoid 激活函数 交叉熵损失函数 我们利用 ModelHelper 来帮助创建模型, 它会创建以下两个互相联系的 nets: 一个执行参数初始化(ref.init_net) 一个执行训练逻辑(ref.exec_net) 1234567891011121314151617# Create the input datadata = np.random.rand(16, 100).astype(np.float32)# Create labels for the data as integers [0, 9].label = (np.random.rand(16) * 10).astype(np.int32)workspace.FeedBlob("data", data)workspace.FeedBlob("label", label)# Create model using a model helperm = model_helper.ModelHelper(name="my first net")weight = m.param_init_net.XavierFill([], 'fc_w', shape=[10, 100])bias = m.param_init_net.ConstantFill([], 'fc_b', shape=[10, ])fc_1 = m.net.FC(["data", "fc_w", "fc_b"], "fc1")pred = m.net.Sigmoid(fc_1, "pred")softmax, loss = m.net.SoftmaxWithLoss([pred, "label"], ["softmax", "loss"]) 上面的代码首先在内存中创建了数据和标签的 blobs. 数据的第一维代表 batch size, 即16. 许多 Caffe2 的 operators 都可以利用 ModelHelper 直接获取, 因此我们用它创建了一系列 operators, 包括: FC, Sigmoid 和 SoftmaxWithLoss.ModelHelper 会创建两个 nets: m.param_init_net 用于初始化指定的参数, 只需要执行一次即可. m.net 用于执行训练逻辑, 该过程对用户透明, 且是自动运行的.网络的定义存储在一个 protobuf 结构当中(Google’s Protocal Buffer), 可以通过下面的方式来进行检查网络 1print(m.net.Proto()) 输出如下: 123456789101112131415161718192021222324252627name: &quot;my first net&quot;op &#123; input: &quot;data&quot; input: &quot;fc_w&quot; input: &quot;fc_b&quot; output: &quot;fc1&quot; name: &quot;&quot; type: &quot;FC&quot;&#125;op &#123; input: &quot;fc1&quot; output: &quot;pred&quot; name: &quot;&quot; type: &quot;Sigmoid&quot;&#125;op &#123; input: &quot;pred&quot; input: &quot;label&quot; output: &quot;softmax&quot; output: &quot;loss&quot; name: &quot;&quot; type: &quot;SoftmaxWithLoss&quot;&#125;external_input: &quot;data&quot;external_input: &quot;fc_w&quot;external_input: &quot;fc_b&quot;external_input: &quot;label&quot; 通过下面的代码可以查看参数初始化网络 1print(m.param_init_net.Proto()) 输出如下: 1234567891011121314151617181920name: &quot;my first net_init&quot;op &#123; output: &quot;fc_w&quot; name: &quot;&quot; type: &quot;XavierFill&quot; arg &#123; name: &quot;shape&quot; ints: 10 ints: 100 &#125;&#125;op &#123; output: &quot;fc_b&quot; name: &quot;&quot; type: &quot;ConstantFill&quot; arg &#123; name: &quot;shape&quot; ints: 10 &#125;&#125; 可以看到有两个 operators, 它们会分别对 FC 的权重和偏置参数进行初始化. Executing现在, 既然我们已经定义好了用于网络训练的 operators, 那么就可以利用下面的代码来训练我们的简单模型.首先, 调用一次参数初始化网络: 1workspace.RunNetOnce(m.param_init_net) 注意, 通常情况下, 上面的代码会将 param_init_net 的 protobuffer 结构传送给 C++ 运行时以供执行. 接下来, 创建真正用于训练的网络 1workspace.CreateNet(m.net) 我们只需要创建该网络一次, 然后可以多次执行它 123456789workspace.CreateNet(m.net, overwrite=True) # 这里需要将 overwrite 设置为 True, 官方教程没有设值, 运行时会出现RuntimeError# Run 100 × 10 iterationsfor _ in range(100): data = np.random.rand(16, 100).astype(np.float32) label = (np.random.rand(16)*10).astype(np.int32) workspace.FeedBlob("data", data) workspace.FeedBlob("label", label) workspace.RunNet(m.name, num_iter=10) #Run net 10 times 注意, 因为我们已经在 workspace 中定义过 net 了, 因此, 我们只需要传送 m.name 给 RunNet 函数即可, 而无需再次定义网络.在训练迭代完成后, 可以通过 Fetch 来查看计算的结果 12print(workspace.Fetch("softmax"))print(workspace.Fetch("loss")) Backward pass上面的网络仅仅包含 foward pass, 因此它不会学到任何东西, 我们可以通过在每一个 operator 中添加一个梯度计算操作来实现 backward pass. 我们需要在调用 RunNetOnce() 之前在网络中插入下面的 operator: 1m.AddGradientOperators([loss]) 接着可以看看新的网络的结构 1print(m.net.Proto()) 输出如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566name: &quot;my first net&quot;op &#123; input: &quot;data&quot; input: &quot;fc_w&quot; input: &quot;fc_b&quot; output: &quot;fc1&quot; name: &quot;&quot; type: &quot;FC&quot;&#125;op &#123; input: &quot;fc1&quot; output: &quot;pred&quot; name: &quot;&quot; type: &quot;Sigmoid&quot;&#125;op &#123; input: &quot;pred&quot; input: &quot;label&quot; output: &quot;softmax&quot; output: &quot;loss&quot; name: &quot;&quot; type: &quot;SoftmaxWithLoss&quot;&#125;op &#123; input: &quot;loss&quot; output: &quot;loss_autogen_grad&quot; name: &quot;&quot; type: &quot;ConstantFill&quot; arg &#123; name: &quot;value&quot; f: 1.0 &#125;&#125;op &#123; input: &quot;pred&quot; input: &quot;label&quot; input: &quot;softmax&quot; input: &quot;loss_autogen_grad&quot; output: &quot;pred_grad&quot; name: &quot;&quot; type: &quot;SoftmaxWithLossGradient&quot; is_gradient_op: true&#125;op &#123; input: &quot;pred&quot; input: &quot;pred_grad&quot; output: &quot;fc1_grad&quot; name: &quot;&quot; type: &quot;SigmoidGradient&quot; is_gradient_op: true&#125;op &#123; input: &quot;data&quot; input: &quot;fc_w&quot; input: &quot;fc1_grad&quot; output: &quot;fc_w_grad&quot; output: &quot;fc_b_grad&quot; output: &quot;data_grad&quot; name: &quot;&quot; type: &quot;FCGradient&quot; is_gradient_op: true&#125;external_input: &quot;data&quot;external_input: &quot;fc_w&quot;external_input: &quot;fc_b&quot;external_input: &quot;label&quot; 可以看到, 网络中增加了4个新的 operators, 其输入为 backward 的计算结果, 输入为相应参数的梯度.]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>Caffe2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rethinking Pretraining]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-RethinkingPretraining%2F</url>
    <content type="text"><![CDATA[核心亮点摘要本文利用随机初始化来训练目标检测和实例分割的网络模型, 并给出了不弱于使用预训练模型的实验结果. 结果显示, 即使在使用针对预训练模型优化的超参数时, 随机初始化的训练也能达到和预训练模型初始化差不多的精度水平, 只不过迭代次数相对更多. 本文的实验结果即使在下面三种情况中依然适用: 1), 使用训练数据的 $10%$; 2), 使用更深更宽的模型(deeper, wider); 3), 使用不同的任务和评价标准. 实验结果表明, 预训练模型初始化在训练初期可以加速收敛, 但是训练模型并没有提供正则化能力, 也不能确保提升任务精度. 这样的观察结果对传统的依赖于预训练模型的任务来说是一个挑战, 同时我们也期望该结果可以对人们重新思考计算机视觉中的 fine-tuning 提供起到一些启发.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Relation-Network (CVPR, 2018)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Relation-Network-CVPR2018%2F</url>
    <content type="text"><![CDATA[文章: Relation Networks for Object Detection作者: Han Hu1, Jiayuan Gu, Zheng Zhang, Jifeng Dai1, Yichen Wei备注: MSRA, Peking University, Oral Paper 摘要尽管长久以来, 人们都任务对图像中物体之间的关系进行建模可以提升目标检测模型的精度, 但是, 就目前而言, 所有先进的 目标检测模型都是将物体作为一个独立的个体进行检测的, 并没有在学习的过程中利用到它们之间的关系(Relations).本文提出了一个物体关系模型, 它会通过物体之间的特征和纹理来 同时 处理一系列物体之间的关系, 因此允许对这些关系进行建模. 本文提出的方法是轻量级的, 无需额外的监督信息, 并且可以很容易嵌入到现有的模型当中. 在现代的目标检测流水线当中, 本文的方法对于目标识别和去除重复步骤的改进是有效的, 验证了在基于CNN的检测模型中对物体关系建模的有效性. 本文的模型是收个完全的端到端的目标检测模型. 介绍近年来, 基于深度学习的目标检测取得了很多成功, 但是, 依然没能够利用到物体之间的相关关系信息, 其中一个难点在于目前的检测模型都比较简单, 无法对复杂度物体关系进行建模.本文的方法受到了自然语言处理中 attention 模型的启发, attention 模型中一个元素可以被某个集合的元素的累积权重所影响. 这个累积权重是在模型学习的过程当中学习到的, 近年来, attention 模型在图像描述领域也得到了许多成功应用.在本文中, 我们首次提出了适用于目标检测任务的自适应的 attention 模型. 该模型建立与一个基本的 attention 模型, 不同之处在于其主要元素不再是 words, 而是 objects. 物体(objects)具有2D的空间布局, 并且具有不同的 scale 和 aspect ratio, 这些信息相对于一维的 words 来说更加复杂. 因此, 本文提出的模型会将原来的 attention 权重扩展成两部分: 原始的权重和一个新的几何(geometric)权重. 后者会对物体间的空间关系进行建模, 并且仅仅考虑它们之间的相对几何关系, 使得整个模型具有平移不变性—-这正是物体识别期望的一个性质( 物体检测希望的是平移可变性, 这里会不会有些问题? ). 通过实验证明, 这里新添加的几何权重(geometric weights) 是非常重要的.本文的模型称为 object relation module, 它和 attention 模型具有相同的优点. 它会接受多个输入, 并且以并行的方式进行处理(相对于序列模型的串行), 并且使可导和 in-place(输入输出的维度相同) 的, 因此, 本文提出的模型可以作为一个基本的 building block 灵活的嵌入到现有的各个模型当中去.如图1所示, 我们可以将 relation module 嵌入到现有的模型当中去, 来提升 instance recognition step, 同时 duplicate removal step. 从原理上来说, 我们的方法与目前的大部分检测方法都不相同, 并且可以弥补目标的许多检测方法. 本文的方法采用了一个新的维度: 一系列的物体在被处理时, 会同时对其他物体的识别产生影响, 而不是将每个物体单独识别. 相关工作Object Relation in post-processing: 这些方法在 pre-DP 时代取得了不错的效果, 但是在 deep ConvNets 时代却没能表现出其有效性. Sequential relation modeling: LSTM. 在目标检测任务中, 有方法建议令先找到的物体会帮助寻找下一个物体, 但是没能证明该方法的有效性. Human centered scenarios: 关注与人相关的关系检测, 但是需要额外的监督标签. Duplicate removal: 去重, NMS, GossipNet(learn duplicate removal), Attention modules in NLP and physical system modeling: Attention Module. 物体关系模型(Object Relation Module)我们首先回顾一个简单的 Attention 模型, 名为 Scaled Dot-Product Attention. 假设输入为 $d_k$ 维的 queries 和 keys, 并且具有 $d_v$ 维的values. 点积会在 query 和所有的 keys 之间进行, 以获取它们之间的相似度, 我们利用 SoftMax 函数来获取 values 的维度. 具体来说, 给定一个 query q, keys(packed into matrices K), values(packed into V), 则输出如下: v^{out} = softmax( \frac{qK^t}{\sqrt{d_k}}) V \tag 1接下来我们描述一下物体关系的计算. 令 $f_G$ 代表物体的几何信息, 即边框的四个坐标, $f_A$ 代表物体的特征信息, 具体形式视任务而定. 当给定 $N$ 个物体 $\{ f_A^n, f_G^n \}, n=1, …, N$ 时, 第 $n$ 个物体和其他所有物体之间的关系特征 $f_R(n)$ 为: f_R(n) = \sum_{m} \omega^{mn} \cdot (W_V \cdot f_A^m) \tag 2上面的输出是第 $n$ 个物体与所有所有物体的特征信息关系的权重和, 通过 $W_V$ 进行线性转换, 关系权重 $\omega^{mn}$ 表明了对其他物体对当前物体的影响程度, 计算公式如下: \omega^{mn} = \frac{\omega_G^{mn}\cdot exp(w_A^{mn})}{\sum_k \omega_G^{kn}\cdot exp(\omega_A^{kn})} \tag 3物体特征的权重 $\omega_A^{mn}$ 计算公式如下: \omega_A^{mn} = \frac{dot(W_k f_A^m, W_Q f_A^n)}{\sqrt(d_k)} \tag 4这里的 $W_K$ 和 $W_Q$ 都是评价标准(matrices), 它们会将原始的图片特征 $f_A^m$ 和 $f_A^n$ 投影到一个子空间中去, 并在此空间可以描述这些特征的好坏, 投影之后的维度是 $d_k$.几何权重(Geometry weight)计算如下: \omega_G^{mn} = max{0, W_G \cdot \xi_G(f_G^m, f_G^n)} \tag 5这里有两步, 首先, 两个物体的几何特征会被嵌入到一个更高的维度 $\omega_G$ 上, 然后, 为了保证平移不变性和尺寸不变性, 我们会利用一个4个的相对几何信息来代替: (log(\frac{|x_m - x_n}{w_m}), log(\frac{|y_m - y_n}{h_m}), log(\frac{w_n}{w_m}), log(\frac{h_n}{h_m})) 然后, 嵌入后的相对坐标会通过一个权重矩阵 $W_G$ 转化成一个标量, 并且用ReLU 来激活. 几何特征的attention有效性正如表1(a)所示. 一个物体关系模型总共会累积 $N_r$ 个关系特征, 并且通过加上下面的项来增加输入的图片特征: $$f_A^n = f_A^n + Concat[f_R^1(n), ..., f_R^{N_r}(n)], \text{for all} n \tag 6上式的流程可以总结出算法1, 如下所示: 上式算法可以通过基本的操作实现, 如图2所示 空间复杂度和计算复杂度如下所示: O(Space) = N_r (2d_f d_k + d_g) + d_f^2O(Comp) = N d_f(2N_r d_k + d_f) + N^2 N_r(d_g + d_k + d_f/N_r + 1)通常情况下: $N_r = 16, d_k = 64, d_g = 64$. 关系模型具有相同的输入和输出, 使得可以更容易作为 building block 添加到现有模型当中. 目标检测关系模型(Relation Networks For Objects Detection)目标检测的流程可以分布以下四步: 在整张图片上生成特征图谱 生成候选区域框 执行实例识别(instance recognition) 去重(duplicate removal): NMS 本文提出的物体关系模型主要作用于后两步, 即令起提升 instance recognition 的能力以及具有学习去重的能力. Our implementation of different architectures:用 ResNet 作为backbone, 用 RPN 来生成候选区域框, 并对以下几种模型进测试. Faster RCNN FPN DCN抛去以上三者整体结构的区别不说, 它们都是用了同样的 head 网络, 即利用 RoI pooling 后接两个全连接层来生成用于分类和回归的最终的特征图谱. Relation for Instance Recognition: {RoIFeat}_n]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[InceptionV4 and Inception-ResNet]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-InceptionV4-InceptionResNet%2F</url>
    <content type="text"><![CDATA[文章:Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning作者: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi 模型 特点 说明 Inception V1 Inception V2 Inception V3 Inception V4 InceptionResNet]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Inception V3]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-InceptionV3%2F</url>
    <content type="text"><![CDATA[文章: Rethinking the Inception Architecture for Computer Vision作者: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna备注: Google, Inception V3 核心摘要近年来, 越来越深的网络模型使得各个任务的 benchmark 都提升了不少, 但是, 在很多情况下, 我们还需要考虑模型计算效率和参数量. 本文我们将通过适当地使用 factorized convolutions(卷积分解) 和 aggressive regularization 来尽可能的是增加计算效率. 介绍AlexNet, VGGNet 虽然很成功, 但是他们的计算成本太大, 模型参数量较多, 与之相比, Inception 模型不仅参数量小, 特征提取能力也较强. 但是, Inception 模型较为复杂, 这使得我们很难对模型进行修改. 在本文中, 我们会先描述一个一般化的原则和优化想法, 使得可以高效的扩大卷积网络的大小. General Design Principles接下来我们会叙述几条基于大规模多结构的神经网络的设计原则 避免使用 representational bottlenecks, 尤其是在网络的较浅层. 前馈神经网络可以被表示成一个有向无环图, 这定义了一个非常明确的信息流. 对于任何输入, 都可以获得很大的信息. 因此, 我们应避免使用 bottlenecks 过度压缩信息. 在通常情况下, 特征的尺寸应该从输入到输出平缓降低. 理论上来说, 降维后的信息不能完全提供足够的信息内容给后续的结构, 而仅仅只是对主要信息的一种粗略的概括. 高维表示更容易在网络本地进行处理. 空间聚合可以在降低的维度 embedding 中完成, 而不需要太多或任何表征能力的损失. 比如, 在执行一个更大的卷积操作(如3×3)之前, 我们可以在 spatial aggregation 之间先降低维度, 而这不会带来严重的负面影响. 我们检测其原因是因为相邻单元之间的相关性很强, 所以导致在降维的时候损失较小. 降维有助于加速训练. 权衡网络模型深度的宽度. 提升模型的宽度和深度都可以提升模型的性能, 但是, 最好的方式是结合这两种方式, 以便使得模型的复杂度可以均衡的分布在网络的深度和宽度中. 上面的原则不建议直接使用, 更好的办法是在你不确定如何提升模型性能时进行权衡和尝试. Factorizing Convolutions with Large Filter.GooLeNet 的成功原因之一得益于广泛的使用降维. 这可以看做是 factorizing convolutions(对卷积进行因式分解) 的一种特殊情况. 在一个视觉网络中, 某点的输出与它相邻的其他点的响应之间有很高的相关性. 因此, 我们可以在聚合之前将这些响应进行降维, 在这理论上, 应该能够产生相同的局部特征表示. 由于 Inception 网络是全卷积的, 每一个权重都会与多处响应相关联, 计算成本的降低会带来参数量的降低. 这意味着 通过恰当的因式分解, 我们可以得到更多解耦的参数, 从而可以带来更快的训练速度. 分解成更小的卷积(Factorization into smaller convolutions)较大的卷积核尺寸(如5×5, 7×7)往往意味着很高的计算成本. 例如, 5×5 的计算成本为 3×3 卷积核的 25/9 = 2.78 倍. 但是, 如果直接替换为 3×3 的卷积核, 那么就会在特征表达能力上造成一些损失. 幸运的是, 我们可以通过多层小卷积核添加的方式来替换大卷积核, 如图1所示, 他们的感受野是相当的, 但是前者的参数只有后者的 $\frac{9+9}{25} = 28 %$. 图4, 图5展示了用两个3×3来替换 5×5 卷积核的示意图. 但是这种替换会引出两个问题, 其一为是否为造成特征表达能力的降低, 其二是如果我们的主要目标时分解计算的线性部分, 那么是否还应该在第一层保持线性激活? 即是否在第一层使用非线性激活函数? 对此, 通过实验证明, 使用线性激活比使用非线性激活的效果差一些, 如图2所示. 空间分解为不对称卷积(Spatial Factorization into Asymmetric Convolutions)上面的结果说明大于 3×3 的卷积核通常都可以分解为一系列 3×3 卷积核堆叠. 那么如果继续分解, 我们可以将 3×3 的卷积核分解为 3×1 的卷积核和 1×3 的卷积核, 这样一来, 参数量就变成了6, 降低了 33%, 如图3所示(将 3×3 分解成两个 2×2 的卷积核, 只能降低 11% 的参数量). 理论上, 我们可以将任何 $n\times n$ 的卷积核用一个 $n\times 1$ 和一个 $1\times n$ 的卷积核替代, 如图6所示. 在实际使用中, 我们发现这种分解方式在网络的浅层并不能很好的工作, 但是在网络的中层可以取得很好的效果(特征图谱大小在 12~20 之间的网络层). 辅助分类器的效用(Utility of Auxiliary Classifiers)Inception V1 首次引入辅助分类器来提升深度网络的收敛性, 其最初动机是为了可以及时利用那些浅层网络中有用的梯度来帮助模型快速收敛, 从而缓解深度神经网络中的梯度消失问题. 有趣的是, 我们发现这个辅助分类器并不会加快训练初期的收敛速度: 对于带有辅助分类器和不带辅助分类器的两个网络, 在模型达到较高精度以前, 他们的性能看起来是差不多的. 但是 当到了训练后期, 带有辅助分支的网络开始超越没有任何辅助分支的网络, 进而达到更高的精度.并且, 在 Inception V1 中使用了两个辅助分支, 我们发现, 将浅层的辅助分支去除并不会对最终的模型质量产生任何不利影响. 有效缩小网格尺寸(Efficient Grid Size Reduction)传统的卷积网络通过池化操作来降低特征图谱的网格尺寸, 但是为了避免降低特征表达能力, 对于一个 $d\times d\times k$ 的特征图谱, 我们通常会先利用一个卷积层使它的通道数增加到 $2k$, 然后再利用池化层来降低它的图谱尺寸到 $\frac{d}{2}$, 因此, 这一步需要的计算量为 $2d^2 k^2$. 我们可以将卷积层和池化层的位置互换, 这样一来, 计算量就会降为 $2(\frac{d}{2})^2 k^2$, 但是, 这样会导致网络的特征表达能力下降, 造成 representational bottlenecks, 如图9所示. 因此, 我们推荐另一种降低计算量的方式, 如图10所示, 我们可以利用两个并行的 block P 和 block C 来达到目的, 其中 P 代表池化, C 代表卷积. Inception-v2表1展示了本文网络的整体布局 注意到我们将原来的 7×7 卷积转换成了3个 3×3 卷积. 图8 Model Regularization via Label Smoothing表3 展示了 ILSVRC 2012 的测试结果 表4 Training MethodologyPerformance on Lower Resolution Input表2 Experimental Results and Comparisons表5]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Inception V2]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-InceptionV2%2F</url>
    <content type="text"><![CDATA[文章: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift作者: Sergey Ioffe, Christian Szegedy备注: Google, Inception V2 Inception V2论文概览Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 要点整理GoogLeNet设计的初衷就是要又准又快，而如果只是单纯的堆叠网络虽然可以提高准确率，但是会导致计算效率有明显的下降，所以如何在不增加过多计算量的同时提高网络的表达能力就成为了一个问题。 Inception V2版本的解决方案就是 修改Inception的内部计算逻辑，提出了比较特殊的“卷积”计算结构。 卷积分解（Factorizing Convolutions）主要受到VGG的启发 大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数，比如5×5卷积核的参数就有25个（不算depth和filters的数量）。因此，GoogLeNet团队提出可以用2个连续的3×3卷积层组成的小网络来代替单个的5×5卷积层，即在 保持感受野范围的同时又减少了参数量，如下图： 大量实验表明，这种分解并不会造成表达缺失，那么是否可以进一步分解，GoogLeNet团队尝试了n×1的卷积核，如下图所示，用3个3×1取代3×3卷积： 因此，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。GoogLeNet团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在12到20之间）。 降低特征图大小一般情况下，如果想让图像缩小，可以有如下两种方式： 前者先做pooling会导致在做卷积的时候缺少特征，后者是在卷积计算后，提取完特征以后进行的正常的图像缩小，但是计算量很大（卷积计算比pooling计算复杂）。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，最后再进行合并）。 网络结构使用Inception V2作改进版的GoogLeNet，网络结构图如下： 注：上表中的Figure 5指没有进化的Inception，Figure 6是指小卷积版的Inception（用3x3卷积核代替5x5卷积核），Figure 7是指不对称版的Inception（用1xn、nx1卷积核代替nxn卷积核）。 Batch NormalizatinBatch Normalization, 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法. 详见:]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cpp-踩坑]]></title>
    <url>%2Fz_post%2FCpp-%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[string.substr坑源: leetcode, 第140题, Word Brak II s.substr(word.size()) 可以通过, 但是s.substr(0, word.size()) 报错 runtime error. 原因:1basic_string substr(size_type pos=0, size_type count=npos) const; 当只指定一个参数时, 该参数表示的是 pos 的位置, 而 count 则默认会是字符串中剩余字符的数量. 如果 pos 的值大于字符串的size, 则会报运行时错误(runtime error).]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch-踩坑]]></title>
    <url>%2Fz_post%2FPyTorch-%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[模型与参数的类型不符Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same TensorIterator expected type torch.cuda.FloatTensor but got torch.FloatTensor 要么需要在每一处新建立的tensor上将其手动移动到 cuda 或者 cpu 上, 要么利用下面的语句设置默认设备和类型12if torch.cuda.is_available(): torch.set_default_tensor_type('torch.cuda.FloatTensor')]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RFB Net (ECCV, 2018)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-RFBNet-ECCV2018%2F</url>
    <content type="text"><![CDATA[文章: Receptive Field Block Net for Accurate and Fast Object Detection作者: Songtao Liu, Di Huang, and Yunhong Wang备注: Beihang University 核心亮点本文从感受野大小的角度出发, 提出了 RFB 模块, 可以融合多个感受野特征, 进而提升轻量级网络(SSD)的特征表达能力相比于不断增加模型复杂度(深度,宽度)来增强特征的表达能力, 本文通过一种人工设计的机制来增强轻量级模型的特征表达能力, 以期获得一种既快又好的检测模型. 摘要目前精确度最高的目标检测模型往往面临着巨大的计算开销, 而轻量级的目标检测模型在精度上却不够高. 本文通过利用人工设计来增强轻量级模型的特征, 以期获得一个既快又好的检测模型. 受到人类视觉系统感受野的启发, 文本提出了一个感受野模块(RF Block module), 它将 RFs 的 size 和 eccentricity 之间的关系考虑在内, 来增强特征的分辨能力和鲁棒性. 之后, 我们将 RFB 集成到了 SSD 之中, 建立了一个 RFB Net 检测器. 实验结果显示, 本文的 RFB Net 可以达到目前最高的性能表现. 介绍通过讨论 two-stage 和 one-stage 模型各自的特点和优势, 本文发现, 相比于不断增加模型复杂度(深度,宽度)来增强特征的表达能力, 另一种可选的做法通过一种人工设计的机制来增强轻量级模型的特征表达能力, 以期获得一种既快又好的检测模型. 另一方面, 多项研究发现, 感受野的大小是视网膜图谱离心率的函数, 并且在不同的图谱上, 离心率会逐渐升高, 如图1所示. 目前的深度网络模型, 大多将不同层之间的感受野设置成相同大小, 这就有可能降低提取到的特征的表达能力. Inception 系列虽然融合了多个尺寸的感受野特征, 但是所有的卷积核仍然是在同一个中心进行采样的. Deformable CNN 尝试自适应的改变卷积核采样点, 尽管采样网格十分灵活, 但是依然没有考虑感受野的 eccentricity 属性, 这使得所有处于感受野当中的像素点都都输出响应具有同等的贡献度, 这会导致一些更重要的信息没有被突出出来.本文根据人类视觉感受野的机制, 提出了 Receptive Field Block(RFB), 来增强轻量级的特征学习能力, 使得他们可以组建出更快更好的检测模型. 具体来说, RFB 通过使用不同大小的卷积核来实现多分支的 pooling 操作, 应用空洞卷积(dilated convolution)来控制它们的离心率(eccentricities), 并且进行 reshape 之后生成最终的特征表示, 如图2所示. 之后, 我们会将该模块集成到 SSD 网络之中, 形成一个更快更好的 one-stage 目标检测模型, 称为 RFB Net. 本文的贡献主要有以下三点: 提出了 RFB 模块, 用于提升轻量级 CNN 网络的特征表达能力 提出了基于 RFB Net 的目标检测模型, 并且通过实验证明了该模型可以在维持 one-stage 模型(SSD)复杂度的条件下增强模型的精度. 实验表明本文的 RFB Net 可以在实时运行的速度下在 VOC 和 COCO 数据集上达到 SOTA 的性能, 并且证明了 RFB 模型具有很好的泛化性能(可以连接到 MobileNet 之上). 相关工作Two-stage detector: RCNN, Fast, Faster, R-FCN, FPN, Mask R-CNNOne-stage detector: YOLO, SSD, DSSD, RetinaNetReceptive filed: Inception(多个感受野尺寸共同作用), ASPP, Deformable CNN. 图3展示了这三种方式与本文的 RFB 的区别. Receptive Field Block本文提出的 RFB 是一个多分支的卷积块, 其内部结构主要包含两个部分: 具有不同卷积核大小的多分支的卷积层, 以及紧跟其后的空洞池化或空洞卷积层. 前一部分和 Inception 相同, 复杂模拟不同尺寸的感受野, 后一部分生成 pRFs(population Receptive Fields) 尺寸和人类视觉离心率之间的关系. Multi-branch convolution layer: 本文使用 Inception V4 和 Inception-ResNet V2 来构成多分支卷积层.Dilated pooling or convolution layer: 也称为 astrous convolution layer. 图4展示了本文的 RFB 模块示意图. RFB Net Detection Architecture本文的 RFB Net 是基于 SSD 和 RFB 模块进行构建的, 其中, RFB 模块会嵌入到 SSD 网络中, 主要的改动在于将 SSD 顶层(head/top)的卷积层替换为 RFB 模块, 如图5所示. Lightweight backbone: 保持 SSD 的选择, 使用 VGG16 作为 backbone. (即使还有其他的选择, 但是为了与 SSD 形成对比, 决定选择 VGG16). RFB on multi-scale feature maps: 在 SSD 中, 使用了多个不同大小的卷积特征图谱参与预测, 在本文的实现中, 会将较大的特征图谱后接的卷积层替换为 RFB 模块. 如图5所示. Training Settings framework: Pytorch strategies: follow SSD, 包括数据增广, 难负样例挖掘等等 new conv-layers: MSRA initialization 实验Pascal VOC 2007 消融实验(Ablation Study) COCO Other BackBone]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IdentityMappings (ECCV, 2016)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ResNetIdentityMappings-ECCV2016%2F</url>
    <content type="text"><![CDATA[文章: Identity Mappings in Deep Residual Networks作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun备注: MSRA 核心亮点摘要Identity Mappings在第一篇ResNet论文中提到, 1202层的ResNet出现了性能退化的问题. 本文主要是对residual block的一个改进, 也就是将BN和ReLU放到计算权重之前进行, 称为”预激活” , 如下图所示: 关于Deep ResNet的分析https://blog.csdn.net/wspba/article/details/60750007 用简单缩放来代替恒等连接设计一个简单的缩放: $h(x_l) = \lambda_l x_l$ 来代替恒等连接: x_{l+1} = \lambda_l x_l + F(x_l, W_l)于是,继续通过递归我们可以得到: x_L =(\prod_{i=l}^{L-1}) x_l + \sum_{i=l}^{L-1}{\hat F(x_i, W_i)}对上面的式子求导, 可以得到: 可以看到, 在该式子中, 由于 $\lambda$ 连乘项的存在, 可能会使这个因子变的很大或者消失, 从而阻断从短接反向传来的信号, 进而对优化造成困难 关于Skip Connections的其他实验Constant scaling考虑对 $F$ 的缩放, 训练结果显式优化变的更加困难, 因此不建议缩放 因为 $F$ 对应的是连加项, 不会出现连乘项, 所以不能说因子很指数增长或消失 Exclusive gatingShortcut-only gating1×1 卷积shortcut在ResNet34的时候, 使用了1×1的卷积(即方案C), 并且取得了较好的结果, 表明1×1卷尺短接还是有效果的. 但是当残差单元变多时, 并不能起到很好的效果 值得注意的是1××\times1的卷积捷径连接引入了更多的参数，本应该比恒等捷径连接具有更加强大的表达能力。事实上，shortcut-only gating 和1××\times1的卷积涵盖了恒等捷径连接的解空间(即，他们能够以恒等捷径连接的形式进行优化)。然而，它们的训练误差比恒等捷径连接的训练误差要高得多，这表明了这些模型退化问题的原因是优化问题，而不是表达能力的问题 Dropout shortcut这个在统计学上相当于给短接强加了一个 $\lambda=0.5$ 的缩放, 这和constant scaling很类似, 同样阻碍了信号的传播 激活函数的使用通过重新安排激活函数(ReLU和/或BN)来使得 $f$ 成为一个恒等映射. 最原始的残差连接如下图a所示, b~e展示了其他形式. 图中所有单元的组成成分相同, 只是顺序不同, e形式取得了最后的结果, 也就是full pre-activation形式 对以上形式讨论如下: BN after addition: 图b, 此种做法正好反其道而行之, 此时 $f$ 不仅包含了 ReLU, 还包含了BN, 最终导致的结果就是阻碍了信息的传递, 是性能下降 ReLU before addition: 图c, 这是一种很直接的做法, 也很天真, 直接将ReLU移动到加法之前, 这导致了F的输出非负, 然我们我们希望残差函数的值是在政府无穷区间内的 Post-activation or Pre-activation: 如图c和d, 图d通过一种非对称的转换, 使得当前块的激活函数成为一个块的预激活项, 具体转换如下图所示: 对上图的解释就是, 在原始的设计中, 激活函数会对两条路径的下一个残差单元造成影响: y_{l+1} = f(y_l) + F(f(y_l), W_{l+1})而通过这种非对称的转换, 能够让激活函数 $\hat f$ 对于任意的 $l$ , 都只对$F$ 路径造成影响: y_{l+1} = y_l + F(\hat f(y_l), W_{l+1})于是, 新的激活函数就变成了一个恒等映射. 后激活与预激活的区别是有元素级加法的存在而造成的,一个含有N层的平铺网络，包含有N−1个激活层(BN/ReLU)，而我们如何考虑它们是否是后激活或者预激活都不要紧。但是对附加的分支层来说，激活函数的位置就变得很重要了。只使用ReLU预激活的结果与原始ResNet-110/164的已经很接近。 只是用ReLU的预激活vs完全预激活 从图d中, 我们可以看到, ReLU层不与BN层连接使用，因此无法共享BN所带来的好处, 因此, 很自然的,我们将BN层移到ReLU的前面, 最终, 性能获得了较大的提升, 超过了原始ResNet-110/164 分析文章发现预激活的影响具有两个方面: 由于$f$变成了恒等映射,优化变的更加简单 在预激活中使用BN能提高模型的正则化能力]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RexNeXt (CVPR, 2017)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-RexNeXt-CVPR2017%2F</url>
    <content type="text"><![CDATA[文章: Aggregated Residual Transformations for Deep Neural Networks作者: Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, Kaiming He备注: UC San Diego, FAIR 核心亮点本文提出了一种新的网络模型架构 ResNeXt, 通过利用多路分支的特征提取方法, 提出了一种新的基于 ResNet 残差模块的网络组成模块, 并且引入了一个新的维度 cardinality. 该网络模型可以在于对应的 ResNet 相同的复杂度下, 提升模型的精度(相对于最新的 ResNet 和 Inception-ResNet).同时, 还通过实验证明, 可以在不增加复杂度的同时, 通过增加维度 cardinality 来提升模型精度, 比更深或者更宽的 ResNet 网络更加高效. 摘要本文提出了一个简单的, 高度模型化的针对图像分类问题的网络结构. 本文的网络是通过重复堆叠 building block 组成的, 这些 building block 整合了一系列具有相同拓扑结构的变体(transformations). 本文提出的简单的设计思路可以生成一种同质的, 多分支的结构. 这种方法产生了一个新的维度, 我们将其称为基(变体的数量, the size of the set of transformations). 在 ImageNet-1K 数据集上, 我们可以在保证模型复杂度的限制条件下, 通过提升基的大小来提高模型的准确率. 更重要的是, 相比于更深和更宽的网络, 提升基的大小更加有效. 我们将本文的模型命名为 ResNeXt, 本模型在 ILSVRC2016 上取得了第二名. 本文还在 ImageNet-5K 和 COCO 数据集上进行了实验, 结果均表明 ResNeXt 的性能比 ResNet 好. 介绍目前, 计算机视觉任务已经慢慢从特征工程转向了网络工程, 但是, 随着网络深度的增加, 设计良好的网络结构, 变的异常困难. VGG-nets 保留了简单同时有效的网络结构, 它通过将多个卷积层堆叠的方式来组成神经网络, 这种堆叠式结构在 ResNet 中也得到了保留, 并在这种结构的基础上, 开发出了性能强劲的网络模型.与 VGG-nets 不同的是, Inception 系列的模型通过精心的拓扑结构设计, 也取得了很好的模型准确度. Inception 模型的一个重要的属性就是 split-transform-merge strategy. 在 Inception 模块中, 输入数据会被划分成一些更低维度的 embeddings(通过1×1卷积), 然后通过一系列特定的卷积层(3×3, 5×5)进行转换, 最后通过 concatenation 融合起来. 这种方式所使用的空间复杂度是用单层卷积的空间复杂度的一个子集. 因此, Inception 模块可以利用更低的复杂度获取更高的特征表示能力. 尽管通过精心的布置和组合, Inception 模块组成的网络可以取得较好的性能表现, 但是, 当面对一个新的任务或数据集时, 往往无法很快的找到合适的模块组合和参数设置.本文提出了一种基于 VGG/ResNet 的 repeating layers 策略的模型, 同时还利用了 split-transform-merge 策略, 如图1所示(二者复杂度相同, 但是右边具有更高的精度). 本文的方法引入了一个新的维度 cardinality, 实验表明, 通过提升该维度, 可以更有效的提升模型的精度(相比于更深和更宽, ResNeXt 101 的精度高于 ResNet-200, 但是仅仅只有其一半的复杂度), 我们将模型命名为 ResNeXt (暗示 next dimension). 模板(Template)本文提出的模型设计思路遵循 VGG/ResNets 的 repeating layers 策略. 首先包含一组堆叠的残差模块, 这些残差模块具有相同的拓扑结构, 并且服从两条基本规则: (1), 如果处理的特征图谱具有相同的尺寸大小, 这些这些 block 的超参数设置相同(filters); (2), 每次当特征图谱的尺寸缩小两倍时, 卷积核的深度也会放大两倍. 第二条规则使得每一个 block 的复杂度(FLOPs, floating-point operations)是差不多相同的. 根据这两条规则, 我们只需要设计出一个模板(template), 进而模板中所有的模块都会相应的确定(相比于 Inception 设计更加简单), 如表1所示. 回顾简单神经元最简单的神经元是通过内积计算的, 而实际上, 内积也可以被看做是一个 aggregating tansformation: \sum_{i=1}^D w_i x_i \tag 1该公式的计算操作会通过一个神经元输出, 如图2所示. 上面的操作可以被重新定义成一组关于 splitting, transforming 以及 aggregating 的组合(conbination). Splitting: 向量 $\vec x$ 被划分成了低维度的 embedding, 每一个维度为 $x_i$ Transforming: 低维度的相同表示通过权重 $w_i$ 进行 transform. Aggregating: 通过求和公式 $\sum_{i=1}^D$ 将 transformations 连接起来. 聚合变换(Aggregated Transformations)根据上面的简单神经元的讨论, 下面我们考虑将 elementary trasformation($w_i x_i$) 用一个更加一般化的函数来替代, 这个函数本身也可以是一个神经网络, 如下所示: F(x) = \sum_{i=1}^C T_i (x) \tag 2上式中的 $T_i(x)$ 可以是任意形式的函数, 通常情况下, $T_i(x)$ 会将 $x$ 映射到一个更低的维度上去, 形成一个 embedding. $C$ 代表了 Transformations 的个数, 我们将其定义为 cardinality.在本文中, 我们使用了一种简单的方式来设计 transformation function: 所有的 $T_i$ 都具有相同的拓扑结构(图1右侧).我们将(2)式的 aggregated transformation 写成残差函数的形式: y = x + \sum_{i=1}^C T_i(x) \tag 3图3展示了本文模型与 Inception-ResNet 之间的关系, 图3(a)中的一些操作和图3(b)很相似, 而图3(b)看起来很像是包含 branching 和 concatenating 的 Inception-ResNet block. 但是与 Inception 和Inception=ResNet 模块不同的是, 本文的模型在不同的 paths 之间共享同样的拓扑结构, 因此, 我们的模型在设计方面需要的成本更小. 图3(c)和图4展示了 group convolutions(Alex Net). 模型容量(Model Capacity)实验表明, 本文的模型可以在维持模型复杂度和参数量的前提下提升模型的准确率. 当我们在维持复杂度的前提下调节 cardinalities C 的时候, 我们希望尽可能的不去改动其他的超参数. 我们选择去调节 bottleneck 的宽度(如图1右侧中的4-d), 因为这可以独立于 block 的 input 和 output.在图1左侧中, 原始的 ResNet bottleneck block 的参数量为 $256\cdot 64 + 3\cdot 3\cdot 64\cdot 64 + 64\cdot 256 \approx 70k$ 以及成比例的 FLOPs. 而我们的 template (图1右侧) 的参数量为: C\cdot ( 256\cdot d + 3\cdot 3\cdot d\cdot d + d\cdot 256) \tag 4当 $C=32, d=4$ 是, 上式约为 $70k$. 表2展示了 cardinality $C$ 和 bottleneck width $d$ 之间的关系. 实现细节(Implementation details) input image: 224×224 randomly cropped from resized image resized image: scale and aspect ratio augmentation shortcuts: option B in ResNet 在conv3,4,5中的 downsampling 操作通过 stride 为2的 3×3 卷积完成 SGD mini-batch size: 256 on 8 GPUs (32 per GPU) weight decay: 0.0001 momentum: 0.9 learning rate: 0.1, 会降低3次, 每次降低10倍(每次更新学习率都会使得错误率出现断崖式下跌) 权重初始化: Xavier module: 图3(c) BN: 在图3(c)中的卷积层之后 ReLU: 在 block 的输出中, ReLU 在 shortcut 之后使用, 其情况下, 均在 BN 之后使用. 图3中的三种形式通过合理安排 BN 和 ReLU 的位置可以互相等价. 我们选择图3(c)是因为它更加简洁, 速度更快. 实验(Experiments)Experiments on ImageNet-1KCardinality vs. Width: Increasing Cardinality vs. Deeper/Wider表4显示出提升 Cardinality 可以降低错误率, 但是加深或者加宽(channel 维度升高) ResNet 提升的精度幅度较小. 下标展示了残差结构的 shortcut 分别在 ResNet 和 ResNeXt 中的影响: Performance: ResNeXt: 0.95s per mini-batchResNet-101: 0.70s per mini-batch Comparisons with SOTA results: Experiments on ImageNet-5K Experiments on CIFAR Experiments on COCO object detection]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cascade R-CNN (CVPR, 2018)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CascadeRCNN-CVPR2018%2F</url>
    <content type="text"><![CDATA[文章: Cascade R-CNN: Delving into High Quality Object Detection作者: Zhaowei Cai, Nuno Vasconcelos备注: UC San Diego 核心亮点本文针对检测问题中正负样本区分的 IoU 阈值选择问题提出了一种新的目标检测框架, Cascade R-CNN周所周知, 在 two-stage 的目标检测模型当中, 需要设置 IoU 阈值来区分正样本和负样本, 通常, 阈值选的越高, 正样本的框就与真实框越接近, 但是这样就会使得正样本的数量大大降低, 训练时容易产生过拟合问题, 而如果阈值选的较低, 就会产生大量的假正例样本. 根据经验和实验证明可知, 当输入的 proposals 和真实框的 IoU 的值, 与训练器训练时采用的 IoU 的阈值比较接近的时候, 训练器的性能会比较好, 为此, 作者提出了一种级联式的阈值训练方法, 先在较低的阈值上训练检测器, 得到具体更高 IoU 的候选框输出, 然后在此基础上进行训练, 不断提升 IoU 的阈值, 这样一来, 最终生成的候选框质量会变得更高 (与真实框的 IoU 更大). 作者提出这种框架的启发来自于图1(c), 整体来说, 输入的 proposals 的 IoU 在经过检测器边框回归以后, 其输出的边框与真实框会有更大的 IoU, 因此可以将这个具有更大 IoU 的框作为下一个检测器的输入, 同时调高训练时的 IoU, 进而得到质量更高的框 论文细节摘要在目标检测任务中, 交并比 (IoU, Intersection over Union) 常常被用来区分正样本和负样本. 当正样本的 IoU 阈值设置的比较低时, 通常会导致产生更多的噪声. 但是, 如果一味的生成 IoU 阈值, 则检测器的性能就会有所下降. 产生这一现象的因素主要有两点: 1), 由于阈值升高后, 大量样本都会无法被当做正样本训练, 是的正样本的数据集大大降低, 从而容易在训练时产生过拟合; 2, 在训练阶段检测器优化的(人为设定的) IoU 阈值和假设的(真实的) IoU 阈值之间的不匹配. 为了解决上述问题, 本文提出了一个 multi-stage 的目标检测模型架构, 称为 Cascade R-CNN. 它包含一系列在递增的 IoU 阈值上训练的检测器, 使得对假阳性样本更有选择性. 所有的检测器都会一个 stage 接着一个 stage 的训练, 利用之前训练的检测器的结果来进一步提升下一次训练的结果, 从而得到更好的检测器. 这种逐步 resampling 可以更好的保证所有的检测器都具有相等的正样本集合, 缓解了过拟合问题. 相同的 cascade 流程会继续应用在 inference 阶段, 确保可以令每一个 stage 的检测器性能与假设性能之间更加匹配. 该算法由一系列随着 IoU 阈值增加而训练的检测器组成, 使得检测器逐渐对假正例样本更有选择性. 这些检测器会被阶段性 (stage by stage) 的训练, 而如果当前检测器的输出是一个好的分布, 就会用于训练下一个阶段的检测器, 从而得到更好的检测器. 对逐渐改进的假设进行冲采样, 保证所有的检测器都具有一组同等大小的正样本集合, 从而缓解了过拟合问题. 相同的 cascade 流程会继续应用在 inference 阶段, 确保可以令每一个 stage 的检测器与假设之间更加匹配. Cascade R-CNN 的一个简单实现在 COCO 数据集上胜过了其他所有的单模型检测器. 并且, 实验表明, Cascade R-CNN 可以应用在多种模型架构之中, 并且无论 baseline detector 的性能是强还是弱, Cascade R-CNN 总是能够进一步提升模型的性能 (consistent gains). 介绍目标检测任务需要同时解决分类和定位两个问题, 这两个问题都不太好解决, 因此往往检测器会面对许多 close false positives, 简单来说就是指许多密集重复的无用框 (假正例). 检测器必须要找到真正的正样本, 同时还要抑制这些假正例. 在 two-stage 检测模型中, 我们将分类和候选框回归任务分为两个阶段执行, 此时需要设置一个 IoU 阈值来标记候选框的正负, 通常情况下, 我们设置一个固定的 IoU 阈值 ($\mu$) 进行训练和预测, 但是, 这种设置 (如, $\mu=0.5$) 是建立在对正样本的要求不高的前提下的. 如图1(a)所示, 阈值设定的不好时, 往往产生出很多的噪声候选框. 在本文中, 我们将假设 IoU 定义为与真实框之间的 IoU, 而将训练时使用的 IoU 阈值 $\mu$ 定义为检测器 IoU. 我们的目标是研究一个穷举问题, 即找到高质量的检测器 IoU 阈值, 该检测器会输出更少的 close false positives (注意, 我们是希望得到更少的假正例, 而不是希望负样本的数量变低, 这两个是有区别的, 不要搞混), 如图1(b)所示. 本文的基本思想是, 如果只使用一个单一的模型, 那么我们就只能在一个单一的级别上优化检测器的 IoU 阈值. 这是著名的 cost-sensitive 学习迭代. 我们与之不同的地方在于, 我们是基于一组 IoU 阈值来进行优化的, 而不是基于假正例样本率.核心思想如图1(c)和(d)所示, 图中展示了三种 IoU 阈值下的检测器的定位和检测性能. 定位性能是关于输入的候选框 IoU (候选框是预测生成的, 因此每次的候选框的假设 IoU 都有所不同)的函数, 检测性能是关于 IoU 阈值的函数. 从图1(c)可以看出, 当输入的 proposals 的 IoU 在0.5~0.6之间时, 训练时采用 $\mu=0.5$ 可以获得最大的 IoU 输出(预测结果的框与真实框的 IoU 越大, 说明这些框的质量越好), 而当输入的 proposals 的 IoU 在 0.6~0.75 之间时, 训练时采用 $\mu=0.6$ 时的性能最好, 再之后就是 $\mu=0.7$ 时的性能最好. 可以得出, 只有 proposal 自身的阈值和训练器训练时用的阈值较为接近时, 训练器的性能才更好. 才从图1(d)中可以看出, 当我们选用较高的训练阈值 $\mu=0.7$ 时, 不论输入的 proposals 的 IoU 为多少, 检测器的性能都会较低 (虽然在IoU较高时可以与 $\mu=0.5$ 持平). 这是因为, 当我们选取了较高的 $IoU$ 阈值时, 那么就会有大量的框会舍去, 使得参与训练的正样本数量急剧下降, 进而导致了过拟合问题.在这片文章中, 为了解决上述问题, 我们提出了一种新的检测框架: Cascade R-CNN. 它是 R-CNN 的一种 multi-stage 扩展, 用于解决 close false positives 问题. Cascade R-CNN 是按阶段训练的 (stage by stage), 它会用一个 stage 的输出来训练下一个检测器. 这是通过观察图1(c)中, 每一个检测器输出的 IoU 总是比输入的 IoU 更好而受到的启发. Cascade R-CNN 的流程很想 boostraping 方法. 但是不同之处在于 Cascade R-CNN 的冲采样过程不是为了挖掘难反例. 相反, 通过调节 bounding boxes, 每一个 stage 都会去寻找更好(size更小)的 close false positives 集合来训练下一个 stage. 在具体操作时, 一系列的检测器会在一组递增的 IoU 阈值集合上训练, 以此避免过拟合问题(直接在大的 IoU 上训练会导致过拟合). 在测试阶段, 会执行同样的流程. 举例说明:有三个串联起来的用0.5/0.6/0.7的阈值训练出来的detector，有一个 IoU 约为0.55的proposal，经过0.5的detector，输出的物体框的 IoU 变为0.75；将此框再经过 0.6 的detector，输出的 IoU 变为 0.82；再经过 0.7 的detector，最终IoU变为 0.87. 这比任何一个单独的detector的结果都要好。同时，因为每经过一个 detector，其输出的 proposal 的 IoU 都更高，样本质量更好了，那么即使我下一个 detector 阈值设置得比较高，也不会有太多的样本被刷掉，这样就可以保证样本数量避免过拟合问题。 Object Detection在本文章, 我们将 Faster R-CNN 模型进行扩展, 如图3(a)所示, 第一阶段是一个 proposal sub-network (H0), 将其作用于整个图片, 还生成主要的检测假设(即 anchor boxes). 在第二个阶段, 这些生成的候选框 (hypotheses) 会被一个 roi detection sub-network (H1) 处理, 我们将其记为 detection head. 最终, 分类 score (C) 和 bounding box (B) 会应用到每一个候选区域框 (hypothesis)上. 本文主要是构建一种 multi-stage 的模型框架, 如图3(d)所示. Bounding Box Regression可以从图1中看到, 在得到了 anchor boxes 后, Faster R-CNN 只进行了一次 box regression. 因此, 有一些工作认为单次的 box regression 是不够的, 故而提出了 iterative bounding box regression, 记为 iterative BBox, 它的实现结构如图3(b)所示, 回归网络中所有的 head 都相同. 这种方法, 忽略了两个问题: 第一, 对于具有更高 IoU 的输入来说, 较低的阈值 ($\mu=0.5$) 往往是一种次优解 (如图1所示); 第二, 如图2所示, bounding box 的分布在每次迭代后都会发生很大的变化, 但是固定的阈值使得模型每次更新时都是以初始分布为目标的. 这些问题使得 iterative BBox 对人为设置的参数要求较高, 通常情况下, 迭代的使用同样的边框回归并不会有太多提升. Detection QualityCascaded R-CNNCascaded Bounding Box Regression如图1(c)所示, 我们很难令一个单一的回归器在所有的 quality levels(输入的框的 IoU 级别) 上获得完美的表现. 我们可以将较难的回归任务分解成一系列较小的步骤, 在 Cascaded R-CNN 中, 我们将其组织成如图3(d)中的结构, 写成公式表达如下: f(x,b) = f_T \circ f_{T-1} \circ ... \circ上式中, $T$ 是 cascade stages 的数量. 该式和图3(b)所示的 iterative BBox 方法有很多不同之处. 第一, 尽管 iterative BBox 是一种用来提升 bounding boxes 的后处理步骤, cascaded regression 是一种用于在不同 stages 改变分布假设的重采样过程. 第二, 由于会同时在训练和预测阶段使用 cascade 策略, 因此训练和预测阶段之间的分布没有依赖性. 第三, 在不同的 stages 上, 会对重新采样后的分布会训练不同的回归器 $\{f_T, f_{T-1}, …, f_1 \}$. 这些特点使得我们的模型可以产生更加精确的 BBox, 而不需要过多的 human engineering. Cascaded Detection如图4所示, RPN 网络最初的假设分布更多的集中在 low quality 的box上, 而在应用了 Cascade R-CNN 以后, 通过不断重新采样的方式, 使得最终的大部分框能够具有更高的 IoU . Experimental ResultsBaseline Networks: Faster R-CNN, R-FCN, FPN. 下图5(a), 可以看出, 虚线始终位于对应实线的上方, 说明应用了 Cascade R-CNN 以后, 模型产生的框的 mAP 变高了. 从图(b)中可以看出, 当输入的框的 IoU 较大时(通过不断添加真实框来增大输入的 IoU 的大小), $\mu$ 值较大的检测器可以获得更高的 IoU. 图6显示了所有的 Cascade R-CNN 检测器在所有 stage 上面的 mAP值, 从图6可以看出, 经过 Cascade R-CNN 以后, 输入的框的 IoU提升了, 是的阈值为 $\mu = 0.7$ 的检测器的 mAP 提升了, 不仅如此, 我们还可以看到, 在经过 Cascade R-CNN 以后, 即使是对具有更高 IoU 的输入, $\mu =0.5$ 的检测器也比 stage-1 阶段的 mAP值高, 这说明本文提出的 Cascade R-CNN 框架可以有效的提升检测器的性能. 在图7(a)中, 我们将本文的 Cascade R-CNN 与 Iterative Box 进行了对比, 在图1中, 我们可以看出, 使用单个回归器不断迭代的方式会降低输出的 IoU 大小. 相反, 使用本文的 Cascade R-CNN 方法, 可以在新的 stage 中生成更高的 IoU.在图7(b)中, 使用同一个检测器, 但是赋予不同的 $\mu$ 值时, 当 $\mu=0.6$ 时 mAP 最高, 当 $\mu=0.7$ 时 mAP最高, 而融合吼的模型结果也没有获得较大的提升. 从表1可以看出, Iterative BBox 和 intergral loss 检测器相对于 baseline 方法都可以提升模型的精度, 但是本文的 Cascade R-CNN 具有最好的精度表现. 消融实验 Stage-wise Comparison: 表2总结了每个 stage 的性能表现, 注意到, stage-1 已经超过了 baseline detector, 这是因为经过 multi-stage 学习后, stage-1 的检测能力也得到了一定的提升. 总体趋势显示越深的 cascade stage 具有越高的 quality localization. (但是考虑到模型复杂度和训练难度问题, 也不能叠加太多 stage, 一般2,3层差不多) IoU Thresholds: 表3前两行显示, 相对于每一个阶段使用固定的 IoU 阈值 (如0.5), 采用递增式的 IoU 阈值可以获得更好的效果 (对于 close false positives 更具有选择性). 但是同样的, 即使使用相同的 IoU 阈值来训练每一个阶段, 也比 baseline 的 mAP 高. Regression Statistics: 表3第一行和第三行对了使用和不使用 sequential regression statictics 时的模型性能差异. 表4总结了 stages 的个数对模型的影响. 添加两层 stages 可以大幅度提升 baseline 的精度, 第三层可以小幅度的的提升模型精度, 但是当叠加到第4层时, 模型精度就会收到一定影响并有略微下降.(尽管如此, 具有4个 stages 的检测器在较高的 IoU (AP90)下可以取得最好的精度表现) Comparison with the state-of-the-art: 表5显示了本文的 Cascade R-CNN 模型与现有模型之间的性能对比. 表6显示了在不同的 baseline 模型上应用 Cascade R-CNN 之后的性能表现 表7显示在不同的 backbone 网络中, Cascade R-CNN 仍然能够大幅度提升模型的 mAP]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 标准模板库STL完整笔记]]></title>
    <url>%2Fz_post%2FCpp-STL%2F</url>
    <content type="text"><![CDATA[vector连续存储结果, 每个元素在内存上是连续的, 支持 高效的随机访问和尾端插入/删除操作, 但其他位置的插入/删除操作效率低下, 可以看做是一个数组, 但是与数组的区别为: 内存空间的扩展. vector 支持动态大小的数据存储, 而数组则必须指定大小, 但需要扩展空间时, 需要手动实现. vector的内存分配原理及实现:在STL内部实现时, 首先分配一个较大的内存空间预备存储, 即capacity()函数返回的大小, 当超过此分配的空间时, 会再重新分配一块更大的内存空间(VS6.0是两倍, VS2005是1.5倍). 通常默认的内存分配能完成大部分情况下的存储操作. 扩展空间的步骤: 配置一块新空间 将就元素一一移动到新地址中 把原来的空间释放掉 vector的数据安排以及操作方式, 与数组模板Array十分相似, 两者唯一的差别在于空间利用的灵活性, Array的空间扩展需要手动实现 deque双端队列: double-end queue连续存储结果, 即每个元素在内存上是连续的, 类似于vector, 不同之处在于, deque提供了两级数组结构, 第一级完全类似于vector, 代表实际容器, 另一级维护容器的首位地址, 这样, deque除了具有vector的所有功能之外, 还支持高校的首/尾端的插入/删除操作. 优点: 随机访问方便, 支持[]操作符和.at()访问函数 可在两端进行push, pop操作 缺点:占用内存多 list非连续存储结构, 具有两链表结构, 每个元素维护一对前向和后向指针, 因此支持前向/后向遍历. 支持高效的随机插入/删除操作, 但是随机访问效率低下, 且由于需要额外维护指针, 开销也比较大. 每一个节点都包括一个信息块info, 一个前驱指针Pre, 一个后驱指针Post. 优先: 不使用连续内存完成插入和删除操作 在内部方便的进行插入和删除操作 可以在两端push, pop 缺点: 不能进行随机访问, 即不支持[]操作符和.at()访问函数 相对于vector占用内存多 list与vector的区别 vector为存储的对象分配一块连续的地址空间, 随机访问效率很高. 但是插入和删除需要移动大量的数据, 效率较低. 尤其当vector内部元素较复杂, 需要调用复制构造函数时, 效率更低. list中的对象是离散的, 随机访问需要遍历整个链表, 访问效率比vector低, 但是在list中插入元素, 尤其在首尾插入时, 效率很高. vector 是 单向的 的, 而 list 是双向的 (vector为什么单向???) vector 中的 iterator 在使用后就释放了, 但是 list 不同, 它的迭代器在使用后还可以继续使用, 是链表所特有的. queuedequemap1、map简介Map是STL的一个关联容器，它提供一对一（其中第一个可以称为关键字，每个关键字只能在map中出现一次，第二个可能称为该关键字的值）的数据 处理能力，由于这个特性，它完成有可能在我们处理一对一数据的时候，在编程上提供快速通道。这里说下map内部数据的组织，map内部自建一颗红黑树(一 种非严格意义上的平衡二叉树)，这颗树具有对数据自动排序的功能，所以在map内部所有的数据都是有序的，后边我们会见识到有序的好处。map是一类关联式容器。它的特点是增加和删除节点对迭代器的影响很小，除了那个操作节点，对其他的节点都没有什么影响。对于迭代器来说，可以修改实值，而不能修改key。 2、map的功能 自动建立Key － value的对应。key 和 value可以是任意你需要的类型。 根据key值快速查找记录，查找的复杂度基本是Log(N)，如果有1000个记录，最多查找10次，1,000,000个记录，最多查找20次。 快速插入Key -Value 记录。 快速删除记录 根据Key 修改value记录。 遍历所有记录。-3. 使用map 1#include &lt;map&gt; //注意，STL头文件没有扩展名.h 4. map的构造函数map共提供了6个构造函数，这块涉及到内存分配器这些东西，略过不表，在下面我们将接触到一些map的构造方法，这里要说下的就是，我们通常用如下方法构造一个map：1map&lt;int, string&gt; mapStudent; 5. 数据的插入unordered_map与map的区别在STL中, map对应的数据结构是红黑树, 红黑树是一种近似于平衡的二叉查找树, 里面的数据是有序的, 在红黑树上做查找的时间为 $O(lonN)$. 而unordered_map对应哈希表, 哈希表的特点就是查找效率高, 时间复杂度基本为 $O(1)$, 而额外空间复杂度较高. 基本使用1234567891011#include &lt;iostream&gt;#include &lt;unordered_map&gt;#include &lt;string&gt;int main()&#123; std::unordered_map&lt;int, std::string&gt; hmap; hmap.insert(std::make_pair(1, "Scala")); hmap.insert(&#123;3, "three"&#125;); hmap.insert(&#123; &#123;4, "Four"&#125;, &#123;5, "Five"&#125;&#125;); cout&lt;&lt;hmap[1]&#125; 键的类型默认情况下, unordered_map 只支持将基本类型作为 key, 如下面的代码是不合法的:1unordered_map&lt;pair&lt;int, int&gt;, int&gt; hash; 而map可以使用pair类型作为key. 关于 hash_mapC++中的hash_map是标准模板库(STL)的一部分, 但是它不是标准库(Standard Library)的一部分. 有很多编译器(GNU, VS)的实现都提供了这个数据结构.C++11标准库引入了unordered_map, 其功能和hash_map相同. set基于红黑树实现 unordered_set基于哈希表实现, 会将传入的值处理成相应的键值, 该键值对应着一个特定的位置, 因此, unordered_set 的各项操作的复杂度平均为常数级(最差为线性), 同时, unordered_set 也是一种 set, 因此, 其关键字不能有重复(重复的会自动合并). 12345678910111213unordered_set&lt;string&gt; stringSet;stringSet.insert("code");stringSet.insert("fast");string key1 = "fast";stringSet.find(key1); // return iter, 指向 faststring key2 = "slow"stringSet.find(key2); // return stringSet.end()vector&lt;int&gt; nums &#123;1,2,3,4,5,6,7,8,9&#125;;unordered_set&lt;int&gt; sets(nums.begin(), nums.end())int key;sets.count(key); // 返回拥有关键key的元素个数, 即只会返回1或0. priority_queue大顶堆默认比较函数为less, 对应为大顶堆1priority_queue&lt;int&gt; q; 小顶堆使用比较函数greater, 对应为小顶堆 1priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 自定义排序方法优先出列时会对 !cmp 判定, 所以如果希望小的在前, 那么就应该返回 元素1 &gt; 元素2 123456struct cmp&#123; bool operator()(int &amp;a, int &amp;b) const&#123; // 仿函数 return a &gt; b ; // 排序后, 小的在前 . &#125;&#125;]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Faster-RCNN 源码实现 (PyTorch)]]></title>
    <url>%2Fz_post%2FPyTorch-FasterRCNN%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[我们知道, FasterRCNN 中主要包含以下几个重要的组成部分: BackBone: VGG, ResNet等 RoIPool: RoI 池化层 RPN: 候选框区域推荐网络, FasterRCNN最主要的贡献点 接下来, 我们以 PyTorch 深度学习框架为例, 介绍一下 FasterRCNN 的具体实现(源码地址: https://github.com/jwyang/faster-rcnn.pytorch). RPNRPN 网络是在 FasterRCNN 中提出来的, 也算是 FasterRCNN 的核心所在. rpn.py 文件:该文件定义了 RPN 的网络结构.由于 FasterRCNN 模型的突出贡献在于提出了 RPN 网络, 并且要实现 FasterRCNN 模型, 首先就需要实现 RPN 网络结构, 因此, 我们先来看一下如何利用 PyTorch 实现 RPN 的网络结构. init 函数123456789101112131415161718192021222324252627282930# ./lib/model/rpn/rpn.pyclass _RPN(nn.Module): def __init__(self, din): # 代表输入的特征图谱的深度, 如 512 super(_RPN, self).__init__() self.din = din # from model.utils.config import cfg self.anchor_scales = cfg.ANCHOR_SCALES self.anchor_ratios = cfg.ANCHOR_RATIOS self.feat_stride = cfg.FEAT_STRIDE[0] # 图片转化成特征图谱后缩小的尺寸倍数 # 定义 RPN 网络的卷积层 self.RPN_Conv = nn.Conv2d(self.din, 512, 3, 1, 1, bias=True) # kernel size为3, stride 和 padding 为1, 所以输出图谱的尺寸不变. # 定义前景和后景的分类层 self.nc_score_out = len(self.anchor_scales) * len(self.anchor_ratios) * 2 # 输出深度为 2(前/后景) * 9 (9个anchors) self.RPN_cls_score = nn.Conv2d(512, self.nc_score_out, 1, 1, 0) # 输出尺寸不变, 深度变为 2*9. 对应9个anchor box的前后景概率. # 定义anchor box的坐标偏移量预测层 self.nc_bbox_out = len(self.anchor_scales) * len(self.anchor_ratios) * 4 # 输出的深度(channels), 对应 9 个anchor boxes 的 4 个坐标 self.RPN_bbox_pred = nn.Con2d(512, self.nc_bbox_out, 1, 1, 0) # 定义 proposal 层, from .proposal_layer import _ProposalLayer self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios) # 定义 anchor 匹配层(将anchor于gt匹配), from .anchor_target_layer import _AnchorTargetLayer self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios) self.rpn_loss_cls = 0 self.rpn_loss_box = 0 在 RPN 网络类的初始化函数中, 可以看出, 除了定义预测预测分类和box坐标的两个卷积层外, 最关键的两行代码分别来自于 _ProposalLayer 和 _AnchorTargetLayer 这两个类, 前者定义在proposal_layer.py文件中, 后者定义在anchor_target_layer.py文件中. 因此, 在继续分析 RPN 网络的其他函数之前, 我建议你先看看这两个类的内部实现(点击名字直接跳转). reshape 函数将指定 tensor 的维度改变.1234567891011# ./lib/model/rpn/rpn.py@staticmethoddef reshape(x, d): input_shape = x.size() x = x.view( input_shape[0], # batch 不变 int(d), int(float(input_shape[1]*input_shape[2]) / float(d)), input_shape[3] # ? ) return x forward 函数123456789101112131415161718192021222324252627282930313233343536# ./lib/model/rpn/rpn.pydef foward(self, base_feat, im_info, gt_boxes, num_boxes): batch_size = base_feat.size(0) # 首先得到经过RPN网络第一层卷积的特征图谱 rpn_conv1 = F.relu(self.RPN_Conv(base_feat), inplace = True) # 获得rpn分类score, 1×1的卷积网络 rpn_cls_score = self.RPN_cls_score(rpn_conv1) rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2) # 将score按照前后景概率reshape rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, 1) # 利用softmax在第一个维度上, 也就是前后景概率的维度上, 将socre转换为概率 rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out) # 转化成每一类的概率 # 获取到 anchor boxes 的 offsets rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv1) # proposal layer 候选框提取层 cfg_key = "TRAIN" if self.training else "TEST" # 输入给 RPN_proposal 对应的 forward 函数的 input 是一个包含4个元素的元组 rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data, im_info, cfg_KEY)) self.rpn_loss_cls = 0 self.rpn_loss_box = 0 # 生成训练时产生的预测结果, 并且构造rpn损失 if self.training: assert gt_boxes is not None # 利用 RPN_anchor_target 得到rpn的计算结果 rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes)) # TODO loss的计算 # 计算分类 loss # 计算 bbox 回归 loss proposal_layer.py 文件该文件中定义了类 _ProposalLayer, 其功能主要是根据规则的 anchor box 生成对应的 proposal box init() 函数1234567891011121314# ./lib/model/rpn/proposal_layer.pyclass _ProposalLayer(nn.Module): """ 该类通过在一系列的标准box(anchor box)上应用 estimated bounding-box transformations 来输出目标检测的候选框 """ def __init__(self, feat_stride, scales, ratios): super(_ProposalLayer, self).__init__() self._feat_stride = feat_stride # 图谱缩小的倍数 # from .generate_anchors import generate_anchors self._anchors = torch.from_numpy(generate_anchors(scales=np.array(scales), ratios=np.array(ratios))).float() self._num_anchors = self._anchors.size(0) 在上面的初始化函数中, 我们可以看到, 调用了 generate_anchors (位于文件 generate_anchor.py中)函数来生成标准的box (anchor box). forward() 函数接下来, 我们具体看一下该类的 foraward() 函数的实现方法, 其实现过程体现了 RoI 的生成方式. 1234567891011121314151617181920# ./lib/model/rpn/proposal_layer.pydef foward(self, input): # 算法: # 对于每一个 location i 上的 (H,W) # 生成以cell i 为中心的 A 个 anchor boxes # 将所有预测的到 bbox deltas 应用到 A 个 anchors 上面 # 在图像上截取该 boxes # 移除掉那些宽度或高度不满足要求的 predicted boxes # 按照score从高到低对proposal排序 # 应用NMS筛选一定数量的proposals # 在NMS筛选出的proposals中选取N个 # 返回 top N proposals scores = input[0][:, self._num_anchors:, :, :] bbox_deltas = input[1] im_info = input[2] cfg_key = input[3] pre_nms_topN = cfg 其他函数12# ./lib/model/rpn/proposal_layer.pydef backward(self, ): 12# ./lib/model/rpn/proposal_layer.pydef reshape(self, bottom, top): 12# ./lib/model/rpn/proposal_layer.pydef _filter_boxes(): generate_anchors.py 文件该文件的功能主要用于生成规则的 anchor box.在 class _ProposalLayer 的初始化函数中, 我们可以看到, 调用了 generate_anchors() 函数来生成标准的box (anchor box), 该函数的具体实现如下: 1234567# ./lib/model/rpn/generate_anchors.pydef generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)): # ** 代表幂次, 所以 scales = [2^3, 2^4, 2^5] = [8,16,32] """base_anchor 的大小为 16×16的, 其坐标为(0,0,15,15)""" base_anchor = np.array([1, 1, base_size, base_size]) - 1 # base_anchor = array([ 0, 0, 15, 15]) # _ratio_enum 为本文件内定义的函数, 作用为相对于每一个anchor枚举所有可能ratios的anchor box.(注意, base_anchor的size只是作用一个过渡使用, 后面的语句会利用scales参数将其覆盖) ratio_anchors = _ratio_enum(base_anchor, ratios) # 在给定anchor下, 根据scale的值枚举所有可能的anchor box anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales) for i in range(ratio_anchors.shape[0])]) whctrs 函数1234567# ./lib/model/rpn/generate_anchors.pydef _whctrs(anchor): # 返回一个anchor的宽, 高, 以及中心点的(x,y)坐标值 w = anchor[2] - anchor[0] + 1 # eg:15-0+1 = 16 h = anchor[3] - anchor[1] + 1 # eg:15-0+1 = 16 x_ctr = anchor[0] + 0.5 * (w-1) # eg: 0+0.5*(16-1) = 7.5 y_ctr = anchor[1] + 0.5 * (h-1) # eg:0+0.5*(16-1) = 7.5 mkanchors 函数12345678910# ./lib/model/rpn/generate_anchors.pydef _mkanchors(ws, hs, x_ctr, y_ctr): # 给定一组围绕中心点(x_ctr, y_ctr) 的 widths(ws) 和 heights(hs) 序列, 输出对应的 anchors ws = ws[:, np.newaxis] hs = hs[:, np.newaxis] # anchors里面的坐标分别对应着左上角的坐标和右下角的坐标 anchors = np.hstack((x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1), x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1))) ratio_enum 函数相对于每一个anchor, 遍历其所有可能ratios对应的anchors123456789# ./lib/model/rpn/generate_anchors.pydef _ratio_enum(anchor, ratios): w, h, x_ctr, y_ctr = whctrs(anchor) # 返回一个anchor的宽, 高, 以及中心点的(x,y)坐标值 size = w * h size_ratios = size / ratios ws = np.round(np.sqrt(size_ratios)) hs = np.round(ws * ratios) anchors = _mkanchors(ws, hs, x_ctr, y_ctr) return anchors scale_enum() 函数12345678# ./lib/model/rpn/generate_anchors.pydef _scale_enum(anchor, scales): # 根据给定的anchor(box), 枚举其所有可能scales的anchors(boxes) w, h, x_ctr, y_ctr = _whctrs(anchor) ws = w * scales hs = h * scales anchors = _mkanchors(ws, hs, x_ctr, y_ctr) return anchors anchor_target_layer.py 文件该文件中定义了class _AnchorTargetLayer(nn.Module), 可以将 anchors 和 groun-truth 匹配起来, 然后生成相应的分类标签和bounding-box的回归targets. init() 函数12# ./lib/model/rpn/anchor_target_layer.py/class _AnchorTargetLayer():def __init__(self, feat_stride, scales, ratios): forward() 函数12# ./lib/model/rpn/anchor_target_layer.py/class _AnchorTargetLayer():def __init(self, feat_stride, scales, ratios): backward() 函数12# ./lib/model/rpn/anchor_target_layer.py/class _AnchorTargetLayer():def backward(): reshape() 函数12# ./lib/model/rpn/anchor_target_layer.py/class _AnchorTargetLayer():def reshape(): unmap() 函数12# ./lib/model/rpn/anchor_target_layer.py 不是类内部的函数def _unmap(): compute_targets_batch() 函数12# ./lib/model/rpn/anchor_target_layer.py 不是类内部的函数def _compute_targets_batch(): bbox_transform.py 文件bbox_transform() 函数bbox_transform_batch() 函数bbox_transform_inv() 函数BackBoneFasterRCNN 采用的 backbone 主要有两种, 一种是经典简单的 VGG16 网络, 一种是提取能力更强的 ResNet网络, 接下来我们对这两个网络的实现进行代码说明. VGG16首先, 我们可以利用 PyTorch 的 torchvision.models 来载入 VGG16 模型(当然也可以自己实现, 不过这不在本文的讨论范围内), 从卷积核的size等信息可以看出, 这已经是优化过的 vgg16 网络, 在网络层参数设置上和原始的 vgg16 有略微不同, 但大体上结构是相似的, 如下所示: 12import torchvisionvgg = models.vgg16() 可以看一下 vgg16 网络的内部结构(可以依照此结构来复现 vgg16 网络):1print(vgg) 输出如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) )) ResNetResNet 的结构稍微复杂一些. 这里就不再贴出了, 不过和 VGGNet 相同, 都是利用 torchvision.mdoels 模块来导入的. Faster RCNN 模型结构在了解了以上两种模型骨架之后, 我们首先创建 Faster RCNN 的整个结构(包含 RoIPool 和 RPN, 不过, 这里只是先用作占位, 具体实现在后面). 123456789101112131415class _fasterRCNN(nn.Module): # 以单下划线开头, 表明为内部函数 """faster RCNN""" def __init__(self, classes, class_agnostic): super(_FasterRCNN, self).__init__() self.classes = classes # self.n_classes = len(self.classes) # 类别个数 self.class_agnostic = class_agnostic # 标志是否是类别不可知的, 默认为False, 即类别是可知的 # loss self.RCNN_loss_cls = 0 # 分类损失 self.RCNN_loss_bbox = 0 # 边框回归损失 # 定义RPN网络 # from model.rpn.rpn import _RPN self.RCNN_rpn = _RPN(self.dout_base_model) self. RoI Pooling]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>PyTorch</tag>
        <tag>计算机视觉</tag>
        <tag>源码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Speed Accuracy TradeOffs (CVPR, 2017)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Speed-Acc-CVPR2017%2F</url>
    <content type="text"><![CDATA[文章: Speed/accuracy trade-offs for modern convolutional object detectors作者: Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy备注: Google 核心亮点本文实现了一个灵活统一的目标检测框架, 并对三个主流的目标检测模型做了详细客观的分析和讨论通过该框架, 本文对目前主流的各个模型(Faster, R-FCN, SSD)影响精确度和速度的各个因素展开了详细的分析和讨论, 以此希望能够帮助从业者在面对真实应用场景时, 能够选择适当的模型来解决问题. 同时, 本文还发现了一些新的现象(减少 proposals box 的数量), 使得可以在保持精度的前提下, 提升模型的速度. 论文细节摘要本篇文章的目的主要是为在给定平台和应用环境下选择一个合适的目标检测模型提供指导, 即要达到合适的 speed/memory/accuracy 平衡. 为此, 我们研究了多种方法来平衡现代卷积网络检测模型的速度, 准确度, 内存消耗等. 近年来有大量成功的检测模型被提出, 但是, 我们很难直接将这些模型互相比较, 因为它们采用的特征提取器不同(eg, VGG, ResNet), 采用的图片尺寸不同, 硬件设备和软件平台也不同. 因此, 本文提供了一个基于 FasterRCNN, R-FCN, SSD 的统一实现版本, 我们称之为- meta-architectures, 然后通过使用不同的特征提取器, 不同的关键参数来跟踪这些模型之间的差异. 最终的模型有两个极端, 一是极度关注速度, 要求最终的模型可以运行在移动设备上, 二是极度关注准确度, 要求能够在 COCO 数据集上达到最高的分数. 介绍目前有众多成功的模型, 但是却很难决定哪种场景下使用哪种模型, mAP 评价标准并不能反映所有问题, 还需要同时考虑模型的运行速度和内存消耗.目前, 只有很少的模型讨论的运行速度(R-FCN, SSD, YOLO), 但是它们大多都只是声称它们达到了某个 fps, 并没有深入的展开关于速度和精度之间的讨论.在这篇文章中, 我们希望以一种详尽而公平的方式来探讨这些模型的速度的精度之间的平衡关系. 在评估时, 我们不使用任何 tricks(ensembling, multi-crop, flipping等等), 仅仅评估单一模型的性能, 对于时间的测量, 我们仅关注预测阶段的运行时间, 而不在意训练时长.本文的贡献主要有以下几点: 提供了一个关于现代卷积检测系统的具体调研报告, 并阐述了这些先进模型在设计模式上共有的通性. 用 TensorFlow 实现了一个灵活统一的检测框架 meta-architectures, 包含 FasterRCNN, R-FCN 和 SSD 三种模型 本文发现, 通过使用较少的候选区域框可以大大提高 FasterRCNN 的检测速度, 并且在精度上不会有太大损失. 同时, 我们还发现 SSDs 的性能表现在面对不同的特征提取器时, 不像 FasterRCNN 和 R-FCN 那么敏感. 并且我们在 acc/speed 曲线上确定了 sweet spots, 这些点上的 acc 只有在牺牲 speed 的情况下才能够提升. 我们尝试了一些以前从未出现过的 meta-architecture 和 feature-extractor 的结合方式, 并讨论了如何利用这些方式来训练 winning entry of the 2016 COCO object detection challenge. Meta-architectures在我们的文章中, 我们主要讨论三种主流模型: SSD, FasterRCNN 和 R-FCN. 在这三个模型的原文中各自使用了特定的特征提取器(eg, VGG, ResNet). 现在我们将模型和特征提取器解耦, 重新审视这些模型结构. SSD将画框和分类预测同时进行, 代表一众 one-stage 检测方法 FasterRCNNFasterRCNN 是自 2015 年以来最主流的 two-stage 目标检测模型, 它首先提出了 RPN 网络, 使得候选框推荐的步骤可以整合到神经网络中去, FasterRCNN 也衍生出了多种的版本, 代表着经典的 two-stage 模型. R-FCN尽管 FasterRCNN 比 FastRCNN 快了一个数量级, 但是相对于 one-stage 方法, 它仍然很慢. 为此, 有人提出了 R-FCN 检测模型, 它和 FasterRCNN 模型类似, 但是将候选框的划取阶段移到了网络模型的最后一层, 使得有更多的卷积层可以共享计算结果, 同时还提出了 PSRoI(position-sensitive), 大大加快了模型的运算速度. Experimental setup各个模型的实现在所用框架, 优化程度, 数据集等都有所不同, 因此, 单单比较 COCO 或 ImageNet 的 mAP 是不全面的. 因此, 为了更好地比较各个模型之间的差异, 我们用 TensorFLow 实现了一个目标检测框架, 从而可以让我们较为客观公平的进行对比. Architectural configurationFeature extractors: VGG-16, ResNet-101, Inception v2, Inception v3, Inception ResNet v2, MobileNet.对于 FasterRCNN 和 R-FCN 来说, 我们需要确定使用特征提取器的哪一层卷积特征图谱来预测候选区域框. 我本文的实验中, 我们尽可能的使用原文中的设置, 如果原文没有提到的, 我们则尽可能的选择相类似的设置.在 SSD 中, 因为使用了多个不同尺度的特征图谱来预测 box 的位置和分类, 因此, 特征图谱的选择是至关重要的. 在 VGG 中, 原文使用了 conv4_3, fc7, 以及后续的几层卷积层, 与原文不同的是, 我们在每一个附加层之后都使用了 BN 层. Number of proposalsFasterRCNN &amp; R-FCN : 10~300 (trade-off) Output stride setting for Resnet and Inception ResNet采用stride 16, 将 conv5_1 的stride从2变为1, 并在后面使用 Atrous 卷积(Dilation 卷积) 来弥补缩小的感受野. 另外, 通过改变 conv4_1 的stride, 还测试了 stride 8 的情况. 相比于 stride 16, stride 8 的 mAP 提升了 5%, 但是运行时间也变慢了 63%. Matching 同样采用原文推荐的参数设置来将候选框与真实框匹配. Box encoding: 与原文相同: (b_a;a) = [10\cdot \frac{x_c}{w_a}, 10\cdot \frac{y_c}{h_a}, 5\cdot \log w, 5\cdot \log h] 需要注意的是, 标量 10 和 5 在原文的代码实现中都有使用, 即使在原文中没有提及. Location loss: Smooth L1 Input size configuration: M=300 / 600. Training and hyperparameter tuning; 对于 FasterRCNN 和 R-FCN, 我们用 TF 的 crop_and_resize 操作来代替 RoIPooling 和 PSRoIPooling, 该操作是利用双线性插值进行反向投影的, 其求导机制和原文中的类似. Benchmarking procedure: 32GB RAM, Intex Xeon E5-1650 v2 processor, Nvidia GTX Titan X. 下面的表2总结了本文使用的特征提取器 Results 分析通常来说, R-FCN 和 SSD 模型要比 Faster RCNN 模块快得多, 但是 Faster RCNN 的精确度更高. 但是, FasterRCNN 可以通过降低候选区域框的数量来提升速度. The effect of the feature extractor: 整体来说, 越强的特征提取器与 mAP 分数成正比, 但是对于 SSD 来说, 这种提升并不明显 (为什么 Inception v2 的 FasterRCNN 和 R-FCN 的 mAP 值那么低?) The effect of object size: The effect of image size: 当 image size 从 600 降到 300 时, 精度度平均会降低 15.88%, 同时 inference time 也会降低 27.4%. The effect of the number of proposals: proposals 可以大幅度降低测试时间, 同时 mAP 值只会降低一点(Inception ResNet v2, 300 -&gt; 10, 35.4% -&gt; 29%). 我们找到的 sweet point 大约是 50 个候选区域框, 此时可以在保持 300 候选区域框精度的 96% 的前提下, 将测试速度提升 3 倍. FLOPs analysis:FLOPs(multiply-adds) Memory analysis: Good localization at .75 IOU means good localization at all IOU thresholds: 表4总结了我们模型的性能(融合了5个FasterRCNN), 并且突出了如何在 COCO 评价标准上提升性能.在模型融合时, 我们选取了5个FasterRCNN模型, 每个模型都是基于ResNet 和 Inception Resnet的, 他们的 stride 不同, 并且使用了不同的损失函数, 以及不完全相同的训练数据. 最终使用ResNet论文中的附录A的方法融合这些模型的检测结果. 表5总结了最后选定的模型的性能表现. 模型融合以及 multi-crop inference 大约使模型的精度提升了7个百分点. 表6比较了单个模型和模型融合之间的性能差异]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCN-ICCV 2017]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-DCN-ICCV2017%2F</url>
    <content type="text"><![CDATA[文章: Deformable Convolutional Networks作者: ifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei说明: Microsoft Research Asia 核心亮点1) 引入了可以自调节感受野大小的deformable convolution和deformable RoI 模块该模块通过额外学习一组采样偏移量来决定卷积操作和RoI pooling操作的采样位置, 通过这种方式, 是的网络模型可以根据输入的图谱自动调节感受野的大小的分布. 2) 上面的两种deformable模块均可以无痛的添加到现有模型中由于deformable convolution和deformable RoI 模块并不会改变原始的输入输出大小, 因此可以很轻易的替换到现有网络中, 并且可以有其他多种提升精度的trick想叠加, 在多个视觉任务上(检测, 分割)都表现出色. 论文细节摘要卷积神经网络由于其模型内部固定的几何结构, 使得它固有的受限于模型的几何变换. 在这片文章中, 作者引入了两种新的模块来增强CNN模型的transformation modeling能力, 分别为可变形的卷积模块和可变形的RoI池化模块(deformable convolution and deformable RoI pooling). 这两个模块都是基于增强模型中的空间采样点的思想提出的. 这些新的模块可以还轻易的替换掉CNN网络中的原始部分, 并且可以轻易地进行端到端的训练. 下面, 我们将会展示在CNN网络中学习秘籍的空间形变, 有助于提升传统的视觉任务, 如物体检测, 语义分割等等. 介绍在视觉识别任务中, 有一个很关键的点在于怎样才能最大程度的适应物体的几何变换, 如物体尺寸, 姿态, 观察角度, 局部形变等等. 在通常情况下, 具有两种做法, 第一是建立足够的数据集来包含这些可能的形变, 以便让网络能够学习到足够的形变知识. 这一类方法的典型应用就是数据增广. 第二种方法就是使用支持形变不变性的特征表示或算法, 如SIFT(CNN具有平移不变性, 但不具有形变不变性).上面的两种方法有一些明显的缺点. 首先, 就是要求对形变类型已知, 只有在这种假设成立的前提下, 才能有选择的应用最合适的数据增广方法. 第二, 即使在知道形变可能类型时, 利用人工设计特征算子依然是一个不容易的工作, 尤其适当形变类型较为复杂时. CNN虽然取得了很大成功, 但是CNN仍然面临这这两问题. 它们对物体形变的适应能力实际上大多来自于海量数据集, 大型模型, 以及一些简单的人工设计模块的支持.总而言之, CNN固有的优先于具有大量未知形变的任务, 这种限制来自于CNN模型本身的结构模块(不论是卷积层还是fc层, 层的参数及输出向量都是固定的). 因此, 在CNN内部, 缺少相应的内部机制来处理几何形变的问题. 举个例子来说, 在同一层卷积层中所有激活单元感受野大小都是一样的, 而这并不是high level的卷积层所期望看到的, 因为正常来说, 不同位置与物体之间的联系紧密程度是不同的. 另外, 目前大多数方法都是基于主边框进行回归预测的, 这实际上是一种次优化的方法, 尤其是对于非规则物体来说.在这篇文章中, 我们提出了两个新的模块可以增强CNN对物体集合形变的适应能力. 首先是可形变卷积模块, 它将2D的偏移量添加到规则的网格采样位置中, 可以自由的形成各种形变的卷积, 如图1所示. 图中的不同的偏移量都是通过额外的卷积层从之前的特征图谱中学习到的. 因此, 可形变是以一种局部的, 密集的, 自适应的方法建立在输入特征之上的. 第二部分是可形变RoI pooling. 它会将每个bin位置的offset添加到对应位置上. 同样, RoI pooling的offset也是学习得到的.以上两种可形变模块都是轻量级的, 并且只引入了很少的参数和计算量, 可以很容易的替换掉标准的CNN网络中去. 产生的网络我们称之为可形变卷积网络(Deformable ConvNets). Deformable Convolutional Networks可形变操作的位移是2D的, 代表着不能跨通道执行, 所有的操作都是在同一的channel上执行的. Deformable Convolution一个2D的卷积操作由可以分为两步: 使用某个坐标偏移量集合 $R$ 在输入的特征图谱上进行采样 将采样点的值与卷积核对应位置的权重相乘再求和, 既得输出图谱某点的值 先看一下正常的卷积操作, 将正常卷积核的坐标偏移量定义为 $R$ : R=\{ (-1,-1), (-1,0), ..., (0,1), (1,1) \}上面的公式定义了一个 dilation 为 1 的 3× 3 大小的卷积核. 那么对应feature map上的每个点的值, 可通过下式计算: y(p_0) = \sum_{p_n\in R} w(p_n)\cdot x(p_0 + p_n)上式中, $p_0$ 对应的是输出特征图谱上的点坐标, $y(p_0)$ 对应的是该点的值, $p_n$ 对应的是常规偏移量集合 $R$ 中的坐标偏移量, $w(p_n)$ 代表该偏移量对应的权重值, $x(p_0 + p_n)$ 代表输入特征图谱在坐标 $p_0+p_n$ 上的值. 可以看出, 这里的 $R$ 实际上也代表了输出特征图谱 $p_0$ 点的感受野范围. 在deformable 卷积中, 常规坐标偏移量集合 $R$ 会与另一个额外的偏移量集合 $\{\Delta p_n | n =1,…N\}, 其中, N=|R|$ 共同决定卷积核感受野的采样坐标, 于是, 输出特征图谱上面的点的计算公式变成: y(p_0) = \sum_{p_n\in R} w(p_n) \cdot x(p_0 + p_n + \Delta p_n)可以看到, 现在的采样点不再是一个规则的矩形了. 在实现中, 偏移量 $\Delta p_n$ 是浮点数类型, 因此, 我们将通过双线性插值来实现(将卷积的输出通过插值的方法计算): x(p) = \sum_q G(q,p)\cdot x(q)上式中, $p$ 代表任意一个浮点坐标值( $p=p_0+p_n+\Delta p_n$ ). $q$ 代表输入特征图谱 $x$ 中的所有整数点坐标(实际计算时, 只有 $p$ 周围的四个整数坐标点有用). $G(\cdot, \cdot)$ 代表双线性插值函数. 如图2所示可形变卷积的过程, 在同一个卷积层当中, 卷积核的空间大小和dilation都是相同的. 因此, 对于一个卷积层来说, 它会新增一个 $2N$ 维的参数, 对应着 $N$ 个2D偏移坐标( $N$ 就是上面提到的卷积核的参数个数). 在训练的时候, 我们不仅需要学习卷积核的参数, 还需要学习卷积核的偏移量参数(通过上面的式子用BP算法更新偏移量参数). 注意, 同一层的多个卷积核?各自/共同?持有一个offset Deformable RoI PoolingRoI pooling 目前被广泛的运用于各种目标检测模型当中, 它可以将不同尺度的矩形区域转换成固定尺寸的图谱. RoI Pooling: 给定一个特征图谱 $x$ 和一个大小为 $w\times h$ , 左上角坐标为 $p_0$ 的RoI, 对其使用RoI pooling, 将其划分成 $k\times k$ 大小的网格, 并且输出一个 $k\times k$ 大小的特征图谱 $y$. 那么, 对与 $y$ 中坐标为 $(i,j), 0\leq i,j \leq k$ 的网格bins来说, 其输出的值为 y(i,j) = \sum_{p\in bin(i,j)} x(p_0 + p) / n_ij上式中, $n_ij$ 是bin中含有的像素点的个数. 同理, 我们可以将上面的标准RoI格式写成deformable RoI公式, 如下所示: y(i,j) = \sum_{p\in bin(i,j)} x(p_0 + p + \Delta p_{ij}) / n_{ij}. 上式同样包含浮点型坐标, 因此也用双线性插值实现. 图3说明了如何获得offsets. 首先, 利用标准的RoI pooling生成池化后的特征图谱. 在特征图谱上, 会用一个 $fc$ 层来生成归一化的offsets $\Delta \hat p_{ij}$, 然后通过对应元素相乘(element-wise product) $\Delta p_{ij} = \gamma \cdot \Delta \hat p_{ij} \odot (w,h)$ 将其转换成上式中的offsets $\Delta p_{ij}$. 式中, $\gamma$ 是一个预先定义的标量(默认为0.1), 来控制offset的大小. 注意, offset归一化对于学习RoI size的不变性来说是有必要的 Position-Sensitive(PS) RoI Pooling PS RoI pooling是全卷积的. 通过卷积层, 所有的输入特征图谱首先会被转换成 $k^2$ 大小的score maps.(对于每一类都有这样的一个maps, 因此共有 $C+1$ 个score maps), 如图4的底部分支所示. //TODO Deformable ConvNets可以看出, 不论是Deformable convolution还是 Deformable RoI pooling, 它们的输出都和常规版本的卷积核RoI的输出大小相同. 因此, 我们可以很自然的用Deformable模块替换现有目标检测模型中的常规模块. 在训练时, 为了学习offsets参数而新增的卷积层和fc层都被初始化为0, 它们的学习率被设置为现有目标检测模型学习率的 $\beta$ 倍(通常为1, 在FasterRCNN中, fc的 $\beta$ 设为0.01). 上面的网络可以通过基于双线性插值的BP算法来优化.为了将deformable convnets整合到现有的先进检测模型中, 我们可以将目标检测模型看成以下两部分:第一部分是一个深层的全卷积网络, 用于生成整个图片的特征图谱. 第二部分, 是根据特征任务设计的具体的浅层网络, 它从feature maps中获取最终计算结果. 下面来详细说明一下这两个部分.Deformable Convolution for Feature Extraction: 本文使用了两个先进的特征提取模型: ResNet-101 和 Inception-ResNet. 均在ImageNet数据集上预训练.这两个模型都包含多个卷积块, 一个平均池化层和一个1000路的fc层用于分类. 我们将平均池化层和fc层移除. 然后添加一个 1×1 的卷积层(随机初始化)将通道维数降为 1024 维. 根据前人工作(R-FCN), 我们将最后一个卷积块的stride从32降为16, 以此来提升特征图谱的resolution, 具体来说, 就是在最后一个卷积块的最开始, 将stride从2变成1, 为了弥补这种改变, 同时会将dilation从1变成2. 然后, 我们将Deformable Convolutional应用于最后的几层卷积层, 通过实验(Table 1), 我们发现对3层卷积层应用Deformable Convolutional可以起到最好的效果. Segmentation and Detection Networks DeepLab, Category-Aware RPN, FasterR-CNN, R-FCN等. Understanding Deformable ConvNets当使用了Deformable ConvNets以后, 卷积核的感受野发生了变化, 正如下图5和图6所示所示. 表2提供了一些数值量证明了Deformable ConvNets的有效性. 从中可以看出: Deformable filter的感受野大小是和物体大小相关的, 这说明卷积核的形变已经从图片中学到了很多的有效信息 背景区域的卷积核大小介于中等尺寸物体和大型物体之间, 这说明在识别背景区域时, 一个相对较大的感受野是有必要的. Deformable RoI pooling的有效性如图7所示: In Context of Related WorksSpatial Transform Networks(STN),Active Convolution,Effective Receptive Field,Atrous convolution,Deformable Parts Models(DPM),DeepID-Net,Spatial manipulation in RoI pooling.Transformation invariant features and their learning 实验Semantic Segmentation: 使用 PASCAL VOC 和 CityScapes 数据集. Object Detectin: 使用 PASCAL VOC 和 COCO 数据集. 默认的ResNet-101使用了 dilation为2, size为3×3的 atrous convolution. 我们还尝试了更多其它的可能参数, 如下表3所示. 表中数据说明: 当使用较大的dilatioin时, 所有任务的准确度都有所提升, 说明默认网络的感受野太小了(较大了dilation可以提供较大的感受野) 不同的任务和模型其最优的dilatioin参数不同, 但是deformable convolution总是能取得最高的准确度 对于具有RoI结构的网络来说, deformable RoI同样有效 如下表4所示, 可以看出, deformable结构对于目标检测任务同样有效: 表5贴出了deformable模型的复杂度和运行时间, 可以看到, 模型增加的参数量和运行时间都是在可接受范围内的]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Softer-NMS-Arvix 2018]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-SofterNMS-Arxiv2018%2F</url>
    <content type="text"><![CDATA[作者: Yihui He, Xiangyu Zhang, Kris Kitani, Marios Savvides发表:机构: CMU &amp; Face++ 核心亮点提出了一种新的边框回归损失函数和NMS算法作者提出了一种 基于KL散度的边框回归损失函数, 可以同时学习到边框的形变量和位置变化量. 最终产生的位置变化量会与位置的精确度有很强的联系, 然后将其使用在本文提出的 新的NMS 算法上, 以此提高准确度. 论文细节摘要在目前的目标检测模型中, NMS是十分重要的一步处理步骤. 但是, 有时候, 较精确的候选框可能并没有很高的socre, 这时候使用NMS就会导致物体位置的预测精度降低. 在这篇文章中, 作者提出了一种 新的边框回归损失函数, 可以同时学习到边框的形变量和位置变化量. 最终产生的位置变化量会与位置的精确度有很强的联系, 然后将其使用在本文提出的 新的NMS 算法上, 以此提高准确度. 在MS-COCO数据集上, 将 VGG-16 Faster RCNN的 AP 从 23.6 提高到了 29.1, 将 ResNet-50 FPN Fast RCNN 的 AP 从 36.8 提高到了 37.8 . 介绍目前, 目标检测模型主要分为one-stage和two-stage两种, 本文主要关注two-stage模型. 本文主要关注候选区域框可能出现的以下两种问题: 第一, 当物体周围所有的dounding box都是不准确的, 如图1(a)所示. 第二, 较准确的box的score不高, 而不准确box的score较高, 如图1(b)所示. 上面两种问题都说明了box的位置和box的score不是强相关的. 收到这两种问题的启发, 本文提出使用 KL loss 来作为物体边框回归loss. 具体来说, 首先将bounding box的预测值和真实值分别建模成高斯分布和Dirac delta function(狄拉克 $\delta$ 函数). 然后, 训练模型, 以期降低来自于这两个分布的KL散度边框回归损失函数. 最后, 提出一个基于权重平均的soft NMS算法, 简言之就是当box具有较高的confidence事, 它就会得到较大的权重, 而不管它的分类score是多少. 利用KL Loss来训练边框回归模型 本文的检测模型的头部结构如图2所示. 我们的目的是估计边框的位置置信度, 具体来说, 我们的网络将会预测下面的高斯分布而不仅仅是边框回归: P_\theta (x) = \frac{1}{2\pi \sigma^2}e^{-\frac{(x-x_e)^2}{2\sigma^2}}上式中, $x_e$ 代表预测的边框的位置期望, $\sigma$ 代表标准差. 这些值将从Fast RCNN的头部(fc7)产生, 注意, fc7使用的是绝对值激活函数(而不是ReLU), 主要目的是尽量避免大量的 $\sigma$ 值为0., 当 $\sigma \rightarrow 0$ 时, 说明网络对当前预测的边框位置期望持有很大的置信度. 同样, 真实边框也可以写成高斯函数的形式, 实际上就是如下Dirac delta 函数: P_D(x) = \delta (x - x_g)其中, $x_g$ 是真实边框的位置.我们的目标时找到使得 $P_\theta (x)$ 和 $P_D(x)$ 之间KL散度最小的参数 $\hat\theta$, 即: \hat\theta = \arg\min_{\theta} D_{KL}(P_D(x) || P_{\theta}(x))综上, 本文的模型将使用KL散度作为边框回归损失函数 $L_{reg}$, 分类损失函数 $L_{cls}$ 维持不变(与其他模型一样) L_{reg} = D_{KL}(P_D(x) || P_{\theta}(x)) = ... = \frac{(x_g - x_e)^2}{2\sigma^2} + \frac{1}{2}log(\sigma^2) + \frac{1}{2}log(2\pi) + H(P_D(x))如图3所示, 当预测位置 $x_e$ 不准确时, 我们就希望方差 $\sigma^2$ 越小越好, 这样一来, 损失函数 $L_{reg}$ 就会变小. //TODO Softer-NMS在获取到预测位置的标准偏差以后, 通过平均权重将bounding boxes融合, 如下面的算法流程所示, 主要使用两行代码来修改原始的NMS算法. 首先, 使用标准的NMS或者soft NMS算法来候选框进行选择. 然后, 对于每一个box $M$, 我们计算它基于周围及自身box的权重均值的新的location. 举个例子, 对于第 $i$ 个box 的坐标 $x1$来说, 它的新坐标 $x1_i$ 计算如下: x1_i = \frac{\sum_j x1_j / \sigma^2_x1,j}{\sum_j 1/ \sigma^2_x1,j}, \text{subject to } IoU(x1_j, x1_i) > N_t当bounding box的iou大于一定阈值 $N_t$ 时, 就会被考虑加入到权重均值当追溯去. 在这里, 我们不需要设置分类score的阈值, 因为即使是较低的socre有时它的localization socre却较高. 图4展示了在应用softerNMS以后, 我们有时候可以避免文章开头提高的两种错误情况.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Non-local Neural Networks (CVPR, 2018)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-NonLocal-CVPR2018%2F</url>
    <content type="text"><![CDATA[文章: Non-local Neural Networks作者: Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He 核心亮点1) 提出了 non-local operations 来解决 CNN 网络中的 long-range dependencies 问题传统 CNN 的卷积操作由于输出神经元只会与输入图谱上的一部分区域有关系, 因此, 在面对那些 long-range dependencies 的时候, 往往不能捕获到足够的信息来表征数据, 为此, 作者提出了 non-locl operations, 其相当于构造了一个和特征图谱尺寸一样大的卷积核, 从而可以维持更多信息. 2) non-local module 可以作为一种通用的模块应用在各项任务上作者通过实验证明, non-local 的有效性不仅仅局限于某一类特殊任务(如视频分类), 同时还可以轻易的整合到其他现有模型中, 如将其整合到 MaskRCNN 中, 可以当做是一种 trick 来提供 MaskRCNN 在目标检测/分割, 姿态识别等任务上的性能表现. 论文细节摘要不论是卷积网络还是递归网络, 它们都是作用在某一块局部区域 (local neighborhood) 的operations. 在本文中, 我们提出了 non-local operations 作为一种通用的神经网络的 building blocks 来捕捉基于 long-range 的依赖关系. 受到经典的 non-local means 方法的启发, 本文的 non-local operation 会将某一位置的响应当做是一种从特征图谱所有位置的加权和来计算. 该 building block 可以插入到现在计算机视觉的许多模型当中, 进而可以提升分类, 检测, 分割等视觉任务的性能表现. 介绍在深度神经网络中, 捕获 long-range dependencies 信息是十分重要的, 如面向序列数据的 LSTM, 面向图像数据的更大的感受野等. 但是不论是 convolutional 还是 recurrent 操作, 它们都是在一个 local neighborhood 上进行计算的. 因此, 只能通过不断的重复执行才能够捕获到足够的 long-range dependencies 信息(卷积计算之间的重叠区域). 这种 Repeating local operations 具有很多缺点. 第一, 计算的低效性; 第二, 会造成很多优化时的困难; 最后, 会产生多次反射, 这使得很难在相距较远两个位置的点传递反向和前向的计算结果.(???, 这三个缺点具体什么意思?)在本篇文章中, 我们提出了一种用于捕获 long-range dependencies 信息的简单, 高效, 通用的神经网络构建模块, 称为 non-local. 本文的 non-local 是从经典的 non-local mean operation 泛化而来的. 直观来说, non-local operaions 会计算输入的特征图谱上所有点加权和的响应(如图1). 这些点既可以代表空间位置, 也可以代表时间, 时空等, 暗示着 non-local 可以应用于图片, 序列和视频相关任务. 使用 non-local operaions 具有以下几点优点: 相比于 CNN 和 RNN 的逐步计算的劣势, non-local 操作 可以直接从任意两点中获取到 long-range dependencies. 根据实验结果可知, non-local operations 是十分高效的, 并且即使在只有几层网络层时, 也能取得很好的效果. 最后, 本文的 nocal operaions 会维持输入变量的尺寸大小, 并且可以很容易的与现有的其他 operations 结合使用.我们用 video classification 任务来展示 non-local 的有效性. 在视频数据中, long-range interactions 不仅出现在 空间位置上的 distant pixels, 还会出现在时间维度上的 distant pixels. 通过一个单一的 non-local block (basic unit), 便可以捕获到这些 spacetime dependencies, 如果将多个 non-local block 组合起来形成 non-local neural networks, 便可以提高 video classification 任务的准确度(不加任何tricks). 另外, non-local 网络要比 3D 卷积网络的计算性价比更高. 为了说明 non-local 的一般性, 我们还在 COCO 数据集上进行了目标检测/分割, 姿态识别等任务的实验, 在基于 MaskRCNN 的网络基础上, 我们的 non-local blocks 可以用较少的计算开销进一步提升模型的精度. 相关工作Non-local image: Non-local means 是一种经典的过滤算法, 它会计算整幅图片的像素值的加权平均和, 使得一些较远元素可以贡献一些位置上的响应. FeedForward modeling for sequences: 近年来很多前馈网络被用于解决语音和自然语言处理, 它们通过更大的感受野来实现 long-range dependencies. Self-attention: 本文的工作和机器翻译中的 self-attention 机制有关. Interaction networks Video classification architectures Non-local Neural Networks下面首先给出 non-local operations 的一般性定义, 然后会给出几种特定的变体 Formulation根据 non-local mean operation, 我们可以在深度卷积网络中定义如下的一般化的 non-local operation: y_i = \frac{1}{\zeta (x)} \sum_{\forall j}f(x_i, x_j) g(x_j) \tag 1上式中, $i$ 代表了 output 的 position 响应, 而 $j$ 枚举了所有可能的 position. $x$ 是 input signal (一般为特征图谱), $y$ 是 output signal (与 $x$ 的 size 相同). $f$ 会返回一个标量, $g$ 也会返回一个标量, $\zeta (x)$ 的作用是对响应进行归一化. 该公式的 non-local 特性主要体现在考虑了所有可能的 position ($\forall j$), 而卷积网络只会考虑 output position 周围位置的像素点.non-local 是一个非常灵活的模块, 它可以被添加到深度神经网络的浅层网络当中去(不像fc那样处于深层网络), 这使得我们可以构建更加丰富的模型结构来结合 non-local 和 local 信息. Instantiations接下来, 我们举例说明几种常用的 $f$ 和 $g$. 有趣的是, 通过实验(表2a)发现, 本文的 non-local 模块对于 $f$ 和 $g$ 的选择并不是特别敏感, 这意味着 non-local 的通用性正是提升各个模型在不同任务上性能表现的主要原因.为了简化, 我们考虑将 $g$ 写成线性形式: $g(x_j) = W_g x_j$, 这里的矩阵 $W_g$ 正是模型需要学习的参数, 在实现时, 通常会通过 1×1(或 1×1×1) 的卷积 来实现. 接下来, 我们来讨论 $f$ 的选择 Gaussian: 最容易想到的选择 f(x_i, x_j) = e^{x_i^T x_j}在这里, $x_i^T x_j$ 是两个向量的点积, 则会返回一个标量, 有时候也可以使用欧几里得距离, 不过点积的实现更加容易. 归一化因子 $\zeta (x) = \sum_{\forall j} f(x_i, x_j)$ Embedded Gaussian: 这是高斯函数的一个简单扩展 f(x_i, x_j) = e^{\theta (x_i)^T} \phi(x_j)在上式中, $\theta (x_i) = W_{\theta} x_i$ , $\phi(x_j) = W_{\phi} x_j$ , 分别为两种 embeddings. 同时, 归一化因子 $\zeta(x) = \sum_{\forall j} f(x_i, x_j)$. Dot product: $f$ 也可以定义成点乘 f(x_i, x_j) = \theta(x_i)^T \phi(x_j)这里我们采用了 embedded 版本, 并且令归一化因子 $\zeta = N$, $N$ 是 $x$ 中 positions 的数量, 而不是 $f$ 的和. Concatenation: Concatenation 曾被用于 Relation Network 来解决 visual reasoning 问题, 形式如下 f(x_i, x_j) = ReLU(w^T_f[\theta (x_i), \phi (x_j)])上式中, $[\cdot, \cdot]$ 代表这 concatenation 操作, $w_f$ 代表着将 concatenated vector 映射到标量的权重向量, 同样, 令 $\zeta(x) = N$. Non-local Block我们将上面介绍的公式(1) (non-local operation)包装进一个 non-local block 中, 使其可以整合到许多现有的网络结构当中, 我们将 non-local 定义成如下格式: z_i = W_z y_i + x_i上式中, $y_i$ 即为公式(1)的返回结果, 而 $+x_i$ 代表着残差连接. 残差连接使得我们可以将一个全新的 non-local block 插入到任何预训练的模型中, 而不会坡缓其原始的行为(eg, $W_z$ 初始化为0). 一个关于 non-block 的实例如图2所示. 当将 non-local block 应用于一个 high-level 的特征图谱时, 其带来的计算成本是很低的, 如在图2中, 通常情况下, $T=4, H=W=14 or 7$. Implementation of Non-local Blocks: 我们将 $W_g, W_{\theta}, W_{\phi}$ 的 channels 设置为 $x$ channels 的一半. 另一个 subsampling 的 trick 是将公式(1)改为: $y_i = \frac{1}{\zeta (\hat x)} \sum_{\forall j} f(x_i, \hat x_j) g(\hat x_j)$, 其中 $\hat x$ 是 $x$ 的 subsampled 版本. 这个 trick 可以使计算量减少 1/4, 并且不会改变 non-local 的行为, 仅仅只会令计算变得更加稀疏. 通过在图2中的 $\phi$ 和 $g$ 之后加一层 max pooling layer 即可实现. Video Classification Models略 Experiments on Video Classification Extension: Experiments on COCOObject detection and instance segmentationn: 我们修改了 MaskRCNN 的 backbone, 在其 res4 的后面添加了一个 non-local block. 与原文不同是, 我们使用了端到端的联合训练(原文是将 RPN 和 RoIAlign等分开训练), 这使得我们的 baseline 提高了.表5显示了在 COCO 数据集上的 box 和 mask AP. 我们可以看到, 一个单独的 non-local block 可以提升所有 Res50/101 和 X152 的baseline. 另外, 下面的性能提升只需要付出很小的计算代价(不超过原模型的 5%), 我们同样尝试了使用更多的 non-local 模块, 但是发现这会降低模型的性能. 表6显示了 non-local 在姿态识别任务上的性能提升.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoupleNet-Coupling Global Structure with Local Parts for Object Detection]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CoupleNet-ICCV2017%2F</url>
    <content type="text"><![CDATA[核心亮点在进行区域分类时, 同时使用了全局信息,上下文信息和局部信息综合判断提出了一个新颖的全卷积网络, 并称之为CoupleNet, 它可以在目标检测中结合使用全局和局部信息. 具体来说, CoupleNet会将由RPN网络产生的候选区域送入到coupling module中, 该模块包含两个分支. 第一条分支利用position-sensitive RoI pooling来捕获物体的局部信息, 另一条分支利用RoI pooling对全局上下文信息进行编码. 接着, 我们设计了不同的coupling策略和归一化方法来使用这些不同分支格子的优势. 论文细节摘要尽管R-FCN在保证检测准确度的同时, 取得了更快的检测速度, 但是position-sensitive score maps的设计依然忽略了图中的整体结构的全局信息. 为了充分利用并结合局部和全局信息, 本文提出了一个新颖的全卷积网络, 并称之为CoupleNet, 它可以在目标检测中结合使用全局和局部信息. 具体来说, CoupleNet会将由RPN网络产生的候选区域送入到coupling module中, 该模块包含两个分支. 第一条分支利用position-sensitive RoI pooling来捕获物体的局部信息, 另一条分支利用RoI pooling对全局上下文信息进行编码. 接着, 我们设计了不同的coupling策略和归一化方法来使用这些不同分支格子的优势. 最终, 本文的模型达到了SOTA. 介绍典型的基于候选区域的检测算法如FasterRCNN使用了单独的网络来生成候选区域, 这使得检测速度很慢, 而R-FCN利用PSRoI pooling(position-sensitive RoI)对其进行了改进, 在保证精度的情况下获得了更快的检测速度. 但是, R-FCN网络依然没能利用到全局结构信息, 如图1所示, 当只利用局部信息时, 检测框内物体对沙发的预测概率只有0.08, 这显然是是不合理的, 而如果只利用全局信息, 也只能得到0.45的预测概率, 但是如果结合这两部分信息, 就能得到0.78的预测结果, 我们更乐意接受这个结果. 本文的主要贡献有以下三点: 本文提出了一个统一的全卷积网络, 可以联合地学习到目标检测任务中的局部信息, 全局信息和相关的上下文信息 本文设计了多个不同的归一化方法和coupling策略, 用以挖掘全局信息和局部信息之间的兼容性和互补性 本文的模型在三个主流数据集(VOC07,VOC12,COCO)取得了SOTA CoupleNet网络结构 CounpleNet的网络结构如图2所示, 主要包含两条不同的分支: 一个局部的part-sensitive全卷积网络, 用于学习特定物体的局部信息, 记为local FCN; 一个全局的region-sensitive全卷积网络, 用于对物体整体结构的全局信息和上下文信息进行编码, 记为global FCN.本文首先利用ResNet-101 (移除了最后的全局平均层和fc层)对图片进行卷积操作, 得到相应的特征图谱, 并利用RPN网络得到相应的候选区域, RPN网络与后续的CoupleNet共享特征图谱计算结果. 然后conv5上对应的候选区域会流向两个不同的分支: local FCN 和 global FC. 最后, 从 local FCN 和 gocal FCN 中得到的结果会被结合在一起, 作为最终的物体socre输出. Local FCN 为了在local FCN高效的捕获特定区域的信息, 本文通过利用通道数为 $k^2(C+1)$ 的 $1\times 1$ 的卷积层构建了一系列的 part=sensitive socre map, 其中 $k$ 代表我们将物体划分成 $k\times k$ 个局部部位(local parts), $C+1$ 代表类别. 因此, 对于任意一个类别, 都会有 $k^2$ 个通道, 并且每一个通道会负责物体的一个特定局部部位. 最终的类别score将由这 $k^2$ 个结果投票产生. 这里, 我们使用了 R-FCN 的 position-sentitive RoI pooling 层来提取物体的特定部位, 并且是三简单的平均池化来进行投票. 如此一来, 我们就会得到一个 $C+1$ 维度的向量, 代表着当前候选区域属于每一类的概率. 这个过程相当于是把一个对物体类别的强分类器转换成了许多弱分类器, 如此便可以起到ensemble part models的作用, 使得分类更加准确. 如图3(a)所示, 对于一个被裁剪的人像来说, 神经网络对人的全局信息无法很高的响应, 但是如果仅从局部特征角度出发, 如人的鼻子, 眼睛等, local FCN可以十分高效的捕获到这些特定区域的特征. 因此, 我们认为 local FCN 更加关注物体的内部结构和组成要素等信息, 这些信息可以高效的反映出物体的局部属性, 特别是当物体被遮挡或者整体边界不完整的情况. 但是, 对于那些具有简单空间结构以及那些包括了相当多背景区域的物体来说(如, 餐桌), 单单只靠 local FCN 很难做出鲁棒性较高的预测结构. 因此有必要加入全局结构信息来增强网络的分辨能力. Global FCN对于 global FCN, 本文通过使用整体的区域级别的特征来描述物体的全局信息. 首先, 我们将一个 1024 维度的 $1\times 1$ 卷积层接在ResNet101的最后一个残差模块之后, 用于降低维度. 由于候选区域的尺寸不唯一, 因此, 本文会插入一个 RoI pooling 层来提取固定长度的特征向量作为物体的全局结构信息. 第二, 本文使用了两个卷积层(分别为 $k\times k$ 和 $1\times 1$)来更进一步的抽象物体的全局信息. 最后, $1\times 1$ 的卷积结果会被送入分类器, 分类器的输出也是一个 $C+1$ 维度的向量(与local FCN一样). 此外, 上下文信息是视觉识别任务中最基本,最重要的元素之一. 例如, 船通常是在水里的, 而不太可能是在空中的. 尽管, 在深层网络中, 较深的网络的特征图谱具有更大的感受野, 可以相对获得更多的空间上下文信息, 但是实际中深层特征图谱所包含的上下文信息要理理论上少很多. 因此, 很有必要显式的去收集物体的周围信息, 以减少错分类的可能性. 为了增强 global FCN 的特征表达能力, 本文将上下文信息引入到网络中作为一种有效的附加信息. 具体来说, 本文将物体的RoI区域扩大为原来的2倍. 然后将这两种RoI(原始的和扩大后的)通过RoIpooling后再连接在一起(concatenated), 接着送入之后的子网络.(如图2后下部分所示, 实际上, global分支可以看做是一种特殊的FasterRCNN). 由于RoI pooling操作, global FCN可以将物体区域作为物体的整体特征进行描述, 因此, 它可以轻松的处理那些完整的物体, 如图3(b)所示. Coupling structure为了让global FCN 和 local FCN 返回的结果在数量级上匹配, 本文对它们的输出使用了归一化操作. 主要利用了两种方案来进行归一化: L2归一化层或者 $1 \times 1$ 卷积层. 同时, 如何将local和global输出结合起来也是一个关键问题. 在本文中, 我们探究了三种不同的coupling方法: 对应位相加(element-wise sum), 对应位相乘(element-wise product), 以及对应位取最大(element-wise maximum). 实验结果表明, 使用 $1\times 1$ 卷积配合对应位相加可以取得最好的实验效果. 实验使用L2归一化效果很差(甚至比不使用归一化的结果还要差), 推测原因可能是L2归一化以后会使得score之间的gap变小, 进而造成错分类. 而使用 $1\times 1$ 卷积进行归一化时, 网络会自动学习并调节local和global归一化以后的尺寸大小. 对于coupling策略的选择, 对应位相加是一种很有效的连接方式, 在ResNet也使用了这种连接, 而对应位相乘有时候会造成梯度的不稳定, 从而导致训练难以收敛. 对应位取最大则会丢失掉更多的信息, 同时也就丢失了结合局部和全局信息的优势. 正如我们之前讨论的那样, CoupleNet在面对遮挡, 截断以及包括大量背景的目标时(如沙发,人,桌子,椅子等等), 可以表现出很强的识别能力.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用PyTorch自己动手从零实现YOLOv3]]></title>
    <url>%2Fz_post%2FPyTorch-YOLOv3%2F</url>
    <content type="text"><![CDATA[学习一个算法最好的方式就是自己尝试着去实现它! 因此, 在这片博文里面, 我会为大家讲解如何用PyTorch从零开始实现一个YOLOv3目标检测模型. 这里我们假设大家对YOLOv3的各个细节都比较熟悉, 因此就不对YOLOv3做过多介绍, 如果对YOLOv3不太懂的话, 可以再看看原文, 或者看看我写的YOLOv3解析. 模型实现总共会分为以下四部分: (一) 搭建YOLO模型框架 (二) 实现模型框架的前向传播过程 (三) 目标函数相关实现及NMS算法实现 (四) 设计输入输出流(input and output pipelines) 参考文献https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/ 【链接】从零开始实现YOLOv3（part2）https://zhuanlan.zhihu.com/p/36920744]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[visdom-数据可视化工具(支持PyTorch和Numpy)]]></title>
    <url>%2Fz_post%2FPyTorch-visdom%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/32025746visdom 是一个灵活的可视化工具, 可用来对实时, 大量的数据进行分析, 组织, 共享等, 还可以实现 远程 数据的可视化. 几个基本概念]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[itertools模块-高效的迭代器操作]]></title>
    <url>%2Fz_post%2FPython-itertools%2F</url>
    <content type="text"><![CDATA[http://funhacks.net/2017/02/13/itertools/#product itertools 模块提供的迭代器函数有以下几种类型: 无限迭代器: 生成一个无限序列, 比如自然数序列: 1, 2, 3, 4, … 有限迭代器: 接受一个或多个序列作为参数, 进行组合, 分组, 过滤等操作 组合生成器: 序列的排列, 组合, 求序列的笛卡尔积等等 无线迭代器优先迭代器组合生成器productproduct 用于求多个可迭代对象的笛卡尔积, 它跟嵌套的 for 循环等价, 其一般使用形式如下:1product(iter1, iter2, ..., iterN, [repeat=1]) 其中, repeat是一个关键字参数, 用于指定重复生成序列的次数]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用PyTorch实现经典VGG网络]]></title>
    <url>%2Fz_post%2FPyTorch-VGG%2F</url>
    <content type="text"></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Single-Shot Refinement Neural Network for Object Detection]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-RefineDet-CVPR2018%2F</url>
    <content type="text"><![CDATA[作者: Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, Stan Z.Li发表: CVPR2018 论文亮点:结合了one-stage方法和two-stage方法各自的优势, 提出了一个基于single-shot的检测模型:模型主要包含两大模块, 分别是anchor精化模块和物体检测模块. 网络采用了类似FPN的思想, 通过 Transfer Connection Block 将特征图谱在两个模块之间传送, 不仅提升了的精度, 同时还在速度方面取得了与one-stage方案相媲美的表现 论文细节摘要本文提出了一个基于single-shot的检测模型, 称为RefineDet, 它在精度上高于现有的two-stage方法, 同时, 可以和one-stage方法的速度相媲美. RefineDet包含两个内部连接的模块(如图1): anchor精化模块(anchor refinement module): 过滤掉负样本的anchors, 以减少分类器的搜索空间; 对anchors的位置和size进行粗糙的调整, 以便为后续的回归网络提供更好的初始化状态. 物体检测模块(object detection module): 用refined anchors作为输入进行回归预测. 同时, 设计一个传送连接模块(transfer connection block), 将anchor refinement module里面的特征进行传送, 以此来预测框的位置, size 和 类别标签. 由于本文使用了多任务联合损失函数, 因此可以进行端到端的训练. 介绍在作者看来, 目前的two-stage方法(Faster RCNN, R-FCN, FPN), 相比于One-Stage方法来说具有三个优势: 具有启发式规则来处理正负样本不均衡问题 具有两个级联的物体边框回归阶段(边框更加精确) 提取了更加丰富的物体特征(anchor使得提取过程更精细) 为了结合One-Stage和Two-Stage方法的优势, 同时克服他们的缺点, 本文的RefineDet设计了两个内部连接的模块: anchor refinement module(ARM) 和 object detection module(ODM).(如图1所示) 网络结构网络结构的整体视图如图1所示, 和SSD类似, RefineDet会基于前向传播网络预测出固定数量的bounding box和对应的类别score. 本文的网络主要包含ARM和ODM两大部分. ARM: 对经典网络结构(VGG-16, ResNet-101)进行改造, 去掉分类层, 并加上一些附属结构ODM: ODM由TCBs (Transfer Connection Block)和预测层(3×3 卷积层)组成, 会输出物体的类别score和相对于refined anchor box的相对位置坐标. Transfer Connection Block: 用于链接ARM和ODM, 引入TCBs的目的主要是为了将ARM中不同层的特征转换成ODM接受的形式, 这样一来,ODM和ARM就可以共享特征向量. 值得注意的是, 在ARM中, 本文仅仅对于anchor相关的特征图谱使用TCBs. 其实很像FPN的想法, 但是与它又不太一样. TCBs的结构如图2所示 Two-Step Cascaded Regression: 当前的two-stage方法仅仅依赖于一次边框回归过程, 其主要是基于在不同尺度的特征图谱上来预测不同size的物体的位置, 因此预测的精度较低, 尤其是在面对小物体时. 因此, 本文的模型采用了Two-Step Cascaded Regression. 首先, 利用ARM来调节anchor的位置和大小, 以便为后续的ODM的回归预测提供更好的anchor初始状态. 具体来说, 我们会将特征图谱进行网格划分, 然后对于每一个cell可以得到n个anchor boxes, 最开始的时候, 每一个anchor box的位置相对于它的cell来说都是固定的. 在每一个特征图谱的cell上面, 我们都会预测4个相对坐标(refined网格相对于origin网格的位移坐标). 因此, 我们可以在每一个cell上面, 产生n个refined anchors. 在获得了refined anchor boxes以后, 我们将它们传送到对应的ODM中去(不同的feature map对应不同的ODM) 来预测物体类别,边框位置和大小等信息. 互相关联的ARM和ODM具有相同的维度, 对于每一个anchor box, ODM都会生成 $c+4$ 个输出( $c$ 为物体类别数 ). 这个预测过程与SSD很相似, 但是与之不同的是本文使用了Refined anchor boxes, 从而可以获得更精确的结果. Negative Anchor Filtering: 同以往检测模型一样, 本文不希望训练过多的(容易分类的)简单样本, 同时需要减轻样本不均衡问题, 因此, 本文设计了一个 negative anchor过滤机制. 具体来说, 就是在训练阶段, 对于一个refined anchor box, 如果它的负样本概率大于一个阈值(如0.99), 那么我们就不去训练它. 这样一来, 网络只会训练 难负样本(refined hard negative anchor boxes) 以及 所有的 正样本(refined positive anchor boxes). 同样, 在预测阶段, 对于大于阈值的负样本, 也会对其放弃检测. 训练和预测(Training and Inference)Data Augmentatin: 采用了和SSD相同的数据增广方法 Backbone Network: 使用了 VGG-16 和 ResNet-101 作为骨架网络, 分别在两个网络后面多加了一些卷积层或者残差模块, 以提取更高level的特征. Anchors Design and Matching: 为了处理不同的物体尺度问题, 本文选择了4个特征层, stride size分别为8, 16, 32 和 64. Hard Negative Mining: 采用了和SSD类似的难样例挖掘算法. Loss Function: RefineDet 的损失函数包含两部分, 即ARM的损失和ODM的损失. 对于ARM损失来说, 我们给每个anchor赋予一个二值标签(是或不是物体). 对于ODM损失来说, 它会接受refined之后的anchor, 进行更进一步的边框预测和类别预测. Optimization: “vavier”初始化用于额外增加的层(两层). Inference:]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cpp-RTTI机制]]></title>
    <url>%2Fz_post%2FCpp-RTTI%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[RTTI概念RTTI (Run Time Type Identification) 即运行时类型识别, 程序能够使用基类类型的指针或引用来检查来检查这些指针或引用所指的对象的实际派生类型. RTTI机制的产生C++ 是一种静态类型语言, 其具体类型是在编译器就确定的, 不能在运行时更改. 然而由于面向对象程序设计中多态性的要求, C++中的指针或引用, 可能指向与它实际类型不符的其他类型(子类). 有时我们需要将一个多态指针转换为其实际指针对象的类型, 此时就需要知道运行时的类型信息. typeid 和 dynamic_cast 操作符RTTI提供了两个非常有用的操作符: typeid 和 dynamic_cast. typeid操作符: 返回指针和引用所指的实际类型 dynamic_cast操作符: 将基类类型的指针或引用安全的转换为其派生类类型的指针或引用 我们知道C++的多态性是有虚函数实现的, 对于多态性的对象, 无法在程序编译阶段确定对象的类型. 为了在运行时获得一个对象的类型, 可以使用typeid函数, 该函数返回一个对type_info类对象的引用, 要使用typeid时必须使用头文件&lt;typeinfo&gt;, 因为typeid是一个返回类型为type_info的引用, 因此, 首先看一下type_info类 type_info 类成员函数: name(): 返回类型的名称 raw_name(): 返回名字编码(Name Mangling)算法产生的新名称. hash_code(): 返回当前类型对应的hash值. hash值是一个可以用来标志当前类型的函数, 有点类型学生的学号, 公民的身份证号, 银行卡号等等. 不过hash值有赖于编译器的实现, 在不同的编译器下可能会有不同的整数, 但它们都能唯一的标识某个类型. 遗憾的是, C++标准只对type_info类做了很有限的规定, 不仅成员函数少, 功能弱, 而且各个平台的实现不一致. 例如最常用的name()函数, int类型和Base(类)类型在VC/VS下的输出结果分别是int和class Base, 而在GCC下的输出结果分别是i和4Base. C++标准规定, type_info类至少要有如下所示的4个public属性的成员函数, 其他的扩展函数编译器开发者可以自由发挥, 不做限制. const char *name() const: 返回一个能表示类型名称的字符串, 但是C++标准没有规定这个字符串是什么形式的, 因此出现了不同编译器返回的字符串形式不同的情况. bool before (const type_info &amp;rhs) const: 判断一个类型是否位于另一个类型前面, rhs参数是一个type_info对象的引用, 但是C++标准并没有规定类型的排列规则, 不同的编译器有不同的排列规则, 程序员也可以自定义. 要特别注意的是, 这个排列顺序和继承顺序是没有关系的, 基类并不一定位于派生类的前面. bool operator==(const type_info &amp;rhs) const: 重载运算符==, 判断两个类型是否相同, rhs参数是一个type_info对象的引用 bool operator!=(const type_info &amp;rhs) const: 重载运算符!=, 判断两个类型是否不同, rhs参数是一个type_info对象的引用]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用Numpy实现一个简单的神经网络]]></title>
    <url>%2Fz_post%2FPython-%E7%94%A8Numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[本示例来自于PyTorch的官网上的一个warm-up小示例, 觉得很有代表性, 所有这里单独记录一下.对于numpy来说, 它对计算图, 深度学习, 梯度等等概念几乎是不知道的, 但是, 如果我们了解简单神经网络的具体结构, 那么我们就可以很轻易的用numpy来实现这个简单网络, 对此, 我们通常需要自己来实现前向计算和反向计算的逻辑, 下面我们来实现一个具有两层隐藏层的简单网络: 12345678910111213141516171819202122232425262728293031323334353637import numpy as np# N 为batch size, D_in 为输入维度# H 为隐藏层的维度, D_out 为输出的维度N, D_in, H, D_out = 64, 1000, 100, 10# 创建随机的输入和输出数据x = np.random.randn(N, D_in) # N × D_in 的矩阵y = np.random.randn(N, D_out) # N × D_out 的矩阵# 对两个隐藏层w1,w2进行初始化w1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)# 设置学习率learning_rate = 1e-6for t in range(500): # 前向传播: 计算预测结果 y_pred h = x.dot(w1) # x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100 h_relu = np.maximum(h,0) y = h_relu.dot(w2) # h_relu维度为 64×100, w2维度为100×10, y的维度为64×10 # 计算损失 loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播根据loss更新w1和w2的值 grad_y_pred = 2.0*(y_pred - y) # 对y_pred求导 grad_w2 = h_relu.T.dot(grad_y_pred) # 对w2求导, 微分矩阵应该与w2的size相同 grad_h_relu = grad_y_pred.dot(w2.T) # 对h_relu求导 grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = grad_h_relu # 经过relu, 将小于0的梯度归0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2 在执行上述代码以后, w1和w2的值会是的预测出来的pred_y与y之间的平方损失越来越小.]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SSD 源码实现 (PyTorch)]]></title>
    <url>%2Fz_post%2FPyTorch-SSD%2F</url>
    <content type="text"><![CDATA[概览SSD 和 YOLO 都是非常主流的 one-stage 目标检测模型, 并且相对于 two-stage 的 RCNN 系列来说, SSD 的实现更加的简明易懂, 接下来我将从以下几个方面展开对 SSD 模型的源码实现讲解: 模型结构定义 DefaultBox 生成候选框 解析预测结果 MultiBox 损失函数 Augmentations Trick 模型训练 模型预测 模型验证 其他辅助代码 可以看出, 虽然 SSD 模型本身并不复杂, 但是也正是由于 one-stage 模型较简单的原因, 其检测的准确率相对于 two-stage 模型较低, 因此, 通常需要借助许多训练和检测时的 Tricks 来提升模型的精确度, 这些代码我们会放在第三部分讲解. 下面, 我们按照顺序首先对 SSD 模型结构定义的源码进行解析.(项目地址: https://github.com/amdegroot/ssd.pytorch) 模型结构定义本部分代码主要位于 ssd.py 文件里面, 在本文件中, 定义了SSD的模型结构. 主要包含以下类和函数, 整体概览如下:1234567891011121314151617181920# ssd.pyclass SSD(nn.Module): # 自定义SSD网络 def __init__(self, phase, size, base, extras, head, num_classes): # ... SSD 模型初始化 def forward(self, x): # ... 定义forward函数, 将设计好的layers和ops应用到输入图片 x 上 def load_weights(self, base_file): # ... 加载参数权重值def vgg(cfg, i, batch_norm=False): # ... 搭建vgg网络def add_extras(cfg, i, batch_norm=False): # ... 向VGG网络中添加额外的层用于feature scalingdef multibox(vgg, extra_layers, cfg, num_classes): # ... 构建multibox结构base = &#123;...&#125; # vgg 网络结构参数extras = &#123;...&#125; # extras 层参数mbox = &#123;...&#125; # multibox 相关参数def build_ssd(phase, size=300, num_classes=21): # ... 构建模型函数, 调用上面的函数进行构建 为了方便理解, 我们不按照文件中的定义顺序解析, 而是根据文件中函数的调用关系来从外而内, 从上而下的进行解析, 解析顺序如下: build_ssd(…) 函数 vgg(…) 函数 add_extras(…) 函数 multibox(…) 函数 SSD(nn.Module) 类 build_ssd(…) 函数在其他文件通常利用build_ssd(phase, size=300, num_classes=21)函数来创建模型, 下面先看看该函数的具体实现: 123456789101112131415161718192021222324252627282930313233343536373839# ssd.pyclass SSD(nn.Module): # 自定义SSD网络 def __init__(self, phase, size, base, extras, head, num_classes): # ... def forward(self, x): # ... def load_weights(self, base_file): # ...def vgg(cfg, i, batch_norm=False): # ... 搭建vgg网络def add_extras(cfg, i, batch_norm=False): # ... 向VGG网络中添加额外的层用于feature scalingdef multibox(vgg, extra_layers, cfg, num_classes): # ... 构建multibox结构base = &#123; # vgg 网络结构参数 '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '500': []&#125;extras = &#123; # extras 层参数 '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '500': []&#125;mbox = &#123; # multibox 相关参数 '300': [4, 6, 6, 6, 4, 4], '500': []&#125;def build_ssd(phase, size=300, num_classes=21): # 构建模型函数, 调用上面的函数进行构建 if phase != "test" and phase != "train": # 只能是训练或者预测阶段 print("ERROR: Phase: " + phase + " not recognized") return if size != 300: print("ERROR: You specified size " + repr(size) + ". However, "+ "currently only SSD300 is supported!") # 仅仅支持300size的SSD return base_, extras_, head_ = multibox(vgg(base[str(size)], 3), add_extras(extras[str(size), 1024), mbox[str(size)], num_classes ) return SSD(phase, size, base_, extras_, head_, num_classes) 可以看到, build_ssd(...)函数主要使用了multibox(...)函数来获取base_, extras_, head_, 在调用multibox(...)函数的同时, 还分别调用了vgg(...)函数, add_extras(...)函数, 并将其返回值作为参数. 之后, 利用这些信息初始化了SSD网络. 那么下面, 我们就先查看一下这些函数定义和作用 vgg(…) 函数我们以调用顺序为依据, 先对multibox(...)函数的内部实现进行解析, 但是在查看multibox(...)函数之前, 我们首先需要看看其参数的由来, 首先是vgg(...)函数, 因为 SSD 是以 VGG 网络作为 backbone 的, 因此该函数主要定义了 VGG 网络的结果, 根据调用语句vgg(base[str(size)], 3)可以看出, 调用vgg时向其传入了两个参数, 分别为base[str(size)] 和3, 对应的就是base[&#39;300&#39;]和3. 123456789101112131415161718192021222324# ssd.pydef vgg(cfg, i, batch_norm = False): # cfg = base['300'] = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], # i = 3 layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] if v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels=in_channels, out_channels=v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Con2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers 上面的写法是 ssd.pytorch 代码中的原始写法, 代码风格体现了 PyTorch 灵活的编程特性, 但是这种写法不是那么直观, 需要很详细的解读才能看出来这个网络的整个结构是什么样的. 建议大家结合 VGG 网络的整个结构来解读这部分代码, 核心思想就是通过预定义的 cfg=base={...} 里面的参数来设置 vgg 网络卷积层和池化层的参数设置, 由于 vgg 网络的模型结构很经典, 有很多文章都写的很详细, 这里就不再啰嗦了, 我们主要来看一下 SSD 网络中比较重要的点, 也就是下面的 extras_layers. add_extras(…) 函数想必了解 SSD 模型的朋友都知道, SSD 模型中是利用多个不同层级上的 feature map 来进行同时进行边框回归和物体分类任务的, 除了使用 vgg 最深层的卷积层以外, SSD 还添加了几个卷积层, 专门用于执行回归和分类任务(如文章开头图2所示), 因此, 我们在定义完 VGG 网络以后, 需要额外定义这些新添加的卷积层. 接下来, 我们根据论文中的参数设置, 来看一下 add_extras(...) 的内部实现, 根据调用语句add_extras(extras[str(size)], 1024) 可知, 该函数中参数cfg = extras[&#39;300&#39;], i=1024.123456789101112131415161718# ssd.pydef add_extras(cfg, i, batch_norm=False): # cfg = [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256] # i = 1024 layers = [] in_channels = i flag = False for k, v in enumerate(cfg): if in_channels != 'S': if v == 'S': # (1,3)[True] = 3, (1,3)[False] = 1 layers += [nn.Conv2d(in_channels=in_channels, out_channels=cfg[k+1], kernel_size=(1, 3)[flag], stride=2, padding=1)] else: layers += [nn.Conv2d(in_channels=in_channels, out_channels=v, kernel_size=(1, 3)[flag])] flag = not flag in_channels = v return layers 同样的问题, 上面的定义不是很直观, 因此我将上面的代码用 PyTorch 重写了, 重写后的代码更容易看出网络的结构信息, 同时可读性也较强, 代码如下所示(与上面的代码完全等价): 1234567891011def add_extras(): exts1_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1) exts1_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1) exts2_1 = nn.Conv2d(512, 128, 1, 1, 0) exts2_2 = nn.Conv2d(128, 256, 3, 2, 1) exts3_1 = nn.Conv2d(256, 128, 1, 1, 0) exts3_2 = nn.Conv2d(128, 256, 3, 1, 0) exts4_1 = nn.Conv2d(256, 128, 1, 1, 0) exts4_2 = nn.Conv2d(128, 256, 3, 1, 0) return [exts1_1, exts1_2, exts2_1, exts2_2, exts3_1, exts3_2, exts4_1, exts4_2] 在定义完整个的网络结构以后, 我们就需要定义最后的 head 层, 也就是特定的任务层, 因为 SSD 是 one-stage 模型, 因此它是同时在特征图谱上产生预测边框和预测分类的, 我们根据类别的数量来设置相应的网络预测层参数, 注意需要用到多个特征图谱, 也就是说要有多个预测层(原文中用了6个卷积特征图谱, 其中2个来自于 vgg 网络, 4个来自于 extras 层), 代码实现如下: multibox(…) 函数multibox(...) 总共有4个参数, 现在我们已经得到了两个参数, 分别是vgg(...)函数返回的layers, 以及add_extras(...)函数返回的layers, 后面两个参数根据调用语句可知分别为mbox[str(size)](mbox[&#39;300&#39;])和num_classes(默认为21). 下面, 看一下multibox(...)函数的具体内部实现: 12345678910111213141516# ssd.pydef multibox(vgg, extra_layers, cfg, num_classes): # cfg = [4, 6, 6, 6, 4, 4] # num_classes = 21 # ssd总共会选择6个卷积特征图谱进行预测, 分别为, vggnet的conv4_3, 以及extras_layers的5段卷积的输出(每段由两个卷积层组成, 具体可看extras_layers的实现). # 也就是说, loc_layers 和 conf_layers 分别具有6个预测层. loc_layers = [] conf_layers = [] vgg_source = [21, -2] for k, v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k]*4, kernel_size=3, padding=1] conf_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k]*num_classes, kernel_size=3, padding=1)] for k, v in enumerate(extra_layers[1::2], 2): loc_layers += [nn.Conv2d(v.out_channels, cfg[k]*4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(v.out_channels, cfg[k]*num_classes, kernel_size=3, padding=1)] return vgg, extra_layers, (loc_layers, conf_layers) 同样, 我们可以将上面的代码写成可读性更强的形式:1234567891011121314151617181920212223242526272829# ssd.pydef multibox(vgg, extras, num_classes): loc_layers = [] conf_layers = [] #vgg_source=[21, -2] # 21 denote conv4_3, -2 denote conv7 # 定义6个坐标预测层, 输出的通道数就是每个像素点上会产生的 default box 的数量 loc1 = nn.Conv2d(vgg[21].out_channels, 4*4, 3, 1, 1) # 利用conv4_3的特征图谱, 也就是 vgg 网络 List 中的第 21 个元素的输出(注意不是第21层, 因为这中间还包含了不带参数的池化层). loc2 = nn.Conv2d(vgg[-2].out_channels, 6*4, 3, 1, 1) # Conv7 loc3 = nn.Conv2d(vgg[1].out_channels, 6*4, 3, 1, 1) # exts1_2 loc4 = nn.Conv2d(extras[3].out_channels, 6*4, 3, 1, 1) # exts2_2 loc5 = nn.Conv2d(extras[5].out_channels, 4*4, 3, 1, 1) # exts3_2 loc6 = nn.Conv2d(extras[7].out_channels, 4*4, 3, 1, 1) # exts4_2 loc_layers = [loc1, loc2, loc3, loc4, loc5, loc6] # 定义分类层, 和定位层差不多, 只不过输出的通道数不一样, 因为对于每一个像素点上的每一个default box, # 都需要预测出属于任意一个类的概率, 因此通道数为 default box 的数量乘以类别数. conf1 = nn.Conv2d(vgg[21].out_channels, 4*num_classes, 3, 1, 1) conf2 = nn.Conv2d(vgg[-2].out_channels, 6*num_classes, 3, 1, 1) conf3 = nn.Conv2d(extras[1].out_channels, 6*num_classes, 3, 1, 1) conf4 = nn.Conv2d(extras[3].out_channels, 6*num_classes, 3, 1, 1) conf5 = nn.Conv2d(extras[5].out_channels, 4*num_classes, 3, 1, 1) conf6 = nn.Conv2d(extras[7].out_channels, 4*num_classes, 3, 1, 1) conf_layers = [conf1, conf2, conf3, conf4, conf5, conf6] # loc_layers: [b×w1×h1×4*4, b×w2×h2×6*4, b×w3×h3×6*4, b×w4×h4×6*4, b×w5×h5×4*4, b×w6×h6×4*4] # conf_layers: [b×w1×h1×4*C, b×w2×h2×6*C, b×w3×h3×6*C, b×w4×h4×6*C, b×w5×h5×4*C, b×w6×h6×4*C] C为num_classes # 注意pytorch中卷积层的输入输出维度是:[N×C×H×W], 上面的顺序有点错误, 不过改起来太麻烦 return loc_layers, conf_layers 定义完网络中所有层的关键结构以后, 我们就可以利用这些结构来定义 SSD 网络了, 下面就介绍一下 SSD 类的实现. SSD(nn.Module) 类在 build_ssd(...) 函数的最后, 利用语句return SSD(phase, size, base_, extras_, head_, num_classes)调用的返回了一个SSD类的对象, 下面, 我们就来看一下看类的内部细节(这也是SSD模型的主要框架实现) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# ssd.pyclass SSD(nn.Module): # SSD网络是由 VGG 网络后街 multibox 卷积层 组成的, 每一个 multibox 层会有如下分支: # - 用于class conf scores的卷积层 # - 用于localization predictions的卷积层 # - 与priorbox layer相关联, 产生默认的bounding box # 参数: # phase: test/train # size: 输入图片的尺寸 # base: VGG16的层 # extras: 将输出结果送到multibox loc和conf layers的额外的层 # head: "multibox head", 包含一系列的loc和conf卷积层. def __init__(self, phase, size, base, extras, head, num_classes): # super(SSD, self) 首先找到 SSD 的父类, 然后把类SSD的对象转换为父类的对象 super(SSD, self).__init__() self.phase = phase self.num_classes = num_classes self.cfg = (coco, voc)[num_classes == 21] self.priorbox = PriorBox(self.cfg) # layers/functions/prior_box.py class PriorBox(object) self.priors = Variable(self.priorbox.forward(), volatile=True) # from torch.autograd import Variable self.size = size self.vgg = nn.ModuleList(base) self.L2Norm = L2Norm(512,20) # layers/modules/l2norm.py class L2Norm(nn.Module) self.extras = nn.ModuleList(extras) self.loc = nn.ModuleList(head[0]) # head = (loc_layers, conf_layers) self.conf = nn.ModuleList(head[1]) if phase = "test": self.softmax = nn.Softmax(dim=-1) # 用于囧穿概率 self.detect = Detect(num_classes, 0, 200, 0.01, 0.45) # layers/functions/detection.py class Detect # 用于将预测结果转换成对应的坐标和类别编号形式, 方便可视化. def forward(self, x): # 定义forward函数, 将设计好的layers和ops应用到输入图片 x 上 # 参数: x, 输入的batch 图片, Shape: [batch, 3, 300, 300] # 返回值: 取决于不同阶段 # test: 预测的类别标签, confidence score, 以及相关的location. # Shape: [batch, topk, 7] # train: 关于以下输出的元素组成的列表 # 1: confidence layers, Shape: [batch*num_priors, num_classes] # 2: localization layers, Shape: [batch, num_priors*4] # 3: priorbox layers, Shape: [2, num_priors*4] sources = list() # 这个列表存储的是参与预测的卷积层的输出, 也就是原文中那6个指定的卷积层 loc = list() # 用于存储预测的边框信息 conf = list() # 用于存储预测的类别信息 # 计算vgg直到conv4_3的relu for k in range(23): x = self.vgg[k](x) s = self.L2Norm(x) sources.append(s) # 将 conv4_3 的特征层输出添加到 sources 中, 后面会根据 sources 中的元素进行预测 # 将vgg应用到fc7 for k in range(23, len(self.vgg)): x = self.vgg[k](x) sources.append(x) # 同理, 添加到 sources 列表中 # 计算extras layers, 并且将结果存储到sources列表中 for k, v in enumerate(self.extras): x = F.relu(v(x), inplace=True) # import torch.nn.functional as F if k % 2 = 1: # 在extras_layers中, 第1,3,5,7,9(从第0开始)的卷积层的输出会用于预测box位置和类别, 因此, 将其添加到 sources列表中 sources.append(x) # 应用multibox到source layers上, source layers中的元素均为各个用于预测的特征图谱 # apply multibox to source layers # 注意pytorch中卷积层的输入输出维度是:[N×C×H×W] for (x, l, c) in zip(sources, self.loc, self.conf): # permute重新排列维度顺序, PyTorch维度的默认排列顺序为 (N, C, H, W), # 因此, 这里的排列是将其改为 $(N, H, W, C)$. # contiguous返回内存连续的tensor, 由于在执行permute或者transpose等操作之后, tensor的内存地址可能不是连续的, # 然后 view 操作是基于连续地址的, 因此, 需要调用contiguous语句. loc.append(l(x).permute(0,2,3,1).contiguous()) conf.append(c(x).permute(0,2,3,1).contiguous()) # loc: [b×w1×h1×4*4, b×w2×h2×6*4, b×w3×h3×6*4, b×w4×h4×6*4, b×w5×h5×4*4, b×w6×h6×4*4] # conf: [b×w1×h1×4*C, b×w2×h2×6*C, b×w3×h3×6*C, b×w4×h4×6*C, b×w5×h5×4*C, b×w6×h6×4*C] C为num_classes # cat 是 concatenate 的缩写, view返回一个新的tensor, 具有相同的数据但是不同的size, 类似于numpy的reshape # 在调用view之前, 需要先调用contiguous loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1) # 将除batch以外的其他维度合并, 因此, 对于边框坐标来说, 最终的shape为(两维):[batch, num_boxes*4] conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1) # 同理, 最终的shape为(两维):[batch, num_boxes*num_classes] if self.phase == "test": # 这里用到了 detect 对象, 该对象主要由于接预测出来的结果进行解析, 以获得方便可视化的边框坐标和类别编号, 具体实现会在后文讨论. output = self.detect( loc.view(loc.size(0), -1, 4), # 又将shape转换成: [batch, num_boxes, 4], 即[1, 8732, 4] self.softmax(conf.view(conf.size(0), -1, self.num_classes)), # 同理, shape 为[batch, num_boxes, num_classes], 即 [1, 8732, 21] self.priors.type(type(x.data)) # 利用 PriorBox对象获取特征图谱上的 default box, 该参数的shape为: [8732,4]. 关于生成 default box 的方法实际上很简单, 类似于 anchor box, 详细的代码实现会在后文解析. # 这里的 self.priors.type(type(x.data)) 与 self.priors 就结果而言完全等价(自己试验过了), 但是为什么? ) if self.phase == "train": # 如果是训练阶段, 则无需解析预测结果, 直接返回然后求损失. output = ( loc.view(loc.size(0), -1, 4), conf.view(conf.size(0), -1, self.num_classes), self.priors ) return output def load_weights(self, base_file): # 加载权重文件 other, ext = os.path.splitext(base_file) if ext == ".pkl" or ".pth": print("Loading weights into state dict...") self.load_state_dict(torch.load(base_file, map_location=lambda storage, loc: storage)) print("Finished!") else: print("Sorry only .pth and .pkl files supported") 在上面的模型定义中, 我们可以看到使用其他几个类, 分别是 layers/functions/prior_box.py class 的 PriorBox(object), layers/modules/l2norm.py 的 class L2Norm(nn.Module) layers/functions/detection.py 的 class Detect 基本上从他们的名字就可以看出他们的用途, 其中, 最简单的是 l2norm 类, 该类实际上就是实现了 L2归一化(也可以利用 PyTorch API 提供的归一化接口实现). 这一块没什么好讨论的, 朋友们可以自己去源码去查看实现方法, 基本看一遍就能明白了.下面我们着重看一下用于生成 Default box(也可以看成是 anchor box) 的 PriorBox 类, 以及用于解析预测结果, 并将其转换成边框坐标和类别编号的 Detect类. 首先来看如何利用卷积图谱来生成 default box. DefaultBox 生成候选框根据 SSD 的原理, 需要在选定的特征图谱上输出 Default Box, 然后根据这些 Default Box 进行边框回归任务. 首先梳理一下生成 Default Box 的思路. 假如feature maps数量为 $m$, 那么每一个feature map中的default box的尺寸大小计算如下: s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1), k\in [1,m]上式中, $s_{min} = 0.2 , s_{max} = 0.9$. 对于原文中的设置 $m=6 (4, 6, 6, 6, 4, 4)$, 因此就有 $s = \{0.2, 0.34, 0.48, 0.62, 0.76, 0.9\}$然后, 几个不同的aspect ratio, 用 $a_r$ 表示: $a_r = {1,2,3,1/2,1/3}$, 则每一个default boxes 的width 和height就可以得到( $w_k^a h_k^a=a_r$ ): w_k^a = s_k \sqrt{a_r}h_k^a = \frac{s_k}{\sqrt {a_r}}对于宽高比为1的 default box, 我们额外添加了一个 scale 为 $s_k’ = \sqrt{s_k s_{k+1}}$ 的 box, 因此 feature map 上的每一个像素点都对应着6个 default boxes (per feature map localtion).每一个default box的中心, 设置为: $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{f_k})$, 其中, $|f_k|$ 是第 $k$ 个feature map的大小 $i,j$ 对应了 feature map 上所有可能的像素点.在实际使用中, 可以自己根据数据集的特点来安排不同的 default boxes 参数组合 了解原理以后, 就来看一下怎么实现, 输出 Default Box 的代码定义在 layers/functions/prior_box.py 文件中. 代码如下所示: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# `layers/functions/prior_box.py`class PriorBox(object): # 所谓priorbox实际上就是网格中每一个cell推荐的box def __init__(self, cfg): # 在SSD的init中, cfg=(coco, voc)[num_classes=21] # coco, voc的相关配置都来自于data/cfg.py 文件 super(PriorBox, self).__init__() self.image_size = cfg["min_dim"] self.num_priors = len(cfg["aspect_ratios"]) self.variance = cfg["variance"] or [0.1] self.min_sizes = cfg["min_sizes"] self.max_sizes = cfg["max_sizes"] self.steps = cfg["steps"] self.aspect_ratios = cfg["aspect_ratios"] self.clip = cfg["clip"] self.version = cfg["name"] for v in self.variance: if v &lt;= 0: raise ValueError("Variances must be greater than 0") def forward(self): mean = [] for k, f in enumerate(self.feature_maps): # 存放的是feature map的尺寸:38,19,10,5,3,1 # from itertools import product as product for i, j in product(range(f), repeat=2): # 这里实际上可以用最普通的for循环嵌套来代替, 主要目的是产生anchor的坐标(i,j) f_k = self.image_size / self.steps[k] # steps=[8,16,32,64,100,300]. f_k大约为feature map的尺寸 # 求得center的坐标, 浮点类型. 实际上, 这里也可以直接使用整数类型的 `f`, 计算上没太大差别 cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # 这里一定要特别注意 i,j 和cx, cy的对应关系, 因为cy对应的是行, 所以应该零cy与i对应. # aspect_ratios 为1时对应的box s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # 根据原文, 当 aspect_ratios 为1时, 会有一个额外的 box, 如下: # rel size: sqrt(s_k * s_(k+1)) s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # 其余(2, 或 2,3)的宽高比(aspect ratio) for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # 综上, 每个卷积特征图谱上每个像素点最终产生的 box 数量要么为4, 要么为6, 根据不同情况可自行修改. output = torch.Tensor(mean).view(-1,4) if self.clip: output.clamp_(max=1, min=0) # clamp_ 是clamp的原地执行版本 return output # 输出default box坐标(可以理解为anchor box) 最终, 输出的ouput就是一张图片中所有的default box的坐标, 对于论文中的默认设置来说产生的box数量为: 38^2 \times 4+19^2 \times 6+ 10^2 \times 6+5^2 \times 6+3^2 \times 4+1^2 \times 4 = 8732 解析预测结果在模型中, 我们为了加快训练速度, 促使模型收敛, 因此会将相应的 box 的坐标转换成与图片size成比例的小数形式, 因此, 无法直接将模型产生的预测结果可视化. 下面, 我们首先会通过接受 Detect 类来说明如何解析预测结果, 同时, 还会根据源码中提过的 demo 文件来接受如何将对应的结果可视化出来, 首先, 来看一下 Detect 类的定义和实现: 123456789101112131415161718192021222324252627282930313233343536373839404142434445# ./layers/class Detect(Function): # 测试阶段的最后一层, 负责解码预测结果, 应用nms选出合适的框和对应类别的置信度. def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh): self.num_classes = num_classes self.background_label = bkg_label self.top_k = top_k self.conf_thresh = conf_thresh self.nms_thresh = nms_thresh self.variance = voc_config["variance"] def forward(self, loc_data, conf_data, prior_data): # loc_data: [batch, num_priors, 4], [batch, 8732, 4] # conf_data: [batch, num_priors, 21], [batch, 8732, 21] # prior_data: [num_priors, 4], [8732, 4] num = loc_data.size(0) # batch_size num_priors = prior_data.size(0) output = torch.zeros(num, self.num_classes, self.top_k, 5) # output:[b, 21, k, 5] conf_preds = conf_data.view(num, num_priors, self.num_classes).transpose(2,1) # 维度调换 # 将预测结果解码 for i in range(num): # 对每一个image进行解码 decoded_boxes = decode(loc_data[i], prior_data, self.variance)#获取第i个图片的box坐标 conf_scores = conf_preds[i].clone() # 复制第i个image置信度预测结果 for cl in range(1, self.num_classes): # num_classes=21, 所以 cl 的值为 1~20 c_mask = conf_scores[cl].gt(self.conf_thresh) # 返回由0,1组成的数组, 0代表小于thresh, 1代表大于thresh scores = conf_scores[cl][c_mask] # 返回值为1的对应下标的元素值(即返回conf_scores中大于thresh的元素集合) if scores.size(0) == 0: continue # 没有置信度, 说明没有框 l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes) # 获取对应box的二值矩阵 boxes = decoded_boxes[l_mask].view(-1,4) # 获取置信度大于thresh的box的左上角和右下角坐标 # 返回每个类别的最高的score 的下标, 并且除去那些与该box有较大交并比的box ids, count = nms(boxes, scores, self.nms_thresh, self.top_k) # 从这些box里面选出top_k个, count&lt;=top_k # count&lt;=top_k output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[:count]), 1) flt = output.contiguous().view(num,-1,5) _, idx = flt[:, :, 0].sort(1, descending=True) _, rank = idx.sort(1) flt[(rank &lt; self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0) # 注意, view共享tensor, 因此, 对flt的修改也会反应到output上面 return output 在这里, 用到了两个关键的函数 decode() 和 nms(), 这两个函数定义在./layers/box_utils.py文件中, 代码如下所示:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def decode(loc, priors, variances): """Decode locations from predictions using priors to undo the encoding we did for offset regression at train time. Args: loc (tensor): location predictions for loc layers, Shape: [num_priors,4] priors (tensor): Prior boxes in center-offset form. Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: decoded bounding box predictions """ boxes = torch.cat(( priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:], priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1) boxes[:, :2] -= boxes[:, 2:] / 2 boxes[:, 2:] += boxes[:, :2] return boxesdef nms(boxes, scores, overlap=0.5, top_k=200): """Apply non-maximum suppression at test time to avoid detecting too many overlapping bounding boxes for a given object. Args: boxes: (tensor) The location preds for the img, Shape: [num_priors,4]. scores: (tensor) The class predscores for the img, Shape:[num_priors]. overlap: (float) The overlap thresh for suppressing unnecessary boxes. top_k: (int) The Maximum number of box preds to consider. Return: The indices of the kept boxes with respect to num_priors. """ keep = scores.new(scores.size(0)).zero_().long() if boxes.numel() == 0: return keep x1 = boxes[:, 0] y1 = boxes[:, 1] x2 = boxes[:, 2] y2 = boxes[:, 3] area = torch.mul(x2 - x1, y2 - y1) v, idx = scores.sort(0) # sort in ascending order # I = I[v &gt;= 0.01] idx = idx[-top_k:] # indices of the top-k largest vals xx1 = boxes.new() yy1 = boxes.new() xx2 = boxes.new() yy2 = boxes.new() w = boxes.new() h = boxes.new() # keep = torch.Tensor() count = 0 while idx.numel() &gt; 0: i = idx[-1] # index of current largest val # keep.append(i) keep[count] = i count += 1 if idx.size(0) == 1: break idx = idx[:-1] # remove kept element from view # load bboxes of next highest vals torch.index_select(x1, 0, idx, out=xx1) torch.index_select(y1, 0, idx, out=yy1) torch.index_select(x2, 0, idx, out=xx2) torch.index_select(y2, 0, idx, out=yy2) # store element-wise max with next highest score xx1 = torch.clamp(xx1, min=x1[i]) yy1 = torch.clamp(yy1, min=y1[i]) xx2 = torch.clamp(xx2, max=x2[i]) yy2 = torch.clamp(yy2, max=y2[i]) w.resize_as_(xx2) h.resize_as_(yy2) w = xx2 - xx1 h = yy2 - yy1 # check sizes of xx1 and xx2.. after each iteration w = torch.clamp(w, min=0.0) h = torch.clamp(h, min=0.0) inter = w*h # IoU = i / (area(a) + area(b) - i) rem_areas = torch.index_select(area, 0, idx) # load remaining areas) union = (rem_areas - inter) + area[i] IoU = inter/union # store result in iou # keep only elements with an IoU &lt;= overlap idx = idx[IoU.le(overlap)] return keep, count MultiBox 损失函数在layers/modules/multibox_loss.py 中定义了SSD模型的损失函数, 在SSD论文中, 损失函数具体定义如下: L_{loc}(x,l,g) = \sum_{i\in Pos}^N \sum_{m\in\{cx,cy,w,h\}} x_{ij}^k smooth_{L_1}(l_i^m - \hat g_j^m)L_{conf}(x,c) = -\sum_{i\in Pos}^N x_{ij}^p log(\hat c_i^p) - \sum_{i\in Neg} log(\hat c_i^0), 其中, \hat c_i^p = \frac{exp(c_i^p)}{\sum_p exp(c_i^p)}损失函数定义根据上面的公式, 我们可以定义下面的损失函数类, 该类继承了 nn.Module, 因此可以当做是一个 Module 用在训练函数中.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# layers/modules/multibox_loss.pyclass MultiBoxLoss(nn.Module): # 计算目标: # 输出那些与真实框的iou大于一定阈值的框的下标. # 根据与真实框的偏移量输出localization目标 # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3) # 目标损失: # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N # 参数: # c: 类别置信度(class confidences) # l: 预测的框(predicted boxes) # g: 真实框(ground truth boxes) # N: 匹配到的框的数量(number of matched default boxes) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes= num_classes # 列表数 self.threshold = overlap_thresh # 交并比阈值, 0.5 self.background_label = bkg_label # 背景标签, 0 self.use_prior_for_matching = prior_for_matching # True 没卵用 self.do_neg_mining = neg_mining # True, 没卵用 self.negpos_ratio = neg_pos # 负样本和正样本的比例, 3:1 self.neg_overlap = neg_overlap # 0.5 判定负样本的阈值. self.encode_target = encode_target # False 没卵用 self.variance = cfg["variance"] def forward(self, predictions, targets): loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度 num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, 因此 priors 维持不变 num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (默认为voc数据集) # 将priors(default boxes)和ground truth boxes匹配 loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor, # 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了 defaults = priors.data # [8732, 4] # from ..box_utils import match # 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解 match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx) # 注意! 要清楚 Python 中的参数传递机制, 此处在函数内部会改变 loc_t, conf_t 的值, 关于 match 的详细讲解可以看后面的代码解析 if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了 loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t &gt; 0 # 筛选出 &gt;0 的box下标(大部分都是=0的) num_pos = pos.sum(dim=1, keepdim=True) # 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold] # 位置(localization)损失函数, 使用 Smooth L1 函数求损失 # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值 pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4)# 获取预测结果值 loc_t = loc_t[pos_idx].view(-1, 4) # 获取gt值 loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # 计算损失 # 计算最大的置信度, 以进行难负样本挖掘 # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # reshape # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失 loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1)) # 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新 loss_c[pos.view(-1, 1)] = 0 # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标) # 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # 进行降序排序, 并获取到排序的下标 _, loss_idx = loss_c.sort(1, descending=True) # 将下标进行升序排序, 并获取到下标的下标 _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], 统计每个样本中的obj个数 num_pos = pos.long().sum(1, keepdim=True) # 根据obj的个数, 确定负样本的个数(正样本的3倍) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # 获取到负样本的下标 neg = idx_rank &lt; num_neg.expand_as(idx_rank) # 计算包括正样本和负样本的置信度损失 # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据 conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # 按照pos_idx和neg_idx筛选目标数据 targets_weighted = conf_t[(pos+neg).gt(0)] # 计算二者的交叉熵 loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # 将损失函数归一化后返回 N = num_pos.data.sum() loss_l = loss_l / N loss_c = loss_c / N return loss_l, loss_c GT box 与default box 的匹配在上面的代码中, 有一个很重要的函数, 即 match() 函数, 因为我们知道, 当根据特征图谱求出这些 prior box(default box, 8732个)以后, 我们仅仅知道这些 box 的 scale 和 aspect_ratios 信息, 但是如果要计算损失函数, 我们就必须知道与每个 prior box 相对应的 ground truth box 是哪一个, 因此, 我们需要根据交并比来求得这些 box 之间的匹配关系. 匹配算法的核心思想如下: 首先将找到与每个 gtbox 交并比最高的 defaultbox, 记录其下标 然后找到与每个 defaultbox 交并比最高的 gtbox. 注意, 这两步不是一个相互的过程, 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有的priorbox都与G匹配. 为了防止上面的情况, 我们将那些对于gtbox来说, 交并比最高的priorbox, 强制进行互相匹配, 即令 best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环. 根据下标获取每个priorbox对应的gtbox的坐标, 然后对坐标进行相应编码, 并存储起来, 同时将gt类别也存储起来, 到此, 匹配完成. 根据上面的求解思想, 我们可以实现相应的匹配代码, 主要用到了以下几个函数: point_form(boxes): 将 boxes 的坐标信息转换成左上角和右下角的形式 intersect(box_a, box_b): 返回 box_a 与 box_b 集合中元素的交集 jaccard(box_a, box_b): 返回 box_a 与 box_b 集合中元素的交并比 encode(matched, priors, variances): 将 box 信息编码成小数形式, 方便网络训练 match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): 匹配算法, 通过调用上述函数实现匹配功能 完整代码及解析如下所示(位于 ./layers/box_utils.py 文件中):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# ./layers/box_utils.pydef point_form(boxes): # 将(cx, cy, w, h) 形式的box坐标转换成 (xmin, ymin, xmax, ymax) 形式 return torch.cat( (boxes[:2] - boxes[2:]/2), # xmin, ymin (boxes[:2] + boxes[2:]/2), 1) # xmax, ymaxdef intersect(box_a, box_b): # box_a: (truths), (tensor:[num_obj, 4]) # box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4]) # return: (tensor:[num_obj, num_priors]) box_a 与 box_b 两个集合中任意两个 box 的交集, 其中res[i][j]代表box_a中第i个box与box_b中第j个box的交集.(非对称矩阵) # 思路: 先将两个box的维度扩展至相同维度: [num_obj, num_priors, 4], 然后计算面积的交集 # 两个box的交集可以看成是一个新的box, 该box的左上角坐标是box_a和box_b左上角坐标的较大值, 右下角坐标是box_a和box_b的右下角坐标的较小值 A = box_a.size(0) B = box_b.size(0) # box_a 左上角/右下角坐标 expand以后, 维度会变成(A,B,2), 其中, 具体可看 expand 的相关原理. box_b也是同理, 这样做是为了得到a中某个box与b中某个box的左上角(min_xy)的较大者(max) # unsqueeze 为增加维度的数量, expand 为扩展维度的大小 min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A,B,2), box_b[:, :2].unsqueeze(0).expand(A,B,2)) # 在box_a的 A 和 2 之间增加一个维度, 并将维度扩展到 B. box_b 同理 # 求右下角(max_xy)的较小者(min) max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A,B,2), box_b[:, 2:].unsqueeze(0).expand(A,B,2)) inter = torch.clamp((max_xy, min_xy), min=0) # 右下角减去左上角, 如果为负值, 说明没有交集, 置为0 return inter[:, :, 0] * inter[:, :, 0] # 高×宽, 返回交集的面积, shape 刚好为 [A, B]def jaccard(box_a, box_b): # A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B) # box_a: (truths), (tensor:[num_obj, 4]) # box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4]) # return: (tensor:[num_obj, num_priors]), 代表了 box_a 和 box_b 两个集合中任意两个 box之间的交并比 inter = intersect(box_a, box_b) # 求任意两个box的交集面积, shape为[A, B], 即[num_obj, num_priors] area_a = ((box_a[:,2]-box_a[:,0]) * (box_a[:,3]-box_a[:,1])).unsqueeze(1).expand_as(inter) # [A,B] area_b = ((box_b[:,2]-box_b[:,0]) * (box_b[:,3]-box_b[:,1])).unsqueeze(0).expand_as(inter) # [A,B], 这里会将A中的元素复制B次 union = area_a + area_b - inter return inter / union # [A, B], 返回任意两个box之间的交并比, res[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.def encode(matched, priors, variances): # 对边框坐标进行编码, 需要宽度方差和高度方差两个参数, 具体公式可以参见原文公式(2) # matched: [num_priors,4] 存储的是与priorbox匹配的gtbox的坐标. 形式为(xmin, ymin, xmax, ymax) # priors: [num_priors, 4] 存储的是priorbox的坐标. 形式为(cx, cy, w, h) # return : encoded boxes: [num_priors, 4] g_cxy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2] # 用互相匹配的gtbox的中心坐标减去priorbox的中心坐标, 获得中心坐标的偏移量 g_cxy /= (variances[0]*priors[:, 2:]) # 令中心坐标分别除以 d_i^w 和 d_i^h, 正如原文公式所示 #variances[0]为0.1, 令其分别乘以w和h, 得到d_i^w 和 d_i^h g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:] # 令互相匹配的gtbox的宽高除以priorbox的宽高. g_wh = torch.log(g_wh) / variances[1] # 这里这个variances[1]=0.2 不太懂是为什么. return torch.cat([g_cxy, g_wh], 1) # 将编码后的中心坐标和宽高``连接起来, 返回 [num_priors, 4]def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) 确定是否匹配的交并比阈值 # truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标 # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理) # labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号 # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号 overlaps = jaccard(truths, point_form(priors)) # [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比. # 二部图匹配(Bipartite Matching) # [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置 best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, 因此shape为[num_objs,1] # [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_prior_idx.squeeze_(1) # 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度. best_prior_overlap.squeeze_(1) best_truth_idx.squeeze_(0) best_truth_overlap.squeeze_(0) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs], # 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox. # 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比 # 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有 # 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox, # 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环 # 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配. for j in range(best_prior_idx.size(0)): # range:0~num_obj-1 best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox # 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配. # 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值. # 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高, # 即 best_truth_idx[i]= k # 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比, # 即best_prior_idx[k]=l # 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大, # 即但是对于best_prior_idx[j] = i. # 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j. # 即令 priorbox[i] 与 gtbox[j]对应. # 这样做的原因: 防止某个gtbox没有匹配的 prior box. mathes = truths[best_truth_idx] # truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732, # 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标 # 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值. conf = labels[best_truth_idx]+1 # 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732] conf[best_truth_overlap &lt; threshold] = 0 # 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框 loc = encode(matches, priors, variances) # 返回编码后的中心坐标和宽高. loc_t[idx] = loc # 设置第idx张图片的gt编码坐标信息 conf_t[idx] = conf # 设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景) Augmentations Trick众多周知, SSD 模型虽然比较简单, 但是也因此在精度上不够优秀, 故而需要借助较多的 Augmentation Trick 来提升模型的 mAP, 这部分代码位于 utils/augmentations.py 文件中, 由于这部分代码比较琐碎, 并且与 SSD 网络的关系并不大, 所以这里我们只给出一个整体概览, 不做过多注释, 有兴趣的朋友可以自己查看源码. 代码文件内容概览如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def intersect(box_a, box_b): # ...def jaccard_numpy(box_a, box_b): # ...class Compose(object): # ...class Lambda(object): # ...class ConvertFromInts(object): # ...class SubtractMeans(object): # ...class ToAbsoluteCoords(object): # ...class ToPercentCoords(object): # ...class Resize(object): # ...class RandomSaturation(object): # ...class RandomHue(object): # ...class RandomLightingNoise(object): def __init__(self): # ... def __call__(self, image, boxes=None, labels=None): # ...class ConvertColor(object): def __init__(self, current="BGR", transform="HSV"): # ... def __call__(self, image, boxes=None, labels=None): # ...class RandomContrast(object): def __init__(self, lower=0.5, upper=1.5): # ... def __call__(self, image, boxes=None, labels=None): # ...class RandomBrightness(object): def __init__(self, delta=32): # ... def __call__(self, image, boxes=None, labels=None): # ...class ToCV2Image(object): # ...class ToTensor(object): # ...class RandomSampleCrop(object): def __init__(self): # ... def __call__(self, image, boxes=None, labels=None): # ...class Expand(object): def __init__(self, mean): # ... def __call__(self, image, boxes, labels): # ...class RandomMirror(object): def __call__(self, image, boxes, classes): # ...class SwapChannels(object): # ...class PhotometricDistort(object): # ...class SSDAugmentation(object): # ... 模型训练在定义了模型结构和相应的随时函数以后, 接下来就是训练阶段, 训练代码位于train.py文件中, 下面对该文件代码进行解读: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# train.pydef str2bool(v): return v.lower() in ("yes", "true", "t", 1)import argparseparser = argparse.ArgumentParser(description="Single Shot MultiBox Detection")#...parser.add_argument("--cuda", default=True, type=str2bool, help="Use CUDA to train model")#...args = parser.parse_args()if torch.cuda.is_available(): if args.cuda: torch.set_default_tensor_type("torch.cuda.FloatTensor") else: torch.set_default_tensor_type("torch.FloatTensor")else: torch.set_default_tensor_type("torch.FloatTensor")def train():# 该文件中中主要的函数, 在main()中, 仅调用了该函数 if args.dataset == "COCO": if args.dataset_root == VOC_ROOT: # ... cfg = coco # coco位于config.py文件中 # COCODetection类 位于coco.py文件中 # SSDAugmentation类 位于utils/augmentations.py文件中 dataset = COCODetection(root=args.dataset_root, transform=SSDAugmentation(cfg["min_dim"], MEANS)) elif args.dataset == "VOC": if args.dataset_root == COCO_ROOT: #... cfg = voc dataset = VOCDetection(root=args.dataset_root, transform=SSDAugmentation(cfg["min_dim"], MEANS)) if args.visdom: import visdom viz = visdom.Visdom() # from ssd import build_ssd ssd_net = build_ssd("train", cfg["min_dim"], cfg["num_classes"]) net = ssd_net if args.cuda: net = torch.nn.DataParallel(ssd_net) # import torch.backends.cudnn as cudnn cudnn.benchmark = True # 大部分情况下, 这个flag可以让内置的cuDNN的auto-tuner自动寻找最适合当前配置的算法. if args.resume: # resume 类型为 str, 值为checkpoint state_dict file ssd_net.load_weights(args.resume) else: vgg_weights = torch.load(args.save_folder + args.basenet) ssd_net.load_state_dict(vgg_weights) if args.cuda: net = net.cuda() # 将所有的参数都移送到GPU内存中 if not args.resume: ssd_net.extras.apply(weights_init) # 本文件的函数: def weights_init(), 对网络参数执行Xavier初始化. ssd_net.loc.apply(weights_init) ssd_net.conf.apply(weights_init) # import torch.optim as optim optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay) # MultiBoxLoss类 位于layers/modules/multibox_loss.py文件中 criterion = MultiBoxLoss(cfg["num_classes"], 0.5, True, 0, True, 3, 0.5, False, args.cuda) net.train() # loss计数器 loc_loss = 0 conf_loss = 0 epoch = 0 epoch_size = len(dataset) // args.batch_size step_index = 0 if args.visdom: #... # import torch.utils.data as data data_loader = data.DataLoader(dataset, args.batch_size, num_workers=args.num_workers, shuffle=True, collate_fn=detection_collate, pin_memory=True) # 创建batch迭代器 batch_iterator = iter(data_loader) for iteration in range(args.start_iter, cfg["max_iter"]): if args.visdom and iteration != 0 and (iteration % epoch_size==0): update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None, "append", epoch_size) loc_loss = 0 conf_loss = 0 epoch += 1 if iteration in cfg["lr_steps"]: step_index += 1 # 每经过固定迭代次数, 就将lr衰减1/10 adjust_learning_rate(optimizer, args.gamma, step_index) # load train data images, targets = next(batch_iterator) if args.cuda: images = Variable(images.cuda()) targets = [Variable(ann.cuda(), volatile=True) for ann in targets] else: images = Variable(images) targets = [Variable(ann, valotile=True) for ann in targets] # forward t0 = time.time() out = net(images) # backprop optimizer.zero_grad() loss_l, loss_c = criterion(out, targets) # criterion = MultiBoxLoss(...) loss = loss_l + loss_c loss.backward() optimizer.step() t1 = time.time() loc_loss += loss_l.data[0] conf_loss += loss_c.data[0] if iteratioin % 10 == 0: # print(...) 每隔10次迭代就输出一次训练状态信息 if args.visdom: # update_vis_plot(...) if iteration != 0 and iteration % 5000 ==0: # save model 模型验证下面是模型验证的相关代码, 存在于./test.py文件中, 代码没有太多特殊的处理, 和./train.py文件略有相似. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def test_net(save_folder, net, cuda, testset, transform, thresh): filename = save_folder+"test1.txt" num_images = len(testset) for i in range(num_images): print("Testing image &#123;:d&#125;/&#123;:d&#125;...".format(i+1, num_images)) img = testset.pull_image(i) img_id, annotation = testset.pull_anno(i) x = torch.from_numpy(transform(img)[0]).permute(2,0,1) x = Variable(x.unsqueeze(0)) with open(filename, mode='a') as f: f.write('\n GROUND TRUTH FOR: ' + img_id + '\n') for box in annotation: f.write("label"+" || ".join(str(b) for b in box) + "\n") if cuda: x = x.cuda() y = net(x) detections = y.data # 将检测结果返回到图片上 scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]]) pred_num = 0 for i in range(detections.size(1)): j = 0 while detections[0, i, j, 0] &gt;= 0.6: if pred_num == 0: with open(filename, mode='a') as f: f.write('PREDICTIONS' + '\n') score = detections[0, i, j, 0] label_name = labelmap[i-1] pt = (detections[0, i, j, 1:]*scale).cpu().numpy() coords = (pt[0], pt[1], pt[2], pt[3]) pred_num += 1 with open(filename, mode='a') as f: f.write(str(pred_num)+' label:' + label_name + ' score' + str(socre) + ' '+ ' || '.join(str(c) for c in coords) + '\n') j += 1def test_voc(): # 加载网络 num_classes = len(VOC_CLASSES) + 1 # 1 为背景 net = build_ssd("test", 300, num_classes) net.load_state_dict(torch.load(args.trained_model)) net.eval() # 将网络只与eval状态, 主要会影响 dropout 和 BN 等网络层 print("Finished loading model!") # 加载数据 testset = VOCDetection(args.voc_root, [("2007", "test")], None, VOCAnnotationTransform()) if args.cuda: net = net.cuda() cudnn.benchmark = True # evaluation test_net(args.save_folder, net, args.cuda, testset, BaseTransform(net.size, (104, 117, 123)), thresh=args.visual_threshold)if __name__ == '__main__': test_voc() 其他辅助代码学习率衰减12345def adjust_learning_rate(optimizer, gamma, step): lr = args.lr * (gamma ** (step)) ## **为幂乘 for param_group in optimizer.param_groups: param_group["lr"] = lr Xavier 初始化123456789# tran.pydef xavier(param): init.xavier_uniform(param) # import torch.nn.init as initdef weights_init(m): if isinstance(m, nn.Conv2d): # 只对卷积层初始化 xavier(m.weight.data) m.bias.data.zero_()]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>源码解析</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSSD-Deconvolutional Single Shot Detector]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-DSSD-Arxiv2017%2F</url>
    <content type="text"><![CDATA[核心亮点(1) 利用反卷积模块向特征图谱中添加更多的上下文信息主要是对SSD的一点改进, SSD使用了不同阶段的卷积特征图谱进行目标检测, 而DSSD受到人体姿态识别任务的启发, 将这些不同阶段的卷积特征图谱通过反卷积模块连接起来, 然后再进行目标检测的预测任务. (2), 预测模块采用Residual模块这个不算是亮点, 不过也是改动之一, 基本来说就说原始的SSD是直接在特征图谱预测结果并计算损失的, 而DSSD在预测之前会先经过一个Residual模块做进一步的特征提取, 然后在进行预测. 论文细节摘要这篇文章最主要的贡献在于提出了一个可以将额外的上下文信息引入到现有大多数目标检测模型的方法. 为了达到这个目标, 本文首先将Resnet101 分类网络和SSD模型结合起来, 然后对SSD+Resnet101的模型进行了扩展, 添加了反卷积层, 以此引入了针对更大范围尺度的目标物上下文特征信息, 进而提高了准确率(特别是在小物体上面). 这种方法在进行高度阐述时, 很容易就能讲明白, 但是在真正实现时, 却很难成功. 因此本文精心的添加了一些具有已经学习好的特征信息的阶段, 特别是在反卷积时的前向计算的连接上, 最终使得上面的方法得以起效. DSSD 模型base network如图1上半部分所示, SSD模型是用一个经典网络做为基础网络, 然后后接几层额外的特征层构建的. 原始的SSD采用的是VGGnet, 但是大量的工作都是用ResNet获得了更好的效果, 因此, 本文也选用ResNet-101网络作为backbone. 并且将额外的卷积层(也换成Residual模块)接在 conv5_x block后面, 同时会分别从 conv3_x, conv5_x两个卷积段预测score和offsets. 个人觉得奇怪的一点是, 为什么单单把VGG换成ResNet并没有提高mAP?(VOC数据集, 前者77.5, 后者76.4) 而是在使用了其他辅助模块后才提高的 prediction module MS-CNN指出, 提升每个任务的子网络有助于提高准确率, 根据这个原则, 我们在每一个预测层都添加了一个residual block, 如图2(c)所示. 同时也对其他形式的predictin module进行了尝试(图2,a,b,c). Deconvolutional SSD为了增加更多高级别的上下文信息, 文章将prediction module 移动到了一系列的反卷积层之后, 并与原始SSD的额外卷积层形成了一种非对称的沙漏结构(hourglass network, 灵感来自于一篇人体姿态检测论文), 如图1所示. 每一个卷积层的特征图谱会和之前的层一起经过一个反卷积模块(该模块细节在后面介绍), 这就相当于在特征图谱中加入了更多的上下文信息. 这里的反卷积只有很浅的几层, 作者这样设计的原因一是不想增加过多的计算时间, 二是由于迁移学习方法可以帮助更好更快的模型收敛, 同时可以获得更高的精度, 因此无需设计过深的网络. 反卷积层很重要的一个点在于计算成本的增加, 除了反卷积操作本身的计算, 还体现在从之前层中添加信息时 Deconvolution Module为了帮助整合反卷积层和之前层的特征信息, 文章引入了一种反卷积模块, 如图3所示. 图3中左下角是从原始SSD中得到的特征图谱, 左上角是论文Learning to refine object segment中提出的反卷积层. 在当前模块中的每一个卷积层, 都使用了BN层, 在放大特征图谱时, 我们使用了学习到的反卷积层, 而不是双线性插值法. 在测试了多种conbination方法后(element-wise sum, element-wise product), 根据实验结果决定采用对应为相乘的结合方式(VOC数据集, 前者78.4, 后者78.6, 提升了0.2的mAP). Training除了一些参数设置的不同外, 训练策略基本遵循SSD. 实验 从COCO数据集的结果来看, DSSD在小物体方面并没有提升, 而在大物体方面获得了很大的提升, 推测原因主要是因为ResNet-101在大物体的特征提取能力上, 要远强于VGGNet.另外, 可以看出, 由于采用了更深的ResNet网络, 同时增加了反卷积过程, 使得FPS降低不少.(即使在测试阶段利用计算公式移除了BN层, 这个trick可以提升1.2~1.5倍的检测速度).]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11 新特性]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-Cpp11%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[智能指针初始化列表decltype 关键字]]></content>
      <categories>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cpp-整型上下限INT_MAX和INT_MIN及其运算]]></title>
    <url>%2Fz_post%2FCpp-%E6%95%B4%E5%9E%8B%E4%B8%8A%E4%B8%8B%E9%99%90INT-MAX%E5%92%8CINT-MIN%E5%8F%8A%E5%85%B6%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[INT_MAX,INT_MIN数值大小因为int占4字节32位，根据二进制编码的规则，INT_MAX = 2^31-1，INT_MIN= -2^31.C/C++中，所有超过该限值的数，都会出现溢出，出现warning，但是并不会出现error。如果想表示的整数超过了该限值，可以使用长整型long long 占8字节64位。 关于INT_MAX INT_MIN的运算由于二进制编码按原码、补码和反码的规则进行运算，所有程序中对INT_MAX和INT_MIN的运算应当格外注意，在出现溢出的时候，不遵循数学规则。 INT_MAX + 1 = INT_MIN INT_MIN - 1 = INT_MAX abs(INT_MIN) = -INT_MIN = INT_MIN 另外要注意的是: INT_MAX + 1 &lt; INT_MAX， INT_MIN - 1 &gt; INT_MIN, abs(INT_MIN) &lt; 0. 虽然abs(INT_MIN) = -INT_MIN = INT_MIN, 但是可以利用unsigned int来获取最小负值的绝对值:1unsigned int un = abs(INT_MIN) // un = 2147483648]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[glob模块-文件路径查找]]></title>
    <url>%2Fz_post%2FPython-glob%2F</url>
    <content type="text"><![CDATA[glob模块是Python最简单的模块之一, 内容非常少, 用它可以查找符合特定规则的文件路径名, 查找文件时只会用到三个匹配符: : 匹配0个或多个字符 ? : 匹配单个字符 [] : 匹配指定范围内的字符, 如[0-9]匹配数字 glob.glob返回所有匹配的文件路径列表, 它只有一个参数pathname, 定义了文件路径匹配的规则, 这里可以是绝对路径或者相对路径:123456import globpathes_list = glob.glob("~/Pictures/*.jpg")# 获取Pictures下的所有图片relative_pathes_list = glob.glob("../*.py")# 获取上级目录中的所有.py文件 glob.iglob获取一个可遍历的对象, 使用它可以逐个获取匹配的文件路径名. 与glob.glob()的区别是: glob.glob()会同时获取到所有的匹配路径, 而glob.iglob()一次只获取一个匹配路径.1234f = glob.iglob("../*.py")print f # &lt;generator object iglob at 0x00B9FF80&gt;for py in f: print(py)]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tarfile和zipfile模块-压缩解压tar归档文件]]></title>
    <url>%2Fz_post%2FPython-tarfile-zipfile%2F</url>
    <content type="text"><![CDATA[tarfiletarfile是Python自带的用于方便读取tar归档文件的模块. zipfile与tarfile对应的是zipfile模块, 用于处理zip压缩.]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[requests模块-HTTP客户端库]]></title>
    <url>%2Fz_post%2FPython-requests%2F</url>
    <content type="text"><![CDATA[requests 是一个很实用的Python HTTP客户端库, 在编写爬虫和测试服务器响应数据时会经常用到, 可以说, requests 完全满足如今网络的需求. get请求1r = requests.get("http://httpbin.org/get")]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CIFAR-10 (Caffe2)]]></title>
    <url>%2Fz_post%2FCaffe2-CIFAR-10%2F</url>
    <content type="text"><![CDATA[本示例主要包含以下几个主要部分: 下载Cifar10数据集 将Images写入Imdbs 定义并且训练model 保存训练好的model 加载训练好的model 在testing imdb上面执行inference 继续训练以提高test accuracy 测试retrained model 导入必要的包123456789101112131415161718192021222324252627from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom __future__ import unicode_literals%matplotlib inlinefrom matplotlib import pyplot as pltimport numpy as npimport osimport lmdbimport shutilfrom imageio import imreadimport caffe2.python.predictor.predictor_exporter as pefrom caffe2.proto import caffe2_pb2from caffe2.python.predictor import mobile_exporterfrom caffe2.python import( brew, core, model_helper, net_drawer, optimizer, visualize, workspace)# 设置全局初始化信息 , 如果需要看到更加详细的初始化信息, 需要将caffe2_log_level参数设置为 1core.GlobalInit(['caffe2', '--caffe2_log_level=0']) 下载并解压数据集1234567891011121314151617181920212223242526272829303132import requestsimport tarfile# 设置下载路径相关变量# data_folder为data下载并解压的地方data_folder = os.path.join(os.path.expanduser('~'), 'Works', 'test', 'caffe2', 'tutorial_data', 'cifar10')# root_folder为checkpoints文件和.pb模型定义文件存放的地方root_folder = os.path.join(os.path.expanduser('~'), 'Works', 'test', 'caffe2', 'tutorial_files', 'tutorial_cifar10')url = "http://pjreddie.com/media/files/cifar.tgz" # url to datafilename = url.split("/")[-1] # download file namedownload_path = os.path.join(data_folder, filename) # path to extract data toif not os.path.isdir(data_folder): os.makedirs(data_folder)# 如果data不存在, 就下载并解压之if not os.path.exists(download_path.strip('.tgz')): # Download data r = requests.get(url, stream=True) print("Downloading... &#123;&#125; to &#123;&#125;".format(url, download_path)) open(download_path, 'wb').write(r.content) print("Finished downloading...") # Unpack images from tgz file print('Extracting images from tarball...') tar = tarfile.open(download_path, 'r') for item in tar: tar.extract(item, data_folder) print("Completed download and extraction!")else: print("Image directory already exists. Moving on...") 接下来看一看数据集中的一些示例:12345678910111213import globfrom skimage import io# Grab 5 image paths from training set to displaysample_imgs = glob.glob(os.path.join(data_folder, "cifar", "train") + '/*.png')[:5]# Plot imagesf, ax = plt.subplots(1, 5, figsize=(10,10))plt.tight_layout()for i in range(5): ax[i].set_title(sample_imgs[i].split("_")[-1].split(".")[0]) ax[i].axis('off') ax[i].imshow(io.imread(sample_imgs[i]).astype(np.uint8)) 创建label文件, 写入LMDBs现在我们已经拥有了数据, 接下来需要写入LMDBs用于训练, 验证和测试. 为了根据每个类别来分离图片, 下面将会使用到一个经常在caffe框架中使用的技术, 创建label files. 具体来说, label files就是将每个.png图片对应到它的类别上面去:/path/to/im1.png 7/path/to/im2.png 3/path/to/im3.png 5/path/to/im4.png 0… 根据不同的数据集, 创建labels的方式有些许区别 1234567891011121314151617181920212223242526272829303132333435# Paths to train and test directoriestraining_dir_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'cifar', 'train')testing_dir_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'cifar', 'test')# Paths to label filestraining_labels_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'training_dictionary.txt')validation_labels_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'validation_dictionary.txt')testing_labels_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'testing_dictionary.txt')# Paths to LMDBstraining_lmdb_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'training_lmdb')validation_lmdb_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'validation_lmdb')testing_lmdb_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'testing_lmdb')# Path to labels.txtlabels_path = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks', 'tutorial_data', 'cifar10', 'cifar', 'labels.txt')# Open label file handlerlabels_handler = open(labels_path, "r")# Create classes dictionary to map string labels to integer labelsclasses = &#123;&#125;i = 0lines = labels_handler.readlines()for line in sorted(lines): line = line.rstrip() classes[line] = i i += 1labels_handler.close()print("classes:", classes)#输出如下:# classes: &#123;'automobile': 1, 'airplane': 0, 'truck': 9, 'ship': 8, 'dog': 5, 'horse': 7, 'cat': 3, 'frog': 6, 'deer': 4, 'bird': 2&#125;]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-辅助函数]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-net%E8%BE%85%E5%8A%A9%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[utils/net.py这个文件经常会被使用到, 它主要的作用提供一些可以使 Caffe2 networks 更方便使用的辅助函数, 该文件解读如下(该文件中包含多个辅助函数, 按照函数在文件中的顺序从上到下逐个解读)]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>Caffe2</tag>
        <tag>Detectron</tag>
        <tag>源码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[time-日期与时间]]></title>
    <url>%2Fz_post%2FPython-time%2F</url>
    <content type="text"><![CDATA[Python中的日期和时间Python程序能用很多方式处理日期和时间, 转换日期格式是一个常见的功能. Python提供了 time 和 calendar模块可以用来格式化日期和时间. 时间间隔是以秒为单位的浮点小数. 每个时间戳都以自1970年1月1日午夜(历元)经过了多长时间来表示. time.time()用于获取当前的时间戳, 单位为秒1234import timeticks = time.time()print(ticks) # 1540387897.1916292 时间戳单位最适于做日期运算, 但是1970年之前的日期就无法以此表示了. 太遥远的日期也不行, UNIX和Windows只支持到2038年. 时间元组]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-日志工具类]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-%E6%97%A5%E5%BF%97%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-训练状态跟踪]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-%E8%AE%AD%E7%BB%83%E7%8A%B6%E6%80%81%E8%B7%9F%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[TrainingStats 类training_stats = TrainingStats(model) 在训练文件 detectron/utils/train.py 中创建(create_model())并配置(setup_model_for_training)完模型以后, 调用了位于detectron/utils/training_stats.py 中class TrainingStats()类, 调用语句如下所示:123456# detectron/utils/train.pydef train_model(): #... training_stats = TrainingStats(model) #... 下面, 来看看这个类具体的内部实现:1234567891011121314151617181920212223242526# detectron/utils/training_stats.py# 该类用于跟踪关键的训练状态统计值class TrainingStats: def __init__(self, model): # TODO 这个window size是指什么? smoothing tracked values?? self.WIN_SZ = 20 # 输出logging的iterations间隔 self.LOG_PERIOD = 20 self.smoothed_losses_and_metrics = &#123; # from detectron.utils.logging import SmoothedValue # 该类用于跟踪一系列值, 同时提供访问smoothed values的借口(基于WIN_SZ或者global series average). key: SmoothedValue(self.WIN_SZ) for key in model.losses + model.metrics &#125; self.losses_and_metrics = &#123; key: 0 for key in model.losses + model.metrics &#125; self.smoothed_total_loss = SmoothedValue(self.WIN_SZ) self.smoothed_mb_qsize = SmoothedValue(self.WIN_SZ) self.iter_total_loss = np.nan self.iter_timer = Timer() # from detectron.utils.timer import Timer self.model = model#... SmoothedValue 类从上面的代码可以看到, TrainingStats 类中的成员大多为SmoothedValue类对象, 该类的定义位于detectron/utils/logging.py 中, 下面先来看看这个文件的内部实现:123456789101112131415161718192021222324252627# detectron/utils/logging.pydef log_json_stats(stats, sort_keys=True): # ...class SmoothedValue(object): # 该类用于跟踪一系列值, 同时提供访问滑动值smoothed values的借口(基于WIN_SZ或者global series average). def __init__(self, window_size): # from collections import deque self.deque = deque(maxlen = window_size) self.series = [] self.total = 0.0 self.count = 0 def AddValue(self, value): # 将指定的值value加入到对象中的各个成员变量中 self.deque.append(value) self.series.append(value) self.total += value self.count += 1 def GetMedianValue(self): return np.median(self.deque) # axis为None , 则按照一维数组来计算deque中的中位数 def GetAverageValue(self): return np.mean(self.deque) # 同理, 返回deque的平均值 def GetGlobalAverageValue(self): return self.total / self.count # 返回所有值的平均值, 而不仅仅只是窗口内的平均值#... 从上面的代码我们可以看出, 实际上SmoothedValue 类是用来维护滑动平均值的, 同时还会维护一个滑动中位数和总平均值. Timer()接着, 在初始化函数中, class TrainingStats类的成员变量 self.iter_timer是class Timer的类对象, 该类位于detectron/utils/timer.py文件中, 主要封装了python的time模块, 下面具体看一下实现细节1234567891011121314151617181920212223242526detectron/utils/timer.pyclass Timer(object): def __init__(self): self.reset() # 调用类自身的reset()函数 def tic(self): # 这里使用了time.time 而不是 time.clock, 原因是因为time.clock对于多线程任务来说可能存在一些问题 self.start_time = time.time() # 成员变量 start_time def toc(self, average=True): self.diff = time.time() - self.start_time # diff的值为当前时间与开始时间之间的间隔(单位为秒) self.total_time += self.diff # 每调用一次toc函数, totaltime都会统计一次时间间隔 self.calls += 1 # 记录调用toc的次数 self.average_time = self.total_time / self.calls if average: return self.average_time else: return self.diff def reset(self): # 将Timer内统计时间全部归0., 注意是浮点类型 self.total_time = 0. self.calls = 0. self.start_time = 0. self.diff = 0. self.average_time = 0. 再看 TrainingStats 类接下来, 我们继续看 TrainingStats 类中的其他方法:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# detectron/utils/timer.pyclass TrainingStats(object): def __init__(self, model): #... def IterTic(self): self.iter_timer.tic() # 调用Timer类的tic方法, 记录当前time.time()时间 def IterTic(self): return self.iter_timer.toc(average=False) # 返回距离上次掉要tic方法的时间间隔(单位为秒) def ResetIterTimer(self): self.iter_timer.reset() # 重置所有时间相关的统计数据 def UpdateIterStats(self): # 更新跟踪的迭代统计信息 for k in self.losses_and_metrics.keys(): if k in self.model.losses: # import detectron.utils.net as nu self.losses_and_metrics[k] = nu.sum_multi_gpu_blob(k) # 计算多个gpu上的数据和 else: self.losses_and_metrics[k] = nu.average_multi_gpu_blob(k) for k, v in self.smoothed_losses_and_metrics.items(): v.AddValue(self.losses_and_metrics[k]) self.iter_total_loss = np.sum( np.array([self.losses_and_metrics[k] for k in self.model.losses]) ) self.smoothed_total_loss.AddValue(self.iter_total_loss) self.smoothed_mb_qsize.AddValue( self.model.roi_data_loader._minibatch_queue.qsize() ) def LogIterStats(self, cur_iter, lr): # 记录跟踪的统计信息 if(cur_iter % self.LOG_PERIOD == 0 or cur_iter == cfg.SOLVER.MAX_ITER - 1): stats = self.GetStats(cur_iter, lr) log_json_stats(stats) def GetStats(self, cur_iter, lr): eta_seconds = self.iter_timer.average_time * ( cfg.SOLVER.MAX_ITER - cur_iter ) # 剩余时间 eta = str(datetime.timedelta(seconds=int(eta_seconds))) mem_stats = c2_py_utils.GetGPUMemoryUsageStats() mem_usage = np.max(mem_stats['max_by_gpu'][:cfg.NUM_GPUS]) stats = dict( iter=cur_iter, lr=float(lr), time=self.iter_timer.average_time, loss=self.smoothed_total_loss.GetMedianValue(), eta=eta, mb_qsize=int( np.round(self.smoothed_mb_qsize.GetMedianValue()), ), mem=int(np.ceil(mem_usage / 1024 / 1024)) # 将字节转换成GB ) for k, v in self.smoothed_losses_and_metrics.items(): stats[k] = v.GetMedianValue() return stats]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Group Normalization]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ECCV2018-GroupNormalization%2F</url>
    <content type="text"><![CDATA[作者: Yuxin Wu and Kaiming He发表: ECCV 2018, Best Paper Honorable Mention 核心亮点针对BN对batch size的依赖问题, 提出了一种新的通用型归一化方法提出了一个用于替代BN的简单算法, 称之为GN(Group Normalization). GN将输入图谱的通道分成不同的组, 并且计算每一组的mean和variance, 然后将其进行归一化. GN的计算复杂度与batch size 的大小是相互独立的, 并且它的准确度在不同范围内的batch size下仍然是稳定的. 并且在整体表现和不同任务上的效果均强于其他类型的归一化方法(LN,IN等) 论文细节摘要BN是深度学习中的一项里程碑式的技术, 它可以让不同网络进行训练. 但是按照batch进行归一化的过程会引入一些问题: 当batch的size比较小时, 由于batch的统计期望十分不精确, 会使得BN的误差大幅度增加. 这一点限制了BN在训练大型模型时的使用(如目标检测, 分割等等, 由于GPU内存的限制往往batch的值很小). 因此, 本文提出了一个用于替代BN的简单算法, 称之为GN(Group Normalization). GN将输入图谱的通道分成不同的组, 并且计算每一组的mean和variance, 然后将其进行归一化. GN的计算复杂度与batch size 的大小是相互独立的, 并且它的准确度在不同范围内的batch size下仍然是稳定的. 在ResNet50中, 当batch size为2时, GN的错误率比BN要低10.6%, 当batch size为经典值时(32,64等), GN的表现也可以媲美BN, 同时, 也超越了其他归一化方法(Layer, Instance, Weight Normalization等). 不仅如此, GN还可以很自然的从预训练的模型中进行fine-tuning. GN在多个任务上都表现出了很好的效果, 可以用其替换掉BN. (只需数行代码即可实现GN). 介绍BN在多项任务中都起到了很好的实验效果, 但是BN的有效性必须建立在足够大的batch size之上(如32/GPU). 当batch size 比较小时, BN往往无法取得好的效果, 并且还会提升模型的误差, 如图1所示. 由于硬件GPU显存大小的设置, 在使用较大的模型训练分辨率较高的图片时, 往往不能设置很高的batch size. 因此, 本文提出了GN算法来作为BN的代替. 相关工作Normalization: BN在很多任务上都取得了很好的效果(BN通常会在每一层都执行), 但是, BN依赖于batch的平均值和方差, 这使得batch size的大小对BN的效果有较大的影响, 同时, 在测试阶段, 单个的图片是没有均值和方差的, 所以只能用整个数据集的均值和方差来代替, 通常会使用滑动平均来维护这两个变量, 也就是说, 在使用BN时, 如果数据集改变了, 则均值和方差就会有较大改变, 这就造成了训练阶段和测试阶段的不一致性, 由此也会带来一些问题.另一些Normalization方法, 尝试避开batch size, 如Layer Normalization(LN)和Instance Normalization等, 但他们通常是针对RNN/LSTM模型的, 并且在大多数的视觉任务上表现不如BN好, 还有Weight Normalization, 它不是对网络层的输入进行归一化, 而是对网络层的参数进行归一化, 同样, 在大多数的视觉任务上, 表现都不如BN. Addressing small batches: Ioffe在NIPS2017上提出Batch Renormalization(BR)尝试解决BN的小batch size问题. 它引入了两个额外的参数来限制BN的估计期望和方差, 减少它们在小batch size时的不稳定问题. 但是BR本质上还是依赖于batch的, 当batch较小时, 其性能也会相应下降(不过下降较少).也有的工作尝试避免使用小batch size. CVPR2018的一篇文章通过同步BN的方法, 使得模型可以在多GPU上计算平均值和方差, 但是, 这个方法并没有才本质上解决BN的问题, 相反的, 它的方法更像是一种工程和硬件上的解决方案, 希望以此来达到BN的要求. 不仅如此, 这种同步BN方案也使得模型在工业中的大规模分布式训练下无法利用异步的优化方法, 如ASGD. Group-wise computation: Group convolution计算在多个模型中都有提及. 但是本文的GP并不需要组卷积计算, 它是一个一般化的网络层(generic layer). Group Normalization特征图谱中的各个channels之前也并不是完全互相独立的. 公式典型的归一化公式形式(BN,LN,IN,GN)为: \hat x_i = \frac{x_i - \mu_i}{\sigma_i}上式中, $x$ 代表某一层的特征, $i$ 代表下标, 在2D图像中, $i=(i_N, i_C, i_H, i_W)$ , 是一个4D的向量. $\mu$ 和 $\sigma$ 分别为期望和标准差, 通过下式计算得到: \mu_i = \frac{\sum_{k\in S_i} x_k}{m}, \sigma_i = \sqrt{\frac{sum_{k\in S_i} (x_k-\mu_i)^2}{m} + \epsilon}大多数的Normalization方法的不同之处就在于集合 $S_i$ 的不同(如图2所示): BN: $S_i = \{k | k_C = i_C \}$, 代表BN是在求每一个channel的均值和方差. 也即 $C$ 不变, 求固定 $C$ 时 $(N,H,W)$ 的均值和方差 LN: $S_i = \{k | k_N = i_N \}$, 代表 $N$ 不变, 求固定 $N$ 以后 $(C,H,W)$ 的均值和方差(可以看出, 此时的均值和方差已经不受batch size的影响) IN: $S_i = \{k | k_N = i_N, k_C = i_C\}$, 代表 $N$ 和 $C$ 都固定时 , $(H,W)$ 的均值和方差. 也就是说是对单个特征中, 单个通道上的均值和方差. 上面所有的Normalization方法(BN,LN,IN)都使用了线性偏移来补偿可能引起的数据分布表征丢失问题: y_i = \gamma x_i + \betaGroup Norm: 在Group Norm中, $\mu$, $\sigma$ 以及集合 $S_i$ 的定义如下: S_i = \{ k | k_N = i_N, \lfloor \frac{k_C}{C/G} \rfloor = \lfloor \frac{i_C}{C/G} \rfloor \}上式中, $G$ 是group的数量, 是一个超参数(默认为32), $C/G$ 是每一个group中的channels的数量, $\lfloor \frac{k_C}{C/G} \rfloor = \lfloor \frac{i_C}{C/G} \rfloor \}$ 意味着下标 $i$ 和 $k$ 在同一个group内. GN计算的 $\mu$ 和 $\sigma$ 是处于同一个group的所有通道上的 $(H,W)$ 的均值和方差, 具体的计算方式如图2最右侧所示, 在该图实例中, G=2, 并且每一个group具有3个channels. GN同样会在每一个channel中使用参数 $\gamma$ 和 $\beta$ 对数据执行线性偏移.(注意是每一个通道, 而不是group, 也就是同一个group中的不同通道下的 $\gamma$ 和 $\beta$ 参数是不一样的!) Relation to Prior Work: 很明显, LN和IN实际上可以看做是GN的一种特例情况(如图2). 当 $G=1$ 时, GN就变成LN, 当 $G=C$ 时, GP就变成了IN. 实现就和GN的思想一样, GN实现起来也十分简单, 下图是GN基于python和tensorflow的实现代码, 从代码中可以看到, $\gamma$ 和 $\beta$ 的shape为 [1,C,1,1], 也就是说, $\gamma$ 和 $\beta$ 在同一个group中的不同channel来说, 值是不一样的. 实验]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R-FCN-NIPS2016]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-NIPS2016-R-FCN%2F</url>
    <content type="text"><![CDATA[文章: R-FCN: Object Detection via Region-based Fully Convolutional Networks 核心 全卷积网络怎分类任务上表现较好, 但是在目标检测任务往往精度不行, 这是因为在一般情况下, 分类任务具有平移不变性, 而检测任务却要求对目标的平移做出正确响应. 在Faster RCNN类的方法中RoI pooling之前都是卷积, 具有平移不变性, 但是一旦经过RoI pooling 之后, 后面的网络结果就不再具备平移不变性了. 因此, 本文了position sensitive score map来将目标位置的信息融合进RoI 对于Faster RCNN等基于感兴趣区域的检测方法来说, 实际上是分成了几个subnetwork, 第一个用来在整张图上做比较耗时的conv, 这些操作与region无关, 是计算共享的. 第二个subnetwork是用来产生候选区域(如RPN), 第三个subnetwork是用来分类或者进一步对box进行回归的, 这个subnetwork和region是有关系的, 衔接在这个subnetwork和前两个subnework中间的就是RoI pooling. 本文与FasterRCNN相比(前91层共享, RoI pooling之后, 后10层不共享)不同, 将ResNet所有的101层都放在的前面共享的subnetwork中, 最后用来进行prediction的卷积只有1层, 大大减少了计算量. 最终, 从实验结果来看, 本文提出的用于目标检测任务的全卷积网络在精度上仍然不如标准的Faster RCNN, 但是在test time上要好于FasterRCNN (因为有大部分层都变成参数共享的了). 论文细节摘要本文提出了一个用于精确高效进行物体检测的基于区域的全卷积网络. 在Fast/Faster RCNN中, 使用了计算成本很大的子网络来提取候选区域, 与之相比, 本文的基于区域的检测器是全卷积的, 因此几乎所有的计算都可以共享. 为了达到这个目标, 本文提出了一个位置敏感的score maps来解决图像分类问题中的平移不变性和物体检测中的平移可变性之间的鸿沟. 因此, 本文的方法可以很自然的使用全卷积图像分类网络, 比如ResNet. 本文的方法在检测阶段的速度大约为FasterRCNN的2.5~20倍. 介绍最近的图像分类网络如ResNet何GoogleNets都是通过全卷积网络设计的.(只有最后一层是全连接的, 并且该层会在进行目标检测任务时被去掉). 我们讨论了之前提到的不自然的设计是由于图像分类的平移不变性和目标检测的平移可变性之间的鸿沟造成的. 一方面, 在图像级别上的分类任务倾向于平移不变性, 因此, 深度卷积网络的结构会尽可能是使得结果的检测具有一定的平移不变性. 另一方面, 物体检测任务需要坐标表示, 这是一种平移可变性的表现. 为了解决这个问题, ResNet的检测方法是插入了一个RoI pooling 层—-该层可以用来打破神经网络原有的平移不变性, RoI后续的卷积层在对不同区域进行评价时, 实际上已经不再具有平移不变性(出现在天空位置时, 行人的预测概率较低). 但是, 这种RoI的设计模型牺牲了训练和测试的高效性, 因为它引入了大量额外的快计算. 在这篇文章中, 我们构建了一个用于物体检测任务的基于区域的全卷积网络的框架模型. 我们的网络模型由共享的全卷积网络组成. 为了与FCN的平移可变性相协调, 我们构建了一个位置敏感型的score maps. 每一个score maps都根据相对空间位置将位置信息进行了编码. 在FCN之上, 我们添加了一个位置敏感的RoI pooling layer 用于指导从score maps中获取的信息, 并且在其后没有再使用带权重的层(全连接or全卷积). Our approachOverview: 和RCNN一样, 本文使用了 two-stage 的物体检测策略, 包括:1) 区域推荐, 2)区域分类. 本文通过RPN网络来提取候选框(RPN网络本身也是一个全卷积网络). 和Faster RCNN一样, 我们令RPN和R-FCN网络的权重参数共享. 下面的图2显示了这个系统的整体视图 (位于上方的RPN网络和位于下方的R-FCN网络使用的卷积图谱都是来自同一段卷积网络, 因此参数共享): 当给定感兴趣区域(proposals regions)后, R-FCN网络会用于对RoIs分类. 在R-FCN中, 所有的可学习的参数都是卷及参数, 并且都是在同一张图片上计算得到的. 最后一层卷积层对于每一类都会输出 $k^2$ 个位置敏感的socre maps, 因此总共有对于具有 $C$ 个物体类别来说, 输出层具有 $k^2(C+1)$ 个通道. $k^2$ 主要是根据用于描述相关位置的 $k\times k$ 的空间网格来决定. 比如, 当 $k \times k = 3\times 3$ 时, 对于每一个物体类别都会有9个score maps 根据下列情况进行编码: {top-left, top-center, top-right, … , bottom-right}. R-FCN最后会有一个位置敏感的RoI pooling层, 这一层会将最后一层卷积层的输出全部整合, 并为每个RoI生成对应的score. 和之前的工作不同, 本文的位置敏感型的RoI层会执行selective pooling, 并且每一个$k\times k$ bin 都会整个仅一个score map. 下面的图展示了示例: Backbone architecture: R-FCN的网络主体是基于ResNet-101的, 我们将平均池化层和fc层移除, 只使用前面的卷积网络来计算特征图谱. 由于卷积段输出的维度为2048, 因此我们使用 $1\tims 1$ 的卷积层来进行降维, 使其维度变成1024. 然后我们使用通道数为 $k^2(C+1)$ 的卷积层来生成score maps. Position-sensitive score maps &amp; Position-sentitive RoI pooling. 为了具体的对每个RoI的位置信息进行编码, 我们将每一个RoI划分成 $k\times k$ 大小的普通网格, 然后, 最后一层卷积层对每一个类别都会生成 $k^2$ score maps, 在第 $(i,j)$ 个bin中 $(0 \leq i,j \leq k-1)$. 我们定义位置敏感的RoI pooling计算操作如下所示: r_c(i,j | \theta) = \sum_{(x,y)\in bin(i,j)} z_{i,j,c}(x+x_0, y+y_0 | \theta)/n]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Improving Object Detection With One Line of Code]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-SoftNMS-ICCV2017%2F</url>
    <content type="text"><![CDATA[作者: Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S.Davis发表: ICCV2017机构: Center For Automation Research 核心亮点提出了一种NMS的变体, 通过利用该变体, 基本上可以提升任何模型的检测准确率作者们提出了一种新式的NMS算法, 并且利用该算法, 可以普遍提高当前现有模型的召回率(尤其是面对重叠程度大的物体), 同时, 由于可以不增加复杂度的情况下直接用该算法替换传统NMS算法, 因此, 在替换SoftNMS时, 无需更改模型的任何参数, 也无需重新训练模型, 就可以达到提升召回率的作用. (对mAP的提升大约为1%左右) 论文细节传统NMS: 先将score倒序排列, 然后取socres值最大的box并将其置于final box列表中, 计算所有剩余box与该box的重叠度, 大于某一阈值的就将其删除, 然后迭代的使用此方法, 直到final box数量达到要求或者没有多的box了.(在FasterRCNN中, 生成候选框时会使用一次NMS, 预测还会分别对每一个类别都使用一次NMS) 传统NMS存在的问题: 由上面的描述可知, 如果两个物体本身的重叠度过大, 那么其中一个物体的框就会被删除(score被置为0), 从而导致漏解. Soft-NMS: 在将具有最大score的box置于final box之后, 计算所有剩余box与该box的重叠度, 对于那些重叠度大于一定阈值的box, 我们并不将其删除, 而仅仅只是根据重叠程度来降低那些box的socre, 这样一来, 这些box仍旧处于box列表中, 至少socre的值变低了. 具体来说, 如果box的重叠程度高, 那么score的值就会变得很低, 如果重叠程度小, 那么box的score值就只会降低一点, Soft-NMS算法伪代码如下图所示: 设 $s_i$ 为第 $i$ 个box的score, 则在应用SoftNMS时各个box score 的计算公式如下: s_i = \begin{cases} s_i, & iou(M, b_i) < N_t \\ s_i(1-iou(M, b_i)), & iou(M, b_i) \geq N_t \end{cases}上式过于简单直接, 为了函数的连续性, 文章改用了高斯惩罚系数: s_i = \begin{cases} s_i, & iou(M, b_i) < N_t \\ s_i e^{\frac{iou(M, b_i)^2}{\sigma}} , & iou(M, b_i) \geq N_t \end{cases}]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMS相关算法Python实现]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-NMS-Implementation%2F</url>
    <content type="text"><![CDATA[【链接】非极大值抑制算法(NMS)及python实现https://blog.csdn.net/Blateyang/article/details/79113030 【链接】NMS-非极大值抑制-Python实现https://blog.csdn.net/u011436429/article/details/80107042 NMS 概念非极大值抑制(Non-Maximum Suppression, NMS), 顾名思义就是抑制那些不是极大值的元素, 可以理解为局部最大值搜索. 这个局部代表的是一个领域, 领域有两个参数可变, 一是领域的维数, 而是领域的大小. 对于目标检测来说, 非极大值抑制主要用于提取局部区域内分数最高的窗口. 算法实现: 1234567891011121314151617181920212223242526272829import cv2def nms(boxes, scores, overlap=0.5, top_k=200): # boxes: Shape: [num_boxes, 4] # scores: Shape: [num_boxes] keep = scores.def main(): img_path = "./nms_test.jpg" bounding_boxes = [(187, 82, 337, 317), (150, 67, 305, 282), (246, 121, 368, 304)] scores = [0.9, 0.75, 0.8] img = cv2.imread(img_path) ori_img = img.copy() # Draw Parameters font = cv2.FONT_HERSHEY_SIMPLEX font_scale = 1 thickness = 2 nms_threshold = 0.4 for (start_x, start_y, end_x, end_y), confidence in zip(bounding _boxes, scores):if __name__ == "__main__": main()]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉-目标检测训练策略]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[NMSMulti-ScaleTraining Testing 哪些论文使用了? 大约提升多少 #]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-模型创建]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[model_builder.create(...) 方法在调用detectron/utils/train.py文件中的train_model()方法时,在第一行代码中调用了同属该文件的create_model()方法, 而在该方法中, 核心的创建语句为model = model_builder.create(cfg.MODEL.TYPE, train=True), 其中, model_builder.create(...) 是位于文件 detectron/modeling/model_builder.py中create(...)方法, 该方法的详细实现如下所示: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# detectron/modeling/model_builder.pydef generalized_rcnn(model): #...def rfcn(model): #...def retinanet(model): #...def create(model_type_func, train=False, gpu_id = 0): # 通用的模型创建函数, 该函数可以继续分派到特定的模型创建函数中 # 默认情况下, 该函数将以并行模式(并行数量取决于cfg.NUM_GPUS)生成数据 # 但是, 你可以将其限制在特定的GPU上进行(通过gpu_id), 在测试阶段使用optimizer.build_data_parallel_model() # from detectron.modeling.detector import DetectionModelHelper model = DetectionModelHelper( name=model_type_func, # 对于示例: model_type_func=generalized_rcnn train=train, num_classes=cfg.MODEL.NUM_CLASSES, init_params=train ) model.only_build_forward_pass = Fasle model.target_gpu_id = gpu_id return get_func(model_type_func)(model) # get_func(...)函数解析就在下面def get_func(func_name): # 该函数会通过name返回一个函数对象, # func_name必须等于该module中的函数, 或者是与base 'modeling'相关的函数的路径 # 对于示例: func_name = cfg.MODEL.TYPE = generalized_rcnn if func_name == '': return None # import detectron.modeling.name_compat as name_compat # 貌似是因为名字做过改动, 这句话是为了兼容性而存在的 new_func_name = name_compat.get_new_name(func_name) if new_func_name != func_name: # 对于本例: new_func_name = func_name, 名字不变 logger.warn( 'Remapping old function name:&#123;&#125; -&gt; &#123;&#125;'. format(func_name, new_func_name) ) func_name = new_func_name try: parts = func_name.split('.') if len(parts) == 1: return globals()[parts[0]] module_name = 'detectron.modeling.' + '.'.join(parts[:-1]) # 会根据module导入位于'detectrong/modeling'中的module # fast_rcnn_heads, FPN, mask_rcnn_heads 等等 module = importlib.import_module(module_name) return getattr(module, parts[-1]) except Exception: logger.error('Failed to find function: &#123;&#125;'.format(func_name)) raise#... DetectionModelHelper 类从上面的代码中可以看出 create() 函数中最主要的部分是使用了 detectron/modeling/detector.py文件中的 class DetectionModelHelper()类, 下面, 我们就对该类进行解析: 123456789101112131415161718192021222324252627# detectron/modeling/detector.py# from caffe2.python import cnnclass DetectionModelHelper(cnn.CNNModelHelper):# 该类实际上是cnn.CNNModelHelper的一个子类, 这一点很重要 def __init__(self, **kwargs): # 处理属于DetectionModelHelper的特定的参数, 其他的都会传到CNNModelHelper当中 self.train = kwargs.get('train', False) # train self.num_classes = kwargs.get('num_classes', -1) # cfg.MODEL.NUM_CLASSES assert self.num_classes &gt; 0, 'num_classes must be &gt; 0' for k in ('train', 'num_classes'): if k in kwargs: del kwargs[k]# TODO 貌似是因为下面要再次调用构造函数的原因? # 设置数据的排列顺序: batchsize, channels, heiht, width kwargs['order'] = 'NCHW' # TODO, 不懂这个选项的实际作用 kwargs['cudnn_exhaustive_search'] = False super(DetectionModelHelper, self).__init__(**kwargs) # TODO ?? 这样不会造成递归调用吗? 为什么要递归调用构造函数 self.roi_data_loader = None self.losses = [] self.metrics = [] self.do_not_update_params = [] # 位于该列表中的参数不会被更新 self.net.Proto().type = cfg.MODEL.EXECUTION_TYPE self.net.Proto().num_workers = cfg.NUM_GPUS * 4 self.prev_use_cudnn = self.use_cudnn self.gn_params = [] # 位于该列表中的元素是GroupNorm参数]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OHEM-CVPR2016]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CVPR2016-OHEM%2F</url>
    <content type="text"><![CDATA[文章: Training Region-based Object Detectors with Online Hard Example Mining作者: Abhinav Shrivastava, Abhinav Gupta, Ross Girshick 核心亮点提出了一种在线的难样例挖掘算法:作者根据每个RoIs的loss的大小来决定哪些是难样例, 哪些试试简单样例, 通过这种方法, 可以更高效的训练网络, 并且可以使得网络获得更小的训练loss. 同时, OHEM还具有以下两个优点: 消除FastRCNN系列模型中的一些不必要这参数 , 这些参数大多都是为了解决难样例问题服务的, 在使用OHEM以后, 不仅无需在对这些超参数进行调优, 同时还能获得更好的性能表现. OHEM算法可以与其他多种提升模型精度的trick相结合, 对于大多数模型(RCNN系列), 在使用了OHEM以后, 都能够获得精度上的提高, 可以看做是一种普适性的提升精度的方法. 注: 在实现OHEM上, 作者为了提升速度和效率, 特意设计了两个RoI网络, 以减少无用的计算. 论文细节摘要目前, 基于区域选择的CNN目标检测方法已经取得了很大的成功, 但是他们的训练过程仍然包含着许多启发法[注]和超参数(调优过程成本很高). 本文提出了一种针对区域选择目标检测方法的一种十分简单但非常有效的 在线的难样例挖掘(OHEM)算法. 我们的动机来源于数据集中存在的海量的简单样本, 而只有一小部分困难样本. 如果能够自动的选择这些困难样本用于训练, 那么就会使得训练过程更加高效和有效. OHEM 是一种简单且直观的算法, 它可以消除多个启发过程和超参数. 更重要的是, 它可以稳定的提升检测模型的算法性能, 当数据集越来越大, 越来越复杂时, 它的提升效果就越大. [注] 启发法: 启发式方法(试探法)是一种帮你寻找答案的技术, 但它给出的答案是具有偶然性的(subject to chance), 因为启发式方法仅仅告诉你该如何去找, 而没有告诉你要找什么, 它并不会告诉你该如何直接从A点到B点, 他甚至可能连A点和B点在哪里都不知道. 启发式算法的难点是建立符合实际问题的一些列启发式规则, 启发式算法的有点在于它比盲目型的搜索法要高效, 一个经过仔细设计的启发函数, 往往在很快的时间内就可以得到一个搜索问题的最优解. 介绍基于区域的CNN目标检测法使用的数据集中带物体标签的区域和背景区域之前的样本比例存在着巨大失衡. 在DPM中, 这种比例达到了1:100000. 一些算法(如Selective Search)对此进行了处理但失衡比例仍然很大(1:70). bootstrapping(现在多称为 hard negative mining)问题至少已经存在了20年. 并且Bootstrapping技术已经在目标检测领域内流行了十几年(尤其是在训练针对目标检测的SVMs时). 很多现代的基于深度学习的目标检测方法都是用了基于难样例挖掘的SVMs来帮助训练检测模型(RCNN, SPPnet). 但是奇怪的是后来的一些模型(FastRCNN, FasterRCNN等) 都没有使用bootstrapping技术. 一个潜在的原因可能是在深度神经网络中, 存在一些技术上的困难, 是的bootstrapping使用效果不佳. 传统的bootstrapping需要先用一个固定的模型来找到新的样本已准备训练数据, 然后再用一个固定的样本将检测模型激活并训练. 而在训练深度神经网络时, 其需要成千上万的大量样本用于训练, 因此, 我们需要一个 纯粹在线 的难样例选择算法. 在本文中, 我们提出了一个新型的bootstrapping技术称为 online hard example mining(OHEM), 并将其应用到基于深度学习的当前最先进的目标检测模型上. 该算法实际上是对SGD做了一些小改动, 使得训练样本可以从一个非均匀分布都采样得到, 该分布是一个基于当前样本loss的非静态分布. 该方法利用了目标检测问题特殊的结构优势, 那就是每一个SGD的mini-batch中仅仅包含一张或两张图片, 但是会有上千个候选样本. 这样候选样本会被继续按照特定的分布(倾向于那些不同的, 可以造成很高loss的样本)进行采样, 由于采样后的样本只是一小部分样本子集, 因此梯度下降优化算法仍然高效. 将OHEM算法用于标准的Fast RCNN算法以后, 显示除了三点好处: 移除了一些在基于区域推荐的CNN目标检测算法中的启发方法和超参数 稳定且大幅度的提升了mAP 当training set变的更大更复杂时, OHEM的有效性会提升 不仅如此, 从OHEM中获得的性能提升是对最近目标检测领域其他提升性能方法的一种补充, 如multiscale testing和迭代式bounding box回归. 在使用OHEM的同时结合这些tricks, 可以更进一步的提升mAP. 相关工作绝大多数目标检测模型都使用了结合bootstrapping算法的SVMs来作为检测的scoring function.但是有一些特例, FastRCNN和FasterRCNN等没有使用结合bootstrapping的SVMs, 而是完全根据SGD进行训练, 这些模型通过引入一些在线的难样例挖掘算法来解决这个问题(送入minibatch的前后景比例1:3). 下面简单介绍一下 Hard example mining和CNN目标检测以及他们之间的关系 Hard example mining: 目前常用的有两大算法.第一种是用于优化SVMs的: 训练算法会维护一个工作样本集, 并且该样本集会依据特定的规则不断在训练SVMshe更新样本集之间迭代. 这些规则会将简单样本(很容易分类正确)从样本集中移除, 同时会添加困难样本到样本集中.第二种方法用于非SVMs模型, 如浅层神经网络和boosted决策树: 该算法会从正样例和一部分随机负样例组成的数据集开始训练, 在该数据集上训练至收敛以后, 会继续在一个更大的, 包含更多负样例的数据集上进行训练. 这个过程通常只会进行一次迭代, 并且没有任何的收敛性证明. ConvNet-bases detection: 近年来CNN在目标检测领域迅速, 尤其是基于深度学习的目标检测方法(SPPNet, MRVNN Fast RCNN). Hard example selection in deep learning: 目前已经有很多专门针对目标检测算法hard example mining方法. 这些算法的基本思路与我们相同, 但是我们的算法更关注基于区域推荐的目标检测算法的 在线难样例挖掘. Overview of Fast RCNN在Fast RCNN中, 每一个minibatch包含N=2张图片, 每一张图片会采样B/N=128/2=64个候选框. RoI sampling过程使用了多个启发式规则, 如下所示: Foreground RoIs: 每一个RoI都会根据与真实框的IOU大小来分成前景框和后景框 Background RoIs: 在[bg_lo,0.5) 区间的被认为是后景, bg_lo(FastRCNN为0.1)用来充当难样例挖掘的角色, 但是这样会忽视一些不频繁但是很重要的后景区域, OHEM移除了bg_lo阈值. Balancing fg-bg RoIs: 为了处理难样例问题, FastRCNN采取的策略是将minibatch中前后景的比例设置为 1:3 (25%为前景). 这是一个十分重要的启发式规则阈值, 在实验中, 将他移除或者改变都会引起mAP分数的大幅度下降(3 points). 但是利用OHEM, 就可以在不损失mAP的情况下, 移除这个超参数. Our approach本文提出的OHEM算法可以简化复杂的FastRCNN系列模型的训练及调参过程, 同时可以获得更好的训练结果(lower training loss)和更高的测试性能(higher mAP) Online hard example mining在RCNN中使用SVMs,大致有两个阶段, 阶段 a) 首先会筛选并处理指定数目(10or100)个图片, 然后在在这些图片进进行训练直到收敛. 重复这两个阶段直到SVMs找到所有的支持向量为止. RCNN所采用的这个优化策略效率是很低的, 因为在对图片进行筛选和处理的时候, 没有任何模型会进行更新. OHEM算法流程如下: 对于处于第t次SGD迭代的输入图片来说, 首先计算器卷积特征图谱. 然后令RoI网络使用这个特征图谱和 所有 的RoIs (不是minibatch子集, 而是所有), 进行前向传播(仅包含RoI pooling层和几层fc层). 由此计算出的loss代表了当前网络在每一个RoI上的表现好坏. Hard examples的选择方法是将每个RoI的loss进行排序, 然后选择loss最大的 B/N 个样本作为hard examples. 直观上可以看出, 这些样本对于当前网络来说, 是最难正确分类的样本. 同时, 由于RoI pooling层本身计算量不高, 且各个RoI之间的计算可以共享(//TODO,怎么共享??), 因此, 额外的计算成本相对来说并不高. 此外, 参与反向计算过程的样本集合很很小的一个集合, 因此, 相比以前的方法并不会带来更多的计算成本.但是, 这里有一个小警告: 如果两个RoI之间的overlap较大, 那么很有可能这两个RoI的loss直接会有一定关联程度. 也就是说, 这些重叠度较高的RoIs可以映射到特征图谱上的同一块区域中, 又因为分辨率之间的差异性, 就可能导致loss的重复计算. 为了解决这种冗余计算和关联区域, 我们使用NMS算法来降低重复性. 具体来说, 给定RoIs列表和它们对应的losses, NMS迭代的选择那些具有最高loss的RoI, 然后移除所有与高RoI重叠度较高(IOU&gt;0.7)的其他RoI(这些RoI的loss更低).可以看出, 上面的过程并不需要fg-bg比例这个超参数, 如果任何一类被忽视了, 那么这个类对应的loss就会一直升高, 知道这个类被采样的概率变大为止. 对于有些图片来说, 前景区域是简单样例, 此时网络就会只采用背景区域进行训练, 反之, 也有的图片会认为背景区域(草地, 天空)是简单样例. Implementatin details实现OHEM算法的方法有很多, 它们之间有着不同的权衡和考量. 一个明显的方法是修改loss层, 使其完成难样例的选取工作. loss层可以计算所有RoIs的loss, 然后将它们进行排序,并且根据loss的值进行难样例的选择, 同时将所有非难样例的RoIs的loss设置为0 (表示梯度下降时不会考虑这些样例). 这种方式很直接, 但是却不够高效, 因为RoI网络仍然需要申请空间来存储这些RoIs, 并且还要对所有的RoIs进行反向传播计算(非难样例的loss虽然为0, 但也要参与计算流程).为了克服上面的效率问题, 本文提出了一种结构如图2所示. 我们的实现包含两个RoI网络的副本, 其中一个是 只读 的. 这意味着我们的只读RoI网络只需要申请前向传播计算的内存, 而不用像经典RoI网络一样, 需要同时申请前向计算和反向传播时的内存. 对于一次SGD迭代来说, 给定卷积特征图谱, 只读RoI网络会对所有的RoIs执行前向计算并计算loss( $R$ , 图2中的绿色箭头所示). 然后难样例采样模块会按照之前说的采样策略来选择难样例, 然后会将这些难样例输入到一个常规的RoI网络中( $R_{hard-sel}$ 图2中的红色箭头所示). 于是, 这个网络仅仅只会对难样例同时进行前向计算和反向传播计算. 在实际中, 我们使用所有图片的所有RoI作为 $R$, 因此对于只读RoI网络最后的minibatch的size为 $|R|$. (//TODO 嘛意思?)在实验中, N=2(意味着 $|R|\approx 4000$ ), B = 128. 实验与分析OHEM vs. heuristic sampling如下表所示, 没有使用 bg_lo 超参数的OHEM在FasrRCNN上的mAP提高了4.8个点 Robust gradient estimates因为N=2, 所以选出来的框之间很有很大的关联性, 这会不会使得梯度不稳定从而难以收敛. 实验结果表明, OHEM同样对这种情况具有鲁棒性, 即使将N设置为1, mAP也不会降低多少(仅降低了0.2%). (当然, 在硬件条件允许的情况下, 选取越大的N, 效果一般越好) Why just hard examples, when u can use call?如果使用所有的RoI loss参与权重更新(而不仅仅是hard examples)会怎么样呢? 在这种情况下, 那些简单样本将会拥有较小的loss, 从而不会对梯度的更新贡献太大, 整个训练过程会自动的专注于难样例的训练.(这种全训练的结果会使得相对于标准FastRCNN的mAP提升一点, 因为标准的FastRCNN并没有针对难样例, 而是随机选的minibatch), 但是这种全训练很是的训练时间大幅提高. Better optimization利用OHEM可以获得更低的 mean loss per RoI (取自各种方法的第20K次迭代) Computational cost不论是耗时上还是内存占用上, 都变多了, 但总体来说, 是可以接受的 PASXAL VOC and MS COCO results在训练阶段和测试阶段使用multi-scale和multi-stage bbox regression 可以大幅度提高mAP multi-scale: $s\in \{ 480, 576, 688, 864, 900 \}$ for training, $s\in \{480, 576,688,864,1000\}$ for testing. 这些scales和caps的选择主要受制于GPU的显存大小.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[其他-Latex语法]]></title>
    <url>%2Fz_post%2F%E5%85%B6%E4%BB%96-Latex%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[乘法点乘\cdot : $\cdot$\odot : $\odot$ 箭头 样式 指令 样式 指令 样式 指令 样式 指令 样式 指令 样式 指令 $\uparrow$ \uparrow $\Uparrow$ \Uparrow $\downarrow$ \downarrow $\Downarrow$ \Downarrow $\leftarrow$ \leftarrow $\Leftarrow$ \Leftarrow 样式 指令 样式 指令 样式 指令 样式 指令 $\uparrow$ \uparrow $\Uparrow$ \Uparrow $\downarrow$ \downarrow $\Downarrow$ \Downarrow \rightarrow \Rightarrow \updownarrow \Updownarrow \leftrightarrow\Leftrightarrow长箭头表示(其实就是在上述表示前加一个long/Long即可) 符号 符号 MarkDown MarkDown\longleftarrow\Longleftarrow \longrightarrow\Longrightarrow\longleftrightarrow\Longleftrightarrow更多的箭头符号 符号 符号 MarkDown MarkDown\twoheadrightarrow\rightarrowtail\looparrowright\curvearrowright\circlearrowright\Rsh\multimap \leftrightsquigarrow \rightsquigarrow \leadsto \nearrow \searrow \swarrow \nwarrow \nleftarrow \nrightarrow \nLeftarrow \nRightarrow \nleftrightarrow \nLeftrightarrow \dashrightarrow\dashleftarrow \leftleftarrows \leftrightarrows \Lleftarrow \twoheadleftarrow \leftarrowtail \looparrowleft \curvearrowleft \circlearrowleft \Lsh \mapsto \hookleftarrow \hookrightarrow\upharpoonright \upharpoonleft\downharpoonright\downharpoonleft \leftharpoonup\rightharpoonup\leftharpoondown \rightharpoondown \upuparrows \downdownarrows \rightrightarrows \rightleftarrows \rightrightarrows \rightleftarrows \rightleftharpoons\leftrightharpoons]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[其他-MarkDown语法]]></title>
    <url>%2Fz_post%2F%E5%85%B6%E4%BB%96-MarkDown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[json模块]]></title>
    <url>%2Fz_post%2FPython-json%2F</url>
    <content type="text"><![CDATA[JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式, 易于阅读和编辑. 使用JSON函数之前需要导入json库: import json json.dumps该函数用于将Python对象编码成JSON字符串, 语法如下: 123json.dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding="utf-8", default=None, sort_keys=False, **kw) json.loads可以解析json文件, 之后可以像使用字典一样进行操作. json解码为Python类型转换对应表]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[FPN-CVPR2017]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CVPR2017-FPN%2F</url>
    <content type="text"><![CDATA[文章: Feature Pyramid Networks for Object Detectin作者: Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie 核心亮点提出了多尺度的特征金字塔结构将最后一层特征图谱进行不断尽快上采样, 并与每一个金字塔阶级的特征图谱进行加法合并操作, 得到新的表征能力更强的不同金字塔层次的特征图谱, 然后将RoI按照尺寸分别映射到这些特征图谱上, 再在每个特征图谱上进行类别和位置预测. 可以直观感受到, 这种多尺度的特征图谱在面对不同尺寸的物体时, 具有更好的鲁棒性, 尤其是在面对小型物体时. 同时, 这种特征金字塔结构是一种通用的特征提取结构, 可以应用到不同的网络框架中, 显著提高(5~8%)模型的召回率(因为提出了更多不同尺度, 不同特征信息的anchor box), 并且可以广泛提高(2~3%)模型的mAP. 论文细节背景介绍在面对不同尺度的物体检测问题时, 特征金字塔结构是一个非常基本的组成部分, 但是最近的检测模型都舍弃了这一结构(Fast RCNN, Faster RCNN, YOLOv2等), 其一部分原因是因为这个结构对计算和内存的要求较高. 本文在控制资源消耗的情况下, 建立了一个跨所有层的特征金字塔结构网络, 达到了当时的STOA性能. 传统的特征金字塔结构对于计算资源和内存资源的依赖较为严重, 同时深度卷积网络在不同阶段的卷积层, 虽然较好的传递了特征, 但是因为每一层的输出通道数不同, 会导致层与层之间形成一种潜在的语义鸿沟. 较高的分辨率好好具有更多的低级信息(在深层会被过滤掉), 但是多于的信息也会对降低泛化能力, 较低的分辨率则具有权重更高的重要信息, 但是这样也会使得小目标物体难以检测. SSD与FPN中多尺度特征图谱融合的区别SSD算是首批结合多尺度特征金字塔的检测系统, 但是SSD为了避免用到过多的低级特征(高层卷积图谱上的特征), 放弃使用以及计算好的特征特普, 而是从网络的最后一层卷积层开始, 添加新的卷积层, 并在这些新添加的卷积层上进行特征金字塔融合. 这样做一个很直观的结果就是, 它会错过很多高分辨率特征图谱上的特征信息, 而这些特征信息在面对小物体检测时是十分有用的.(这也是SSD对小物体检测较为敏感的原因之一). Feature Pyramid Networks输入: 任意尺寸的单张图片(不进行尺度缩放)输出: 以全卷积的方式在不同层次上输出对应的映射尺寸的特征图谱 构建FPN包含以下步骤: 自下而上的路径: 该方式是根据backbone 卷积网络的前馈计算过程进行的. 特征图通常经过卷积计算后是会越来越小的, 也有一些特征层的输出和原来大小一样, 成为处于同一个网络阶段(“same network stage”). 对于本文, 将每一个”network stage”看做是一个金字塔级别. 然后选择每个阶段的最后一层作为特征图的参考集合(因为最深层理应具有最强的特征表示). 具体来说, 对于ResNets, 使用了每一个阶段的最后一个残差结构的特征激活输出, 将这些残差模块conv2, conv3, conv4, conv5 的输出表示为 $\{C_2, C_3, C_4, C_5\}$, 并且注意到他们相对于输入图像具有 $\{4,8,16,32\}$ 像素的步长. 自上而下的路径以及横向连接:自上而下的路径是通过在更粗糙, 但是语义更强的高层特征图(深层)上进行上采样来得到更高分辨率的图谱, 然后将这些上采样之后的feature map 与 自下而上(自浅而深)的特征图谱通过横向连接的方式拼接在一起.(横向连接的feature map的size是一样大的). 那些bottom-up frature map 具有较为低级的语义信息(低级是指抽象程度低), 但是这些图谱的位置精度更高, 因为它们经过的下采样次数更少. 下面的图3显示了构建top-down feature map的模块. 对于一个粗糙精度的特征图谱, 首先将其上采样至2倍(为了简单, 直接使用最近邻), 然后将上采样后的特征图谱与对应的自下而上的图谱进行按元素相加合并(合并前自下而上的图谱会将经过1×1卷积降低通道数). 这个过程会一直迭代, 知道最浅的卷积图谱也被合并为止. 在最开始的时候, 最深层的合并是使用1×1卷积来生成待合并的top-bottom feature map的. 最后, 会用3×3的卷积在所有合并后的特征图谱上进行卷积操作, 以此来得到最终的特征图谱(以此来消除上采样的混叠效应) $\{P_2, P_3, P_4, P_5\}$, 他们具有与$\{C_2, C_3, C_4, C_5\}$ 相同的大小. 由于所有层的金字塔卷积都是使用的共享的分类器和回归器, 因此文章固定了特征图谱的通道数量(256). 文章使用的是最简单的网络结构, 同时也存在其他更好的连接设计, 但本文的主要目的是探讨FPN的有效性, 因此没要尝试过多的连接组合. 应用FPN是一种用于在卷积网络内部建立特征金字塔的一般化的解决方案, 下面利用Faster RCNN来证明FPN的有效性. Feature Pyramid Networks for RPNRPN会在最后一层的特征图谱上, 利用 3×3 的卷积核, 生成同样大小的特征图谱, 然后在这个新的图谱上画anchor boxes, 并利用两个1×1的卷积核进行二分类和回归任务. 本文用FPN替换了RPN的3×3的卷积核, 然后依然使用两个1×1的卷积核进行二分类和回归任务. 同时, 因为FPN会结合前面的backone网络的所有卷积段, 因此, 对于每一个金字塔层, 只使用一个固定尺寸的anchors. 本文中, 对于$\{P_2, P_3, P_4, P_5\}$,其anchors的大小分别为 ${32^2, 64^2, 128^2, 256^2, 512^2}$, 每一层anchors的宽高比例为{1:2, 1:1, 2:1}, 因此, 总共具有15个anchors(对于每一个location而言) 训练时的标签赋值策略和FasterRCNN是一样的. Feature Pyramid Networks for Fast RCNN将FPN用于FastRCNN时, 需要在不同的层次上赋予不同尺度的RoI大小.(因为RoI pooling是根据物体在原图中的框决定) 本文宽度为 $w$ ,高度为 $h$ 的RoI 通过如下公式分配到特征金字塔的 $P_k$ 等级上: k = \lfloor k_0 + log_2(\frac{\sqrt{wh}}{224})这里 224 是规范的ImageNet预训练的大小, 而 $K_0$ 是则大小为 $w\times h = 224^2$ 的RoI应该映射到的目标级别. 类似于基于ResNet的Faster RCNN使用 $C_4$ 作为单尺度特征映射, 我们将 $k_0$ 设置为4 (也就是说, 与图片一样大的RoI会映射到 $P_1$ 的特征图谱上). 上式表明, 如果RoI的尺度变的更小(如224的0.5倍), 那么该RoI就应该映射到分辨率更高的金字塔图谱上(如 $k=3$ ).(也就是说不同大小的RoI会映射到不同金字塔层级的特征图谱上, 总的来说, 越小的RoI, 会映射到更浅层的特征图谱上, 因为太深的图谱可能已经将小物体信息过滤掉了) 文章将预测器(分类和坐标回归)应用到所有金字塔层级的RoI上面. 需要注意, 预测器在所有层级上的权重都是共享的. . 在ResNet中, 会在conv4的特征图谱上在加上一个conv5, 但是本章已经将conv5用于构建特征金字塔. 所以和ResNet不同, 文章很直接的利用RoI pooling来获取 $7\times 7$ 的特征 (注意不是基于滑动窗口的检测器, 这一点和YOLO差不多), 并且会使用2层1024维的隐藏层(后街ReLU), 然后才会送入最终的预测器层当中(分类和位置回归). 实验用COCO 80ktrain和35k val进行实验. 所有的网络均在ImageNet1k上预训练. 实验细节: 输入图片的尺寸被resize, 其最短边长为800像素. 8块GPU同步训练, 每个GPU的minibatch为两张图片, 每张图片的anchors为256. weight decay为0.0001, momentum为0.9. learning rate 开始的30k图片是0.02, 之后是0.002. 训练时包含了那些处于image之外的anchor boxes(Faster选择忽略). 消融实验实验结果表明, RPN结构可以提高检测的AR(Average Recall)指标, 尤其是在面对小物体和中等物体等多尺度物体时, 会显著提高AR指标. top-down 的特征图谱加强可以使得特征图谱具有很强的语义特征信息和更好的分辨率.(原始的特征图谱之间的语义鸿沟更大, 层与层之间的联系比较简单粗糙) 虽然top-down方式的特征图谱具有很强的语义特征信息和更好的分辨率效果, 但是有过经过不断的降采样和上采样过程, 该特征图谱的位置信息可能会变得不够精确. lateral connections 同时结合具有精确位置信息的特征图谱和具有强语义信息的图谱, 进而达到更好的效果. Pyramid结构的重要性: 如果只在最后一层特征图谱 $P_2$ 上进行检测, 这就像是Faster RCNN的单尺度方法一样, 所有的anchors都在最后一层图谱上, 这种变体比Faster RCNN好但是比FPN差. 直观上来说, 在所有特征层上进行检测, 对不同尺度的物体的鲁棒性要更好. 利用Fast/Faster RCNN进行目标检测利用AP(Average Precision)指标对FPN进行验证 在Faster RCNN上使用FPN, mAP提高了 2%, 其中小物体的mAP提高了2.1%.(固定的候选区域集合) 在面对consistent proposals时(因为RPN和Fast RCNN要共享权重,所以会不断迭代训练), FPN比Faster RCNN的AP高 2.3 点, 比AP@0.5高 3.8 点. FasterRCNN中RPN和FastRCNN的权重共享大约可以提升mAP值0.5左右(0.2~0.7), 同时, 权重共享也可以降低预测时间(0.148 vs 0.172, ResNet50, M40 GPU因为不用计算两个不同的权重参数, RPN与Fast RCNN用的是一个权重参数). FPN没有使用很多流行的提升精度的方法, 如迭代回归, 难样例挖掘, 上下文建模, 数据增强等等. 但是FPN仍然在目标检测, 实例分割, 关键点检测等多项任务上刷新了最高分. 如果使用这些trick, 理论上会获得更高的精度. FPN是一种通用的特征提取方法, 他同样也适用于实例分割任务, 并且可以取得很好的效果.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学题]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E6%95%B0%E5%AD%A6%E9%A2%98%2F</url>
    <content type="text"><![CDATA[概率论矩阵论线性变换的矩阵为什么强调在这组基下矩阵的奇异值和特征值有什么相似之处和区别之处?奇异值分解把线性变换清晰地分解为旋转, 缩放, 投影这三种基本线性变换. 首先, 矩阵是对线性变换的表示, 确定了定义域空间与目标空间的两组基, 就可以很自然的得到该线性变换的矩阵表示 高数一阶矩和二阶矩是什么?期望, 方差 有一个很大的二维数组, 数组里面大部分都是nan值, 有一小部分不是nan值, 求这些不是nan值之间的欧式距离, 并存到另一个数组中.# 正态函数的随机性是怎么实现的均匀分布随机函数是怎么实现的如何用均匀分布来实现正态分布的随机函数技巧题a,b~U[0,1]，互相独立求Max(a,b) 期望 #]]></content>
      <categories>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[面试-面经汇总]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E9%9D%A2%E7%BB%8F%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[优秀面经https://www.cnblogs.com/huanyi0723/p/8470866.html https://www.nowcoder.com/discuss/78195 http://www.zheyibu.com/article/5626.html https://www.jianshu.com/p/6671232cec79 https://www.zhihu.com/question/62482926 在多线程和大量并发环境下，如果有一个平均运行一百万次出现一次的bug， 你如何调试这个bug。解决Bug，第一步就是重现，第二步定位以及Reduce，第三步再来解。所以，不管百万次还是十万次，首先要重现出来，然后找出重现出来的计算机状态。计算机不会欺骗人，每一个问题出来肯定是有原因的，唯一要做的就是如何把这个计算机状态信息还原出来，你可以使用log跟踪等，怎么纪录还原都是工程师的选择。而若能把相关的状态信息拿到，剩下的就是定位是哪里的问题，而这时候最好的就是模拟和Reduce，把问题缩小，排除其它信息干扰。模拟与Reduce成功以后，再想办法解决，然后再来估计解决问题的难度与成本问题等，有些BUG我们是知道，但是解决太麻烦了，影响也不大，就放着。 作者：蓝色链接：https://www.zhihu.com/question/43416744/answer/95944740来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 另一方面，从管理上应该要考虑 bug 的严重性与成本／时间的问题。如果最终能找出问题，需要研究怎样防范相似的 bug。 这个bug很难重现，这个时候你要怎么处理或者重现呢。有一个类指针，指向类实例化的对象，在这个对象程序的运行过程中，程序崩溃了，后来发现是这个类指针的虚函数表被破坏了，现在如何定位这个问题。需要先限定编译器和环节，比如，virtual table 在 Linux 下 GCC 4.9 的实现就是放在read only 段 .rodata，怎么可能被修改？好，就算可以被修改，我第一反应就是上GDB与Valgrind，被破坏的原因很多，你不让我调，我怎么跟你继续说下去，不如直接给我代码，我调给你看？ 那你首先准备一个这样的代码？ 作者：蓝色链接：https://www.zhihu.com/question/43416744/answer/95944740来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 模型压缩cuda作者：oncecoder链接：https://www.nowcoder.com/discuss/23418来源：牛客网 面试官：看你简历上写熟悉CUDA，你能具体讲讲吗。 我：写过图片的resize,padding,卷积，提取hog特征等的gpu代码（kernel函数），效果还不错。 面试官问：具体说说怎么做到提升速度的。 我：把处理安排到gpu的每个thread上。 面试官：那看来你就相当于简单的利用了gpu的多核的特性？ 我一听感觉面试官不是很满意，于是扯了扯：还用了share_memory,const_memory等来提升速度，用了原子操作等来保证安全性。 面试官：你能讲讲使用shared memory为什么快吗？ 我：在某些应用场景下会快，一来和使用场景有关，讲了下哪些场景用这个会好一些，二来可能是硬件方面的原因吧，硬件原理方面的我也不清楚。 然后面试官从内存的金字塔结构，以及gpu的一些特性给我展开讲了很多，这个面试官感觉是gpu方面的行家，人非常好，感觉给我做了个讲座。。。 然后面试官问：你知道warp这个概念吗？ 我说知道，就是gpu底层同时执行的指令数量，现在一般是32.所以在写内核函数的时候，thread的数目最好是32的倍数。其他的不太清楚。 面试官好像点了点头，又给我balabala做了一次讲座。。。。 面试官问：假如要申请一大片空间，一次性申请这么大的，和分多次申请很多小的，但总数一样，哪个快，为什么。 我：在做项目的时候遇到过这种情况，前者会快很多，然后说了原因（答得不太标准，就不误导大家了） 面试官：其实cpu和gpu在这方面是一样的， 都会维护一个表什么的，记不太清楚了。 面试官：怎么看gpu使用情况。我：nvidia-smi(我用的是nvidia的卡) face++ 面经作者：一一后链接：https://www.nowcoder.com/discuss/119900来源：牛客网 一面（30分钟+ 撸项目（很细节，第一个项目每一步都要问为什么不用某个其他的方法） 讲讲adaboost和random frost的相同之处和不同，各自应用范围，实际应用选择 对SVM的理解，简单推导SVM，为什么要用对偶问题（二次规划+核化）具体讲一下为什么要核化，核化的过程 讲一下DL中目标检测的大类和特点（one stage、two stage）为什么two stage比one stage慢，为什么one stage比two stage精度高？one stage在哪些具体方面检测精度不高（ROI+default box的深层理解） 讲讲梯度消失问题及其应对方案（BN、Relu、初始化） 讲讲BN的细节（过程，公式，作用）为什么BN可以加快优化算法的速度 有什么问题 总结：问得很深入，基本都要非常理解才行，提到某个细节之后可能会深入问这个细节更细节的东西，千万别在回答的时候给自己挖坑。 二面：（40分钟+ 自我介绍；链表的倒数第k个结点（双指针） 应用场景题 抛一个不均匀的硬币，设计策略能得到1/2的概率（抛两次）如果要求得到1/3和2/3呢？设计策略（抛四次，我想着抛6次，小哥哥提醒了） 给出一个0到n的随机数生成器，设计策略，让不得到x的条件下，得到其他数的均匀分布（只能生成一次）（hash映射，但是我找不到合适的映射函数，小哥哥提醒了）扩展：不得到两个数呢？m个数呢？（一样） 房子500万，每年涨10%，程序员工资100万，不涨，问多少年能全款买房（几秒钟估算了一下，永远买不起…）（总觉得小哥哥在暗示我什么） 堆介绍，插入元素时调整的时间复杂度（变成二叉树，递归定义）堆排序、其他排序方法介绍和特点（按时间复杂度分了三种去介绍），最常用哪种 有什么问题（小哥哥建议多看些ML的实际场景（我其实想问智商怎么提升…） 总结：二面考基本功，数学算法和ML的熟练运用能力。 三面（院长大佬面）（挂） 自我介绍 用到深度学习的项目（大部分时间聊项目） 深度学习的前沿知识（最新的网络结构、精度最高的目标检测模型等） 有什么问题（大佬很委婉地劝我说他们主要收深度学习方向的…） 作者：jucic 链接：https://www.nowcoder.com/discuss/108078 来源：牛客网 CV岗： 一面： 用C++将一个类改造成线程安全的类 凸优化了解吗 SLAM里面闭环检测是什么怎么做 用深度学习做SLAM了解吗 兼职offer上一原题 交叉熵是什么 二面: 链表反转 快排 三面（院长面） 一直在聊项目 算法细节部分被怼的很厉害 某个函数只能随机产生0或1，利用这个实现一个函数能等概率的返回1-n之间的数，手撕实现代码 一个文件有一亿条整数数据，算一下占用多大磁盘，里面有几十个重复的数据，怎么找出来，内存占用不要超过本身的文件大小 a) 概率是抽样的题目居多，计算正确，错误或者抽中没抽中的概率，与腾讯考察的要求差不多，但稍难其中一题，第一题，问试卷中的10道题，每到5个选项，如果瞎猜，每道题的数学期望是多少，如果每道题猜错的概率是92%，那么每道题的数学期望是多少？b) 计算甲乙两地距离的问题，甲乙分别从AB两地相向而行，甲乙速度比是常数，第一次相遇在距离甲地80KM处，分别到达对方起点后，再返回来相向而行，第二次相遇在距离甲地40KM处，计算甲乙两地相距多远的问题c) 研究基础，问到了RANSAC抽样的问题，将它与概率结合，抽取两个样本，抽取10次，问抽样概率d) ICCV会议2013与2015年分别是在哪里开的e) 选做题：写HOG的伪代码；关于图像模糊问题；问常见的跟踪方法有哪些，简述他们的优缺点，举一个近五年CVPR中流行的跟踪方法，写出它的思想 作者：牛客网链接：https://zhuanlan.zhihu.com/p/29695077来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 中国平安 - 实习生(上海/北京) 1.卷积正向和反向传播 2.Caffe源码 3.斐波那契 memcpy 4.pooling反向 5.项目介绍 6.override overload 纯虚函数等 阿里巴巴 - 2017.3.23 - 实习生 - idst - 非内推 1.linux 修改环境变量 2.sql语句 3.gbdt xgboost区别 4.kaggle项目 30min 5.融合方法，改进 阿里巴巴 - 2017.3.28 - 实习生 - 淘宝搜索 - 内推一面 1.项目介绍(30分钟)—项目过程，融合方法，训练方法，augmentation等 2.batch normalization 3.有没有了解其他机器学习算法 4.介绍一个熟悉的算法(决策树) 5.在线写线性回归 6.对深度学习框架有没有了解，还是只是停留在使用的层面 7.有没有什么想问的 阿里巴巴 - 2017.3.31 - 实习生 - 淘宝搜索 - 内推二面 1.项目介绍 2.kd-tree 3.开放问题 100w商品 50个推荐窗口，怎么安排推荐 腾讯 - 2017.4.10 - 实习生非内推 - 优图实验室 - 一面 1.项目介绍 2.计算卷积核参数数量 3.如何处理深度学习overfitting 4.如何在测试中，加速1000倍(不改变网络结构) 5.pooling层的作用，以及为什么会选择maxpooling 6.有没有从头开始训练一个模型 vgg16 resnet googlenet收敛问题 今日头条 - 2017.4.11 - 日常实习生非内推 - 一面 1.项目介绍 2.如何训练深度学习网络 3.如何处理样本分布不均衡的问题 4.手写代码-反转链表 5.手写代码-前序遍历 今日头条 - 2017.4.11 - 日常实习生非内推 - 二面 1.项目介绍（为什么不尝试xgboost以外的模型） 2.xgboost gbdt区别 3.深度学习训练方法 4.改进方法 5.caffe框架结构 6.手写代码-旋转数组找出最大数字 今日头条 - 2017.4.13 - 日常实习生非内推 - 三面 1.前两面反应较好，聊天 2.对前两个面试官有什么看法 3.有什么问题 #腾讯挺坑的，一面过了，二面面试官打电话确认了面试时间，收到了确认邮件，然后鸽了 腾讯游戏 - 校招内推 - 一面 1.实习介绍 2.介绍svm，为什么要求对偶 3.介绍一个熟悉的算法 4.全局变量 局部变量存储位置不同，静态变量初始化，生存周期 5.python多线程的实现，死锁 6.优化算法 sgd 牛顿法。为什么牛顿法快？及其缺点？ 网易 - 内推校招 - 人工智能事业部 - 一面 1.实习介绍 2.kaggle 深度学习项目介绍 3.几个框架对比 4.模型融合策略和方法 网易 - 内推校招 - 人工智能事业部 - 二面 1.项目介绍，讲你最好的项目 2.实习介绍 3.svm手推 4.kaggle融合的策略和方法 #前3面反映较好，加面 网易 - 内推校招 - 人工智能事业部 - special 加面 1.最好的项目介绍 2.batch normalization算法 3.实习经历 4.cnn现在发展以及不足 5.说对游戏ai感兴趣 - alphago的技术点，强化学习等 华为 - 内推校招 - 1,2,3面 #略 #Nvidia Deeplearning software 面试官很客气，提前定好这次面试时长40分钟 Nvidia - 内推校招 - 一面 1.项目介绍 30min 2.编程题2道 3.过拟合欠拟合 以及其背后本质，偏差方差角度如何理解 #Sensetime 商汤科技 每面30min #号称最难进公司之一？ Sensetime - 2017.9.11 - 校招内推 - 计算机视觉&amp;深度学习 - 一面 1.kaggle比赛 问的比较详细 包括 data augmentation， KNN的trick， 模型融合等 2.实习经历 3.有什么问题 Sensetime - 2017.9.11 - 校招内推 - 计算机视觉&amp;深度学习 - 二面 1.kaggle比赛 2.头条实习 3.python set-list转化 4.caffe框架结构，learning rate设置 5.第K大的数 6.sgd adam区别 7.resnet vgg区别 8.python 变量拷贝规则 9.有什么要问的 Sensetime - 2017.9.11 - 内推校招 计算机视觉&amp;深度学习 - 三面 1.头条实习 比较详细以及为什么头条推荐这么厉害 #面试官是在做dl+推荐，所以比较关心头条所做的东西 2.熟悉什么框架 3.喜欢什么方向，cv还是推荐等，以及个人认为他们的前景 4.学术型硕士还是工程型硕士？ 5.有什么问题 阿里巴巴 - 2017.9.13 - 校招 - 初面 1.头条实习 ——- 特征维度，为什么时延很低，在头条做了哪些，头条的算法 2.深度学习和传统机器学习 3.深度学习最近的发展和技术突破点 4.GBDT是什么 — 加法模型 5.为什么现在推荐可以使用GBDT的内部结点当做LR的特征 — 特征选择和子集评价，还是stack模型融合？ 6.RF GBDT区别 — 方差偏差理论，bagging&amp;boost区别 7.GBDT xgboost区别 —泰勒二阶，并行化，正则项 8.手写MergeSort 9.熟悉什么语言 10.用什么框架 11.深度学习正则化 12.GBDT分布式如何实现 #没有了解过，然后简单说了自己的想法，面试官给我讲了许多这方面 阿里巴巴 - 2017.9.15 - 校招 - 终面 1.头条实习 ——- 模型介绍 2.GBDT xgboost区别 3.kaggle比赛 4.一个整数数组中，寻找3个数乘积最大 5.GBDT与bagging等方法区别 6.linux常用指令 sort grep等 阿里巴巴 - 2017.9.15 - 校招 - 加面 #压力面？ 1.头条实习ffm替换skip gram模型，为什么？效果如何？为什么会有提速效果？线上如何部署等 2.头条所做？训练两个大模型，效果如何？ 3.kaggle比赛 4.vgg16 resnet googlenet区别 5.手写代码-旋转数组找出最小数字 #其余记不清了 大疆 - 2017.9.17 - 校招 - 初面 1.头条实习 2.kaggle项目 待看http://www.cnblogs.com/mrxsc/articles/6266584.html https://zhuanlan.zhihu.com/p/29633019 【链接】依图面试经历（三轮面试，已得offer）https://zhuanlan.zhihu.com/p/27842581 https://www.nowcoder.com/discuss/73739 https://zhuanlan.zhihu.com/p/38067051 【链接】依图科技暑期实习生面试经验https://blog.csdn.net/wslf123/article/details/79924413]]></content>
      <categories>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SSD-Single Shot MultiBox Detector]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-SSD-ECCV2016%2F</url>
    <content type="text"><![CDATA[文章: SSD: Single Shot MultiBox Detector作者: Wei Liu, Dragomir Anguelov, Dumitru Erhan 核心亮点(1) 在不同层的feature map上使用网格法获取候选区域: 某种程度上SSD可以看做是YOLOv1和FasterRCNN的结合, 在不同layer的输出的不同尺度的feature map上划格子, 在格子上利用anchor思想. 因此, . (YOLOv2也使用了Anchor的思想) (2) 使用了数据增广, 难样例挖掘, atrous算法等trick大幅度提升精度和速度这个其实算不上亮点, 只不过作者确实使用这些技术提升性能不少 (3) 相对于那些需要object proposals的两阶段模型, SSD方法全完取消了 proposals generation, pixel resampling 或者 feature resampling 这些阶段 论文细节背景介绍目前, 虽然Faster RCNN取得了很好的检测精度, 但是对于普通设备来说, Faster RCNN过于庞大, 计算时间太久, 不足以达到实时监测. YOLO虽然速度很快, 但是精度太低了, 达不到基本要求. SSD的出现正是为了解决这些问题, 确定在不丢失过度精度的前提下, 提升检测的速度. Single Shot Detector(SSD) 根据上图, 简单说一下SSD的关键要素 输入: 图像以及每个物体对应的ground truth boxes 多特征图谱的anchor思想: 在不同尺度的特征图谱上(如上图的4×4和8×8), 对每个位置上设置多个具有不同大小和长宽比的boxes, 称之为 default boxes. 输出: 对于每一个default box, 都会输出4个相对位移用于边框回归, 同时会输出所有类别的预测概率 匹配: 在预测阶段, 需要将这些 defaults boxes 与 gt boxes 匹配. 在上图中, 最终有两个框(蓝色)与猫所在框匹配, 有一个框(红色)与狗所在框匹配. 这三个框被标记为正样本, 其余剩下的框都被标记为负样本. (可见负样本数量远远大于正样本数量) 损失函数: 边框回归损失(Smooth L1) 和 类别置信度损失(softmax 交叉熵损失) 的权重和. ModelSSD 会产生固定数量的bounding box, 以及每个bounding box的各个类别的预测概率, 最后会使用NMS算法生成罪最终的检测结果. 多尺度feature map: 在卷积网络的不同卷积层后面添加convolutional feature layers, 并且在每一层中都会进行检测任务. Convolutional predictors for detection: 每一个添加的特征层(或者在基础网络里的特征层), 都可以使用一系列的卷积核产生固定大小的predictions, 如图2所示. 对于一个大小为 $m\times n$, 具有 $p$ 通道的特征层, 使用的卷积核就是 $3\times 3\times p$ , 之后会产生相对于 default box 的预测坐标, 已经每一类的预测置信度. 对于特征图上 $m\times n$个位置, 在对每个位置使用卷积核之后, 都会产生一个输出值. (YOLO架构则是用一个全连接层来代替这里的卷积层) Default boxes and aspect ratios: 每一个feature map的cell都会与一系列 default bounding box 相关联. 对于每一个cell来说, 模型会预测与之关联的 default bounding box 相对于该cell的偏移量, 同时会预测这些boxes对应的每一类的出现概率. 具体来说, 对于一个位置上的 $k$ 个boxes中的每一个box, 都会计算出这个box的4个相对位移值 $c$ 个类别的score. 因此, 对于一个feature map来说, 总共需要 $(c+4)\times k$ 个卷积核, 最终该对于大小为 $m\times n$ 的 feature map来说, 其输出结果数量为: $(c+4)\times k\times m\times n$. TrainingSSD与二阶段检测方法在训练时的区别: SSD训练图像中的GT信息需要赋予到那些固定输出的boxes上面(也就是说不仅要使用bounding box的坐标, 还有把类别标签也与每一个box绑定, 这种方法在YOLO, FasterRCNN(只分前后景), MultiBOx中都有用到). Matching strategy: 只要预测框与gt box之间的 jaccard overlap(就是交并比) 大于一个阈值(0.5), 就认为是配对成功, 反之, 认为是背景. Training objective: SSD的损失函数源自于MultiBox的损失函数, 但是SSD对其进行拓展, 使其可以处理多个目标类别. 用 $x_{ij}^p={1,0}$ 表示第 $i$ 个default box 与类别 $p$ 的第 $j$ 个gt box匹配, 否则若不匹配的话, 则 $x_{ij}^p = 0$. 根据上面的策略, 一定会有 $\sum_i x_{ij}^p &gt;1 $ 的情况出现, 意味着对于第 $j$ 个gt box, 很有可能有多个default box与之匹配. 总的目标函数是由Localization loss(loc) 和 confidence loss(conf) 的加权求和得到的: L(x,c,l,g) = \frac{1}{N} (L_{conf}(x,c) + \alpha L_{loc}(x,l,g))式中: $N$是与ground truth box 相匹配的 default boxes个数(如果N为0, 则将该项loss设为0) localization loss(loc) 是 Fast RCNN 中的Smooth L1 loss, 用于对bounding box进行回归, 与Faster RCNN一样, 我们会将真实的gt box坐标值转换成相对于default box( $d$ )中心 $(cx,cy)$ 的偏移量和缩放度, 预测的时候也是对偏移量和缩放度进行预测: L_{loc}(x,l,g) = \sum_{i\in Pos}^N \sum_{m\in\{cx,cy,w,h\}} x_{ij}^k smooth_{L_1}(l_i^m - \hat g_j^m) confidence loss(conf) 是 Softmax 交叉熵loss L_{conf}(x,c) = -\sum_{i\in Pos}^N x_{ij}^p log(\hat c_i^p) - \sum_{i\in Neg} log(\hat c_i^0), 其中, \hat c_i^p = \frac{exp(c_i^p)}{\sum_p exp(c_i^p)} 权重项 \alpha, 默认设置为1. Choosing scales and aspect ratios for default boxes: 大部分CNN网络在越深的层, feature map的尺寸会越来越小, 这样做不仅仅是为了减少计算与内存的需求, 还有个好处就是, feature map往往具有一定程度的平移和尺度不变性.为了处理不同尺度的物体, OverFeat和SPPNet都是通过在feature map上进行不同尺度的pooling, 然后再将这些pooling综合进行获取固定长度的特征向量输出. SSD采用的策略是使用同一个网络中不同层的feature map, 这些feature map也是不同尺度的, 同时也具有共享参数的好处. 本文使用了 8×8 和 4×4 大小的feature map. 假如feature maps数量为 $m$, 那么每一个feature map中的default box的尺寸大小计算如下: s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1), k\in [1,m]上式中, $s_{min} = 0.2 , s_{max} = 0.9$. 对于原文中的设置 $m=6 (4, 6, 6, 6, 4, 4)$, 因此就有 $s = \{0.2, 0.34, 0.48, 0.62, 0.76, 0.9\}$然后, 几个不同的aspect ratio, 用 $a_r$ 表示: $a_r = {1,2,3,1/2,1/3}$, 则每一个default boxes 的width 和height就可以得到( $w_k^a h_k^a=a_r$ ): w_k^a = s_k \sqrt{a_r}h_k^a = \frac{s_k}{\sqrt {a_r}}对于宽高比为1的 default box, 我们额外添加了一个 scale 为 $s_k’ = \sqrt{s_k s_{k+1}}$ 的 box, 因此 feature map 上的每一个像素点都对应着6个 default boxes (per feature map localtion).每一个default box的中心, 设置为: $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{f_k})$, 其中, $|f_k|$ 是第 $k$ 个feature map的大小 $i,j$ 对应了 feature map 上所有可能的像素点.在实际使用中, 可以自己根据数据集的特点来安排不同的 default boxes 参数组合 这种多尺度的feature maps策略, 可以适应不同大小的物体的检测, 如下图, 对于体积较小的猫来说, 它会与 8×8 feature map 的default box匹配, 而对于体积较大的狗来说, 它会与 4×4 feature map 的default box 匹配, 而其余的都会被看做是负样本. Hard negative mining: 在一系列的matching之后, 大多数default boxes都会被认为是负样本. 这会导致正负样本不均衡问题. 对于, SSD会将所有的负样本按照scores loss的高低进行排序(损失高的优先级在前), 然后每次只选择顶端的负样本进行训练, 负样本与正样本之间的比例为 3:1. Data augmentation: 为了增强对物体多尺度和形状的鲁棒性, 对于每一张训练数据, 会随机使用下列数据增广技术: 使用整张图片 采样一个patch (这里的patch我认为就是裁剪子区域的感觉), 使其与物体之间的最小交并比为0.1, 0.3, 0.5, 0.7, 0.9 随机采样一个patch 采样的patch与原始图像大小的比例为[0.1, 1], aspect ratio在 0.5 到 2 之间. 当gt box的中心出现在采样的patch中时, 我们保留重叠部分. 在这些采样步骤之后, 每一个采样的patch被resize到固定的大小, 并且以0.5的概率被水平翻转. 实验 hole filling algorithm??//TODO SSD模型对于bounding box的size非常敏感, 也就是说, SSD对于小物体目标较为敏感, 在检测小物体目标上表明交差, 主要也是因为对于小目标而言, 经过多层卷积之后, 剩余信息过少导致的. 数据增广对于结果的提升非常明显 多feature map对结果的提升是有很大帮助的 使用较多的default boxes, 效果较好(但也不能太多) atrous: atrous是精度有一定提高, 同时对速度有很大提高(约20%) Inference time: SSD会产生大量的bounding boxes, 使用NMS算法只留下top200 (这一步SSD300在VOC20类的每张图像上, 需耗时2.2msec) 上图看起来SSD300比YOLO还要快, 但实际上YOLO的网络层数是24层, 而SSD的层数是12层, 这样比起来有点不太公平( 但是层数多居然精度没SSD高, 这也值得吐槽, 但是个人觉得这是因为YOLOv1的设计比较粗糙, 很多trick没有使用导致的, 所以看看YOLOv2, 和YOLOv3的版本, 结果还是挺不错的)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉-模型结构汇总]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[https://ethereon.github.io/netscope/quickstart.html]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[STL之priority_queue]]></title>
    <url>%2Fz_post%2FCpp-STL%E4%B9%8Bpriority-queue%2F</url>
    <content type="text"></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《STL源码剖析》]]></title>
    <url>%2Fz_post%2FCpp-Book-STL%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[第一章 STL概论与版本简介STL 的提出是为了提高代码的复用性, 提升编程效率. STL 提供六大组件, 彼此可以组合套用: 容器(containers): 各种数据结构, 如 vector, list, deque, set, map 等 class template; 算法(algorithms): 各种常用算法如 sort, search, copy, erase 等 function template; 迭代器(iterators): 扮演容器与算法之间的胶合剂, 从实现角度看, 迭代器是一种将 operator*, operator-&gt;, operator++, operator-- 等指针相关操作予以重载的 class template; 仿函数(functors): 行为类似函数, 可作为算法的某种策略. 配接器(adapters): 一种用来修饰容器(containers), 仿函数(functors), 或者迭代器(iterators)接口的东西. 例如, STL 提供的 queue 和 stack, 虽然看似容器, 其实只能算是一种容器配接器, 因为它们的底层完全借助 deque 完成. 改变 functor 接口者, 称为 function adapter, 改变 container 接口者, 称为 container adapter, 改变 iterator 接口者, 称为 iterator adapter. 配置器(allocators): 负责空间配置与管理, 从实现角度来看, 配置是是一个实现了动态空间配置, 空间管理, 空间释放的 class template. 以上六者的关系为: Container 通过 Allocator 取得数据存储空间, Algorithm 通过 Iterator 存取 Container 内容, Functor 可以协助 Algorithm 完成不同的策略变化, Adapter 可以修饰或套接 Functor. 4.8 priority_queue4.8.1 priority_queue 概述priority_queue 首先是一个队列, 因此它允许在底端加入元素, 并从顶端取出元素, 除此之外别无其他存取元素的途径. priority_queue带有权值观念, 其内部的元素并非依照插入顺序排序, 而是按照元素的权值进行排列, 权值较高者, 排在最前面. 缺省情况下 priority_queue 利用一个 max-heap 完成, 也即 top() 元素表示的是队列中值最大的元素. 4.8.2 priority_queue 定义完整列表priority_queue 的实现在缺省情况下是以 vector 为底部容器, 再加上 heap 相关处理规则, 所以其实现非常简单, 也因此, 我们通常并不认为 priority_queue 并非是一种新的容器, 而只是把它归类为 container adapter(容器配接器). 1// priority_queue 内部完整实现 4.8.3 priority_queue 没有迭代器由于 priority_queue 只有顶端元素 (权值最高者) 才有机会被外界使用, 因此, priority_queue 不提供遍历功能, 也不提供迭代器. 4.8.4 priority_queue 测试实例1//TODO]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-roidb数据结构]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-roidb%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[roidb数据结构roidb的类型是list, 其中的每个元素的数据类型都是dict, roidb列表的长度为数据集的数量(即图片的数量), roidb中每个元素的详细情况如下表所示: for entry in roidb 数据类型 详细说明 entry[&#39;id&#39;] int 代表了当前image的img_id entry[&#39;file_name&#39;] string 表示当前图片的文件名(带有.jpg后缀) entry[&#39;dataset&#39;] string 指明所属的数据集? entry[&#39;image&#39;] string 当前image的文件路径 entry[&#39;flipped&#39;] bool 当前图片是否进行翻转 entry[&#39;height&#39;] int 当前图片的高度 entry[&#39;width&#39;] int 当前图片的宽度 entry[&#39;has_visible_keypoints&#39;] bool 是否含有关键点 entry[&#39;boxes&#39;] float32, numpy数组(num_objs, 4) num_objs为当前图片中的目标物体个数, 4代表bbox的坐标 entry[&#39;segms&#39;] 二维列表[[],[],…] 列表中每个元素都还是一个列表, 其中存储着每个物体的ploygon实例标签 entry[&#39;gt_classes&#39;] int32, numpy数组(num_objs) 指明当前图片中每一个obj的真实类别 entry[&#39;seg_areas&#39;] float32, numpy数组(num_objs) 代表当前图片中每一个obj的掩膜面积 entry[&#39;gt_overlaps&#39;] float32, scipy.sparse.csr_matrix数据(num_objs, 81) 代表每一个obj与81个不同类别的overlap entry[&#39;is_crowd&#39;] bool, numpy数组(num_objs) 代表当前掩膜是否为群落 entry[‘box_to_gt_ind_map’] int32, numpy数组(num_objs) 该列表存储着box的顺序下标值, 同样是一维数组, 直接拼接,将每一个roi映射到一个index上, index是在entry[‘gt_classes’]&gt;0的rois列表的下标 combined_roidb_for_training() 方法在目标检测类任务中, 有一个很重要的数据结构roidb, 它将作为基本的数据结构在数据队列中存在, Detectron 的数据载入类 RoIDdataLoader 也是将该数据结构作为成员变量使用的, 因此, 有必要对这个数据结构展开分析. 首先, 在运行训练脚本时, 就会调用到 detectron/utils/train.py 中的 train()函数, 而train()函数内部又会调用当前文件的add_model_training_inputs() 函数, 在这个函数内部, 就会调用到 detectron/datasets/roidb 文件中的 combined_roidb_for_training() 函数, 该函数的返回值正是roidb, 这是贯穿整个训练过程的训练数据, 故我们对此函数进行分析. 该函数代码解析如下: 12345678910111213141516171819202122232425# detectron/datasets/roidb.py# 加载并连接一个或多个数据集的roidbs, along with optional object proposals# 每个roidb entry都带有特定的元数据类型, 对其进行准备工作后进行训练def combined_roidb_for_training(dataset_names, proposal_files): def get_roidb(dataset_name, proposal_file): # 注意 dataset_name 没有 's' # from detectron.datasets.json_dataset import JsonDataset # 可以看到, roidb 是利用JsonDataset类对象的get_roidb()方法获取的 # 因此, 我们先在下面看一下这个类的实现细节 ds = JsonDataset(dataset_name) roidb = ds.get_roidb( gt=True, proposal_file=proposal_file, crowd_filter_thresh=cfg.TRAIN.CROWD_FILTER_THRESH ) if cfg.TRAIN.USE_FLIPPED: logger.info("Appending horizontally-flipped training examples...") extend_with_flipped_entries(roidb, ds) logger.info("Loaded dataset: &#123;:s&#125;".format(ds.name)) return roidb if isinstance(dataset_names, basestring): #... #... get_roidb() 方法在上面的函数中我们可以发现, combined_roidb_for_training函数内部又定义了另一个函数get_roidb(), 而该函数主要是基于detectron/datasets/json_dataset.py中的JsonDataset类及该类的成员方法get_roidb实现的, 因此, 我们先跳到json_dataset.py文件中去看看这个类的内部实现是怎样的: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# detectron/datasets/json_dataset.pyclass JsonDataset(object): # 这个类的设计主要是基于COCO的json格式数据集 # 当我们需要训练自己的数据集时, 最好的方式就是将自己的数据集的格式改为 # COCO数据集的json格式, 这样一来, 我们就无需重写数据载入代码了. def __init__(self, name): assert dataset_catalog.contains(name), \ "Unknown dataset name: &#123;&#125;".format(name) assert... #... # 准备数据集的类别信息 category_ids = self.COCO.getCatIds() # 1~80, 对应80个类 # coco的loadCats函数, 必须指定需要加载的cat的id, 否则返回空列表 # 若指定后, 则返回id对应的类别信息, 每个类别信息是一个字典, 包括'name','id','supercategory'三个字段 # 获取每个类的名字, person, bicycle,bus等等, 返回的名字在列表中的位置与id在cat_ids列表中的位置一致 categories = [c['name'] for c in sefl.COCO.loadCats(category_ids)] # 建立类别的name 与 id之间的对应关系, 其中cat_name为key,cat_id为值 self.category_to_id_map = dict(zip(categories, category_ids)) # 注意, 没有'__background__' self.classes = ['__background__'] + categories # 将'__background__'添加到categories类别名字列表中 self.num_classes = len(self.classes) # coco下标最大值为90,但实际上只有80个类, 有的地方跳过了, 因此id不是连续的, self.json_category_id_to_contiguous_id = &#123; v: i + 1 # key为coco的非连续id, value为1~80的连续id, 均为整数 for i, v in enumerate(self.COCO.getCatIds()) &#125; self.contiguous_category_id_to_json_id = &#123; v: k # key为1~80的连续id, value为coco的非连续id, 均为整数 for k, v in self.json_category_id_to_contiguous_id.items() &#125; self._init_keypoints() # 调用类内的keypoints初始化方法. def get_roidb( self, gt=False, proposal_file=None, min_proposal_size=2, proposal_limit=-1, crowd_filter_thresh=0 ): """ 返回json dataset对应的roidb数据, 提供以下四种选项: - 在roidb中包含gt boxes - 添加位于proposal file里面的特定proposals - 基于最短边长的proposals过滤器 - 基于群落区域交集的proposals过滤器 """ assert gt is True or crowd_filter_thresh == 0, \ "Crowd filter threshold must be 0 if gt " \ "annotations are not included." # 这里调用了COCO的官方API, 关于COCO数据集的结构和标注格式解析, 可以查看我的另一篇文章 # 没有指定筛选条件, 获取数据集标签中所有的图片id image_ids = self.COCO.getImgIds() image_ids.sort() # 将id按照从小到大的顺序排列 # roidb为列表结构, 列表中的每一项是一个字典, 代表着对应imageid的标签内容. # 键值包括:coco_url, license, width, filename, height, flickr_url, id, date_captured roidb = copy.deepcopy(self.COCO.loadImgs(image_ids)) for entry in roidb: # 调用了本类的私有函数 _prep_roidb_entry(), entry为字典. # 主要是为entry赋初值, 占位符等等, 包含box, segms,等各种字段, 详细信息可以看下面的函数解析 # 注意, 这里的字段值都是预测值相关的值, 因此也会局域gt_overlap等字段 self._prep_roidb_entry(entry) if gt: # 如果参数声明是gt信息, 则会调用_add_gt_annotations # 访问标注文件, 以便添加相关字段信息, 具体看下面的相关函数解析 self.debug_timer.tic() for entry in roidb: # 注意, 是单独对每个entry调用该函数, 因此每次会载入指定imgid的相关标签 # 关于_add_gt_annotations函数具体解析可以看后面的部分 self._add_gt_annotations(entry) logger.debug( '_add_gt_annotations took &#123;:.3f&#125;s'. format(self.debug_timer.toc(average=False)) ) if proposal_file is not None: self.debug_timer.tic() # 加载proposals文件到roidb中, 关于此函数的详细解析可以看后文 self._add_proposals_from_file( roidb, proposal_file, min_proposal_size, proposal_limit, crowd_filter_thresh ) logger.debug( '_add_proposals_from_file took &#123;:.3f&#125;s'. format(self.debug_timer.toc(average=False)) ) # 类外部的函数, 用于计算与每个roidb相关的box的类别 _add_class_assignments(roidb) return roidb _prep_roidb_entry() 方法数据准备函数 _prep_roidb_entry() 的实现解析123456789101112131415161718192021222324252627282930313233343536373839404142434445# detectron/datasets/json_dataset.pyclass JsonDataset(object): def __init__(...): #... def get_roidb(...): #... # 该函数主要将空的元数据添加到roidb entry中 def _prep_roidb_entry(self, entry): # entry的'dataset'关键字, 值为self. entry['dataset'] = self im_path = os.path.join(self.image_directory, self.image_prefix+entry['file_name']) assert os.path.exists(im_path), "Image \"&#123;&#125; \" not found".format(im_path) # entry的'image'关键字, 值为当前imageid对应的image路径 entry['image'] = im_path entry['flipped'] = False # 禁止反转 entry['has_visible_keypoints'] = False # 下面entry键的对应值均为空, 暂为占位键 # entry的'boxes'关键字,值为n×4的numpy数组, n代表box的数量,这里暂时为0 entry['boxes'] = np.empty((0,4), dtype=np.float32) entry['segms'] = [] # entry的'segms'关键字, 值为一个列表,暂时为空 # entry的'gt_classes'关键字, 是个一维数组, 维度与box的数量n对应,暂时为0 entry['gt_classes'] = np.empty((0), dtype=np.int32) # 代表掩膜的面积, 供n项, 与boxes数目相对 entry['seg_areas'] = np.empty((0), dtype=np.float32) # TODO, 这里是一个矩阵压缩, 矩阵大小为n×c, c为类别数量, 没太搞懂要压缩成什么? entry['gt_overlaps'] = scipy.sparse.csr_matrix( np.empty((0, self.num_classes), dtype=np.float32) ) # 同样是n行1列, n与boxes数目对应, 表示是否为`一群物体` entry['is_crowd'] = np.empty((0), dtype=np.bool) # shape大小与roi相关, 将每一个roi映射到一个index上 # index是在entry['gt_classes']&gt;0的rois列表的下标 TODO还是不太清楚映射关系 entry['box_to_gt_ind_map'] = np.empty((0), dtype=np.int32) # 关键点信息, 默认情况下不设置 if self.keypoints is not None: entry['gt_keypoints'] = np.empty( (0, 3, self.num_keypoints), dtype=np.int32 ) # 删除那些从json file中获取到的不需要的字段 for k in ['date_captured', 'url', 'license', 'file_name']: if k in entry: del entry[k] _add_gt_annotations() 方法加载标注文件的函数 _add_gt_annotations()的实现解析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# detectron/datasets/json_dataset.pyclass JsonDataset(object): def __init__(...): #... def get_roidb(...): #... def _prep_roidb_entry(self, entry): #... # 该函数将标注文件的元数据添加到roidb entry中 def _add_gt_annotations(self, entry): # 获取指定imgid的annid列表 (对应多个box) ann_ids = self.COCO.getAnnIds(imgIds=entry['id'], iscrowd=None) # 根据annids的id列表, 获取这些id对应的标注信息, objs是一个列表 # 列表中的每一个元素都是一个字典,字典的内容是标注文件中的内容,包含bbox,segmentation等字段 objs = self.COCO.loadAnns(ann_ids) # 下面的代码会对bboxes进行清洗, 因为有些是无效的数据 valid_objs=[] # 存储有效的objs valid_segms=[] # 存储有效的segms width = entry['width'] # 获取entry中的width字段, 代表图片的宽度 height = entry['height'] # 获取entry中的height字段, 代表图片的高度 for obj in objs: # crowd区域采用RLE编码 # import detectron.utils.segms as segm_utils # 用于判断当前的segmentation是polygon编码还是rle编码, 前者是列表类型, 后者是字典类型 # 返回True为polygon编码, 返回Fasle为rle编码 if segm_utils.is_poly(obj['segmentation']): # poly编码必须含有&gt;=3个点才能组成一个多边形, 因此需要&gt;=6个坐标点 # 类似于这样的检查操作只在PLOYGON中存在, 在面对RLE时无需检查, 可以直接接受后面的检查 obj['segmentation'] = [ p for p in obj['segmentation'] if len(p) &gt;=6 ] if obj['area'] &lt; cfg.TRAIN.GT_MIN_AREA: continue # 如果面积不达标, 则认为该标注无效, 不将其加入valid列表 if 'ignore' in obj and obj['ignore'] == 1: continue # import detectron.utils.boxes as box_utils # 将[x1,y1,w,h]的边框格式转换成[x1,y1,x2,y2]的格式 x1, y1, x2, y2 = box_utils.xywh_to_xyxy(obj['bbox']) # 将[x1,y1,x2,y2]的边框坐标限制在图片的[width,height]范围内, 防止越界 x1, y1, x2, y2 = box_utils.clip_xyxy_to_image( x1, y1, x2, y2, height, width ) if obj['area'] &gt; 0 and x2 &gt; x1 and y2 &gt; y1: # 若数据有效, 则加入到列表当中 obj['clean_bbox'] = [x1, y1, x2, y2] valid_objs.append(obj) valid_segms.append(obj['segmentation']) # 将数据的segms存在列表中(RLE/PLOYGON) num_valid_objs = len(valid_objs) # num_valid_objs持有objs的有效个数 # 注意, 下面的数据内容都被初始化为0 # boxes为 有效objs数×4 的numpy数组, 用来表示每个objs的边框坐标 boxes = np.zeros((num_valid_objs,4), dtype=entry['seg_areas'].dtype) # 每个objs的真实类别 gt_classes = np.zeros((num_valid_objs), dtype=entry['gt_classes'].dtype) gt_overlaps = np.zeros( # 形状为 有效objs数×num_class数 的numpy数组, 表示与每个类的IoU大小 (num_valid_objs, self.num_classes), dtype=entry['gt_overlaps'].dtype ) # 掩膜面积 seg_areas = np.zeros((num_valid_objs), dtype=entry['seg_areas'].dtype) # 是否crowd is_crowd = np.zeros((num_valid_objs), dtype=entry['is_crowd'].dtype) # 这个是??? box_to_gt_ind_map = np.zeros( (num_valid_objs), dtype=entry['box_to_gt_ind_map'].dtype ) if self.keypoints is not None: gt_keypoints = np.zeros( (num_valid_objs, 3, self.num_keypoints), dtype=entry['gt_keypoints'].dtype ) # 图片是否有可视的关键点? im_has_visible_keypoints = False for ix, obj in enumerate(valid_objs):# ix为下标, obj为下标对应元素 # category_id为coco类别id,json_category_id_to_contiguous_id 为字典类型 # 其中, key为coco的非连续id, value为1~80的连续id, 均为整数, 所以这里是将coco的非连续id转换成对应的连续id cls = self.json_category_id_to_contiguous_id[obj['category_id']] boxes[ix, :] = obj['clean_box'] # 将当前obj的box填入boxes列表 gt_classes[ix] = cls # 将连续id填入gt_classes列表 seg_areas[ix] = obj['area'] # 将area填入seg_areas列表 is_crowd[ix] = obj['iscrowd'] box_to_gt_ind_map[ix] = ix # 该列表存储着box的顺序下标值 if self.keypoints is not None: # ... if obj['iscrowd']: # 如果当前物体是crowd的话, 则将所有类别的overlap都设置为-1, # 这样一来在训练的时候, 这些物体都会被排除在外!! gt_overlaps[ix, :] = -1.0 else: gt_overlaps[ix, cls] = 1.0 # 仅仅将对应类的overlap设置为1, 其他为0 # 将gt的boxes添加到entry中, 注意axis为0, 则会按照第0维拼接, 即最后是一个n×4的数组 # 注意, entry['boxes']初始时候是空的, 因此这就相当于是只添加了真实的框 entry['boxes'] = np.append(entry['boxes'], boxes, axis=0) # 由于segms是以列表形式存储, 所以利用列表的extend方法来将valid_segms添加到其中 entry['segms'].extend(valid_segms) # gt_classes的类型内一维numpy数组(维度为有效obj的数量), 因此这里不用指定axis的值, 直接按照一维数组拼接即可 entry['gt_classes'] = np.append(entry['gt_classes'], gt_classes) # 同理, 一维numpy数组(维度为有效obj的数量), 无须指定axis的值 entry['seg_areas'] = np.append(entry['seg_areas'], seg_areas) # gt_overlaps为 num_objs × num_classes的numpy数组, 表示每个obj与任意一个类的重叠度 # 因为entry['gt_overlaps']的类型为scipy.sparse.csr.csr_matrix, 因此这里需要调用toarray方法将其转换成numpy数组, 然后再与gt_overlaps拼接, #由于entry['gt_overlaps']的维度为 0 × 81 , 因此拼接后的维度为num_objs × num_classes的numpy数组 entry['gt_overlaps'] = np.append( entry['gt_overlaps'].toarray(), gt_overlaps, axis=0 ) # 再将其包装成scipy.sparse.csr.csr_matrix类型 entry['gt_overlaps'] = scipy.sparse.csr_matrix(entry['gt_overlaps']) # 一维numpy数组, 可直接拼接 entry['is_crowd'] = np.append(entry['is_crowd'], is_crowd) # 该列表存储着box的顺序下标值, 同样是一维数组, 直接拼接 entry['box_to_gt_ind_map'] = np.append( entry['box_to_gt_ind_map'], box_to_gt_ind_map ) if self.keypoints is not None: entry['gt_keypoints'] = np.append( entry['gt_keypoints'], gt_keypoints, axis=0 ) entry['has_visible_keypoints'] = im_has_visible_keypoints _add_proposals_from_file()123456789101112131415# detectron/datasets/json_dataset.pyclass JsonDataset(object): def __init__(...): #... def get_roidb(...): #... def _prep_roidb_entry(self, entry): #... def _add_gt_annotations(self, entry): #... # def _add_proposals_from_file( self, roidb, proposal_file, min_proposal_size, top_k, crowd_thresh ): 续解combined_roidb_for_training() 方法接下来, 重新回到刚才detectron/datasets/roidb.py 文件 的 combined_roidb_for_training 函数中, 继续往下看:123456789101112131415161718192021222324252627282930313233343536373839404142# detectron/datasets/roidb.py# 加载并连接一个或多个数据集的roidbs, along with optional object proposals# 每个roidb entry都带有特定的元数据类型, 对其进行准备工作后进行训练def combined_roidb_for_training(dataset_names, proposal_files): def get_roidb(dataset_name, proposal_file): # 注意没有 's' # from detectron.datasets.json_dataset import JsonDataset # 可以看到, roidb 是利用JsonDataset类对象的get_roidb()方法获取的 # 注意gt参数是True, 所以表明加载的是训练集的真实数据及其标签 ds = JsonDataset(dataset_name) roidb = ds.get_roidb( gt=True, proposal_file=proposal_file, crowd_filter_thresh=cfg.TRAIN.CROWD_FILTER_THRESH ) # 如果图片翻转属性为真, 则对加载好以后的数据集进行翻转操作 if cfg.TRAIN.USE_FLIPPED: logger.info("Appending horizontally-flipped training examples...") extend_with_flipped_entries(roidb, ds) logger.info("Loaded dataset: &#123;:s&#125;".format(ds.name)) # 以上, 数据集加载操作完成, 将roidb数据结构返回 return roidb if isinstance(dataset_names, basestring): dataset_names=(dataset_names, ) if isinstance(proposal_files, basestring): proposal_files = (proposal_files, ) if len(proposal_files) == 0: proposal_files = (None, ) * len(dataset_names) assert len(dataset_names) == len(proposal_files) roidbs = [get_roidb(*args) for args in zip(dataset_names, proposal_files)] roidb = roidbs[0] for r in roidbs[1:]: roidb.extend(r) roidb = filter_for_training(roidb) logger.info("Computing bounding-box regression targets...") # 为训练bounding-box 回归其添加必要的information add_bbox_regression_targets(roidb) logger.info("done") _compute_and_log_stats(roidb) return roidb]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Queue模块-队列]]></title>
    <url>%2Fz_post%2FPython-Queue%2F</url>
    <content type="text"><![CDATA[Queue 是Python标准库中的线程安全的队列 (FIFO) 实现, 提供了一个适用于多线程变成的先进先出的数据结构, 主要用于在生产者和消费者线程之间的信息传递 Queue-基本FIFO队列1class Queue.Queue(maxsize=0) Queue.Queue 提供了一个基本的FIFO容器, maxsize 指明了队列中能存放的数据个数的上线. 一旦达到上限, 就会导致队列阻塞, 指责队列中的数据被消费掉. 如果将 maxsize 的值设置为小于或者等于 0, 则队列的大小没有限制. ```pyimport]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python-Python2和Python3的兼容问题]]></title>
    <url>%2Fz_post%2FPython-Python2%E5%92%8CPython3%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在看社区里面开源的深度学习或者python项目时, 经常会看到诸如__future__或者six.moves 这种名字很奇怪的包, 实际上, 这些包都是为了实现python2与python3的兼容性而设置的, 在python社区里, 很多公司或者个人为了只维护一套代码, 一般都会写同时兼容Python2/3的代码. futurepython3 出来的时候, python的设计者们当然也考虑过代码之间的兼容问题, 许多为兼容性设计的功能可以通过 __future__ 这个包来导入, 较常用的例如: 1234567891011from __future__ import print_function# 让python2可以使用python3的print函数, 同时禁用python2的print语句from __future__ import unicode_literal# 像python3一样, 字符串字面量的类型为文本( 文本是python2中的unicode, python中的str), 而不是字节( python2中的str, python3中的bytes)from __future__ import absolute_import#from __future__ import division# 让python2像python3一样, int/int = float, int // int = int 上面四条语句导入以后, python2中的很多语法就和python3差不多了 six由于 python2/3 之间还有很多别的差异, 因此只用 __future__ 是不够的. 用 six 这个 第三方模块 可以解决这个问题. 例如, Python2 和 Python3 中字符串类型名字不同, six可以把它们变成统一的第三种形式: six.text_type , six.binary_type]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[collections模块-集合型数据结构]]></title>
    <url>%2Fz_post%2FPython-collections%2F</url>
    <content type="text"><![CDATA[collections 是Python的一个集合模块, 它在内置数据结构 (dict, list, set, tuple) 的基础上, 提供了一些额外的集合型数据结构: 类型 说明 备注 deque 双端队列, 可以快速的从头/尾两端添加或删除元素 New in 2.4 defaultdict 带有默认值的字典 New in version 2.5 namedtuple 命名元组, 可以使用名字访问元素 New in version 2.6 Counter 计数器, 用于对某项数据进行计数 New in version 2.7 OrderedDict 有序字典, 按key对字典元素排序 New in version 2.7 ChainMap 合并多个map(dict), 但保持原数据结构 New in version 3.3 UserDict 将字典包装起来使得创建字典的子类更容易 UserList 列表对象的包装器 UserString 字符串对象的包装器 dequedeque 是double-ended queue 的缩写, 即双端队列. List存储数据的优势在于按索引查找元素很快, 但是插入和删除元素就很慢了, 因为List是基于数组实现的. deque 是为了高效插入和删除的双向列表, 适合用于跌列和栈, 而且线程安全, 其原型如下: 1collections.deque([iterable[, maxlen]]) deque 除了list固有的方法外, 还新增了 appendleft / popleft 等方法允许高效的在队列的开头插入或删除元素, 其时间复杂度为 $O(1)$ defaultdictnamedtupleCounterOrderedDictOrderedDict 是 dict 的一个自雷, 支持所有的 dict 方法, 它能够保持 dict 的有序性, 其原型如下: 1collections.OrderedDict([items])]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【置顶】深度学习框架底层实现原理-caffe2源码探究]]></title>
    <url>%2Fz_post%2FCaffe2-caffe2%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo博客相关问题及解决方案汇总]]></title>
    <url>%2Fz_post%2F%E5%85%B6%E4%BB%96-hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[数学公式渲染异常https://www.once4623.site/2017/10/03/2017-10-04--Use-MathJax-In-Hexo-Next/ hexo 实现置顶功能hexo 站内搜索https://www.jianshu.com/p/519b45730824 https://blog.csdn.net/ksws0292756/article/details/82714984 一直处于加载状态首先看是不是有哪些文件的title命名方式有问题, 已知的问题有: 不能有包含半角的冒号(全角可以), 这个可以用hexo g得知, 如果有问题, 则hexo g会报错 方法一: hexo clean + hexo g 重新生成search.xml文件 方法二: 更新searchdb插件: npm install hexo-generator-searchdb --save 方法三: 文章太多时, 尝试将_comfig.yml中Local Search下的limits设置的更高一些]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MultiBox-CVPR2014]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CVPR2014-MultiBox%2F</url>
    <content type="text"><![CDATA[文章: Scalable Object Detection using Deep Neural Networks作者: Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov 核心亮点(1) 回归问题:将物体检测问题定义为输出多个bounding box的回归问题. 同时每个bounding box会输出关于是否包含目标物体的置信度, 使得模型更加紧凑和高效 (2) 损失函数:将训练bounding box检测器作为整个网络训练过程的一部分, 也就是说在损失函数中包含了关于bounding box的损失项. 通过联合训练, 不仅利用了神经网络强大的特征表示能力, 而且将检测器的训练集成到了网络中 (3) 无类别监督训练作者将本文的目标边框检测器在无监督的样本下训练, 由于本方法主要完成的功能就是画框, 并不会输出框中包含的物体类别, 因此训练的时候无需知道样本的类别信息. 这也使得该方法的计算复杂度与类别信息几乎无关, 可以轻易的推广到未知的类别当中. (当然也可以进行相关类别的训练, 对每个类别都训练一个检测器, 模型的总参数会随着类别数线性增加) 关键技术 作者将bounding box的检测过程集成到了神经网络中, 使其转变成了一个回归问题, 通过BP算法优化下面的损失函数即可获得预测的框, 相比于SS算法, 计算复杂度更低. $x_{ij}=1$ 当且仅当第 $i$ 个预测框与第 $j$ 个真实框匹配. $l_i$ 和 $g_j$ 分别是预测框和真实框的归一化后的坐标, $c_i$ 代表置信度: F_{match}(x,l) = \frac{1}{2} \sum_{i,j} x_{ij} \|l_i - g_j\|_2^2F_{conf}(x,c) = -\sum{i,j} x_{i,j} log(c_i) - \sum_i (1 - \sum_j x_{ij}) log(1-c_i)F(x,l,c) = \alpha F_{match}(x,l) + F_{conf}(x,c)x^* = \arg \min_x F(x,l,c)\text{subject to } x_{ij} \in \{0, 1\}, \sum_i x_{ij}=1利用BP算法分别对 $l_i$ 和 $c_i$ 求导, 以便更新相关参数使其损失函数值更低. \frac{\partial F}{\partial l_i} = \sum_j (l_i - g_j) x^*_{ij}\frac{\partial F}{\partial c_i} = \frac{\sum_j x^*_{ij} c_i}{c_i(1-c_i)}论文细节背景介绍在(2014年)之前的工作中, 对于目标检测任务都是对整个图片进行检测, 无法检测出同一张图片中的多个目标物. 于是, 本文就提出了一种目标检测模型, 可以在一张图片中预测多个bounding boxes, 并且每个box都对应了包含某个类别物体的置信度. 作者使用了一个单一的DNN网络, 来生成候选区域框, 并且每个区域框都会带有一个置信度, 代表这框内包含物体的可能性大小. Model: 模型最后一层的神经元的输出值代表着每个框的坐标和对应的置信度. Bounding Box: 将左上角和右下角的坐标分别作为四个神经元的输出值. 这些坐标都是经过归一化的. Confidence: 每个Box对应的置信度会单独作为一个神经元节点输出. 在预测阶段, 可以利用该模型输出 $K$ 个bounding box预测结果, 同时可以利用NMS算法得到置信度更高的Box集合, 然后将这些集合送到分类器中进行分类. 训练目标: 假设对于一个训练样本, 具有 $M$ 个已经标注好的GT bounding box. 然后, 检测器会生成 $K$ 个预测的bounding box, $K$ 的值一般远远大于 $M$. 因此, 我们仅仅需要优化 $K$ 中与 $M$ 个GT匹配度最高的一个子集合. 优化的时候, 我们尽可能的提高这些子集合内部的预测框的置信度, 同时降低其他那些不在子集合里面的框的置信度. 对此, 形式化描述为下面的函数: F_{match}(x,l) = \frac{1}{2} \sum_{i,j} x_{ij} \|l_i - g_j\|_2^2上式中, $x_{ij}=1$ 当且仅当第 $i$ 个预测框与第 $j$ 个真实框匹配. $l_i$ 和 $g_j$ 分别是预测框和真实框的归一化后的坐标. 此外, 我们还希望对预测框的置信度进行优化, 将匹配框的置信度最大化, 这个过程转换成最小化下面的式子: F_{conf}(x,c) = -\sum{i,j} x_{i,j} log(c_i) - \sum_i (1 - \sum_j x_{ij}) log(1-c_i)从上式可以看到, $\sum_j x_{ij} = 1$ 当且仅当预测框 $i$ 可以匹配到某个真实框. 在这种情况下, $c_i$ 将王越来越大的方向优化. 上面这个式子正式交叉熵. 结合上面的两个公式, 最终的损失函数如下所示, 其中 $\alpha$ 用于调节两部分的权重: F(x,l,c) = \alpha F_{match}(x,l) + F_{conf}(x,c)优化: 对于每一个训练样本, 都希望按照如下最优化问题求得 $x^*$ (也就是最优化预测框与真实框的匹配方案) : x^* = \arg \min_x F(x,l,c)\text{subject to } x_{ij} \in \{0, 1\}, \sum_i x_{ij}=1由于标记物体的数量非常少, 所以上面公式的计算复杂度并不高. 对于上面的公式, 可以利用BP算法分别对 $l_i$ 和 $c_i$ 求导, 以便更新相关参数使其损失函数值更低. Training Details: 使用了三个小改动, 进一步提升了精度的速度]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协变(covariance)与逆变(contravariance)]]></title>
    <url>%2Fz_post%2FCpp-%E5%8D%8F%E5%8F%98%E4%B8%8E%E9%80%86%E5%8F%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[thread和threading模块-多线程处理]]></title>
    <url>%2Fz_post%2FPython-thread%E5%92%8Cthreading%2F</url>
    <content type="text"><![CDATA[Python的标准库提供了两个多线程模块: thread 和 threading. 其中, thread 是低级模块, threading 是高级模块, 对thread进行了封装, 大多数情况下, 我们只需要使用threading这个高级模块. python中使用线程有两种方式: 函数或者用类来包装线程对象]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-数据载入]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-%E6%95%B0%E6%8D%AE%E8%BD%BD%E5%85%A5%2F</url>
    <content type="text"><![CDATA[Coordinator 类由于 RoIDataLoader 类将 Coordinator 类对象作为成员变量, 因此我们先看一下这个类的作用和底层实现, 该类位于detectron/utils/coordinator.py文件中, 定义如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344#detectron/utils/coordinator.py# 从名字可以看出, 该类的作用主要是协调各个数据载入管道之间的信息同步# 实现上, 该类主要封装了threading多线程模块的一些功能class Coordinator(object): def __init__(self): # import threading self._event = threading.Event() def request_stop(self): log.debug("Coordinator stopping") self._event.set() def should_stop(self): # 当Event()对象使用set()方法后, is_set()方法返回镇 return self._event.is_set() #... @contextlib.contextmanager 上下文环境管理器 def stop_on_exception(self): try: yield except Exception: if not self.should_stop(): traceback.print_exc() self.request_stop()def coordinated_get(coordinator, queue): while not coordinator.should_stop(): try: # 从队列中获取数据 return queue.get(block=True, timeout=1.0) except Queue.Empty: continue raise Exception("Coordinator stopped during get()")def coordinated_put(coordinator, queue, element): while not coordinator.shuold_stop(): try: queue.put(element, block=True, timeout=1.0) return except Queue.Full: continue raise Exception("Coordinator stopped during put()") RoIDataLoader 类在之前分析的tools/train_net.py 文件中, 关于数据载入的部分被封装在了detectron/roi_data/loader.py文件中的RoIDataLoader类中, 而数据载入对于任何模型和工程来说, 都是非常重要的一步, 下面, 我们就具体看看这个类的底层实现是怎么样的. 文件开头, 有一段非常详细的注释: 1234567891011121314151617181920212223# detectron/roi_data/loader.py"""Detectron data loader. The design is generic and abstracted away from anydetails of the minibatch. A minibatch is a dictionary of blob name keys andtheir associated numpy (float32 or int32) ndarray values.Outline of the data loader design:loader thread\loader thread \ / GPU 1 enqueue thread -&gt; feed -&gt; EnqueueOp... -&gt; minibatch queue -&gt; ...loader thread / \ GPU N enqueue thread -&gt; feed -&gt; EnqueueOploader thread/&lt;---------------------------- CPU -----------------------------|---- GPU ----&gt;A pool of loader threads construct minibatches that are put onto the sharedminibatch queue. Each GPU has an enqueue thread that pulls a minibatch off theminibatch queue, feeds the minibatch blobs into the workspace, and then runsan EnqueueBlobsOp to place the minibatch blobs into the GPU's blobs queue.During each fprop the first thing the network does is run a DequeueBlobsOpin order to populate the workspace with the blobs from a queued minibatch.""" 从上面的注释我们可以看出, 这个文件定义了Detectron的数据载入器data loader, 这个类的设计是一种抽象的一般化的设计, 并且会与所有minibatch的细节隔离开来. 在这个类中, minibatch被记录为一个字典结构, 它的key值为blob name, 其value值为对应的numpy ndarray. 每一个GPU都具有一个enqueue线程, 可以从minibatch queue中获取数据, 然后会将minibatch blobs喂到workspace中去, 之后运行 EnqueueBlobsOp 来将minibatch blobs 放置到 GPU的blob queue中. 在每一次前向传播过程中, 模型最先做的事情就是运行 DequeueBlobsOp 来构建工作空间. 下面, 看一下RoIDataLoader类的具体实现:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# detectron/roi_data/loader.pyclass RoIDataLoader(object): def __init__( self, roidb, num_loaders = 4, minibatch_queue_size=64, blobs_queue_capacity=8 ): self._roidb = roidb self._lock = threading.Lock() self._perm = deque(range(len(self._roidb))) self._cur = 0 # _perm cursor # minibatch队列会在CPU内存当中持有准备好的训练数据 # 当训练N&gt;1个GPUs时, 在minibatch队列中的每一个元素 # 实际上是只是一部分minibatch, 对整个minibatch贡献了 # 1/N的样例 # from six.moves import queue as Queue self._minibatch_queue = Queue.Queue(maxsize=minibatch_queue_size) # TODO, 其他参数的初始化 # from detectron.utils.coordinator import Coordinator self.coordinator = Coordinator() # 加载mini-batches, 并且将它们放进mini-batch 队列中. def minibatch_loader_thread(slef): # coordinator的上下文管理器, 当有异常出现时会调用coordinator.request_stop()方法 with self.coordinator.stop_on_exception(): while not self.coordinator.should_stop(): # RoIDataLoader的成员函数, 返回用于下一个minibatch的blobs, # 函数内部调用了另一个成员函数_get_next_minibatch_inds() # 该函数返回下一个minibatch的roidb的下标 # 还调用了detectron/roi_data/minibatch.py文件中的get_minibatch方法 # 该方法会在给定roidb的情况下, 从中构造一个minibatch blobs = self.get_next_minibatch() # from collections import OrderedDict # Blobs必须根据self.get_output_names()在队列中进行排序 ordered_blobs = OrderedDict() for key in self.get_output_names(): assert blobs[key].dtype in (np.int32, np.float32), \ "Blob &#123;&#125; of dtype &#123;&#125; must have dtype of" \ "np.int32 or np.float32".format(key, blobs[key].dtype) ordered_blobs[key] = blobs[key] # from detectron.utils.coordinator import coordianted_put # 此处是将minibatch中数据blobs放入队列的关键代码 coordinated_put(self.coordinator, self._minibatch_queue, ordered_blobs) logger.info("Stopping mini-batch loading thread") # 将mini-batches从mini-batch队列中转移到BlobsQueue中. def enqueue_blobs_thread(self, gpu_id, blob_names): with self.coordinator.stop_on_exception(): while not self.coordinator.should_stop(): if self._minibatch_queue.qsize == 0: logger.warning("Mini-batch queue is empty") blobs = coordinated_get(self.coordinate, self._minibatch_queue)]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SPPNet-ECCV2014]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-SPPNet-ECCV2014%2F</url>
    <content type="text"><![CDATA[文章: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun 核心亮点(1) 提出了一种新的池化方法—-空间金字塔池化SPP: 可以接受任意尺寸的输入图片,并生成固定长度的表征向量 可以进行多尺度的联合训练, 提升模型精度 这种池化方法是比较general的, 可以提升不同模型架构的性能(分类任务) (2) 将SPP用于目标检测, 并且提出了先求卷积特征图谱, 后取区域的的策略: 大大提升了模型训练和预测的速度(在预测阶段, 比RCNN快24~102倍, 同时取得了更好的精度). 注1: 在特征图谱上使用检测方法不是首次提出, 而SPP的贡献可以结合了deep CNN结构强大的特征提取能力和SPP的灵活性, 使得精度和速度同时提高注2: 相比于RCNN, SPPNet使用了EdgeBoxes( $0.2s/img$ )的方法来进行候选区域推荐, 而不是Selective Search( $1\sim 2s/img$ )注3: SPPNet在ILSVRC2014的目标检测任务上取得第二名, 在图片分类任务上取得第三名 论文细节背景介绍指明固定图片输入尺寸的缺点: 需要剪裁或wrap图片, 无法包含整个图片信息, 导致信息丢失或引起形变 破除尺寸限制的做法: 在最后一个卷积层之后, 添加SPP层, SPP层可以将不同尺寸的feature map池化成固定长度的特征向量., 之后, 仍然与普通网络一样, 后街全连接层或者其他分类器 SPP本文的核心就在于SPPNet, 其主要用法是在卷积神经网络的最后一层卷积特征图谱上, 通过多尺度的网格划分, 接受任意尺寸的特征图谱输入, 同时输出固定长度的特征向量, 以此来实现网络模型接受任意尺寸图片输入的目的. SPPNet实现原理如下图所示: 首先, 设定好固定的网格划分方法, 以便得到spatial bins, 如上图, 有三种不同spatial bins, 网格划分力度分别为 4×4, 2×2 和 1×1, 因此, spatial bins的数量为:$4\times 4+2\times 2+ 1\times 1 = 21 = M$, 图中的256代表最后一层卷积层的filter的数量, 也就是特征图谱的depth = $k$. 因此, SPP层的输出为 $kM$ 维的一维向量. 注意: 这里最粗粒度的spatial bins 是对整张特征图谱上进行pooling, 这实际上是为了获得一些全局信息.(之前也很很多work集成了这种全局pooling方法, 貌似有助于提升精度, 同时由于是全局信息, 所以相当general, 可以一定程度上起到降低过拟合的作用) SPP可以看做是Bag-of-Words(BoW)模型的一种扩展 SPP用于deep CNNs时, 具有一些瞩目的性质: 相比于sliding window pooling的方式, SPP可以生成固定尺寸的特征向量 SPP使用了多尺寸的spatial bins, 而sliding window只是用了单一的window size. 这使得SPP对于物体的形变更加鲁棒 由于输入尺寸的灵活性, SPP可以在不同尺度下提取图片特征(可以在训练时接受多尺寸土木, 降低过拟合风险) SPP使用了MutiBoxes来进行候选区域推荐. Deep Networks With Spatial Pyramid Pooling卷积层和特征图谱卷积层可以接受任意尺寸的图片, 并且可以在特征图谱上得到相应的响应. 根据BoW的思想, 在 deep convolutional features 上也可以使用(与Bow)相似的pooling方式使得输出固定长度的特征向量 Training the Network虽然上面的work可以接受任意尺度的图片输入, 但在实际训练中, GPU相关组件如caffe或者cuda-convnet还是倾向于接受固定尺寸的输入. 因此作者介绍了他们的training solution, 以便在使用GPU高计算能力的同时, 保留SPP的pooling特点. Single-size training假设最后一层卷积层的特征图谱的size为 $a\times a$. pyramid level为 $n\times n$ bins. 那么实现该pyramid pooling的方式可以看做是 $win=\lceil a / n\rceil , stride = \lfloor a/n \rfloor$ 的sliding window pooling. 对于不同级别的pyramid, 使用同样的方法, 最终将所有的bins的输出连接起来组成一个一维向量. 上面用一句话总结: 在实际实现中, 使用sliding window pooling的方式来实现spp的, 对于单一尺寸的输入, 可以提前计算好需要的windows size, 编码时直接常量定义相关pooling即可. Multi-size training对于不同的尺寸输入, 分别计算不同的windows, 然后定义相关的pooling. (需要重新定义网络的pooling参数) 为了减少训练不同size网络的间接成本, 作者会先训练一个网络, 在一个epoch完成后, 会转向训练另一个网络(两个网络共享参数, 注意pooling是没有参数的) 也就是说, 对于两个网络, spp的参数设置是根据图片的尺寸来调节的, 因为要得到固定长度的特征向量, 所以如果图片尺寸较大, 那么最后一层卷积层的特征图谱也就较大, 那么久需要比较大的sliding windows size 来实现spp. 作者为了能一次性训练不同尺寸图片, 采用了这种迭代训练的方式(反正 pooling层是没有参数的 , 只不过是定义时需要指定window size). 进行多尺度训练的目的: 为了协同不同尺度下的图片特征提取, 同时利用已有的优化技术. 注意: 以上策略仅仅在training阶段使用. SPP-Net For Image ClassificationExperiments on ImageNet 2012 Classifization训练策略: images resize 使得图片的短边长度为256, 然后对其进行224×224 crop(四角+中心). 图像增强: horizontal flipping, color altering. dropout: 用于2层全连接层 lr: 0.01, 两次衰减10分之1 Baseling Network ArchitecturesZF-5, Convnet*-5, Overfeat-5/7 Multi-level Pooling Improves Accuracy需要注意的是, 模型精度的不是因为更多参数(pooling没有参数), 但是因为考虑了这种spatial结构, 从而使得模型的精度提升. Multi-size Training Improves Accuracy顾名思义, 作者使用了多尺度的训练, 同样提升了精度 Full-image Representations Improve Accuracy作者将Full-Image和crop(224×224, center)的策略进行了实验比较, 发现未经裁剪的Full-Image的精度更高, 说明维持原始图片的完整输入是很有必要的. 作者发现, 即使使用了很多视角下的crop结果进行投票, 额外的增加两个full-image(with flipping) 仍然可以提升模型 0.2%的精度 Multi-view Testing on Feature Maps受到目标检测算法的启发, 作者发现整合在feature maps上面进行不同视角的预测, 会使得模型精度有所提高. 实验结果证明, 在features maps上面进行10-view投票相比于直接在原始图上面进行10-view投票, 精度提高了0.1% (top-5 error) 作者使用了是不同image size, 结合了不同的view(18种, center+四角+四边中心+filp,), 总共有96种view(18*5 + 6(图片size只有224的时候)). 将top-5 error 从10.95% 降到 9.36%, 再结合two full image view, 降到了9.14%. Overfeat 也是从feature map中获取不同views的, 但它不能处理多尺度的图片 Experiments on VOC 2007 ClassificationExperiments on Caltech101SPP-Net For Object DetectionRCNN: 先从原始图像中选出大约2000个框, 然后将这些图像区域缩放到227×227大小, 然后对每个区域进行卷积分类, 使用了SVM分类器. 特征提取存在大量重复计算, 占用了绝大部分的时间 改进: 从feature maps提取对应框,只进行一次卷积计算: 之前的DPM在HOG特征图谱上选框, SS在SIFT特征图谱上选框, Overfeat在深度卷积特征图谱上选框, 但是Overfeat必须固定图片尺寸. 相比之前的这些方法, SPP具有在深度卷积特征图谱上的灵活性, 可以接受任意尺寸的输入, Detection Algorithm使用 “fast” mode的SS来生成2000个候选区域框, 然后将image 进行resize ,使其 min(w,h) = s. 接着对整张图片计算卷积特征图谱. 然后在特征图谱上选框, 接着对这些框进行spp, 最后使用svm进行分类. 使用了strandard hard negative mining来训练svm. Detection ResultsComplexity and Running Time改用了MultiBox算法, 处理每张图片只需要0.2左右, 原来的SS算法大约需要1~2s才可以. Model Combination for Detection在ImageNet上训练另一个模型, 使用相同的网络结构, 但是随机初始化状态不同. 如此得到两个模型, 他们的性能表现差不多. 首先分别用这两个模型给所有的测试样例的候选框进行打分, 然后利用NMS消除这些候选框(包含两个模型的预测结果)中的重复框, 这样一来, 就会保留下置信度更高的框. mAP从 59.1%, 59.2% 提升到了 60.9% ILSVRC 2014 DetectionConclusionAppendix AMean SubtractionImplementatioin of Pooling BinsMapping a Window to Featrue Maps]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-Python面试总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-Python%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[https://github.com/L1nwatch/interview_collect/tree/master/Python%20%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7 Python有哪些特点和优点 可解释 动态特性 面向对象 简明简单 python中, 对于基本类型, 每次的=赋值都会生成新的地址:1234567a = 1print(id(a))# 4490543616a = 2print(id(a))# 4490543648 深拷贝和浅拷贝的区别深拷贝是完全的拷贝, 当对一个对象的深拷贝做出改变时, 不会影响原对象的状态, 使用b = copy.deepcopy(a) 来完成深拷贝 浅拷贝是将一个对象的引用拷贝到另一个对象上, 如果在拷贝中进行改动, 则会影响到原对象的状态, 使用b = copy.copy(a) 完成浅拷贝 python中对象的赋值操作都是进行对象引用的传递(也就是浅拷贝的形态) 对于非容器类型(如数字, 字符串, 和其他原子类型的对象), 没有拷贝一说(除非使用=运算符, 否则经过拷贝的对象中的非容器类型, 与原对象的元素都是相对独立的,不会互相影响) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import copyorigin = ["will", 28, ["python", "C++"]]op_equal = origincp_shallow = copy.copy(origin)cp_deep = copy.deepcopy(origin)print("origin:")print(id(origin))print(origin)print([id(item) for item in origin])print()origin[0] = "new_will"origin[1] = 30origin[2].append("Caffe2")print("changed origin:")print(id(origin))print(origin)print([id(item) for item in origin])print()print("op_equal:")print(id(op_equal))print(op_equal)print([id(item) for item in op_equal])print()print("cp_shallow:")print(id(cp_shallow))print(cp_shallow)print([id(item) for item in cp_shallow])print()print("cp_deep")print(id(cp_deep))print(cp_deep)print([id(item) for item in cp_deep])print()origin:4339513864['will', 28, ['python', 'C++']][4334051088, 4330878304, 4339513928]changed origin:4339513864['new_will', 30, ['python', 'C++', 'Caffe2']][4339502384, 4330878368, 4339513928]# str和int在修改时会被赋予新的地址空间, 同时替换对应的旧元素op_equal:4339513864['new_will', 30, ['python', 'C++', 'Caffe2']][4339502384, 4330878368, 4339513928]# =赋值, 会将所有元素及对象的地址传递, 不会生成新的变量及对象, 与原对象完全关联cp_shallow:4339513736['will', 28, ['python', 'C++', 'Caffe2']][4334051088, 4330878304, 4339513928]#浅拷贝会生成一个新对象, 但是, 对于对象内的元素, 浅拷贝只会使用原始元素的引用# 对于非容器类型, 没有拷贝深浅之分, 因为原对象改变后, 其会指向一个新的地址, 而浅拷贝的地址不受影响, 所以二者是独立的# 但如果是可变类型, 则不会生成新的地址, 这会的话原对象的改变会对浅拷贝对象造成影响cp_deep4339513672['will', 28, ['python', 'C++']][4334051088, 4330878304, 4339513608]# 深拷贝, 与原对象完全独立, 没有任何关系 列表和元组之间的区别是:二者的区别主要是列表是可变的, 而元组是不可变的 Python 三元运算子1[on true] if [expression] else [on false] Python的继承单继承: 一个类继承自单个基类多继承: 一个类继承自多个基类多级继承: 一个类继承自单个继承, 后者则继承自另一个基类分层继承: 多个类继承自单个基类混合继承: 两种或多种类型继承的混合 Python中是如何管理内存的?Python有一个四有堆空间来保存所有的对象和数据结构, 作为开发者, 无法对其进行访问, 是由解释器在管理它, 但是有了核心API之后, 可以访问一些工具, Python内存管理器控制内存分配.]]></content>
      <categories>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[logging模块-打印日志信息]]></title>
    <url>%2Fz_post%2FPython-logging%2F</url>
    <content type="text"><![CDATA[简单使用1234567import logginglogging.debug("debug msg")logging.info("info msg")logging.warn("warn msg")logging.error("error msg")logging.critical("critical msg") 默认情况下, logging模块将日志打印到屏幕上, 只有日志级别高于WARNING的日志信息才回输出]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Detectron 源码解析-模型训练]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[运行Faster-RCNN指令123python tools/train_net.py \ --cfg configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml \ OUTPUT_DIR /tmp/detectron-output 上面两行指令第一行指定了config文件的位置, 第二行指定了输出文件的位置. 在tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml文件中, 主要包含了以下几个关键信息: MODEL: 包含模型类型, 卷积网络的骨干, 样本类别数目 SOLVER: 包含一些参数的值, 如: 权重衰减系数, LR_POLICY, GAMMA, MAX_ITER等等 FPN: 包含若干布尔值, 指示是否开启FPN, MULTILEVEL_ROIS/RPN 等等 FAST_RCNN: 与ROI相关的一些信息 TRAIN: 指定初始化权重文件的url, 以及数据集相关信息 TEST: 指定TEST数据集相关信息, NMS阈值 OUTPUT_DIR: . 被命令行的参数覆盖 上图为tools/train_net.py文件函数调用的关系图, 几乎所有的模型都是通过这个脚本运行的, 源码分析如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# tools/train_net.pyfrom __future import ... # 导入__future__的相关包, 兼容python2#...import cv2 #由于cv2自身的bug, cv2必须在caffe2之前被导入#...from caffe2.python import workspace# 导入detectron相关from detectron.core.config import assert_and_infer_cfgfrom detectron.core.config import cfgfrom detectron.utils.logging import setup_logging# helpful utilities for working with caffe2import detectron.utils.c2 as c2_utils# 训练文件import detectron.utils.train# 导入Detectron需要的相关contirb ops(nccl ops)c2_utils.import_contrib_ops()# 导入Detectron opsc2_utils.import_detectron_ops()cv2.ocl.setUseOpenCL(False) #源码注释已经说得很清楚, 就是为了安全而特意禁用opencldef parse_args(): # 定义了各种命令行参数, 一般只需要关注--cfg参数, 其他默认即可def main(): # 初始化caffe2的全局环境, #如果希望看到更加详细的初始化信息, 可以将log_level设置为1 workspace.GlobalInit( ['caffe2', '--caffe2_log_level=0', '--caffe2_gpu_memory_tracking=1'] ) ## 设置logging, __name__ = '__main__', 使用了python的logging模块, logger = setup_logging(__name__) #... # 解析命令行参数 args = parse_args() #... if args.cfg_file is not None: # from detectron.core.config import merge_cfg_from_file merge_cfg_from_file(args.cfg_file) if args.opts is not None: # from detectron.core.config import merge_cfg_from_list merge_cfg_from_list(args.opts) assert_and_infer_cfg() # 获取并显示当前的nvidia相关信息 smi_output, cuda_ver, cudnn_ver = c2_utils.get_nvidia_info() logger.info(...) #... # 设置随机种子, 以便每次训练时的网络状态都不同 np.random.seed(cfg.RNG_SEED) # 最重要的一行代码, 执行训练! # import detectron.utils.train checkpoints = detectron.utils.train.train_model() # 测试训练好的模型 if not args.skip_test: test_model(...)def test_model(...): # 测试训练好的模型 #...if __name__ == "__main__": main() train.py 文件可以看到, 上面只是一个启动训练代码的脚本文件, 接下来看一下train.py文件中的train_model()函数的详细情况: 123456#detectron/utils/train.pydef train_model(): """循环训练模型""" model, weights_file, start_iter, checkpoints, output_dir = create_model() #... create_model()在这里, train_model()函数的开始调用了create_model()函数, 在进行后面的代码分析之前, 需要先看看这个函数具体做了什么, 返回了什么:1234567891011121314151617181920212223242526272829303132333435363738394041#detectron/utils/train.pydef train_model(): #...def handle_critical_error(model,msg): #...def create_model(): """创建模型, 并且寻找保存的checkpoints用以继续训练""" logger = logging.getLogger(__name__) start_iter = 0 checkpoints = &#123;&#125; # from detectron.core.config import get_output_dir # def get_output_dir(datasets, training=True) # 这里只是用了数据集的名字, 并不是使用数据集 # 在OUTPUT文件夹(由config定义)会创建一个新的文件夹, 并返回该路径: # &lt;output-dir&gt;/&lt;train|test&gt;/&lt;dataset-name&gt;/&lt;model-type&gt;/ # 对于本例来说, cfg.TRAIN.DATASETS = ('coco_2014_train',) output_dir = get_output_dir(cfg.TRAIN.DATASETS, train = True) # 默认为R-50.pkl的 url 下载地址 weights_file = cfg.TRAIN.WEIGHTS if cfg.TRAIN.AUTO_RESUME: #TODO, 这个参数在哪里?? #... # 对于本例, cfg.MODEL.TYPE = generalized_rcnn logger.info("Building model: &#123;&#125;".format(cfg.MODEL.TYPE)) # from detectron.modeling import model_builder # def create(model_type_func, train=False, gpu_id=0) # 创建一个一般化的模型, 然后将其转化成特定的模型 # 函数内部使用了detectron/modeling/detector.py 中的DetectionModelHelper类 # 同时将cfg.MODEL.NUM_CLASSES信息传递给了该类 # cfg.MODEL.TYPE = generalized_rcnn # TODO: 此行代码涉及的文件和代码较多, 暂时先不深入探讨, 可以认为是创建了一个模型 model = model_builder.create(cfg.MODEL.TYPE, train=True) if cfg.MEMONGER: # TODO, 这个参数在哪里? optimize_memory(model) workspace.RunNetOnce(model.param_init_net) return model, wright_file, start_iter, checkpoints, output_dir setup_model_for_training(...)接着回到刚才的train_model()函数:12345678910111213141516171819#detectron/utils/train.pydef train_model(): """循环训练模型""" # 由上面的函数可知: # model为DetectionModelHelper类创建的模型对象 # weights_file即为cfg文件里定义的权重文件的url # start_iter = 0 # checkpoints =&#123;&#125; 字典类型哦! # output_dir为根据cfg.OUTPUT_DIR和DATASETS名字创建的输出目录 model, weights_file, start_iter, checkpoints, output_dir = create_model() if "final" in checkpoints: # 如果已经训练完成, 则直接返回 return checkpoints # 这里调用了本文件的函数, 主要是为训练模型做准备工作 setup_model_for_training(model, weights_file, output_dir) #... 具体看一下setup_model_for_training函数的内部实现 1234567891011121314151617181920212223242526272829303132333435363738394041#detectron/utils/train.py# 加载保存的权重文件, 同时在 C2 workspace 中创建 networkdef setup_model_for_training(model, weights_file, output_dir): logger = logging.getLogger(__name__) #调用了本文件的函数, 主要用于加载训练数据集, 并且将训练输入绑定到model中 # def add_model_training_inputs(model), 无返回值 # 核心代码: # from detectron.datasets.roidb import combined_roidb_for_trainig # roidb = combined_roidb_for_training(cfg.TRAIN.DATASETS, cfg.TRAIN.PROPOSAL_FILES) # model_builder.add_training_inputs(model, roidb = roidb) # 添加roidb, 内部会调用model_builder.add_training_inputs, # 该函数会为model创建用于训练网络的input ops 和blobs, 需要在调用了model_builder.create()之后调用 # 创建位于detectron.roi_data.loader中的RoIDataLoader对象进行数据载入 # model.roi_data_loader = RoIDataLoader() # TODO: 数据集加载过程涉及文件较多,后面会单独解析, 此处不太多讨论 add_model_training_inputs(model) # import detectron.utils.net as nu if weights_file: # 从权重文件中初始化模型, 覆盖随机初始化参数权重, 并且指定gpu nu.initialize_gpu_from_weights_file(model, weights_file, gpu_id=0) # 多GPU训练时,训练同步GPU之间的参数信息 # if cfg.NUM_GPUS = 1 直接return, 所以这句话在单GPU下可有可无 nu.broadcast_parameters(model) # 正式创建caffe2网络 workspace.CreateNet(model.net) #...log info # 本文件内的函数: def dump_proto_files(model, output_dir) # 保存训练网络的参数初始化的prototxt 描述信息 # 分别为output_dir里面的net.pbtxt和param_init_net.pbtxt文件 dump_proto_files(model, output_dir) # 载入mini-batches, 同时enqueuing blobs model.roi_data_loader.register_sigint_handler() model.roi_data_loader.start(prefill=True) return output_dir add_model_training_inputs(model)12345678910#detectron/utils/train.pydef add_training_inputs(model): logger = logging.getLogger(__name__) logger.info('Loading dataset: &#123;&#125;'.format(cfg.TRAIN.DATASETS)) roidb = combined_roidb_for_training( cfg.TRAIN.DATASETS, cfg.TRAIN.PROPOSAL_FILES ) logger.info('&#123;:d&#125; roidb entries'.format(len(roidb))) model_builder.add_training_inputs(model, roidb=roidb) 再看 train_model()接着回到刚才的train_model()函数:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#detectron/utils/train.pydef train_model(): """循环训练模型""" # 调用本文件的create_model()函数, 由上面的函数可知: # model为DetectionModelHelper类创建的模型对象 # weights_file即为cfg文件里定义的权重文件的url # start_iter = 0 # checkpoints =&#123;&#125; 字典类型哦! # output_dir为根据cfg.OUTPUT_DIR和DATASETS名字创建的输出目录 model, weights_file, start_iter, checkpoints, output_dir = create_model() if "final" in checkpoints: # 如果已经训练完成, 则直接返回 return checkpoints # 这里调用了本文件的函数, 主要是为训练模型做准备工作, 加载权重, 创建网络等等 setup_model_for_training(model, weights_file, output_dir) # from detectron.utils.training_stats import TrainingStats # def __init__(self, model) # 跟踪模型训练时的关键统计数据 training_stats = TrainingStats(model) # TODO SNAPSHOT_ITERS参数在哪? CHECKPOINT_PERIOD = int(cfg.TRAIN.SNAPSHOT_ITERS/cfg.NUM_GPUS) #循环训练过程cfg.SOLVER.MAX_ITER=60000次 for cur_iter in range(start_iter, cfg.SOLVER.MAX_ITER): # model.roi_data_loader是位于detectron.roi_data.loader中的RoIDataLoader对象 # def has_stopped(self): return self.coordinator.should_stop() # coordinator是位于detectron.utils.coordinator中的Coordinator类的对象, 该类用于多线程处理数据队列 if model.roi_data_loader.has_stopped(): handle_critical_error(model, 'roi_data_loader failed') # from detectron.utils.training_stats import TrainingStats # 用于训练计时, 计时开始 training_stats.IterTic() #更新学习率 lr = model.UpdateWorkspaceLr(cur_iter, lr_policy.get_lr_at_iter(cur_iter)) # 运行网络!关键步骤 workspace.RunNet(model.net.Proto().name) if cur_iter == start_iter: #首次迭代打印模型信息 # import detectron.utils.net as nu nu.print_net(model) training_stats.IterToc() # 计时结束 training_stats.UpdateIterStats() training_stats.LogIterStats(cur_iter, lr) # 迭代到指定次数以后, 自动保存模型 if (cur_iter+1) % CHECKPOINT_PERIOD ==0 and cur_iter&gt;start_iter: checkpoints[cur_iter] = os.path.join(output_dir, "model_iter&#123;&#125;.pkl".format(cur_iter)) nu.save_model_to_weights_file(checkpoints[cur_iter], model) if cur_iter == start_iter + training_stats.LOG_PERIOD: training_stats.ResetIterTimer() # 判断损失函数是否出现了nan值, 如果出现,则报错,注意,nan不是inf, nan值代表不存在的值, 如log(-1), 而log(0)是inf, 不是nan if np.isnan(training_stats.iter_total_loss): handle_critical_error(model, "loss is NaN") checkpoints['final'] = os.path.join(output_dir, 'model_final.pkl') nu.save_model_to_weights_file(checkpoints['final'], model) model.roi_data_loader.shutdown() return checkpoints 以上,便是train_model()函数的整体流程, 执行该函数后,训练过程就已经开始, 在detectron中, 所有的模型可数据都是用这个脚本训练的, 根据config文件的具体参数来决定载入哪个model, 或者使用哪个数据集. 但是上面的过程太过笼统, 给人一种隔了一堵墙的感觉. 因此, 我们需要更加深入了理解detectron是如何处理数据的, 又是如何将数据送到模型中去的, 另外, 每个模型的定义又是怎样的. 这些信息我会在后面的文章里一一解读.]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe2 基础]]></title>
    <url>%2Fz_post%2FCaffe2-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[corefrom caffe2.python import core : OperatorsCaffe2中的Operators有点像函数, 从C++的角度来说, 它们都是从一个通用接口派生而来的, 并且通过类型注册(什么意思?), 因此我们可以在runtime下调用不同的operators. 关于operators的接口定义在 caffe2/proto/caffe2.proto 中. 通常情况下, 它会接受一大批输入, 同时会返回一大批输出. 记住, 当我们说在 Caffe2 Python 中创建一个operator时, 还没有任何东西开始运行. 这仅仅是创建了一个protocal buffer(协议缓冲区), 用于指定具体的operator, 在稍后的一段时间内, 它将会被送入到C++后台去执行.(protobuf仅仅是一个针对结构化数据的json-like的序列化工具). Relu下面的代码创建了一个operator:123456789101112op = core.CreateOperator( "Relu", # 我希望运行的operator的类型 ["X"], # 一个列表, 里面存放的是输入的blobs的名字 ["Y"] # 仍然是一个列表, 里面存放的是输出的blobs的名字)print(type(op)) # &lt;class 'caffe2.proto.caffe2_pb2.OperatorDef'&gt;print(op)# input: "X"# output: "Y"# name: ""# type: "Relu" 接下来, 尝试使用operator, 首先将输入blobs添加到workspace中, 然后使用最简单的运行operator的方法, 即调用workspace.RunOperatorOnce(operator):12workspace.FeedBlob("X", np.random.randn(2,3).astype(np.float32))workspace.RunOperatorOnce(op) # 指定需要运行的operator变量 执行了RunOperatorOnce以后, workspace中会多出一个名字为name=&quot;Y&quot;的blobs, 用FetchBlobs()获取该blobs以后, 可以看到Y的值实际上就是对X执行Relu函数以后的结果(对应位置大于0的元素保留, 其他位置归0). 注意, 只有在执行了RunOperatorOnce以后, 名字为Y的blobs才会存在于workspace中, 此时使用FetchBlobs()才能够获取到对应值, 否则, Y是不存在的 CreateOperatoroperators同时也可以接受一些可选参数, 这些参数可以通过键值的方式来指定:12345678op = core.CreateOperator( "GaussianFill", [], # 填充时不需要输入任何数据 ["Z"], # 指定输出的名字 shape=[100,100], # 指定shape, 这里是一个100×100的二维列表 mean=1.0, # 指定平均值 std=1.0 # 指定标准差) NetsNets本质上就是计算图. 一个Net由多个operators组成, 当我们谈到Nets的时候, 我们同时也会谈到 BlobReference, 这是一个对象, 通过它我们可以轻易的将各种operators连接起来. 在创建网络时, 暗示着protocal buffer除了name以外其余都是空的123my_net = core.Net("my_first_net") # 重复调用时, 会不断创建net, 每次都会在后面加上一个后缀以区别不同的net, 如my_first_net, my_first_net_1, my_first_net_2等等print(my_net.Proto()) # name: "my_first_net"print(type(my_net)) # &lt;class 'caffe2.python.core.Net'&gt; 接下来, 向net中创建一些blobs1X = my_net.GaussianFill([], ["X"], mean=0.0, shape=[2,3], std=1.0, run_once=0) 此时, my_net中就创建了一个operator, 其类型为”GaussianFill”. 在这里, 对象X的类型为: &lt;class &#39;caffe2.python.core.BlobReference&#39;&gt;, 它会记录两个值, 一个是blob的名字, 另一个记录该对象是从哪个net中来的(_from_net). 上面没有使用core, 而是直接利用my_net进行operator的创建, 同样也可以使用core中的CreateOperator来创建operator, 并将其添加到对应的net中, 如下所示:12op = core.CreateOperator("op_name", ... )net.Proto().op.append(op) 继续创建W和b:12W = net.GaussianFill([], ["W"], mean=0.0, std=1.0, shape=[5,3], run_once=0)b = net.ConstantFill([], ["b"], shape=[5,], value=1.0, run_once=0) 下面利用了一个简单的语法糖, 由于BlobReference对象知道自己是从哪个net中来的, 因此, 可以直接利用BlobReference对象来创建operators, 下面显示了如何创建FC operator:1Y = X.FC([W,b], ["Y"]) 如果利用net创建则是下面的形式:1Y = my_net.FC([X,W,b], ["Y"]) 此时, 如果利用my_net.Proto()来查看net信息的话, 由于参数变多, 难以观察, 因此 Caffe2 提供了一个精简的minimal graph视图来帮助观察net:1234from caffe2.python import net_drawerfrom IPython import displaygraph = net_drawer.GetPydotGraph(net, rankdir="LR")display.Image(graph.create_pgn(), width=800) 通过上面的代码, 我们创建一个Net, 但是还没有任何东西被运行, 当我们运行network的时候, 会发生下面两件事情: 一个C++的net对象会从protobuf中实例化 实例化的网络的Run()函数会被调用 再做其他事情之前, 我们需要先用ResetWorkspace()清空早前的workspace, 然后有两种方式可以用来run net, 首先看看第一种:123456workspace.ResetWorkspace()print(workspace.Blobs()) # [] 可以看到, 列表为空, 因为X,Y,W,b均是operatorsworkspace.RunNetOnce(net) # 只执行一次, 无需调用CreateNetprint(workspace.Blobs()) # ['W','X','Y','b'], 在执行了RunNetOnce以后, workspace内被添加了有关operators的blobs.for name in workspace.Blobs(): print(name, workspace.FetchBlobs(name)) 下面是第二种创建并执行net的方式, 首先, 清空workspace里面的blobs, 然后创建net对象, 最后, 执行net1234567workspace.ResetWorkspace()print(workspace.Blobs()) #[] 之前的blobs又被清空了workspace.CreateNet(net) # 先创建networkspace.RunNet(net.Proto().name) # 执行net, 传入net.Proto().name参数print(workspace.Blobs()) # ['W','X','Y','b']for name in workspace.Blobs(): print(name, workspace.FetchBlob(name)) RunNetOnce 和 RunNet 之间有一点小区别, 最重要的区别就是计算开销的不同. 因为RunNetOnce包含了序列化protobuf, 同时还要将其在Python和C中传递, 并对network进行初始化, 因此它需要更长的执行时间. workspacefrom caffe2.python import workspace 12import cv2 # NOQA(Must import before importing caffe2 due to bug in cv2)from caffe2.python import workspace GlobalInit()12345# 初始化caffe2的全局环境,#如果希望看到更加详细的初始化信息, 可以将log_level设置为1workspace.GlobalInit( ['caffe2', '--caffe2_log_level=0', '--caffe2_gpu_memory_tracking=1']) caffe2的工作空间由你创建的blobs组成, 并且将其存储在内存当中. 将blob当做是numpy中的一个N维数组, 可以从Basics.ipynb中看出, blob实际上是一个指针, 它指向可以存储任意类型的一个C++对象, 但是最常存储的类型仍然是 Tensor 类型. Blobs()workspace.Blobs()方法可以打印出workspace中的所有存在的blobs (只会显示出blobs的名称). HasBlob()workspace.HasBlob(&quot;blob_name&quot;)用于查询workspace中是否存在名为blob_name的blob, 返回一个布尔值 workspace.FeedBlob()用于向workspace中添加blobs1234X = np.random.randn(2,3).astype(np.float32)print("Generated X from numpy:\n&#123;&#125;".format(X))workspace.FeedBlob("X", X) # 若添加成功, 则返回True, 否则, 返回False# 如果调用该函数时, 名字已经在workspace中存在, 则后添加的值会覆盖之前添加的值 workspace.FetchBlob(&quot;X&quot;)用来获取对应名字的blob的值, 如果获取的名字不在workspace中, 那么就会抛出错误, 我们可以用利用try except语句来输出错误信息:1234try: workspace.FetchBlob("not_exists_name")except RuntimeError as err: print(err) # Can't find blob:... workspace.SwitchWorkspace()同时, 你可以拥有多个不同的workspaces, 每一个workspace都有不同的名字, 你可以调用workspace.SwitchWorkspace(&quot;name&quot;)来在这些不同的名字之间交换. 不同workspace中的blobs是相互独立的, 你可以使用workspace.CurrentWorkspace来查询当前所有的workspace:123456# Switch the workspace. The second argument "True" means creating# the workspace if it is not existsworkpsace.SwitchWorkspace("new_workspace_name", True) # 如果名为"new_workspace_name"的workspace存在, 则切换当前workspace到该workspace, 如果不存在, 那么就创建它.print("Current workspace:&#123;&#125;".format(workspace.CurrentWorkspace())) # 默认的workspace的名称为: defaultprint("Current blobs in the workspace:&#123;&#125;".format(workspace.Blobs())) 利用workspace.ResetWorkspace()可以清空当前workspace中的所有东西, 谨慎使用1workspace.ResetWorkspace() model_helper1from caffe2.python import model_helper param_initXavierFill12m = model_helper.ModelHelper(name="my model")weight = m.param_init_net.XavierFill([], "fc_w", shape=[10, 100]) ConstantFill1bias = m.param_init_net.ConstantFill([], "fc_b", shape=[10, ]) netFC()1fc_1 = m.net.FC(["data", "fc_w", "fc_b"], "fc1") # "data", "fc_w", "fc_b" 都是workspace中的blobs Sigmoid()1pred = m.net.Sigmoid(fc_1, "pred") SoftmaxWithLoss()1softmax, loss = m.net.SoftmaxWithLoss([pred, "label"], ["softmax", "loss"])]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OverFeat-ICLR2014]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ICLR2014-OverFeat%2F</url>
    <content type="text"><![CDATA[文章: OverFeat: Integrated Recognition, Localization and Detectin using Convolutional Networks 作者: 核心亮点Multi-Scale Classification:在分类任务上, 虽然训练时采用和AlexNet相同的multi crop方法, 但是在预测阶段没有使用AlexNet的crop投票策略, 而是提出了Multi-Scale Classification方法, 一句话概括就是 对整个图片以不同的尺寸, 并且对每一个location进行模型预测 利用了全卷积的思想代替全连接, 降低了滑动窗口的计算代价 可以用同一个模型完成分类, 定位, 检测任务:同一个模型, 只需要用回归层替换分类层, 即可完成目标定位任务, 同时利用了贪心策略来融合最终的定位结果 论文细节背景介绍提出了一种新的深度学习方法, 通过预测物体的边界来对目标物体进行定位. 本文的模型是ILSVRC2013的冠军 从表现最好的模型中开源了特征提取器, 并取名为OverFeat 作者为了避免大量的时间消耗, 没有在背景样例中训练, 但同时声称效果也不错?(持怀疑态度) 物体画框的时候, 因为画到了物体的一部分, 而没有画到整个物体, 更别说将框画到物体的正中心了. 因此, 希望可以对框进行预测回归, 使之画出来的框更加准确 与Krizhevsky等人之前的图像分类的paper做了个简单的比较 2. 视觉任务本文研究了三种计算机视觉任务, 分别为: 分类, 定位和检测 3. 分类本文的分类体系架构与Krizhevsky在12年使用的体系结构类似, 但是对模型中的一些细节进行了更多的探索 3.1 模型设计和训练训练数据集: ImageNet 2012 (1.2 million 张图片, 共1000类) 输入尺寸: 训练阶段先将所有的图片downsample, 使其最小尺寸为256, 然后从每张图片(包括反转图片)中裁剪出5张子图(加上反转共10张), 尺寸为221*221, 这与AlexNet的策略相同, 但是在预测阶段, 使用了不同Multi-Scale Classificatioin方法, 具体见下文. mini-batch: 128 模型权重初始化: 随机初始化 $(\mu, \sigma) = (0, 1\times 10^{-2})$ momentmu项为: 0.6 L2 权重衰减系数: $1\times 10^{-5}$ 学习率: $5\times 10^{-2}$, 每经过(30,50,60,70,80)epochs时, 衰减1/2. Dropout: 0.5(用于6,7层的全连接) 模型结果细节图: 3.2 Feature Extractor根据本文的方法, 开源了一个名为”OverFeat”的Feature Extractor, 有两个版本, 分别是精度优先和速度优先. 模型结构以及他们之间的比较都在上面两张图中显示. 3.3 Multi-Scale Classificaion在测试阶段, 之前的Alex-Net中, 使用了multi-view投票来提升性能(一张图片crop出10个子图, 分别为四角和中心,以及他们的反转图片). 但是这个方法有一些问题: 忽视了图片中的很多区域, 并且计算存在冗余重复, 同时, 生成子图的操作都是在同一尺度上进行的, 这有时候不是最佳的尺度选择策略. 与此相反, 本文提出对整个图片以不同的尺寸, 并且对每一个location进行模型预测 , 虽然这种滑动窗口的方法在计算上是不可取的, 但是, 结合下面的3.5节中的策略, 可以缓解计算复杂问题. 上面方法的好处: 可以对同一张图片生成更多不同角度下的预测结果, 并将结果用于投票, 使得最终结果更鲁棒. 但是存在问题: 上面提到的subsampling方法的比例为 2×3×2×3, 也就是36. 这样一来, 模型在只能依赖36个像素点来生成分类向量. 这样粗粒度的分布使得模型的性能表现低于10-view方法. 主要原因是因为网络窗口没有和图片中的物体边界对齐. 于是本文提出了一种方法来克服这个问题, 对最后一个subsampling在不同的offset上进行pooling, 这样做可以缓解这一层的loss of resolution问题, 生成的总的subsampling比例为12, 而不是36. //TODO 这一段啥意思?, 36怎么来的, 12怎么来 下来解释具体是如何进行resolution augmentation的, 本文对于同一张图片, 使用了6种不同的尺寸, 如下图所示(C为类别): 具体过程如下: 对于一个给定的scale的图片, 我们从第5层, 还未pooling的特征图谱开始 对于每一个未经过pooling的特征图谱, 进行 3×3 的pooling操作(非重叠池化), 重复 3×3 次($\Delta x, \Delta y)$ 以不同的offset {0,1,2}). 这样可以得到 3×3 个pooling后的特征图谱. 分类器(6,7,8层)具有一个 5×5 的固定的输入大小, 同时对于每一个location的土整图谱都会生成 C 维的输出向量 根据不同的offset, 可以得到不同的结果 下面的图以一维向量为例讲解了offset pooling的原理, 可以类推到3维结构中 上面的操作可以看做是以一个像素为单位, 在特征图谱上进行了位移操作, 进而得到了不同角度下的图谱结果用于分类, 与之前的crop方法相比, 这个方法没有对图片进行裁剪, 但同时又达到了获取同一张图片不同视角结果的目的. 这样做最大的优点在于: 对于一张图片, 可以宏观的将网络分成两个部分, 分别是特征提取层和分类层, 在特征提取部分, 同一张图片只会进行一次前向计算, 在这计算角度来说, 大大提高了计算效率, 减少了荣冗余计算. 这种极致的pooling策略(exhaustive pooling scheme) 确保了我们可以在分类器和特征图的对象之间获得准确的边界对齐. 3.4 结果实验结果如下表所示: 分类错误率在18个队伍中的排名为第5 (主要本篇文章的两点也不在分类, 还是在目标检测上) 3.5 convNets和滑动窗口效率 首先, 要知道全卷积与全连接层之间的关系, 利用全卷积层代替全连接层以后, 可以接受不同图片尺寸的输入, 同时, 在计算时, 由于卷积操作本身的计算共享特性, 可以使得计算过程更加高效, 以此来缓解滑动窗口方法带来的计算问题. // TODO 这块还是不是很懂 4. 定位 Localization用回归网络代替分类网络, 同时训练网络使其在每一个空间location和scale下预测目标物体的bounding boxes. 然后将回归预测结果与每个位置的分类结果结合在一起. 4.1 生成预测由于可以共享特征提取层的计算结果, 因此只需要重新计算最终的回归层即可. 因为每个位置的分类结果为所有的类别都赋予了一个置信度, 因此, 我们可以将这个置信度作为bounding box的置信度使用. 4.2 回归训练 Regressor Training回归网络将第5层的池化后的特征图谱作为输入. 后面接入两个全连接层, 隐藏神经元个数分别为4096和1024. 最终的输出层具有4个神经元, 用于指定bounding box的坐标. 和分类网络一样, 也是用了基于offset的pooling策略. 该网络的结构如下图所示 训练时, 固定前5层的特征提取层, 然后使用每个样本的真实边界与预测边界之间的L2损失(平方损失)来训练回归网络. 最后的这个回归层根据与特定类相关的, 每一个类都会有一个回归层(也就是说有1000个不同的回归层版本). 当输入图片与目标物体的IOU小于50%时, 则不会进行训练 同时使用了多尺度的输入进行训练(与第三节相同), 以便更好的进行不同尺度下的预测. 4.3 联合预测通过对回归得到的bounding boxes使用贪心融合策略, 来得到单个目标物的预测结果. 具体算法过程如下: 1.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ string 完整笔记]]></title>
    <url>%2Fz_post%2FCpp-string%E7%94%A8%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[在 C++ 中, 用于处理字符串的类 string 虽然有用很多和 STL 相似的性质和用法, 但是一般来说, 我们不认为它是 STL 的一份子, 因为实际上, string 并不是模板, 它其实是某个模板的实例, 即 basic_string&lt;char&gt;. 因此, 这里我们单独讨论关于 string 类的一系列方法和属性, 但是需要知道, string 类在很多地方都与模板的函数或者属性很相似. 成员函数元素访问at() 返回char类型的字符 [] 返回char类型的字符 操作push_back: 将字符添加到字符串结尾 pop_back: 移除末尾字符 类型转换char 转 intint num = std::atoi(str[i]); string 转 intstd::string str;int i = std::stoi(str);12同样, 可以使用 stol(long), stof(float), stod(double) 等 最简单的将int转换成string的方法方法一：C风格的itoa123int a = 10;char* intStr = itoa(a);string str = string(intStr); 方法二：1234int a = 10;stringstream ss;ss &lt;&lt; a;string str = ss.str(); 方法三：C++风格的std::to_string12345#include &lt;string&gt;std::string s = std::to_string(42);auto s = std::to_string(42);]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【置顶】Detectron源码解析]]></title>
    <url>%2Fz_post%2FCaffe2-Detectron%2F</url>
    <content type="text"><![CDATA[roidb数据结构 数据载入 模型训练 根目录dirs:— build: 有关cython构建的文件 — cmake: 各种编译配置文件 — configs: 使用模型时的config文件 — demo: 一些图片的demo — Detectron.egg-info: 源码信息 — detectron: 最主要的文件夹, 存放几乎所有的模型源文件 — docker: 顾名思义, 存放了Dorkerfile&emsp;|— Dockerfile — projects&emsp;|— gn.jpg&emsp;|— README.md — tools files:— CMakeLists.txt — CONTRIBUTING.md — FAQ.md — GETTING_STARTED.md — INSTALL.md — LICENSE — Makefile — MODEL_ZOO.md — NOTICE — README.md — requiresments.txt — setup.py core目录detectron目录]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux-Git用法笔记]]></title>
    <url>%2Fz_post%2FLinux-Git%E7%94%A8%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[初始化当前本地文件夹为仓库 git init 添加文件: 单个: git add readme.md全部: git add -A / git add ./ 提交修改: git commit -m “must write commit” 查看状态: git status 查看日志: git log 版本回退: 回退一个: git reset -hard HRAD^ 回退两个: git reset -hard HARD^^ 回退多个: git reset -hard HEAD~100 首次连接: git remote add origin https//www.github… 提交: git push origin master 更新本地仓库: git fetch origin master 获取 git merge origin/master 合并 如果将别人的仓库拉到了自己的仓库里, 为了push成功: 删除.git相关 git rm rf —cached ./themes/next git add ./themes/next git commit -m “next” git push origin master 子模块: 可以独立提交]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shutil模块-shell命令行指令]]></title>
    <url>%2Fz_post%2FPython-shutil%2F</url>
    <content type="text"><![CDATA[1234567shutil.copyfile("old","new") # 复制文件，都只能是文件shutil.copytree("old","new") # 复制文件夹，都只能是目录，且new必须不存在shutil.copy("old","new") # 复制文件/文件夹，复制 old 为 new（new是文件，若不存在，即新建），复制 old 为至 new 文件夹（文件夹已存在）shutil.move("old","new") # 移动文件/文件夹至 new 文件夹中]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程-FineTuning Torchvision Models]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-FineTuningModels%2F</url>
    <content type="text"><![CDATA[在本篇教程中, 我们将教授你如何 finetune 一个 torchvision models, 这些 models 都已经在 ImageNet 的 1000-class 数据集上进行过预训练. 由于每个模型的结构都不太相同, 因此没有通常的代码模板来适应所有的场景需要. 在这里, 我们将会演示两种类型的迁移学习: finetuning 和 feature extraction. 在 finetuning 中, 我们会从一个预训练好的模型开始, 将该模型的参数应用到新的任务上去, 然后重新训练整个模型. 在 feature extraction 中, 我们同样会从一个预训练好的模型开始, 但是仅仅更新最后的几层网络, 而将之前的网络层参数固定不变. 通常情况下, 这两种迁移学习都包含以下几步: 用预训练模型初始化参数 更改最后的一层或几层神经层, 使其输出适应我们的新数据集 在 optimization 算法中定义我们希望更新的参数 执行训练]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MicroSoft COCO数据集]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%95%B0%E6%8D%AE%E9%9B%86-COCO%2F</url>
    <content type="text"><![CDATA[评价标准COCO数据集介绍COCO数据集具有5种标签类型, 分别为: 目标检测, 关键点检测, 物体分割, 多边形分割 以及 图像描述. 这些标注数据使用JSON格式存储. 所有文件除了标签项以外都共享同样的数据结构, 如下所示: 标签结构各有不同, 如下所示: Object Detection每一个目标实例的标签都具有一系列条目, 包括该目标的类别id以及分割掩膜(segmentation mask). 分割掩膜的格式取决于实例表示的是一个单一的目标(iscrowd=0, 使用polygons标注)还是一群目标(iscrowd=1, 使用RLE标注). 注意, 一个单一的物体(iscrowd=0)在被遮挡的情况下, 可能会需要多个多边形来表示. Crowd标签用来标注一大群目标(如一群人). 另外, 每一个物体都会提供一个闭合的bounding box( box的坐标系从图左上角开始,0-indexed). 最后, 标注结构的类别条目存储着从cat id 到类别的映射以及超类别的名字. 关键点检测Stuff SegmentationStuff Segmentation的格式和object detection的格式几乎一模一样, 但是stuff segmentation无需iscrowd条目, 因为该条默认置为0. 为了方便访问, coco提供了json和png两种标注格式. 在 json 格式中, 每一个出现在图片中的类别都会单独用一个RLE标签条目编码(也就是说同类的会被放到同一个RLE编码里面). category_id 则代表当前的物体类别的id. Panoptic Segmentation 数据集信息标注格式COCO-API 使用方法及源码解析最常用的是 pycocotools/coco.py 文件中的COCO类, 其内部实现如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# pycocotools/coco.py# 首先, 是一大串的注释# COCO API提供了一系列的辅助函数来帮助载入,解析以及可视化COCO数据集的annotations# 该文件定义了如下API 函数:# COCO - COCO api 类, 用于载入coco的annotation 文件, 同时负责准备对应数据结构来存储# decodeMask - 通过rle编码规范, 来对二值mask M进行解码# encodeMask - 使用rle编码规范来对二值mak M进行编码# getAnnIds - 获得满足给定过滤条件的ann ids(标注)# getCatIds - 获得满足给定过滤条件的cat ids(类别)# getImgIds - 获得满足给定过滤条件的img ids(图片)# loadAnns - 根据指定的ids加载anns# loadCats - 根据指定的ids加载cats# loadImgs - 根据指定的ids加载imgs# annToMask - 将annotation里面的segmentation信息转换成二值mask# showAnns - 可视化指定的annotations# loadRes - 加载算法结果同时创建API以便访问它们# download - 从Mscoco.org.server上下载COCO数据集# 接下来, 具体看一下COCO类的实现class COCO: def __init__(self, annotation_file=None): # 构造函数 # 参数annotation_file: 指定了annotation文件的位置 # dataset, anns, cats, imgs均为字典类型数据 self.dataset, self.anns, self.cats, self.imgs = dict(), dict(), dict(), dict() # imgToAnns, catToImgs均为defaultdict数据类型(带有默认值的字典) self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list) if not annotation_file == None: #... # 以只读方式加载json标注文件 dataset = json.load(open(annotation_file, 'r')) assert type(dataset)==dict # 确保读出来的是字典类型 #... # 正式将读取的字典数据赋给该类的成员变量 self.dataset = dataset # 创建索引 self.createIndex() def createIndex(self): # 创建索引 # 三个都是字典数据类型 anns,cats,imgs=&#123;&#125;,&#123;&#125;,&#123;&#125; # 两个defaultdict数据类型 imgToAnns, catToImgs = defaultdict(list), defaultdict(list) if 'annotations' in self.dataset: for ann in self.dataset['annotations']:]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PASCAL VOC数据集]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%95%B0%E6%8D%AE%E9%9B%86-VOC%2F</url>
    <content type="text"><![CDATA[评价标准COCO数据集介绍COCO数据集具有5种标签类型, 分别为: 目标检测, 关键点检测, 物体分割, 多边形分割 以及 图像描述. 这些标注数据使用JSON格式存储. 所有文件除了标签项以外都共享同样的数据结构, 如下所示: 标签结构各有不同, 如下所示: Object Detection每一个目标实例的标签都具有一系列条目, 包括该目标的类别id以及分割掩膜(segmentation mask). 分割掩膜的格式取决于实例表示的是一个单一的目标(iscrowd=0, 使用polygons标注)还是一群目标(iscrowd=1, 使用RLE标注). 注意, 一个单一的物体(iscrowd=0)在被遮挡的情况下, 可能会需要多个多边形来表示. Crowd标签用来标注一大群目标(如一群人). 另外, 每一个物体都会提供一个闭合的bounding box( box的坐标系从图左上角开始,0-indexed). 最后, 标注结构的类别条目存储着从cat id 到类别的映射以及超类别的名字. 关键点检测Stuff SegmentationStuff Segmentation的格式和object detection的格式几乎一模一样, 但是stuff segmentation无需iscrowd条目, 因为该条默认置为0. 为了方便访问, coco提供了json和png两种标注格式. 在 json 格式中, 每一个出现在图片中的类别都会单独用一个RLE标签条目编码(也就是说同类的会被放到同一个RLE编码里面). category_id 则代表当前的物体类别的id. Panoptic Segmentation 数据集信息标注格式COCO-API 使用方法及源码解析最常用的是 pycocotools/coco.py 文件中的COCO类, 其内部实现如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# pycocotools/coco.py# 首先, 是一大串的注释# COCO API提供了一系列的辅助函数来帮助载入,解析以及可视化COCO数据集的annotations# 该文件定义了如下API 函数:# COCO - COCO api 类, 用于载入coco的annotation 文件, 同时负责准备对应数据结构来存储# decodeMask - 通过rle编码规范, 来对二值mask M进行解码# encodeMask - 使用rle编码规范来对二值mak M进行编码# getAnnIds - 获得满足给定过滤条件的ann ids(标注)# getCatIds - 获得满足给定过滤条件的cat ids(类别)# getImgIds - 获得满足给定过滤条件的img ids(图片)# loadAnns - 根据指定的ids加载anns# loadCats - 根据指定的ids加载cats# loadImgs - 根据指定的ids加载imgs# annToMask - 将annotation里面的segmentation信息转换成二值mask# showAnns - 可视化指定的annotations# loadRes - 加载算法结果同时创建API以便访问它们# download - 从Mscoco.org.server上下载COCO数据集# 接下来, 具体看一下COCO类的实现class COCO: def __init__(self, annotation_file=None): # 构造函数 # 参数annotation_file: 指定了annotation文件的位置 # dataset, anns, cats, imgs均为字典类型数据 self.dataset, self.anns, self.cats, self.imgs = dict(), dict(), dict(), dict() # imgToAnns, catToImgs均为defaultdict数据类型(带有默认值的字典) self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list) if not annotation_file == None: #... # 以只读方式加载json标注文件 dataset = json.load(open(annotation_file, 'r')) assert type(dataset)==dict # 确保读出来的是字典类型 #... # 正式将读取的字典数据赋给该类的成员变量 self.dataset = dataset # 创建索引 self.createIndex() def createIndex(self): # 创建索引 # 三个都是字典数据类型 anns,cats,imgs=&#123;&#125;,&#123;&#125;,&#123;&#125; # 两个defaultdict数据类型 imgToAnns, catToImgs = defaultdict(list), defaultdict(list) if 'annotations' in self.dataset: for ann in self.dataset['annotations']:]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法题杂记]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E9%9D%A2%E8%AF%95%E9%A2%98%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[交换两个数的特殊方法加法注意: 有可能导致溢出 123a = a + b; //但是加法可能会最终导致溢出b = a - b;a = a - b; 异或123a = a^b;b = a^b;a = a^b; 上面的实习原理是利用异或的自身特性: 12a ^ a = 0a ^ 0 = a 注意: 用异或交换两个整数存在一个陷阱 当交换两个相同的数字时, 由于异或自身的特性, 会使得这两个数字都变成0, 解决方法是加上一个判断条件: 12345if(a!=b)&#123; a = a^b; b = a^b; a = a^b;&#125; 不开根号求平方根牛顿迭代法假设要对 $a$ 进行开根, 那么就需要找到一个值 $x$, 满足 $x^2 = a$, 我们令 $f(x) = x^2 - a$ , 则只需找到使 $f(x)=0$ 的值 $x$ 即为开根后的值, 也就是说要求 $f(x)$ 与x轴的交点, 在x轴上任选一点 $(x_0, f(x_0))$, 求该点在函数 $f(x)$ 上面的切线方程如下: f(x) - f(x_0) = f'(x_0)(x - x_0)也就是: f(x) - (x_0^2 - a) = 2x_0(x - x_0)我们求该直线与x轴的交点, 即 $f(x)=0$ , 求得 $x$ 的值为: x = x_0 - \frac{x_0^2 -a}{2x_0}如果上面求得的 $x$ 不是 $f(x)$ 与 x 轴的交点, 那么久继续这个过程, 直到结果满足我们需要的精度 123456789int main()&#123; double a = 24;//欲求24的开根 double x = 1.0; while(abs(x*x-a)&gt;0.0001)&#123; x = x- (x*x-a)/(2*x+1e-9); &#125; cout&lt;&lt;x; return 0&#125; 二分法1234567891011int main()&#123; double a = 24;//欲求24的开根 double high = a; double low = 0; while(high-low&gt;0.0001)&#123; double mid = (high+low)/2; if(mid*mid &gt; t) high = mid; if(mid*mid &lt; t) low = mid; &#125; return low;&#125; 有两个数组, 数组很大, 其中一个数组存储着n个人的名字, 另一个数组存储着这n个人的身高?问题一: 现在需要找出m个身高最高的人的名字, 要求时间和空间复杂度最低问题二: 哪些排序算法时间复杂度是 $O(1)$ 的, 哪些不是问题三: 哪些排序算法是稳定的, 哪些是不稳定的 稳定: 冒泡, 插入排序, 归并(合并), 基数排序 不稳定: 选择, 快排, 希尔排序, 堆排序 问题四: 现在需要设计一个算法, 每次从这n个人当中挑出一个人, 要求最终跳出的所有人当中, 身高高的人站的比例大函数轮盘赌算法求下面函数的返回值:12345678int func(x)&#123; int countx=0; while(x)&#123; countx++; x=x&amp;(x-1); &#125; return countx;&#125; 假定x=9999, 答案:8思路: 将x转化为2进制, 看含有1的个数 在多线程和大量并发环境下，如果有一个平均运行一百万次出现一次的bug， 你如何调试这个bug。n个数里面求最大的m个数, (堆)建立size为m的最小堆, 堆顶为最小的值, 堆内的数据都比堆顶小, 所以这m个数字即为n个数里面最大的m个数. 求两个不相交的连续子数组的最大和题目：有一个整数数组n，a和b是n里两个互不相交的子数组。返回sum(a)+sum(b)的最大值。 分析：1234567891011121314151617181920212223242526272829303132333435363738394041int SumOfTwoSubarray(const vector&lt;int&gt; &amp;n)&#123; if (n.size() &lt; 2) &#123; throw exception(); &#125; vector&lt;int&gt; left(n.size()), right(n.size()); int sum = n[0], maxnum = n[0]; left[0] = n[0]; right.back() = n.back(); for (uint32_t i = 1; i &lt; n.size()-1; ++i) &#123; if (sum &gt; 0) sum += n[i]; else sum = n[i]; if (sum &gt; maxnum) &#123; maxnum = sum; &#125; left[i] = maxnum; &#125; sum = n.back(); maxnum = n.back(); for (uint32_t i = n.size()-2; i &gt;=1; --i) &#123; if (sum &gt; 0) sum += n[i]; else sum = n[i]; if (sum &gt; maxnum) &#123; maxnum = sum; &#125; right[i] = maxnum; &#125; int res = 0x80000000; for (uint32_t i = 0; i &lt; n.size() - 1; ++i) res = max(res, left[i] + right[i + 1]); return res;&#125; 新建两个数组left和right，left[i]表示n[0:i]的连续子数组的最大和，right[i]表示n[i:length-1]的连续子数组的最大和。left[i]+right[i+1]的最大值就是答案。 https://blog.csdn.net/bupt8846/article/details/48115931]]></content>
      <categories>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Caffe2基础]]></title>
    <url>%2Fz_post%2FCaffe2-Caffe%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/carle-09/p/9033608.html https://zhuanlan.zhihu.com/p/26451014 简介Caffe2 基本概念Blobs and Workspace, Tensorsblobs: Caffe2中的数据组织形式. 大多数blobs都包含tensor, 并且在Python中会被转换成numpy数组 Workspace: 存储所有blobs 可以通过 workspace.FeedBlob() 和 workspace.FetchBlob() 来feed和fetch numyp数组 Nets and OperatorsCaffe2的基础网络模型抽象是 net(short of network). 一个net是由operator组成的图结构, 每个operator可以接受blobs集合的输入, 并且可以输出一个或多个output blobs Brewing Modelsbrew 是caffe2 中用于搭建模型的新的API, 它封装了新的ModelHelper, 使得搭建模型更加容易 Models and Datasets]]></content>
      <categories>
        <category>Caffe2</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程(五)-Saving and Loading Models]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-Saving-and-Loading-Models%2F</url>
    <content type="text"><![CDATA[本篇教程提高了大量的用例来说明如何保存和加载PyTorch models. 在介绍细节之前, 需要先熟悉下面的三个函数: torch.save: 保存一个序列化对象(serialized object)到磁盘中. 该函数使用的是Python的pickle工具完成序列化的. Models, tensors, 以及由各种对象所组成的字典数据都可以通过该函数进行保存. torch.load: 使用pickle的解包工具(unpickling facilities)来反序列化 pickled object 到内存中. 该函数同样可以操作设备(device)来加载数据 torch.nn.Module.load_state_dict: 利用非序列结构数据state_dict加载模型的参数字典. What is a state_dict?在PyTorch中, torch.nn.Module 模型中的可更新的参数(weighs and biases)在保存在模型参数中(model.parameters()). 而state_dict是一个典型的python字典数据, 它将每一层和层中的参数tensor相互关联起来. 注意到, 只有那些具有可更新参数的层才会被保存在模型的state_dict数据结构中. 优化器对象(Optimizer object-torch.optim)同样也可以拥有state_dict数据结构, 它包含了优化器的相关状态信息(超参数等). 下面看一个state_dict的简单例子. 调用时, 要使用 .state_dict() 来获得字典结构` 123456789101112131415161718192021222324252627282930313233343536# Define modelclass TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 个人建议最好将relu也写在__init__函数内, 否则无法通过模型获知到底使用了什么激活函数(只有通过forward函数才能知道) x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x# Initialize modelmodel = TheModelClass()# Initialize optimizeroptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)# Print model's state_dictprint("Model's state_dict:")for param_tensor in model.state_dict(): print(param_tensor, "\t", model.state_dict()[param_tensor].size())# Print optimizer's state_dictprint("Optimizer's state_dict:")for var_name in optimizer.state_dict(): print(var_name, "\t", optimizer.state_dict()[var_name]) Saving &amp; Loading Model for InferenceSave/Load state_dict(Recommended)Save:1torch.save(model.state_dict(), PATH) Load:123model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.eval() 当为inference阶段保存模型时, 仅仅保存训练好的模型的可更新参数即可. 利用torch.save()函数来保存模型的state_dict可以在之后恢复模型时提供极大的灵活性, 这也是我们推荐使用该方法来保存模型的原因. 模型的保存文件通常以.pt或者.pth结尾 请牢记在执行inference逻辑之前使用了model.eval()来将当前的模式转换成测试模式, 不然的话dropout层和BN层可能会产生一些不一致的测试结果. 请注意, load_state_dict()函数接受的参数是一个字典对象, 而不是模型文件的保存路径. 这意味着你必须先将模型文件解序列成字典以后, 才能将其传给load_state_dict()函数 Save/Load Entire ModelSave:1torch.save(model, PATH) Load:123# Model class must be defined somewheremodel = torch.load(PATH)model.eval() 这段保存/加载的流程使用了最直观的语法以及最少的代码. 以这种方式保存模型时将会用pickle模块把 整个 模型序列化保存. 这种方法的坏处是序列化的数据会和特定的classes绑定, 以及模型保存时固定的目录结构(这句话啥意思?). 造成这种结果的原因在于pickle没有保存模型本身, 而是保存了一个包含类的文件的路径. 因此, 这样的代码会在之后应用到其他工程时以各种方式造成程序崩溃. Saving &amp; Loading a General Checkpoint for Inference and/or Resuming TrainingSave:123456torch.save(&#123;"epoch":epoch, "model_state_dict":model.state_dict(), "optimizer_state_dict":optimizer.state_dict(), "loss": loss, ... &#125;, PATH) Load:123456789101112model = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH)model.load_state_dict(checkpoint["model_state_dict"])optimizer.load_state_dict(checkpoint["optimizer_state_dict"])epoch = checkpoint["epoch"]loss = checkpoint["loss"]model.eval()# --or--model.train() 当保存一个通用的 checkpoint 文件时, 我们不仅仅需要保存模型的 state_dict 信息, 还需要保存一些其他信息. 为此, 我们需要将这些信息组织成字典的形式, 然后利用 torch.save() 函数进行保存. 通常情况下, 在PyTorch中, 这些checkpoints文件使用 .tar 文件后缀. 在加载模型时, 首先要记得初始化模型, 然后利用 torch.load() 函数来你所需要的各项数据. Saving Multiple Models in One FileSave:1234567torch.save(&#123; "modelA_state_dict": modelA.state_dict(), "modelB_state_dict": modelB.state_dict(), "optimizerA_state_dict": optimizerA.state_dict(), "optimizerB_state_dict": optimizerB.state_dict(), ... &#125;, PATH) Load:12345678910111213141516modelA = TheModelAClass(*args, **kwargs)modelB = TheModelBClass(*args, **kwargs)optimizerA = TheOptimizerAClass(*args, **kwargs)optimizerB = TheOptimizerBClass(*args, **kwargs)checkpoint = torch.load(PATH)modelA.load_state_dict(checkpoint["modelA_state_dict"])modelB.load_state_dict(checkpoint["modelB_state_dict"])optimizerA.load_state_dict(checkpoint["optimizerA_state_dict"])optimizerB.load_state_dict(checkpoint["optimizerB_state_dict"])modelA.eval()modelB.eval()# - or -modelA.train()modelB.train() 当需要保存多个不同的模型时(如RNN, CNN), 可以用同样的方式将这些模型的 state_dict 信息保存起来, 并将它们组织成字典的形式, 然后利用torch.save()将他们序列化保存起来, 通常情况下文件以.tar后缀命名. Warmstarting Model Using Parameters from a Different ModelSave:1torch.save(modelA.state_dict(), PATH) Load:12modelB = ThemodelBClass(*args, **kwargs)modelB.load_state_dict(torch.load(PATH), strict=False) Partially loading a model 或者 loading a partial model 在迁移学习或者训练一个复杂模型时是很常见的, 即使只是用很小一部分参数, 也可以起到训练过程的热启动效果, 进而可以帮助模型更快的收敛.不论何时, 当你需要从 partial state_dict 中加载模型时, 都需要将参数 strict 设置为 False. Saving &amp; Loading Model Across DevicesSave on GPU, Load on CPUSave:1torch.save(model.state_dict(), PATH) Load:123device = torch.device("cpu")model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH, map_location=device)) 通过 torch.load() 函数的 map_location 参数来指定将模型的 state_dict 加载到哪个设备上. Save on GPU, Load on GPUSave:1torch.save(model.state_dict(), PATH) Load:1234device = torch.device("cuda:0")model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.to(device) 使用 .to 来将模型中的参数tensor转移到GPU设备上, 需要注意的是, 在进行训练或者预测之间, 还需要调用 tensor 的 .to() 方法来将 tensor 也转移到 GPU 设备上, 另外, 注意, mytensor.to(device) 实际上是在 GPU 中创建了 mytensor 的副本, 而并没有改变 mytensor 的值, 因此, 需要写成这样的形式来是的 mytensor 的值改变: my_tensor = my_tensor.to(torch.device(&quot;cuda&quot;)) Save on CPU, Load on GPUSave:1torch.save(model.state_dict(), PATH) Load:1234device = torch.device("cuda")model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH, map_location="cuda:0"))model.to(device) 由于模型是在CPU上存储的, 因此在模型加载时, 需要设置 torch.load() 函数的 map_location 参数为 cuda:0. 然后, 还需要调用 model 的 .to(device) 方法来将model的参数 tensor 全部转移到 GPU 上去, 另外别忘了将数据也要转移到 GPU 上去, my_tensor = my_tensor.to(torch.device(&quot;cuda&quot;)). Saving torch.nn.DataParallel ModelsSave:1torch.save(model.module.state_dict(), PATH) Load:1# Load to whatever device you want]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-个人简历总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[介绍一下自己一定要想办法主导后面面试的侧重点, 突出自己的优势, 强调自己的擅长领域 ZeroTensor你的项目和tiny dnn有什么区别。比它优势在哪里？你从tiny dnn这个项目中学到了什么？要问的问题?目前我们现在工业界在目标检测的落地应用方面, 通常会怎么做? 具体一点就是说, 是让检测过程本地上做计算, 还是说把模型放到服务器端, 然后在服务器上计算再返回结果? 如果是在本地计算的话? 会不会对本地设备要求有点高? 还有就是RCNN在实际场景用到底用的多不多? 是不是主要还是YOLO用的多一些? DF竞赛如果商汤/face++和我们公司同时给了你offer, 你会怎么选择从三个方面选择-- 薪资 三个方面的权重分别是 4:4:2 说说你的职业规划在竞赛中，使用了有哪些提升最终精度的方法, 每种方法分别提升了多少精度?multi-scale training $\{ 480, 576, 688, 864, 900\}$ and testing $\{480, 576, 688, 864, 1000\}$ 提升mAP 3~5% Iterative bounding box regression (multi-stage box regression): 数据增广. 目标检测大多论文出自何恺明之手, 最大的感觉就是，总是能从网络的baseline中挖掘中值得改进的地方，每一个改进都能够提高目标检测的精度或者速度。 技术总结擅长 C++、Python 编程语言算法基础扎实，具有良好的代码风格和质量意识对CNN 等深度学习技术了解透彻，擅长 TensorFlow 深度学习框架熟悉GPU并行计算及CUDA编程语言 自我评价代码工程能力强,可快速将想法付诸于代码实现, 对计算机技术充满热情，尤其是对计算机视觉领域，有极大的兴趣和求知欲，喜欢面对挑战，敢于尝试，有良好的沟通能力和团队合作精神]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程(四)-Transfer_Learning_Tutorial]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-Transfer-Learning-Tutorial%2F</url>
    <content type="text"><![CDATA[通常情况下, 我们不会从头训练整个神经网络, 更常用的做法是先让模型在一个非常大的数据集上进行预训练, 然后将预训练模型的权重作为当前任务的初始化参数, 或者作为固定的特征提取器来使用. 既通常我们需要面对的是下面两种情形: Finetuning the convnet: 在一个已经训练好的模型上面进行二次训练 ConvNet as fixed feature extractor: 此时, 我们会将整个网络模型的权重参数固定, 并且将最后一层全连接层替换为我们希望的网络层. 此时, 相当于是将前面的整个网络当做是一个特征提取器使用. Load Data我们将会使用torch.utils.data包来载入数据. 我们接下来需要解决的问题是训练一个模型来分类蚂蚁和蜜蜂. 我们总共拥有120张训练图片, 具有75张验证图片. 12345678910111213141516171819202122232425data_transforms = &#123; "train": transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), # 注意转换成tensor后, 像素会变成[0,1]之间的浮点数 transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ]), "val": transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ])&#125;data_dir = "hymenoptera_data"# from torchvision import datasetsimage_datasets = &#123;x:datasets.ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ["train", "val"]&#125;dataloaders = &#123;x:torch.utils.data.DataLoader(image_datasets[x]), batch_size=4, shuffle=True, num_workers=4) for x in ["train", "val"]&#125;dataset_sizes = &#123;x:len(image_datasets[x]) for x in ["train", "val"]&#125;class_names = image_datasets["train"].classesdevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") Visualize a few images1234567891011121314def imshow(inp, title=None): inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updatedinputs, class_ids = next(iter(dataloaders["train"])) # 获取一个batchout = torchvision.utils.make_grid(inputs)imshow(out, title=[class_names[x] for x in class_ids]) Training the model接下来, 让我们定义一个简单的函数来训练模型, 我们会利用LR scheduler对象torch.optim.lr_scheduler设置lr scheduler, 并且保存最好的模型. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(epoch) for phase in ["train", "val"]: if phase == "train": model.train() else: model.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() # forward with torch.set_grad_enabled(phase == "train"): outputs = model(inputs) _, preds = torch.max(outputs,1) # preds代表最大值的坐标, 相当于获取了最大值对应的类别 loss = criterion(outputs, labels) if phase = "train": # 只有处于train模式时, 来更新权重 loss.backward() optimizer.step() # 统计状态 running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds==labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(phase, epoch_loss, epoch_acc) if phase == "val" and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) time_elapsed = time.time() - since print(time_elapsed) print(best_acc) # load best model weights model.load_state_dic(best_model_wts) return model Visualizing the model predictions下面的代码用于显示预测结果 12345678910111213141516171819202122232425def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): # 不计算梯度 for i, (inputs, labels) in enumerate(dataloaders["val"]): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs,1) for j in range(inputs.size()[0]): # 或者batch size images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis("off") ax.set_title(class_names[preds[j]]) imshow(inputs.cpu().data[j]) # 由于imshow不能作用在gpu的数据上, 因此需要先将其移动到cpu上. if images_so_far == num_images: model.train(mode = was_training) return model.train(mode=was_training) FineTuning the convnet加载预训练模型, 并重置最后一层全连接层 123456789101112# from torchvisioin import modelsmodel_ft = models.resnet18(pretrained=True)num_ftrs = model_ft.fc.in_featuresmodel_ft = model_ft.to(device)criterion = nn.CrossEntropyLoss()# 这里是让所有的参数都进行更新迭代optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Train and evaluate调用刚刚定义的训练函数对模型进行训练123model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)visualize_model(model_ft) Convnet as Fixed Feature Extractor假设我们需要将除了最后一层的其它层网络的参数固定(freeze), 为此, 我们需要将这些参数的requires_grad属性设置为False. 123456789101112131415model_conv = torchvision.models.resnet18(pretrained=True)for param in model_conv.parameters(): param.requires_grad = False# 将最后一层fc层重新指向一个新的Module, 其内部参数的requires_grad属性默认为Truenum_ftrs = model_conv.fc.in_featuresmodel_conv.fc = nn.Linear(num_ftrs,2)model_conv = model.to(device)criterion = nn.CrossEntropyLoss()optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate12model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_eopch=25)visualize_model(model_conv)]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os模块-系统操作]]></title>
    <url>%2Fz_post%2FPython-os%2F</url>
    <content type="text"><![CDATA[os.path.isabs(…) # 判断是否绝对路径 os.path.exists(…) # 判断是否真实存在 os.path.isdir(…) # 判断是否是个目录 os.path.isfile(…) # 判断是否是个文件 os.path.split(…) # 分隔目录和文件名/文件夹名 os.path.splitdrive(…) # 分隔盘符(windows系统) os.path.splitext(…) # 分隔文件和扩展名 os.walk() 方法用于通过在目录树中游走输出在目录中的文件名，向上或者向下。 os.mkdir(path[, mode])创建一个目标, 参数可以是相对或者绝对路径, mode的模式默认为0777,如果目录有多级, 则创建最后一级. 如果最后一级目录的上级有不存在的或者目录已经存在, 则会抛出一个OSError os.makedirs(path[, mode])创建 递归 的目录树, 参数可以是相对或者绝对路径, mode的默认模式也是0777.如果子目录已经存在, 则创建失败, 会抛出一个OSError的异常]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib模块-数据绘图]]></title>
    <url>%2Fz_post%2FPython-matplotlib%2F</url>
    <content type="text"><![CDATA[基础知识Matplotlib是一个Python 2D绘图库，可以在各种平台上以各种硬拷贝格式和交互式环境生成出版质量数据。Matplotlib可用于Python脚本，Python和IPython shell，jupyter笔记本，Web应用程序服务器和四个图形用户界面工具。 matplotlib画板一般由以下部分组成 创建了subplot后, 如果发出绘图指令，matplotlib这时就会在你最后一个用过的subplot中（没有则创建）绘制. 12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport numpy as np# 多个figurex = np.linspace(-1, 1, 50)y1 = 2*x + 1y2 = 2**x + 1# 使用figure()函数重新申请一个figure对象# 注意，每次调用figure的时候都会重新申请一个figure对象plt.figure()# 第一个是横坐标的值，第二个是纵坐标的值plt.plot(x, y1)# 第一个参数表示的是编号，第二个表示的是图表的长宽plt.figure(num = 3, figsize=(8, 5))# 当我们需要在画板中绘制两条线的时候，可以使用下面的方法：plt.plot(x, y2)plt.plot(x, y1, color='red', # 线颜色 linewidth=1.0, # 线宽 linestyle='--' # 线样式 )plt.show() # 会将所有的figure对象显示出来, 这里有2个 用matplotlib显示图片1import matplotlib.pyplot as plt # plt 用于显示图片 2D图表风格散点图12matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None,alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, ** kwargs) 参数说明: x/y: 一维向量, 且长度相等 s: 标记大小, 以平方磅为单位的标记面积，指定为下列形式之一： 数值标量 ： 以相同的大小绘制所有标记。 行或列向量 ： 使每个标记具有不同的大小。x、y 和 sz 中的相应元素确定每个标记的位置和面积。sz 的长度必须等于 x 和 y 的长度。 [] ： 使用 36 平方磅的默认面积 c: 标记颜色, 指定为下列形式之一: RGB 三元数或颜色名称 - 使用相同的颜色绘制所有标记。 由 RGB 三元数组成的三列矩阵 - 对每个标记使用不同的颜色。矩阵的每行为对应标记指定一种 RGB 三元数颜色。行数必须等于 x 和 y 的长度。 向量 - 对每个标记使用不同的颜色，并以线性方式将 c 中的值映射到当前颜色图中的颜色。c 的长度必须等于 x 和 y 的长度。要更改坐标区的颜色图，请使用 colormap 函数 marker: 标记样式 linewidths: 线宽 颜色种类 样式种类 直方图 hist12matplotlib.pyplot.hist(x, bins=10, range=None, normed=False, weights=None, cumulative=False, bottom=None, histtype=u'bar', align=u'mid', orientation=u'vertical', rwidth=None, log=False, color=None, label=None, stacked=False, hold=None, **kwargs) 参数说明: x: 横轴对应的数据 bins: 指定条状图的条形个数 range: 横轴的范围, 默认为数据最小和最大的值作为范围 normed: 默认为False, 纵轴表示频数, 如果置为True, 则纵轴表示频率 rwidth: 条状图柱子与柱子之间的距离, 默认为0 饼状图 pie]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[skimage模块-图像处理]]></title>
    <url>%2Fz_post%2FPython-skimage%2F</url>
    <content type="text"><![CDATA[比opencv的速度要慢很多, 但是使用起来更加简单, 真的对速度要求很高的话, 一般都会C++和opecv使用. 所以一般情况下, 首先看skimage能否实现, 不行的话再转用opencv 123import skimagefrom skimage import io # IO is a submodule. Submodules need to be imported from the parent module explicitly.img = io.imread("1.jpg")]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[opencv模块-计算机视觉算法库]]></title>
    <url>%2Fz_post%2FPython-opencv%2F</url>
    <content type="text"><![CDATA[opencv 基础知识读取图片保存图片cv2.putText()(startX, startY) 为左下角坐标1cv2.putText(img, "text test", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness) cv2.rectangle()(x,y) 为左上角坐标(x+h,y+w) 为右下角坐标1cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness) cv2.waitKey()12345keypress = cv2.waitKey(200) # 200为当前图片的显示持续时间if keypress == ord('c') # keypress为按键的整数形式, 所以需要用ord将字符类型转换if cv2.waitKey(200) == 27: # Decimal 27 = Esc opencv与numpyopencv的基础类型为numpy.ndarray, 因此可以直接使用 ndarray 的一些属性的方法 1234import cv2image = cv2.imread('./test.jpg')print(type(image) #&lt;class 'numpy.ndarray'&gt;print(image.shape) #(500, 1069, 3) (高, 宽, 通道), 利用 cv2.merge 方法将 numpy.ndarray 数据转换成opencv的图片数据: 123456789101112# 图片的分辨率为300*200(宽*高)，这里b, g, r设为随机值，注意dtype属性b = np.random.randint(0, 255, (200, 300), dtype=np.uint8)g = np.random.randint(0, 255, (200, 300), dtype=np.uint8)r = np.random.randint(0, 255, (200, 300), dtype=np.uint8)# 合并通道，形成图片img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后# 显示图片cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test') opencv 基本图像操作通道的拆分与合并12345678910111213141516# 图片的分辨率为800*200(宽*高)，这里b, g, r设为随机值，注意dtype属性b = np.random.randint(0, 255, (200, 800), dtype=np.uint8)g = np.random.randint(0, 255, (200, 800), dtype=np.uint8)r = np.random.randint(0, 255, (200, 800), dtype=np.uint8)# 合并通道，形成图片img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后# 显示图片cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test')# 拆分通道, 每个通道都变成了单通道数组[blue, green, red] = cv2.split(img) 用matplotlib显示图像1234b,g,r=cv2.split(img)img2=cv2.merge([r,g,b])plt.imshow(img2)plt.show() 更方便的转换通道的方式:1rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) opencv 核心图像算法cv2123456789101112131415161718import cv2image_path = './test.jpg'src_image = cv2.imread(image_path) # 读取图片size = src_image.shape # 获取图片的尺寸, 返回一个元组: (height, width, depth)copy_image = src_image.copy() # 复制图片cv2.imwrite('./dst_test.jpg', copy_image) # 保存图片cv2.imshow('image', src_image) # 显示图片# 利用下标访问指定像素for x in range(src_image.shape[0]): # 以行为主, 行数=图片height for y in range(src_image.shape[1]): # 列数 = 图片width src_image[x,y] = (255,0,255) # (blue, green, red) 值越高表示对应颜色越显著, 全0为黑, 全255为白]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NumPy模块-线性代数计算库]]></title>
    <url>%2Fz_post%2FPython-numpy%2F</url>
    <content type="text"><![CDATA[numpy基础知识ndarray 对象ndarray对象的属性表示一个 n 维数组, 描述相同类型的元素集合, 基于0的索引访问 创建 ndarray 1numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0) object: 可以返回一个数组或任何(嵌套)序列的对象。 dtype: 数据类型对象 copy: 对象是否被复制 order: C 安行, F 按列, 或者默认(A) subok: 默认情况下，返回的数组被强制为基类数组。 如果为true，则返回子类 ndmin: 指定返回数组的最小维数。 123456789101112131415# 最小维度 import numpy as npa = np.array([[1,2,3], [1,2,4]], dtype= float ,ndmin = 2)print(a)print(a.shape)print(a.dtype)print(a.ndim)print(a.size)#output:[[ 1. 2. 3.] [ 1. 2. 4.]](2, 3)float6426 numpy.array 中的 object 传入参数必须是同一结构的, 否则将进行自动类型转换, 如果指定了 dtype 参数, 则以该参数类型为准(规则与C++类似) ndarray对象的方法ndarray.ptp(axis=None, out=None) ndarray.clip()ndarray.all()ndarray.any()ndarray.swapaxes(axis1, axis2) numpy支持的数据类型 数据类型对象 dtypedtype是一个对象, 这个对象所属类的名字叫做 “数据类型” dtype 通常用于结构化数据类型: 123import numpy as npdt = np.dtype([('age',np.int8)]) print(dt) #输出如下： [('age', 'i1')] numpy基本操作根据下标组成新的数组123a = np.asarray([1,2,3,4,5,6,7,8,9])c = a[[2,3,4]]print(c) # [3,4,5] 比较运算符12345678910111213141516171819a = np.array([[1,2,3], [1,2,4]], dtype= float ,ndmin = 2)# 可以对ndarray中的值依次进行比较运算符, 返回bool数组print(a == 3)#out[[False False True] [False False False]]#out print(a &lt; 3) #out[[ True True False] [ True True False]]#out# 可以根据bool数组对ndarray中的值进行筛选, 返回1维数组print(a[ a&lt;3 ])#out[1. 2. 1. 2.]#out 向量运算矩阵运算矩阵运算主要注意的是axis参数的选择, axis参数的作用是, 沿着axis参数进行运算, 即运算后, 其他维度不变, axis指定的维度消失(所以维度会减1) numpy常用函数ndarray对象的创建np.array该函数会返回一个ndarray对象(注意numpy中不存在array这种数据对象类型). np.ndarray该函数也会返回一个ndarray对象, 但是不推荐使用(为啥?), np.ndarray是一个底层的方法, 会被其他多维数组创建方法所调用 np.zeros()生成默认元素为 0. (注意是float64类型)的ndarray 12345np.zeros ((3,4))array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones()生成默认元素为 1. (注意是float64类型)的ndarray 12345z = np.ones ((3,4))array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) np.arange()指定范围和数值间的间隔生成 array，注意范围包左不包右 1234l = np.arange(0,10,2) # 这里与python中的range不同的是, 必须指定第一个元素, 其次, 这里步长可以为小数, python的则不行[0 2 4 6 8] np.randomnp.random.random:12345random, 生成0~1的随机小数,默认类型为float64, 还有其他更多类型的randomnp.random.random((2,3))array([[ 0.86166627, 0.37756207, 0.94265883], [ 0.9768257 , 0.96915312, 0.33495431]]) np.random.randn: 从标准正态分布中随机输出1numpy.random.randn(d1,d2,...,dn) # d1,d2,...,dn为维度 np.random.rand: 生成[0,1)之间的随机数据1numpy.random.rand(d1,d2,...,dn) # d1,d2,...,dn为维度 ndarray.reshape()可以对ndarray的维度重新进行调整 123456arr = np.arange(15).reshape(3, 5)arrarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 数组拼接np.append()np.append(arr, values, axis=None)参数: arr 返回: np.concatenate()统计类数值np.median()1numpy.median(a, axis=None, out=None, overwrite_input=False, keepdims=False) 沿着指定的轴计算中位数参数: a: 类似数组的变量. 不一定非要是ndarray类型, 只要是任何可以转换成ndarray类型的数据变量都可以 np.mean()1numpy.mean(a, axis=None, dtype=None, out=None, keepdims=&lt;no value&gt;) 返回沿着指定轴的算数平均值参数: a: 类似数组的变量. dtype: 数据类型, 可选参数. 对于整数输入来说, 默认返回值为float64, 对于其他floating输入来说, 返回值类型与输入类型保持一致. np.round()np.newaxisnp.hstack()np.vstack()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程(三)-Learning PyTorch with Examples]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-Learning-PyTorch-with-Examples%2F</url>
    <content type="text"><![CDATA[TensorsWarm-up: numpy对于numpy来说, 它对计算图, 深度学习, 梯度等等概念几乎是不知道的, 但是, 如果我们了解简单神经网络的具体结构, 那么我们就可以很轻易的用numpy来实现这个简单网络, 对此, 我们通常需要自己来实现前向计算和反向计算的逻辑, 下面我们来实现一个具有两层隐藏层的简单网络: 12345678910111213141516171819202122232425262728293031323334353637import numpy as np# N 为batch size, D_in 为输入维度# H 为隐藏层的维度, D_out 为输出的维度N, D_in, H, D_out = 64, 1000, 100, 10# 创建随机的输入和输出数据x = np.random.randn(N, D_in) # N × D_in 的矩阵y = np.random.randn(N, D_out) # N × D_out 的矩阵# 对两个隐藏层w1,w2进行初始化w1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)# 设置学习率learning_rate = 1e-6for t in range(500): # 前向传播: 计算预测结果 y_pred h = x.dot(w1) # x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100 h_relu = np.maximum(h,0) y = h_relu.dot(w2) # h_relu维度为 64×100, w2维度为100×10, y的维度为64×10 # 计算损失 loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播根据loss更新w1和w2的值 grad_y_pred = 2.0*(y_pred - y) # 对y_pred求导 grad_w2 = h_relu.T.dot(grad_y_pred) # 对w2求导, 微分矩阵应该与w2的size相同 grad_h_relu = grad_y_pred.dot(w2.T) # 对h_relu求导 grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = grad_h_relu # 经过relu, 将小于0的梯度归0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2 在执行上述代码以后, w1和w2的值会是的预测出来的pred_y与y之间的平方损失越来越小. PyTorch: Tensors用PyTorch实现一个简单的神经网络在神经网络的实现中, 较麻烦的是梯度的计算过程, 下面利用PyTorch的自动求导来实现一个简单的神经网络(两层隐藏层) 1234567891011121314151617181920212223242526272829303132333435363738import torchdtype = torch.floatdevice = torch.device("cpu")# N为batch size, D_in为input dimension# H为hidden dimension, D_out为output dimensionN, D_in, H, D_out = 64, 1000, 100, 10# 创建输入和输出的Tensors# requires_grad的值默认为False 指明无需计算x和y的梯度x = torch.randn(N, D_in, device=device, dtype=dtype)y = torch.randn(N, D_in, device=device, dtype=dtype)# 初始化两个隐藏层的参数, 注意要将requires_grad的值设置为Truew1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)learning_rate = 1e-6for t in range(500): ## torch.mm / torch.Tensor.mm : matrix multiplication h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) loss = (y_pred - y).pow(2).sum().item() print(t, loss) # BackProp grad_y_pred = 2*(y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h&lt;0] = 0 # 将grad_h_relu中小于0的都置为0, 即为relu的反向传播公式(因为小于0的梯度为0, 大于0的梯度为1) grad_w1 = x.t().mm(grad_h) w1 -= lr * grad_w1 w2 -= lr * grad_w2 Autograd自定义一个具有自动求导功能的PyTorch函数上面的例子是使用手动的方式求梯度的, 当模型参数变多时, 这样的方式显然很不方便. 不过, 借助PyTorch的autograd模块, 可以方便的求取任意参数的导数.在使用PyTorch的自动推导模块autograd时, 前向传播过程会被定义成一个计算图, 图中的节点是Tensors, 图中的边是一些函数, 用于根据 input Tensors 来生成 output Tensors. 比如当 x 是一个Tensor, 并且拥有属性x.requires_grad=True, 那么x.grad就是另一个Tensor, 它持有loss相对于x的梯度. 1234567891011121314151617181920212223242526272829303132333435363738import torchdtype = torch.floatdevice = torch.device("cpu")# N为batch size, D_in为input dimension# H为hidden dimension, D_out为output dimensionN, D_in, H, D_out = 64, 1000, 100, 10# 创建输入和输出的Tensors# requires_grad的值默认为False 指明无需计算x和y的梯度x = torch.randn(N, D_in, device=device, dtype=dtype)y = torch.randn(N, D_in, device=device, dtype=dtype)# 初始化两个隐藏层的参数, 注意要将requires_grad的值设置为Truew1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)learning_rate = 1e-6for t in range(500): # 定义前向计算过程, mm为两个矩阵相乘, clamp可以将数据限定在某一范围内, 实现relu的功能 y_pre = x.mm(w1).clamp(min=0).mm(w2) # 计算loss, loss.item()可以得到loss的矢量值 loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # 只需要调用一条语句, 即可计算出所有requires_grad设置为True的参数的梯度, 可以通过w1或者w2的grad属性来访问各自的梯度. loss.backward() # 所有的ops, 如conv, relu等的backward方法已经在PyTorch内部实现 # 手动更新参数(面对大型网络时, 可以通过调用torch.optim.SGD来自动更新) # 将参数放在 torch.no_grad() 管理环境当中, 这是因为我们无需对grad进行跟踪, 因此, 也需要在更新完参数以后, 将grad重新置为0 , 以便下一次更新 with torch.no_grad(): w1 -= learning_rate*w1.grad w2 -= learning_rate*w2.grad # 为了避免当前求取的梯度的值累加到下一次迭代当中, 调用zero_原地清空grad. 对于nn.Module, 可以调用`zero_grad`来清空所有Module中的参数的梯度. w1.grad.zero_() w2.grad.zero_() Defining new autograd functions在PyTorch中, 每一个具有自动求导功能的operator都由两个作用在Tensors上的函数实现, 分别是用于计算输出的 前向函数 , 以及用于计算梯度的 反向函数. 因此, 我们在可以在PyTorch中通过继承父类torch.autograd.Function, 并实现其中的forward 和 backward 函数来定义自己的自定义autograd functions 1234567891011121314151617181920212223import torchclass MyReLU(torch.autograd.Function): @staticmethod # 将该方法变成静态方法, 使得不用实例化也可以调用, 当前实例化也可以调用 def forward(ctx, input): # ctx 是一个上下文管理器, 它可以利用`ctx.save_for_backward`把任何需要在backward用到的对象都存储起来 ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): # grad_output 为从下游传回来的梯度 input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input&gt;0] = 0 return grad_input# 使用方法: 使用.apply方法来应用自定义的opsy_pred = MyReLU.apply(x.mm(w1)).mm(w2)# 为了方便, 也可以先对MyReLU重命名, 然后调用更简洁的别名relu = MyReLU.applyy_pred = relu(x.mm(w1)).mm(w2) Static GraphsPyTorch采用动态计算图, 而TensorFlow采用静态计算图 静态计算图: 只对计算图定义一次, 而后会多次执行这个计算图.好处: 可以预先对计算图进行优化, 融合一些计算图上的操作, 并且方便在分布式多GPU或多机的训练中优化模型 动态计算图: 每执行一次都会重新定义一张计算图. 控制流就像Python一样, 更容易被人接受, 可以方便的使用for, if等语句来动态的定义计算图, 并且调试起来较为方便. nn Modulenn对于大型网络模型来说, 直接使用autograd有些太过底层(too low-level). 为此在搭建神经网络时, 我们经常会将计算放置到 layers上 , 这些 layers 中的可学习参数会在训练中就行更新. 在TF中, Keras, TF-Slim等提高了封装性更高的高层API, 在PyTorch中, nn 包可以提供这些功能. 在nn包中, 定义了一系列的 Modules , 可以类比为神经网络中的不同层. 一个 Module 会接受一组 input Tensors, 并计算出对应的 output Tensors, 同时会持有一些内部状态(如可学习的权重参数). 在nn包中还定义了一系列有用的 loss functins 可供使用. 下面尝试用 nn 包来实现上面的两层网络: 123456789101112131415161718192021222324252627282930importN, D_in, H, D_out = 64, 1000, 100, 10x = torch.randn(N,D_in)y = torch.randn(N,D_out)# 由于当前的两层网络是序列的, 因此可以使用 torch.nn.Sequential 来定义一个Module, 该 Module 中包含了一些其它的 Modules (如Linear, ReLU等),# Sequential Module会序列化的执行这些 Modules, 并且自动计算其output和grads.# 注意因为是序列化执行的, 因此无需自定义 forward. 这是与 nn.Module 的区别之一.model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out))loss_fn = torch.nn.MESLoss(reduction="sum")lr = 1e04for t in range(500): y_pred = model(x) loss = loss_fn(y_pred, y) print(t, loss.item()) # 在获取梯度前, 先清空梯度缓存 model.zero_grad() loss.backward() with torch.no_grad(): for param in model.parameters(): param -= lr * param.grad optim可以看到, 上面在更新参数时, 我们仍采取的是手动更新的方式, 对于简单的优化算法来说, 这并不是什么难事, 但是如果我们希望使用更加复杂的优化算法如AdaGrad, Adam时, 采用 optim 包提供的高层API可以方便的使用这些优化算法.12345678optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)for t in range(500): y_pred = model(x) loss = loss_fn(y_pred, y) optimizer.zero_grad() # 已经将待优化参数model.parameters()传给优化器了 loss.backward() optimizer.step() # 执行一次参数优化操作(是不是很简单?) Custom nn Modules有时候, 我们需要定义一些相比于序列化模型更加复杂的模型, 此时, 我们可以通过继承nn.Module,同时定义forward前向计算函数来自定义一个 Module. 下面我们就用这种方式来自定义一个具有两层网络的 Module.12345678910111213141516171819202122232425262728class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): # 通常我们将具有参数的层写在__init__函数中, 将不具有参数的ops写在forward中 self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, input): h_relu = self.linear1(input).clamp(min=0) y_pred = self.linear2(h_relu) return y_predN, D_in, H, D_out = 64, 1000, 100, 10x = torch.randn(N, D_in)y = torch.randn(N, D_out)model = TwoLayerNet(D_in, H, D_out)loss_fn = torch.nn.MSELoss(reduction="sum")optim = torch.optim.SGD(model.parameters(), lr=1e-4)for t in range(500): y_pred = model(x) loss = loss_fn(y_pred, y) optim.zero_grad() loss.backward() optim.step() Control Flow + Weight Sharing为了更好的演示动态图和权重共享的特点, 我们会在下面实现一个非常奇怪的模型: 一个全连接的ReLU网络, 中间会随机的使用1~4层隐藏层, 并且重复利用相同的权重来计算最深层的隐藏层输出. 在PyTorch中, 我们可以通过for循环来实现这种动态模型, 并且通过重复调用同一个Module就可以很容易使用多层之间的参数共享, 如下所示:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import randomimport torchclass DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): # 实现三个 nn.Linear 实例, 意味着在模型中只有 三个 nn.Linear 的参数 super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): """ 在PyTorch中, 我们可以通过for循环来随机的选择中间层的层数, 使得每一次 执行forward函数时, 都具有不同的中间层层数. 而这些中间层都来自于同一个Module实例, 因而具有共享的权重参数. """ h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0,3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred;# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold inputs and outputsx = torch.randn(N, D_in)y = torch.randn(N, D_out)# Construct our model by instantiating the class defined abovemodel = DynamicNet(D_in, H, D_out)# Construct our loss function and an Optimizer. Training this strange model with# vanilla stochastic gradient descent is tough, so we use momentumcriterion = torch.nn.MSELoss(reduction='sum')optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)for t in range(500): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) print(t, loss.item()) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step()]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习方法]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[个体与集成集成学习(ensemble learning): 通过构建并结合多个学习器来完成学习任务 同质集成(homogeneous): 集成中只包含同种类型的个体学习器( 神网+神网, 决策树+决策树), 其中的个体学习器称为”基学习器”, 算法为”基学习算法”. 异质集成(heterogenous): 集成中包含不同类型的个体学习器( 神网+决策树 ), 其中的个体学习器称 “组件学习器”或”个体学习器” 要获得好的集成效果, 每个个体学习器应 “好而不同”, 即个体学习器要有一定的准确性, 同时还要有一定的差异性(否则多个学习器会退化成单一学习器,因为每个学习器都差不多) 事实上, 个体学习器的”准确性”和”差异性”本身就存在冲突, 一般的, 当准确性很高之后, 要增加多样性就需要牺牲准确性. 根据个体学习器的生成方式, 目前的集成学习方法大致可分为两大类: 个体学习器间存在依赖关系, 必须串行生成的序列化方法, eg: Boosting 个体学习器间不存在依赖关系, 可同时生成的并行化方法, eg: Bagging 和 随机森林(Random Forest) Boosting工作机制: 先从初始训练集训练出一个基学习器, 再根据基学习器的表现对训练样本进行调整, 使得先前基学习器做错的训练样本在后续受到更多关注, 然后基于调整后的样本分布来训练下一个基学习器, 如此重复进行, 直至基学习器数目达到事先指定的值T, 最终将这T个基学习器进行加权结合 Boosting族有很多种算法, 最著名的代表是 AdaBoost]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python 基础]]></title>
    <url>%2Fz_post%2FPython-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[列表遍历列表:123456789101112131415161718192021222324252627zz_list = ['a', 'b', 'c', 'd']for index in list: print(index) # 0 # 1 # 2 # 3for index in range(len(list)): print(index) # 0 # 1 # 2 # 3for index, val in enumerate(list): print(index, val) # 0 a # 1 b # 2 c # 3 d# 设置遍历的开始序号, val的输出不变for i, val in enumerate(list, 2): print(index, val) # 2 a # 3 b # 4 c # 5 d append() 方法追加单个元素 extend() 方法extend()函数用于在列表末尾一次性追加另一个序列中的多个值(用新列表扩展原来的列表).该方法没有返回值, 会直接在已经存在的列表中添加新的列表内容12345a= [[1,2,3],[4,5,6]]b= [['a','b','c'],['d','e','f']]a.extend(b)print(a)# [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']] 序列切片(双冒号)Python序列切片地址可以写为 [开始(包含) : 结束(不包含) : 步长]. 当开始省略的时候, 默认从第0项开始, 当结尾省略的时候, 默认到数组最后, 当步长省略的时候, 默认为1. 步长可以为负数, 代表从右向左取数.123456a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9]a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] 包含开始下标, 不包含结束下标a[1::2] # [1, 3, 5, 7, 9]a[::3] # [0, 3, 6, 9]a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]a[::-2] # [9, 7, 5, 3, 1] 字典遍历字典:1zz_dict = &#123;'x': 1, 'y':2, 'z':3&#125; 遍历keys: 123456789# 输出均为: x y zfor key in zz_dict: print(key)for key in zz_dict.iterkeys(): print(key)for key in zz_dict.keys(): print(key) 遍历values: 123456# 输出均为 1 2 3for value in zz_dict.itervalues(): print(value)for value in zz_dict.values(): print(value) 遍历keys和values 123456# 输出为: x corresponds to 1 (其余两个也一样)for key, value in zz_dict.iteritems(): print(key, "corresponds to", value)for key, value in zz_dict.items(): print(key, "corresponds to", value) csv12345678import csv# 只读csv文件with open('./csvfile.csv') as csv_file: csv_reader = csv.reader(csv_file) csv_head = next(csv_reader) for csv_row in csv_reader: print(csv_row) Pathreduce() 函数reduce() 函数会对参数序列中元素进行累积 函数将一个数据集合(列表, 元组等) 中的所有数据进行以下操作: 用传给reduce中的函数function(有两个参数)先对集合中的第1,2个元素进行操作, 得到的结果再与第三个数据用function函数运算, 最后得到一个结果 语法: 1reduce(function, iterable[, initializer]) zip() 函数zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个个 元组 ,然后返回有这些元组组成的 对象. ( 相比于python2中返回列表的方式, 这样做的好处是节约了不少的内存 )可以用list()转换或者dict()转换将对象转换成相应的数据类型如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同, 多出来的部分会被舍弃, 利用*号操作符, 可以将元组解压成列表. 123456789101112131415161718192021a = [1,2,3]b = [4,5,6]c = ['a','b','c','d','e','f']zip_ab = zip(a,b)print(zip_ab) # &lt;zip object at 0x104605348&gt;print(dict(zip_ab)) # &#123;1: 4, 2: 5, 3: 6&#125;# !!!注意, 一旦将zip_ab转换成dict以后, zip_ab内部就为空了!! 例如, 再次调用上面的语句:print(dict(zip_ab)) # &#123;&#125;# 但是zip_ab对象本身不会消失, 地址仍然不变print(zip_ab) # &lt;zip object at 0x104605348&gt;zip_abc = zip(a,b,c) # 注意, 三个元素的zip是不能转换成dict类型的print(zip_abc) # &lt;zip object at 0x1046054c8&gt;print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')]zip_abc = zip(a,b,c)z_a, z_b, z_c = zip(*zip_abc) # 利用zip(*)可以将zip对象重新解压, 返回类型是元组print(z_a) # (1,2,3)print(z_b) # (4,5,6)print(z_c) # ('a','b','c') getattr() 函数getattr()函数用于返回一个对象的属性值, 语法如下1getattr(object, name[, default]) 参数: object: 对象 name: 字符串, 对象属性 default: 默认返回值, 如果不提供该参数, 在没有对应属性时, 将触发Attributerror @propertyhttp://python.jobbole.com/80955/ 下划线 _var: 在一个模块中以单下划线开头的变量和函数会被默认当做内部函数, 在使用from a_module import * 导入时, 这部分变量和函数不会被导入. 不过如果使用import a_module导入模块时, 仍然可以用a_module._var的形式访问该变量或函数 var_: 有时候, 一个变量的最适合的名称已经被另一个关键字所占用. 在这种情况下, 可以在名称的末尾附加一个下划线来解决冲突. __var: 双下划线前缀会导致Python解释器重写属性名称, 以避免子类中的命名冲突. 举例来说, 如果在class Test中有一个成员__x, 那么当利用内置函数dir(Test)来查看类的属性时, 会发现__x被解释器重命名为_Test__x. 双下划线的名称修饰同样也适用于方法名称. __var__: 双下划线开头和结尾的是一些 Python 的特殊对象, 如类成员的 __init__, __del__, __name__, __call 等. Python 官方推荐永远不要讲这样的命名方式应用于自己的变量或函数. 有一种说法是说双下划线建议为类的私有成员, 但是 PEP8 当前的官方版本中并没有明说. _: 有时候我们会用一个独立的下划线作为一个名字, 这通常是用来指示某个变量时临时的或者无关紧要的. 星号 *单星号: 将所有参数以 元组(tuple) 的形式导入123456def foo(param1, *param2): print(param1) print(param2)foo(1,2,3,4,5)# 1# (2,3,4,5) 双星号: 将所有参数以 字典 的形式导入123456def bar(param1, **param2): print(param1) print(param2)bar(1, a=2, b=3)# 1# &#123;'a': 2, 'b': 3&#125; 当然这两个用法可以同时出现在一个函数中:12345678910def fun(a, b=10, *args, **kwargs): print(a) print(b) print(args) print(kwargs)fun(1,2,3,4,e=5,f=6)# 1# 2# (3,4)# &#123;'e': 5, 'f': 6&#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程(二)-DataLoadingAndProcessing]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-Data-Loading-and-Processing%2F</url>
    <content type="text"><![CDATA[对于一个新的机器/深度学习任务, 大量的时间都会花费在数据准备上. PyTorch提供了多种辅助工具来帮助用户更方便的处理和加载数据. 本示例主要会用到以下两个包: scikit-image: 用于读取和处理图片 pandas: 用于解析csv文件 导入下面的包123456789101112131415from __future__ import print_function, divisionimport osimport torchimport pandas as pdfrom skimage import io, transformimport numpy as npimport matplotlib.pyplot as pltfrom torch.utils.data import Dataset, DataLoaderfrom torchvision import transforms, utils# Ignore warningsimport warningswarnings.filterwarnings("ignore")plt.ion() # interactive mode 本示例使用的是人脸姿态的数据集, 数据集的标注信息是由68个landmark点组成的, csv文件的格式如下所示:123image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y0805personali01.jpg,27,83,27,98, ... 84,1341084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 利用如下代码可以快速的读取CSV文件里面的标注信息, 并且将其转换成 (N,2) 的数组形式, 其中, N 为 landmarks 点的个数12345678910landmarks_frame = pd.read_csv('faces/face_landmarks.csv')n = 65img_name = landmarks_frame.iloc[n, 0]landmarks = landmarks_frame.iloc[n, 1:].as_matrix()landmarks = landmarks.astype('float').reshape(-1, 2)print('Image name: &#123;&#125;'.format(img_name))print('Landmarks shape: &#123;&#125;'.format(landmarks.shape))print('First 4 Landmarks: &#123;&#125;'.format(landmarks[:4])) 利用下面的函数可以将图像和标注文件中的点显示出来, 方便观察:12345678910def show_landmarks(image, landmarks): """Show image with landmarks""" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # pause a bit so that plots are updatedplt.figure()show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks)plt.show() Dataset classtorch.utils.data.Dataset实际上是一个用来表示数据集的虚类, 我们可以通过集成该类来定义我们自己的数据集, 在继承时, 需要重写以下方法: __len__: 让自定义数据集支持通过len(dataset)来返回dataset的size __getitem__: 让自定义数据集支持通过下标dataset[i]来获取第 $i$ 个数据样本. 接下来, 尝试创建人脸姿态的自定义数据集. 我们将会在__init__函数中读取csv文件, 但是会将读取图片的逻辑代码写在__getitem__方法中. 这么做有助于提高内存使用效率, 因为我们并不需要所有的图片同时存储在内存中, 只需要在用到的时候将指定数量的图片加载到内存中即可. 我们的数据集样本将会是字典形式: {&#39;image&#39;: image, &#39;landmarks&#39;:landmarks}. 我们的数据集将会接受一个可选参数transform, 以便可以将任何需要的图片处理操作应用在数据样本上. 使用transform会使得代码看起来异常整洁干净.123456789101112131415161718192021222324class FaceLandmarksDataset(Dataset): def __init__(self, csv_file, root_dir, transform=None): # 参数: # csv_file(string): csv标签文件的路径 # root_dir(string): 所有图片的文件夹路径 # transform(callable, optioinal): 可选的变换操作 self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks.astype("float").reshape(-1,2) sample = &#123;"image": image, "landmarks":landmarks&#125; if self.transform: sample = self.transform(sample) return sample 接下来, 让我们对这个类进行初始化1234567891011121314face_dataset = FaceLandmarksDataset(csv_file="faces/face_.csv", root_dir="faces/")fig = plt.figure()for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample["image"].shape, sample["landmarks"]) ax = plt.subplot(1,4,i+1) plt.tight_layout() ax.set_title("Sample") ax.axis("off") show_landmarks(**sample) if i==3: plt.show() break Transforms尝试以下三种常见的转换操作: Rescale: 改变图片的尺寸大小 RandomCrop: 对图片进行随机剪裁(数据增广技术) ToTensor: 将numpy图片转换成tensor数据 我们将会把这些操作写成可供调用的类, 而不仅仅是一个简单的函数, 这样做的主要好处是不用每次都传递transform的相关参数. 为了实现可调用的类, 我们需要实现类的 __call__ 方法, 并且根据需要实现 __init__ 方法. 我们可以像下面这样使用这些类:12tsfm = Transform(params)transformed_sample = tsfm(sample) 具体实现如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class Rescale(object): def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample["image"], sample["landmarks"] h, w = image.shape[:2] if isinstance(self.output_size, int): if h&gt;w: new_h, new_w = self.output_size*h/w, self.out_size else: new_h, new_w = self.output_size, self.output_size*w/h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) landmarks = landmarks*[new_w/w, new_h/h] return &#123;"image":img, "landmarks": landmarks&#125;class RandomCrop(object): """Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. """ def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return &#123;'image': image, 'landmarks': landmarks&#125;class ToTensor(object): """Convert ndarrays in sample to Tensors.""" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)&#125; Compose transforms接下来, 需要将定义好的转换操作应用到具体的样本上, 我们首先将特定的操作组合在一起, 然后利用torchvision.transforms.Compose方法直接将操作应用到对应的图片上.1234567891011121314151617scale = Rescale(256)crop = RandomCrop(128)composed = transforms.Compose([Rescale(256), RandomCrop(224)])# Apply each of the above transforms on sample.fig = plt.figure()sample = face_dataset[65]for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample)plt.show() Iterating through the dataset总结一下对数据采样的过程: 从文件中读取一张图片 将transforms应用到图片上 由于transforms是随机应用的, 因此起到了一定的增广效果. 可以利用 for i in range循环操作来对整个数据集进行transforms12345678910transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',root_dir='faces/', transform=transforms.Compose([Rescale(256),RandomCrop(224),ToTensor()]))for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break Afterword: torchvision123456789101112131415import torchfrom torchvision import transforms, datasetsdata_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform)dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4)]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux-常用指令助记]]></title>
    <url>%2Fz_post%2FLinux-%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E5%8A%A9%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[cat指令grep指令ls指令统计文件个数更换国内镜像源清华源:1https://mirrors.tuna.tsinghua.edu.cn/ 中科大源: 1http://mirrors.ustc.edu.cn/help/homebrew-bottles.html 阿里源:123456789```# apt# pip# brew中科大 cd “$(brew —repo)”git remote set-url origin https://mirrors.ustc.edu.cn/brew.git1234# npmhttps://npm.taobao.org/ npm config set registry http://registry.npm.taobao.org```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PyTorch官方教程(一)-A 60 Minute Blitz]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B-60MinuteBlitz%2F</url>
    <content type="text"><![CDATA[What is PyTorch?一个基于Python的科学计算包, 设计目的有两点: numpy在GPUs实现上的替代品 具有高度灵活性和速度的深度学习研究平台 TensorsTensors可以理解成是Numpy中的ndarrays, 只不过Tensors支持GPU加速计算. 12345678910x = torch.empty(5,3)print(x) # 输出 5×3 的未初始化的矩阵, 矩阵元素未初始化, 所以可能是浮点类型的任何职x = torch.rand(5,3)x = torch.zeros(5,4,dtype=torch.long)x = torch.tensor([5.5, 3]) # 直接用常数来初始化一个Tensorx.size() # Tensor的size OperationsPyTorch支持多种语法实现相同的操作. 加法:12345678910111213141516171819202122232425x = torch.rand(5,3)y = torch.rand(5,3)z1 = x + yz2 = torch.add(x,y)z3 = torch.empty(5,3)torch.add(x,y,out=z3)# in-placey.add_(x) # _ 代表原地加法 也就是 y = y+x# 可以想numpy数组一样使用tensor:print(x[:,-1])# Resizing, 利用torch.view来对tensor执行reshape/resize操作x = torch.randn(4, 4)y = x.view(16)z = x.view(-1,8) # -1代表自动推断维度print(x.size(), y.size(), z.size()) # torch.Size([4,4]) torch.Size([16]) torch.Size([2,8])# item()可以获得只有一个元素的tensor的值x = torch.randn(1)print(x.item()) Tensor与Numpy Array从tensor转换成numpy数组: 1234a = torch.ones(5)print(type(a)) # &lt;class 'torch.Tensor'&gt;b = a.numpy()print(type(b)) # &lt;class 'numpy.ndarray'&gt; 注意, 此时a和b共享内存, 即a和b指向的都是同一个数据, 也就是说, 如果改变a的值, 那么b的值也会随之改变!!12print(a.add_(1)) # tensor([2., 2., 2., 2., 2])print(b) # [2., 2., 2., 2., 2] 从numpy数组转换成tensor 12a = np.ones(5)b = torch.from_numpy(a) 同样, a和b是共享内存的 所有位于CPU上的Tensor (除了CharTensor) 都支持转换成对应的numpy数组并且再转换回来. CUDA TensorsTensors可以利用.to方法移动到任何设备上去123456if torch.cuda.is_avaiable(): device = torch.device("cuda") # 创建了一个cuda device对象 y = torch.ones_like(x, device=device) # 直接从GPU上创建tensor x = x.to(device) # 将x移到gpu上, 也可以直接用字符串指明: x = x.to("cuda") z = x+y z.to("cpu", torch.double) Neural Networks可以利用torch.nn包来创建神经网络, nn依靠autograd来定义模型并且对其计算微分. 从nn.Module类派生的子类中会包含模型的layers, 子类的成员函数forward(input)会返回模型的运行结果. 经典的训练神经网络的过程包含以下步骤: 定义具有一些可学习参数(权重)的神经网络 在数据集上创建迭代器 将数据送入到网络中处理 计算loss 对参数进行反向求导 更新参数: $weight = weight - lr*gradient$ 定义一个简单的网络12345678910111213141516171819202122232425262728293031323334353637import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 输出如下1234567Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 当定义好模型的forward()函数以后, backward()函数就会自动利用autograd机制定义, 无需认为定义. 可以通过net.parameters()函数来获取模型中可学习的参数 123params = net.parameter() # params的类型为 &lt;class 'Iterator'&gt;print(len(list(params))) # 具有可学习参数的层数print(list(params)[0].size()) # conv1 的参数 根据网络结构接受的输入, 想网络中传输数据并获取计算结果123input = torch.randn(1,1,32,32) # 四个维度分别为 (N,C,H,W)out = net(input) # 自动调用forward函数进行计算并返回结果print(out) #tensor([[ 0.1246, -0.0511, 0.0235, 0.1766, -0.0359, -0.0334, 0.1161, 0.0534, 0.0282, -0.0202]], grad_fn=&lt;ThAddmmBackward&gt;) 下面的代码可以清空梯度缓存并计算所有需要求导的参数的梯度12net.zero_grad()out.backward(torch.randn(1,10)) # 正如前面所说, 当定义了forward函数以后, 就会自动定义backward函数, 因此可以直接使用 需要注意的是, 整个torch.nn包只支持mini-batches, 所以对于单个样本, 也需要显示指明batch size=1, 即input第一个维度的值为1 也可以对单个样本使用input.unsqueeze(0)来添加一个假的batch dimension. Loss Function一个损失函数往往接收的是一对儿数据 (output, target). 然后根据相应规则计算output和target之间相差多远, 如下所示: 1234567output = net(input)target = torch.randn(10)target = target.view(1,-1) # 令target和output的shape相同.criterion = nn.MSELoss()loss = criterion(output, target)print(loss) # tensor(1.3638, grad_fn=&lt;MseLossBackward&gt;) 利用.grad_fn属性, 可以看到关于loss的计算图:12345print(loss.grad_fn) # 返回MseLossBackward对象#input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d# -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear# -&gt; MSELoss# -&gt; loss 因此, 当调用loss.backward()时, 就会计算出所有(requires_grad=True的)参数关于loss的梯度, 并且这些参数都将具有.grad属性来获得计算好的梯度 BackProp再利用loss.backward()计算梯度之前, 需要先清空已经存在的梯度缓存(因为PyTorch是基于动态图的, 每迭代一次就会留下计算缓存, 到一下次循环时需要手动清楚缓存), 如果不清除的话, 梯度就换累加(注意不是覆盖). 123456net.zero_grad() # 清楚缓存print(net.conv1.bias.grad) # tensor([0., 0., 0., 0., 0., 0.])loss.backward()print(net.conv1.bias.grad) # tensor([ 0.0181, -0.0048, -0.0229, -0.0138, -0.0088, -0.0107]) Update The Weights最简单的更新方法是按照权重的更新公式:123learning_rate = 0.001for f in net.parameters(): f.data.sub_(learning_rate*f.grad.data) 当希望使用一些不同的更新方法如SGD, Adam等时, 可以利用torch.optim包来实现, 如下所示:12345678import torch.optim as optimoptimizer = optim.SGD(net.parameters(), lr=0.01) # 创建优化器optimizer.zero_grad() # 清空缓存output = net(input)loss = criterion(output, target)loss.backward() # 计算梯度optimizer.step() # 执行一次更新 Train A ClassifierWhat About Data?通常情况下, 在处理数据的时候可以使用标准的Python包(opencv, skimage等), 并将其载入成Numpy数组的形式, 然后可以很方便的将其转换成torch.*Tensor数据. 对于图像数据来说, PyTorch提供了torchvision包, 它包含许多常见数据集(Imagenet, CIFAR10, MNIST等等)的加载器, 同时还包含其他一些针对图片的数据转换(data transformers)函数. 对于CIFAR10来说, 它的数据集中图片尺寸为 3×32×32, 总共具有10个不同的类别. 下面就来看一下如何训练一个分类器将这10个类别进行分类. Training An Image Classifier接下来主要包括以下步骤: 使用torchvision加载并归一化CIFAR10的训练数据集和测试数据集. 定义一个卷积神经网络 定义损失函数 在traing data上训练网络 在test datauh测试网络 Loading and normalizing CIFAR10: 导入相关的包123import torchimport torchvisionimport torchvision.transforms as transforms torchvision的输出类型是 PILImage. 我们需要将其转换成 Tensors, 并对其进行归一化, 使其数值处于 [-1, 1] 之间.123456789101112131415# 将多个transforms链接(chained)起来transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 利用下面的代码可以查看CIFAR10中的训练图片样本:1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as np# functions to show an imagedef imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0)))# get some random training imagesdataiter = iter(trainloader)images, labels = dataiter.next()# show imagesimshow(torchvision.utils.make_grid(images))# print labelsprint(' '.join('%5s' % classes[labels[j]] for j in range(4))) 定义卷积神经网络:123456789101112131415161718192021222324import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Model): def __init__(self): super(self, Net).__init__ self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) # 两个max pooling的参数是一样的, 所以定义一个就行, 可以重复使用 self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, input): x = self.pool(F.relu(self.conv1(input))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16*5*5) # 第一个维度为batch size x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) output = self.fc3(x) return outputnet = Net() Define a Loss function and optimizer: 损失函数使用交叉熵, 优化器使用带动量的SGD123import torch.optim as optimcriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 训练网络: 训练网络的时候, 我们需要简单的在数据迭代器上进行循环操作就可以, 只需要注意不断想网络中送入新的数据即可.123456789101112131415161718192021222324for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0print('Finished Training') Test the network on the test data 在测试集上获取模型的准确率, 只需要利用outputs = net(images)即可获得预测的类别概率, 取最大者为预测的类别结果. 123456789101112correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 利用下面的代码可以看到每个类别的准确率: 1234567891011121314151617class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) Training on GPU上面的代码是在CPU上训练的, 那么如何利用PyTorch在GPU上进行训练呢? 实际上, 只需要将模型转移到GPU上即可. 首先定义一个device对象:12device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")print(device) # 输出 cdua:0 接下来, 利用.to()方法将模型转移到GPU上面(同时所有的参数和梯度缓存也会转移到GPU上)1net.to(device) # 也可以直接写成 net.to(device), 但是这样会缺少了设备检查, 不够健壮 接下来, 再向模型投喂数据之前, 就需要先将数据转移到GPU上1inputs, labels = inputs.to(device), labels.to(device) 其余代码均与上面的训练代码相同. Training on multiple GPUs//TODO]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[项目-竞赛-Apollo]]></title>
    <url>%2Fz_post%2F%E9%A1%B9%E7%9B%AE-%E7%AB%9E%E8%B5%9B-Apollo%2F</url>
    <content type="text"><![CDATA[处理流程: https://www.kaggle.com/kmader/data-preprocessing-and-unet-segmentation-gpu https://github.com/matterport/Mask_RCNN/issues/5 遇到的问题 https://www.kaggle.com/c/cvpr-2018-autonomous-driving/discussion/56888 https://github.com/pandas-dev/pandas/issues/18355 https://github.com/matterport/Mask_RCNN/issues/44 https://github.com/matterport/Mask_RCNN/issues/628https://github.com/matterport/Mask_RCNN/issues/658https://github.com/matterport/Mask_RCNN/issues/521 https://github.com/matterport/Mask_RCNN/issues/5]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode算法题(Hard)]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98-LeetCode-3%2F</url>
    <content type="text"><![CDATA[004. Median of Two Sorted ArraysDescriptionThere are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). You may assume nums1 and nums2 cannot be both empty. Example 1: nums1 = [1, 3]nums2 = [2] The median is 2.0Example 2: nums1 = [1, 2]nums2 = [3, 4] The median is (2 + 3)/2 = 2.5 解法一: 根据中位数的特性题目要求需要时间复杂度为 $O(log (m+n))$. 首先我们思考中位数的作用: 中位数可以将一个数组分成两个长度相同的部分, 并且一部分中的数字总比另一部分中的小 那么对于两个数组的情况, 我们需要做的就是找到一个数字, 可以使这两个数组分别分成两部分, 并且两部分长度相同, 前一部分比后一部分的数字小 首先,我们将数组A分成两部分, 由于A有m个数字, 所以它可以有m中不同的分法, 我们以 i 为界限, 将A分成两部分, 前一部分的长度为i (从0到 i-1 ), 后一部分的长度为 m-i (从 i 到 m-1): A[1,2,…,i-1] | A[i, i+1, …, m-1] 同理,数组 B 也可以做如下分割: B[1,2,…,j-1] | B[j, j+1, …, n-1] 因此, 两个数组A和B都被分到了两部分, 将它们合起来, 第一部分的数字为 A[1,2,…,i-1] 和 B[1,2,…,j-1], 第二部分的数字为 A[i, i+1, …, m-1] 和 B[j, j+1, …, n-1], 我们并不关系两部分内部的顺序, 我们只关心一件事, 那就是: 第一部分和第二部分的长度相等, 并且第一部分的数字都比第二部分小 , 于是, i 和 j和取值就必须满足下列关系: i+j = m-i + n-j 或 m-i + n-j + 1 (加1的原因是因为有可能数组总长为奇数, 我们令前一部分比后一部分多1个元素) A[i-1] &lt;= B[j] 或 i==0 (说明A全部在后半段, 因此无需判断A的元素是否小于后半段的第一个B元素) B[j-1] &lt;= A[i] 或 i==m (说明A全部在前半段, 因此无需判断A的元素是否大于前半段的最后一个B元素) 由于上式 i+j = m-i + n-j 或 m-i + n-j + 1 , 因此有 j = (m+n+1)/2 - i ; (向下取整). 故而可以只对 i 进行判断i是否越界, 只要i满足条件, j就不会等于0或n(即不会越界) 根据上面的分析, 解题过程如: 根据两数组的长度, 将短的一方设为A数组 (j要对应较长的那个数组, 否则的话j有可能小于0 ), 令start=0, end=A.size 令 i=(start+end+1)/2 (加1 的原因是因为i代表的含义是前一部分有i个元素) 计算j = (m+n+1)/2 - i 判断当前 i和j是否满足条件,有三种情况(对这三种情况不断重复, 直到i,j位置刚刚好): A[i-1] &gt; B[j] 或 i 越界 , 说明 i 的位置过大, 令 end = i-1 B[j-1] &gt; A[i] 或 i 越界 , 说明 i 的位置过小, 令 start = i+1; 其他情况(A[i-1] &lt;= B[j] 或 i==0 并且 B[j-1] &lt;= A[i] 或 i==m), 说明 i 和 j的位置刚刚好 当i,j位置刚好时, 根据数组整体长度的奇偶, 返回正确的中位数: 奇数: 返回前半段的最大元素 偶数: 返回前半段最大元素和后半段最小元素的平均值-非递归写法 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; if(nums1.size() &gt; nums2.size())&#123; nums1.swap(nums2); &#125; int m = nums1.size(); int n = nums2.size(); int start = 0, end=m; while(start&lt;=end)&#123; int i = (start+end+1) / 2; int j = (n+m+1)/2 - i; if(i&gt;start &amp;&amp; nums1[i-1] &gt; nums2[j]) //注意 , 只要i&gt;start, j就一定不会超出数组限制 end = i-1; //i太大 else if(i&lt;end &amp;&amp; nums2[j-1] &gt; nums1[i]) start = i+1; // i太小 else&#123; int leftmax;// 取左边最大的 if(i==0) leftmax=nums2[j-1]; else if(j==0) leftmax=nums1[i-1]; else leftmax = max(nums1[i-1], nums2[j-1]) ; if( (n+m)%2 == 1) return leftmax; int rightmin; // 取右边最小的 if(i==m) rightmin = nums2[j]; else if(j==n) rightmin = nums1[i]; else rightmin = min(nums1[i] ,nums2[j]); return (leftmax+rightmin) / 2.0; &#125; &#125; // return 0.0; //因为, 两数组不会同时为空, 所以这句话主要用于调试 &#125;&#125;; 递归写法123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; if(nums1.size() &lt;= nums2.size()) return helper(nums1, 0 , nums1.size(),nums2); else return helper(nums2, 0 , nums2.size(),nums1); &#125; double helper(vector&lt;int&gt;&amp; nums1, int start1, int end1, vector&lt;int&gt;&amp; nums2)&#123; int i = (start1+end1+1)/2; int j = (nums1.size()+nums2.size()+1)/2 - i; if(start1 &gt; end1) return 0.0; if( (i==0 || nums1[i-1]&lt;=nums2[j]) &amp;&amp; (i==nums1.size() || nums2[j-1]&lt;=nums1[i]))&#123; // 如果找到i int res11, res12; int res21, res22; // 首先将左边部分的两个数组分别赋值, 如果i或j为0, 说明对应数组在左边 //只有0个元素 , 将其赋值为INT_MIN(因为要取max(res11, res21)) if(i==0) res11= INT_MIN; else res11=nums1[i-1]; if(j==0) res21= INT_MIN; else res21=nums2[j-1]; //同理, 对右边进行处理, 取min(res12, res22) if(i==nums1.size()) res12= INT_MAX; else res12=nums1[i]; if(j==nums2.size()) res22= INT_MAX; else res22=nums2[j]; // 根据数组奇偶个数返回结果 if((nums1.size() + nums2.size())%2 == 1 )&#123; return max(res11, res21); &#125; else&#123; return ( max(res11,res21)+min(res12,res22) ) / 2.0; &#125; &#125;else if(nums1[i-1] &gt; nums2[j])&#123; return helper(nums1, start1, i-1, nums2); &#125;else&#123; return helper(nums1, i+1, end1, nums2); &#125; &#125;&#125;; 010.Description解法一: 递归实现( 速度很慢, 只超过0.97%的提交)采用递归法, 首先判断当前字母后面是否是’ * ‘, ,如果是, 则需要分别进行下面两种情况的判断: 当前字母出现0次 当前字母出现1次或以上(前提是当前字母可以匹配) 其次, 就是终止条件的判断, 总共有三种情况: 都走到了尽头, 返回ture 只有p走到了尽头, 返回false 只有s走到了尽头, 需要查看p后续的值 12345678910111213141516171819202122232425class Solution &#123;public: bool isMatch(string s, string p) &#123; return helper(s, 0, p, 0); &#125; bool helper( string&amp; s, int s_i, string&amp; p, int p_i)&#123; if(s_i == s.size() &amp;&amp; p_i == p.size()) return true; // 若都走到了最后, 则返回ture if(p_i == p.size() ) return false; // 若只有p走到了最后, 返回false if(s_i == s.size() &amp;&amp; p_i+1&lt;p.size() &amp;&amp; p[p_i+1] == '*') //若只有s走到了最后, 则需要看p后面是否有*, 如果有*, 则仍有可能匹配 return helper(s, s_i, p, p_i+2); // 若没有*,返回false else if(s_i == s.size()) return false; if(p_i + 1 &lt; p.size() &amp;&amp; p[p_i+1] == '*')&#123; //如果后面有星号, 则需要进行两种判断 bool b1 = helper(s, s_i, p, p_i+2); // 星号前的字母出现0次 bool b2 = false; if(s[s_i] == p[p_i] || p[p_i] == '.') b2 = helper(s, s_i+1, p , p_i) || helper(s, s_i+1, p, p_i+2);// 出现一次或一次以上 return b1 || b2; &#125;else if(s[s_i] == p[p_i] || p[p_i] == '.' )&#123; return helper(s, s_i+1, p, p_i+1); &#125;else&#123; return false; &#125; &#125;&#125;; 解法二: 动态规划This problem has a typical solution using Dynamic Programming. We define the state P[i][j] to be true if s[0..i) matches p[0..j) and false otherwise. Then the state equations are: P[i][j] = P[i - 1][j - 1], if p[j - 1] != ‘*’ &amp;&amp; (s[i - 1] == p[j - 1] || p[j - 1] == ‘.’); P[i][j] = P[i][j - 2], if p[j - 1] == ‘*’ and the pattern repeats for 0 times; P[i][j] = P[i - 1][j] &amp;&amp; (s[i - 1] == p[j - 2] || p[j - 2] == ‘.’), if p[j - 1] == ‘*’ and the pattern repeats for at least 1 times. Putting these together, we will have the following code. 1234567891011121314151617class Solution &#123;public: bool isMatch(string s, string p) &#123; bool dp[s.size()+1][p.size()+1]&#123;0&#125;; dp[0][0]=true; for(int i =0; i&lt;s.size()+1; i++)&#123; for(int j = 1;j&lt;p.size()+1;j++)&#123; if(p[j-1] == '*') // 注意这里是j-1 dp[i][j] = ( j &gt; 1 &amp;&amp; dp[i][j-2] )|| ( i&gt;0 &amp;&amp; (s[i-1] == p[j-2] || p[j-2] == '.') &amp;&amp; dp[i-1][j]); // 注意这里是j-2, i-1, 一定要知道这些是为什 else dp[i][j] = i&gt;0 &amp;&amp; dp[i-1][j-1] &amp;&amp; (s[i-1] == p[j-1] || p[j-1] == '.'); &#125; &#125; return dp[s.size()][p.size()]; &#125;&#125;; 23.Description解法一: 基于比较的合并时间复杂度: $O(k * max(len))$ k为需要合并和链表个数, 在比较时需要遍历k个链表的头结点, 以便找出最小的空间复杂度: $O(1)$ 将该问题看做是两个有序链表的合并问题, 只不过每次选择最小的节点时, 需要从vector.size()个节点中选择, 同时还要注意及时移除vector中已经走到头的空链表, 并判断size当前的大小, 当vector中的size大小为1时, 说明其余链表都已经合并完成, 此时退出循环, 直接将该链表接入即可. 另外要注意vector为空, 以及vector中全是nullptr链表的特殊情况. 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123; if(lists.size() == 0) return nullptr; //处理[]的情况 ListNode* dummy = new ListNode(0); ListNode* cur_node = dummy; while(lists.size() &gt; 1)&#123; int min_node_index = 0; for(int i = 0; i&lt;lists.size() ;i++)&#123; if(lists[i] == nullptr) &#123; lists.erase(lists.begin()+i); i--; //移除第i个元素后, 下一个元素会自动成为第i个元素,因此, 将当前i-- continue; // continue后, i会++, 最终i指向了下一个元素 &#125; if(lists[min_node_index]-&gt;val &gt; lists[i]-&gt;val)&#123; min_node_index = i; &#125; &#125; if(lists.size() == 0) return nullptr; //主要是应对 [[], []] 的情况, 本身vector的size大于0, 但是经过erase以后size就变成0了, 此时应返回nullptr cur_node-&gt;next = lists[min_node_index]; cur_node = cur_node-&gt;next; lists[min_node_index] = lists[min_node_index]-&gt;next; if(lists[min_node_index] == nullptr) lists.erase(lists.begin()+min_node_index); &#125; cur_node-&gt;next = lists[0]; return dummy-&gt;next; &#125;&#125;; 解法二: 用小顶堆对解法一的比较操作进行优化时间复杂度: $O(logk * max(len))$空间复杂度: $O(k)$ 由于要构造堆, 所以需要额外空间 由于我们只需要找到k个节点里面数值最小的那一个, 因此可以利用Priority Queue (实际上就是大顶堆和小顶堆)对上面的比较操作进行优化, 使得比较操作的复杂度从 $k$ 降到 $logk$. 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123; struct cmp&#123; // 仿函数 bool operator()(ListNode* &amp; node1, ListNode* &amp;node2)&#123; return node1-&gt;val &gt; node2-&gt;val; &#125; &#125;;public: ListNode* mergeKLists(vector&lt;ListNode * &gt;&amp; lists) &#123; priority_queue&lt;ListNode * , vector&lt;ListNode * &gt;, cmp&gt; min_q; for(auto head_node : lists) if(head_node != nullptr) min_q.push(head_node); // 入队列之前检查是否为空 if(min_q.empty()) return nullptr; // 如果队列为空, 说明没有非空元素入队列, 所有链表为空, merge后也为空 ListNode* res = min_q.top(); min_q.pop(); if(res-&gt;next!=nullptr) min_q.push(res-&gt;next); ListNode* cur_node = res; while(!min_q.empty())&#123; cur_node-&gt;next = min_q.top(); min_q.pop(); cur_node = cur_node-&gt;next; // 将cur_node指向top元素, 以便查看top的next是否为空 if(cur_node-&gt;next!=nullptr) min_q.push(cur_node-&gt;next); &#125; return res; &#125;&#125;; 解法三: 转化成双列表合并问题时间复杂度: $O(k * max(len))$空间复杂度: $O(1)$ 双列表合并问题的时间复杂度为 $O(max(m,n))$ , 可以将多链表合并问题看做是k次双列表合并. 解法四: 对解法三进行优化时间复杂度: $O(logk * max(len))$空间复杂度: $O(1)$ 对列表合并时, 每次都是两两合并(不是解法三中的逐一合并), 这样, 只需要经过 $logk$ 次两两合并就可完成所有合并过程. 41. First Missing Positive寻找数组中缺失的最小的正数 DescriptionGiven an unsorted integer array, find the smallest missing positive integer. Example 1: Input: [1,2,0]Output: 3Example 2: Input: [3,4,-1,1]Output: 2Example 3: Input: [7,8,9,11,12]Output: 1Note: Your algorithm should run in O(n) time and uses constant extra space. 解法一: 下标与正数对应时间复杂度: $O(n)$ (但是for循环内部存在while循环, 因此有争议)空间复杂度: $O(1)$ (但是对改变了原始的数组, 这是一个小缺陷) 将下标与正数相应对, 例如对于正数5, 我们就将放置在nums[4]上, 这样一来, 再次遍历数组的时候, 当遇到第一个与下标不对应的数字时, 该下标对应的正数(i+1)就是缺少的正数. 放置正数到正确位置上时, 需要注意几点: swap之后需要继续将原来位置上(nums[4])的数放置到正确的位置上, 这里需要一个while循环 在检查数组时, 如果所有数组内所有数字都处在正确位置上, 那么就应该返回nums.size+1 (包括了数组为空的情况: 返回0+1=1) 写法一: for+while 12345678910111213141516class Solution &#123;public: int firstMissingPositive(vector&lt;int&gt;&amp; nums) &#123; for(int i = 0; i &lt; nums.size(); i++)&#123; while(nums[i] &gt; 0 &amp;&amp; nums[i]&lt;nums.size() &amp;&amp; nums[i] != nums[nums[i]-1]) std::swap(nums[i], nums[nums[i]-1]); &#125; for(int i =0; i&lt;nums.size(); i++)&#123; if(nums[i] != i + 1) return i+1; &#125; return nums.size()+1; &#125;&#125;; 写法二: while12345678910111213141516171819class Solution &#123;public: int firstMissingPositive(vector&lt;int&gt;&amp; nums) &#123; int i = 0; while(i&lt;nums.size())&#123; if(nums[i] &gt; 0 &amp;&amp;nums[i]&lt;nums.size() &amp;&amp; nums[i] != nums[nums[i]-1]) std::swap(nums[i], nums[nums[i]-1]); // 如果进行了swap, 就不要i++ else i++; &#125; for(int i =0; i&lt;nums.size(); i++)&#123; if(nums[i] != i + 1) return i+1; &#125; return nums.size()+1; &#125;&#125;; 解法二: 哈希时间复杂度: $O(n)$ (3次for循环, 毫无争议的 $O(n)$ )空间复杂度: $O(1)$ (但是对改变了原始的数组, 这是一个小缺陷) 注意: 虽然这里的时间复杂度是毫无争议的 $O(n)$ , 但是不一定会上面的速度快, 因为上面只有两次循环, 内第一次内部的循环次数一般情况下都不会很大. 从哈希的角度理解: 可以将数组下标看成是hash的key for any array whose length is l, the first missing positive must be in range [1,…,l+1], so we only have to care about those elements in this range and remove the rest. we can use the array index as the hash to restore the frequency of each number within the range [1,…,l+1] 123456789101112131415161718192021222324class Solution &#123;public: int firstMissingPositive(vector&lt;int&gt;&amp; nums) &#123; // 丢失的最小正数只可能在 [1,2,...,nums.size()+1] 之间 // 这里的pushback是必须的, 因为下面会将不符合要求的元素都置为0, //因此nums[0]需要与0对应, 以代表所有的非法元素, //这点与上面基于swap的方法不同, 上面的swap是让nums[0] 与 1 对应. nums.push_back(0); int length = nums.size(); for(int i =0 ; i&lt;length; i++)&#123; if(nums[i] &lt; 0 || nums[i] &gt;= length) nums[i] = 0; // 将所有不符合丢失正数的数移除, 这一步必须单独用一个for循环做 &#125; for(int i = 0; i&lt;length; i++)&#123; nums[nums[i]%length] += length; &#125; for(int i = 1 ; i&lt;length; i++)&#123; if(nums[i]/length == 0) return i; &#125; return length; &#125;&#125;; 42 Trapping Rain Water数组中每个值代表柱状体的高度, 每个柱状体的宽度都为1, 根据数组内的值组成的高低不同的块, 能够存储多少个bin (1×1)的水 DescriptionGiven n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. The above elevation map is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped. Thanks Marcos for contributing this image! Example: Input: [0,1,0,2,1,0,1,3,2,1,2,1]Output: 6 解法一: 左右指针时间复杂度: $O(n)$空间复杂度: $O(1)$ 分别用两个变量left和right指向左边和右边的柱子, 并再用两个变量maxleft和maxright维护左边最高的柱子和右边最高的柱子, 统计的时候, 先固定left和right当中柱子高度较高的那一个, 然后统计较低柱子上存储的水量. 利用, 如果当前left的高度小于right的高度, 则我们计算left上面能够存储的水量, 有两种情况, 当left柱子的高度大于等于maxleft时, 则left柱子上没法存储水, 因为谁会从左边全部流失(右边比左边高, 所以不会从右边流失). 如果left的高度小于maxleft时, 由于水无法从左边流失, 也不能从右边流失, 因此当前柱子上就会存储水, 存储的水量为maxleft-height[left] (不考虑maxright, 因为maxright大于maxleft). 注意: 此题中的柱子是有 宽度 的, 这一点很重要, 如果柱子的宽度为0 , 那么就是另一种情况了. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Solution &#123;public: int trap(vector&lt;int&gt;&amp; height) &#123; int len = height.size(); int left = 0, right = len-1; int res = 0, maxleft = 0, maxright = 0; while(left &lt;= right)&#123; if(height[left] &lt;= height[right])&#123; //固定较大的一个柱子 if(height[left] &gt; maxleft) maxleft = height[left];// 如果当前柱子的高度大于左边max柱子的高度, 那么该柱子所处位置一定存不下水 else res = res + maxleft - height[left]; // 反之, 该柱子位置上可以存储的水的量为 坐标max高度减去当前的高度 left++; &#125;else&#123; if(height[right] &gt; maxright) maxright = height[right]; else res = res + maxright - height[right]; right--; &#125; &#125; return res; &#125;&#125;;``` # 44. Wildcard Matching字符串模式匹配## DescriptionGiven an input string (s) and a pattern (p), implement wildcard pattern matching with support for '?' and '*'.'?' Matches any single character.'*' Matches any sequence of characters (including the empty sequence).The matching should cover the entire input string (not partial).Note:s could be empty and contains only lowercase letters a-z.p could be empty and contains only lowercase letters a-z, and characters like ? or *.Example 1:Input:s = "aa"p = "a"Output: falseExplanation: "a" does not match the entire string "aa".Example 2:Input:s = "aa"p = "*"Output: trueExplanation: '*' matches any sequence.Example 3:Input:s = "cb"p = "?a"Output: falseExplanation: '?' matches 'c', but the second letter is 'a', which does not match 'b'.Example 4:Input:s = "adceb"p = "*a*b"Output: trueExplanation: The first '*' matches the empty sequence, while the second '*' matches the substring "dce".Example 5:Input:s = "acdcb"p = "a*c?b"Output: false## 解法一: 迭代时间复杂度: $O(m+n)$空间复杂度: $O(1)$```cppclass Solution &#123;public: bool isMatch(string s, string p) &#123; int i = 0, j = 0; int try_i = 0; int star_index = -1; int match = -1; int len_s = s.size(), len_p = p.size(); while(i &lt; len_s)&#123; if( j&lt; len_p &amp;&amp; (s[i] == p[j] || p[j] == '?') )&#123; i++;j++; &#125; else if( j&lt;len_p &amp;&amp; p[j] == '*')&#123; star_index = j; j++; match = i; // 该变量用于回溯 &#125;else if( star_index != -1)&#123; //如果之前是 * , 则进行回溯 j = star_index + 1; match++; i = match; &#125;else return false; &#125; while(p[j] == '*') j++; return i==len_s &amp;&amp; j == len_p; &#125;&#125;; 解法二: DP时间复杂度: $O(n^2)$空间复杂度: $O(n)$ 12345678910111213141516171819class Solution &#123;public: bool isMatch(string s, string p) &#123; int pLen = p.size(), sLen = s.size(), i, j, k, cur, prev; if(!pLen) return sLen == 0; bool matched[2][sLen+1]; fill_n(&amp;matched[0][0], 2*(sLen+1), false); matched[0][0] = true; for(i=1; i&lt;=pLen; ++i) &#123; cur = i%2, prev= 1-cur; matched[cur][0]= matched[prev][0] &amp;&amp; p[i-1]=='*'; if(p[i-1]=='*') for(j=1; j&lt;=sLen; ++j) matched[cur][j] = matched[cur][j-1] || matched[prev][j]; else for(j=1; j&lt;=sLen; ++j) matched[cur][j] = matched[prev][j-1] &amp;&amp; (p[i-1]=='?' || p[i-1]==s[j-1]) ; &#125; return matched[cur][sLen]; &#125;&#125;; 076. Minimum Window Substring求包含子串字符的最小窗口 DescriptionGiven a string S and a string T, find the minimum window in S which will contain all the characters in T in complexity O(n). Example: Input: S = “ADOBECODEBANC”, T = “ABC”Output: “BANC”Note: If there is no such window in S that covers all characters in T, return the empty string “”.If there is such window, you are guaranteed that there will always be only one unique minimum window in S. 解法: 两个变量记录当前窗口大小时间复杂度: $O(n)$空间复杂度: $O(1)$ 1234567891011121314151617181920class Solution &#123;public: string minWindow(string s, string t) &#123; vector&lt;int&gt; hmap(256,0); for(auto c:t) hmap[int(c)]++; int count = t.size(), begin=0, end=0, head=0, cur_window=INT_MAX; while(end&lt;s.size())&#123; // 这里可以直接写成 if(hmap[int(s[end++])]-- &gt; 0) count--; 但是可读性很差, 不建议这样写. if(hmap[int(s[end])] &gt; 0) count--; hmap[int(s[end])]--; end++; while(count==0)&#123; //end 超尾 if( (end-begin) &lt; cur_window) cur_window = end - (head=begin); // 同样, 可以直接写成 if(hmap[int(s[begin++])]++ &gt; 0) count++; 但是可读性很差 if(hmap[int(s[begin])] == 0) count++; hmap[int(s[begin])]++; begin++; &#125; &#125; return cur_window==INT_MAX ? "" : s.substr(head, cur_window); &#125;&#125;; 子串相关题目的模板解法对于大多数的子串相关的问题, 通常可以描述为给定一个字符串, 要求找到满足某些限制条件的子串, 这类都可以用下面的基于哈希表和两个辅助指示变量的模板来求解: 123456789101112131415161718192021int findSubstring(string s)&#123; vector&lt;int&gt; hmap(128,0); int count; // 用于检查子串是否合法 int begin=0, end=0; // 两个指示变量, 分别指向子串的头和尾(end会在++后退出循环, 因此最后end会变成超尾) int len_sub; // 子串的长度 for()&#123; &#125;//对hasp map进行初始化 while(end&lt;s.size())&#123; //if(hmap[s[end++]]-- ? ) &#123; &#125; //修改count //上面的语句可读性很差, 最后拆开来写, 后面也同理, 拆开写 if(hmap[int(s[end])] ? ) &#123; &#125; //修改count hmap[int(s[end])]--; //注意顺序 end++; while( count? )&#123; // 检查count是否满足条件 // update len_sub if(hmap[int(s[begin])] ?) &#123; &#125; //修改count hmap[int(s[begin])]++; begin++; &#125; &#125;&#125; 例如, 对于问题 Longest Substring At Two Distinct Characters 的模板解法如下: 对于问题 Longest Substring Without Repeating Characters 的模板解法如下:12345678910int lengthOfLongestSubstring(string s)&#123; vector&lt;int&gt; map(256,0); int begin=0,end=0,len_sub=0,count=0; while(end&lt;s.size())&#123; if(map[int(s[end])] &gt; 0) count++; map[int(s[end])]++; end++; while(count&gt;0) if(map[int(s[begin])] &gt; 1) count; &#125;&#125; 084. Largest Rectangle in Histogram求最大面积的矩形 DescriptionGiven n non-negative integers representing the histogram’s bar height where the width of each bar is 1, find the area of largest rectangle in the histogram. Example:Input: [2,1,5,6,2,3]Output: 10 解法一: 穷举时间复杂度: $O(n^2)$空间复杂度: $O(1)$ 列出以每一个i上的值为矩形高度的矩形面积, 然后取得最大值12345678910111213141516171819class Solution &#123;public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) &#123; int max_area = 0; for(int i =0; i&lt;heights.size(); i++)&#123; int low = i; while(low&gt;=0 &amp;&amp; heights[low] &gt;=heights[i]) low--; low++; int high = i; while(high&lt;heights.size() &amp;&amp; heights[high] &gt;= heights[i]) high++; high--; int cur_area = heights[i]* (high-low+1); if(max_area&lt;cur_area) max_area = cur_area; &#125; return max_area; &#125;&#125;; 解法二: 解法一的改进-空间换时间时间复杂度: $O(n)$, 前面省略常数项(因为不好确定常数项的值)空间复杂度: $O(2n)$ 从解法一中我们可以看出, 核心的要点就在于求取每一个i对应的矩形的左端和右端, 如下图所示: 那么, 如果我们可以在 $O(1)$ 的时间内获取到左端和右端的值, 则时间复杂度就可以降低到 $O(n)$, 因此, 首先想到的是用数组将每个i对应的左端和右端的值保存起来. 于是, 我们需要先求取这两个数组(左端,右端)的值, 在对左端和右端求值时, 我们要确保时间复杂度不能超过 $O(n)$, 因此, 我们不能每次都重新从i出发分别向左向右遍历(如解法一那样), 反之, 我们可以利用左端和右端中已经求好的值, 对于左端来说, 我们可以利用左端数组跳跃式的向左前进, 对于右端来说, 我们可以利用右端数组跳跃式的向右前进(这里不太好用语言描述, 具体请看程序代码). 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) &#123; int *left = new int[heights.size()]; int *right = new int[heights.size()]; left[0]=-1; for(int i=1; i&lt;heights.size(); i++)&#123; int p = i-1; while(p&gt;=0 &amp;&amp; heights[p] &gt;= heights[i]) p = left[p]; left[i] = p; &#125; right[heights.size()-1] = heights.size(); for(int i=heights.size()-2; i&gt;=0; i--)&#123; int p = i+1; while(p&lt;heights.size() &amp;&amp; heights[p] &gt;= heights[i]) p = right[p]; right[i] = p; &#125; int max_area = 0; for(int i =0; i&lt;heights.size(); i++)&#123; int low = left[i]; int high = right[i]; int cur_area = heights[i]*(high-low-1); if(max_area&lt;cur_area) max_area = cur_area; &#125; return max_area; &#125;&#125;; 解法三: 最优-栈时间复杂度: $O(n)$, 无常数项空间复杂度: $O(n)$, 无常数项 上面的解法二, 虽然时间复杂度为 $O(n)$, 但实际上其时间复杂度是略微高于 $O(n)$, 因为在求取左端右端时, 每次跳跃的次数是大于等于1, 而不是仅为1次的.(只不过大O记法不考虑常数项). 而对于空间复杂度来说, 实际上是 $O(2n)$. 下面我们从另外一个角度出发: 不再以当前i对应的高度为最低, 向左右两边探索, 改为以当前i对应的高度为最低, 仅仅向左边探索, 实现算法如下: 首先, 构造一个空栈 从heights数组的第一个bar开始, 遍历所有的bar值(0~n-1), 并执行以下逻辑: 如果当前栈为空, 或者当前数组bar值大于等于栈顶bar值, 则将bar值下标入栈 否则, 将栈顶出栈, 并以栈顶下标对应的bar值作为最低的高度, 求该高度对应的面积, 因为当前数组bar值小于栈顶下标对应的bar值, 因此可以将当前bar值下标作为right_index, 又因为栈顶bar值下标的前一个元素, 要么小于栈顶, 要么等于栈顶, 不论哪种情况, 都可以将其下标作为left_index(因为栈顶退出对, 次栈顶就会成为新的栈顶, 所以可以包括bar值相等的情况), 得到了高度, right_index, left_index, 即可计算当前栈顶对应的面积, 并与max_area判断, 更新max_area的值 最后, 如果遍历完以后栈顶不为空(说明后面有几个连续的bar值相等, 或者bar只呈递增排序), 则依次强制弹出栈顶计算面积, 并更新max_area. 复杂度分析: 由于入栈出栈的元素仅为heights数组元素, 可以栈的size就是heights数组的大小, 即空间复杂度为 $O(n)$, 时间复杂度从代码中可看出约为 $O(n)$. 1234567891011121314151617181920212223242526272829class Solution &#123;public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) &#123; std::stack&lt;int&gt; s; int max_area = 0; int cur_area = 0; int height_index=0; int i=0; while(i&lt;heights.size())&#123; if(s.empty() || heights[i] &gt;= heights[s.top()]) s.push(i++); else&#123; height_index = s.top(); s.pop(); cur_area = heights[height_index] * ( s.empty()? i : i-s.top()-1 ); // 注意, 如果栈为空, 则说明当前i对应的bar值是前i个bar值中最小的, 所以宽为i, 否则宽为i-s.top()-1 if(cur_area &gt; max_area) max_area = cur_area; &#125; &#125; while(!s.empty())&#123; height_index = s.top(); s.pop(); cur_area = heights[height_index] * ( s.empty()? i : i-s.top()-1 ); // 注意, 如果栈为空, 则说明当前i对应的bar值是前i个bar值中最小的, 所以宽为i, 否则宽为i-s.top()-1 if(cur_area &gt; max_area) max_area = cur_area; &#125; return max_area; &#125;&#125;; 124.求二叉树中, 以任意节点为起始的路径和(这里是将二叉树看成无向图来计算路径的)的最大值, 例如对于下面的二叉树, 具有最大值的为:2-&gt;1-&gt;3 = 6 1 / \2 3 DescriptionGiven a non-empty binary tree, find the maximum path sum. For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Example 1: Input: [1,2,3]123 1 / \2 3 Output: 6Example 2: Input: [-10,9,20,null,null,15,7]12345 -10 / \9 20 / \ 15 7 Output: 42 解法一: 递归对于本题来说, 我们需要求得每个节点所在的路径的最大值, 以下面的例子来说:12345 4 / \ 11 13 / \7 2 我们需要求的最大和的路径为: 7-&gt;11-&gt;4-&gt;13. 而根据二叉树的遍历性质, 我们假设现在已经遍历到节点7, 此时, 左右子树均为空, 所以左右子树的最大和为0, 那么此时节点7所在的路径的最大和为: 左子树+右子树+当前节点值 = 7. 然后, 回溯到了节点11, 此时同理, 节点11所在的路径的最大和为: 左子树+右子树+当前节点值 = 11.(忽略节点2的遍历过程). 接下来对于节点4, 同理也应为: 左子树+右子树+当前节点值. 右子树返回的值很容易看出是13, 但是左子树应该返回多少呢? 由于我们希望求得当前的最大和, 因此, 左子树就应该返回它的最大和, 但是, 不能统计两条路径, 而应该选择以左节点为根节点的左右子树的较大者, 因此, 应该返回的是: max(左节点左子树, 左节点右子树)+左节点的值, 因此, 返回的是: 7+11 = 18. 于是, 节点4对应的最大和就为: 18+13+4. 12345678910111213141516171819202122232425/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: int maxPathSum(TreeNode* root) &#123; int res=INT_MIN; helper(root, res); return res; &#125; int helper(TreeNode* cur_node, int &amp;res)&#123; if(cur_node==nullptr) return 0; int left = std::max(helper(cur_node-&gt;left, res), 0); int right = std::max(helper(cur_node-&gt;right, res), 0); res = std::max(res, left+right+cur_node-&gt;val); return std::max(left, right)+cur_node-&gt;val; &#125;&#125;; 128. Longest Consecutive Sequence返回无序数组中, 可以组成的最长的连续子串的长度 DescriptionGiven an unsorted array of integers, find the length of the longest consecutive elements sequence. Your algorithm should run in O(n) complexity. Example: Input: [100, 4, 200, 1, 3, 2]Output: 4Explanation: The longest consecutive elements sequence is [1, 2, 3, 4]. Therefore its length is 4. 解法一: 排序时间复杂度: $O(nlogn)$空间复杂度: $O(1)$ 先排序, 然后在从头往后遍历, 并用一个变量维护当前的最长连续序列的长度. 解法二: 利用哈希表时间复杂度: $O(n)$空间复杂度: $O(n)$ 利用 unordered_set 将所有的数字存储起来, 然后遍历每一个数字 num, 查看这个数字是否为所在连续序列的开头(即查看 num-1 是否存在). 若 num 就是所在连续序列的开头, 则查看当前序列的长度, 并更新最大长度. 故而时间复杂度为 $O(n+n) = O(n)$. 同时, 因为使用了 unordered_set, 所以空间复杂度为 $O(n)$.123456789101112131415161718class Solution &#123;public: int longestConsecutive(vector&lt;int&gt;&amp; nums) &#123; unordered_set&lt;int&gt; sets(nums.begin(), nums.end()); int longest = 0; for(auto num : sets)&#123; if(sets.find(num-1) == sets.end())&#123; int cur_len = 1; while(sets.find(num+1) !=sets.end())&#123; num++; cur_len++; &#125; if(longest &lt; cur_len) longest = cur_len; &#125; &#125; return longest; &#125;&#125;; 解法三: 另一种哈希表用法时间复杂度: $O(n)$空间复杂度: $O(n)$ 主题思想与解法二相同, 不过是从另一角度来使用 unordered_map, 首先, 依然利用 unordered_map 将 nums 存储起来, 然后遍历 nums, 对于 nums 中的每一个 num, 查看其是否存在于 unordered_map 中, 如果存在, 则分别向左向右查找当前数字 num 所在序列的最左端和最右端的数字, 同时, 将在 unordered_map 中遍历过的数字都移除(因为每个数字只可能唯一的属于一个连续序列). 之后, 利用最左端和最右端来更新最长连续序列的长度. 这样, 遍历的时间复杂度也为 $O(n+n) = O(n)$ 12345678910111213141516171819class Solution &#123;public: int longestConsecutive(vector&lt;int&gt;&amp; nums) &#123; unordered_set&lt;int&gt; sets(nums.begin(), nums.end()); int longest = 0; for(auto num : nums)&#123; if(sets.find(num)!=sets.end())&#123; sets.erase(num); int pre = num-1, next = num+1; while(sets.find(pre)!=sets.end()) sets.erase(pre--); while(sets.find(next)!=sets.end()) sets.erase(next++); if(longest &lt; next-pre) longest = next-pre-1; &#125; &#125; return longest; &#125;&#125;; 140. Word Break IIDescriptionGiven a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences. Note: The same word in the dictionary may be reused multiple times in the segmentation.You may assume the dictionary does not contain duplicate words.Example 1: Input:s = “catsanddog”wordDict = [“cat”, “cats”, “and”, “sand”, “dog”]Output:[ “cats and dog”, “cat sand dog”]Example 2: Input:s = “pineapplepenapple”wordDict = [“apple”, “pen”, “applepen”, “pine”, “pineapple”]Output:[ “pine apple pen apple”, “pineapple pen apple”, “pine applepen apple”]Explanation: Note that you are allowed to reuse a dictionary word.Example 3: Input:s = “catsandog”wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]Output:[] 解法一: DP直接使用回溯法, 有大量重复计算, 导致时间超时, 无法通过 OJ, 因此考虑 DP 思想. 将中间的计算结果缓存起来, 再次遇到的时候无需重复计算, 只需直接使用即可.利用一个哈希表将每个字符串与该字符串能拆分出的句子联系起来, 其中, key 为字符串, value 为字符串拆分后的句子. 假设我们已经求出一个字符串的解为 res, 并将其存入到哈希表中, 此时, 如果在该字符串的前面再加上一个单词(单词表的中任意一个), 那么新的解就应该为: word+&quot; &quot;+res[i]. 代码实现如下. 12345678910111213141516171819202122class Solution &#123;public: vector&lt;string&gt; wordBreak(string s, vector&lt;string&gt;&amp; wordDict) &#123; unordered_map&lt;string, vector&lt;string&gt;&gt; hash_dict; return DP_helper(s, wordDict, hash_dict); &#125; vector&lt;string&gt; DP_helper(string s, vector&lt;string&gt; &amp;wordDict, unordered_map&lt;string, vector&lt;string&gt;&gt; &amp;hash_dict)&#123; if(hash_dict.find(s)!=hash_dict.end()) return hash_dict[s]; if(s=="") return &#123;""&#125;; //这里必须返回具有一个元素("")的vector, 否则下面的push_back语句不会执行 vector&lt;string&gt; res; for(auto word : wordDict)&#123; if(s.substr(0, word.size()) != word) continue; vector&lt;string&gt; res_word = DP_helper(s.substr(word.size()), wordDict, hash_dict); //s.substr(word.size()) 代表截取剩余的字符, 所以有可能出现空字符的情况 for(auto str : res_word)&#123; // 如果返回的是空的vector, 则不会执行该语句, 因此, 不能返回空vector, 当遇到空字符串时, 因该返回 &#123;""&#125;, 即只有一个元素的vector, 该元素为"". res.push_back(word + (str==""? "":" ") + str); //这里根据 str的值来决定是否加空格, 如果str为空, 说明是word是最后一个字符, 则其后不应该添加空格 &#125; &#125; hash_dict[s] = res; return res; &#125;&#125;; 146. LRU Cache实现一个 LRU 缓存器, 即 Least Recently Used (最近最少使用). DescriptionDesign and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put. get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item. Follow up:Could you do both operations in O(1) time complexity? Example: LRUCache cache = new LRUCache( 2 / capacity / ); cache.put(1, 1);cache.put(2, 2);cache.get(1); // returns 1cache.put(3, 3); // evicts key 2cache.get(2); // returns -1 (not found)cache.put(4, 4); // evicts key 1cache.get(1); // returns -1 (not found)cache.get(3); // returns 3cache.get(4); // returns 4 解法一: 利用哈希表和双端链表时间复杂度: $O(n)$, get和put均为 $O(n)$ 空间复杂度: $O(n)$, 哈希表和双端链表 利用哈希表(unordered_map)来存储键值对, 用于实现 $O(1)$ 复杂度的查找和返回特定键对应的值.利用双端链表(list)来维护LRU逻辑, 即每次访问(get)时, 如果键存在, 那么在返回之前, 还应当将list中的键移到最顶端(最后), 首先, 顺序遍历找到该键($O(n)$复杂度), 然后将其删除($O(1)$复杂度), 接着, 将其插入到最后一位上($O(1)$复杂度). 对于插入(put)的情况, 首先判断是否已经存在($O(1)$复杂度), 如果已经存在, 那么将其value值更新并将其移动至最顶端($O(n)$复杂度). 否则, 判断当前是否溢出, 如果溢出, 则将list中的首部key值删除, 并将对应的hash键值对也删除($O(1)$复杂度), 然后执行插入逻辑($O(1)$复杂度). 如果没有溢出, 则直接插入. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class LRUCache &#123;private: int L_capacity; std::unordered_map&lt;int, int&gt; kv_map; std::list&lt;int&gt; key_l;public: LRUCache(int capacity) &#123; L_capacity = capacity; &#125; int get(int key) &#123; if(kv_map.find(key) != kv_map.end())&#123;// 访问了key, 将其移到最顶端 for(auto it=key_l.begin(); it!=key_l.end(); it++)&#123; if(*it == key)&#123; key_l.erase(it); break; &#125; &#125; key_l.push_back(key); // 访问了key, 将其移到最顶端 return kv_map[key]; &#125; return -1; &#125; void put(int key, int value) &#123; if(kv_map.find(key) != kv_map.end())&#123;// 访问了key, 将其移到最顶端 for(auto it=key_l.begin(); it!=key_l.end(); it++)&#123; if(*it == key)&#123; key_l.erase(it); break; &#125; &#125; key_l.push_back(key);// 访问了key, 将其移到最顶端 kv_map[key]=value; //更新value值, 因为有可能同样的key对应的value不同 &#125;else if(key_l.size() == L_capacity)&#123; int evict_key = key_l.front(); key_l.pop_front(); // 删除最少访问的key kv_map.erase(evict_key); // 删除最少访问的key key_l.push_back(key); kv_map.insert(&#123;key, value&#125;); &#125;else&#123; key_l.push_back(key); kv_map.insert(&#123;key, value&#125;); &#125; &#125;&#125;;/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 149. Max Points on a Line求最大的共线点个数 DescriptionGiven n points on a 2D plane, find the maximum number of points that lie on the same straight line. Example 1: Input: [[1,1],[2,2],[3,3]]Output: 3Explanation:^|| o| o| o+——————-&gt;0 1 2 3 4Example 2: Input: [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]Output: 4Explanation:^|| o| o o| o| o o+—————————-&gt;0 1 2 3 4 5 6 解法一: 哈希表时间复杂度: $O(n^2)$, 求取任意两点间的斜率空间复杂度: $O(n)$, 哈希表, 存储斜率 由于要求共线点个数, 就必须获取任意两点间的斜率, 因此, 时间复杂度最少为 $O(n^2)$. 算法流程如下: 对于每一个点来说, 构造一个哈希表, 表中的键为斜率, 表中的值为对应斜率的点的个数, 这里注意, 当我们求完第i个点与第j个点之间的斜率之后, 就不用再求第j个点与第i个点之间的斜率情况了(即令int j = i+1, 而不是int j = 0) 对于重点的情况, 需要单独设置一个变量来记录, 之后将该重复次数加入到该点所在的每条直线上(因为重点也算是共线) 对于斜率不存在的情况, 可以考虑利用INT_MAX来作为键值 精度: 在求取斜率时, 会进行除法, 而在计算机内部, 除法在精度上始终会有一定误差, 会造成斜率相同的两对点在计算成浮点数以后斜率不同, 因此, 要 避免使用除法, 解决办法是利用 最大公约数, 求取y2-y1与x2-x1之间的最大公约数, 然后对进行约分, 用约分后的值作为键来存储, 就不会造成精度上的损失, 但是, 此时需要用pair作为键, 故不能用unordered_map, 而只能用map(搜索的时间复杂度为 $O(logn)$), 另一种可选做法是利用string类型, 将两个int数值转换成string后再拼接, 此时就可以使用unordered_map了(搜索的时间复杂度为 $O(logn)$, 但是int和string的类型转换也需要消耗时间). 当采用公约数以后, 因为没有了除法, 因此可以不用特殊处理斜率不存在的情况, 代码更加简洁. 12345678910111213141516171819202122232425262728293031323334353637383940C/** * Definition for a point. * struct Point &#123; * int x; * int y; * Point() : x(0), y(0) &#123;&#125; * Point(int a, int b) : x(a), y(b) &#123;&#125; * &#125;; */class Solution &#123;public: int maxPoints(vector&lt;Point&gt;&amp; points) &#123; int res=0; for(int i=0; i&lt;points.size(); i++)&#123; int duplicate = 1; map&lt;pair&lt;int,int&gt;, int&gt; lines_hash; //这里用map的原因是因为unordered_map的键的类型只能是基本类型, 不能是pair for(int j=i+1; j&lt;points.size(); j++)&#123; if(points[i].x==points[j].x &amp;&amp; points[i].y==points[j].y)&#123; duplicate++; &#125;else&#123; int a = points[j].y-points[i].y; int b = points[j].x-points[i].x; int d = gcd(a, b); lines_hash[&#123;a/d, b/d&#125;]++; &#125; &#125; res = max(res, duplicate); // 如果points里面只有一个点, 则哈希表中不会有键值, 因此需要先处理只有一个点的情况 for(auto line : lines_hash)&#123; res = max(res, duplicate+line.second); &#125; &#125; return res; &#125; int gcd(int a, int b)&#123; // 求a与b的最大公约数 return (b==0) ? a : gcd(b, a%b); &#125;&#125;;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>算法刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智力题]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E6%99%BA%E5%8A%9B%E9%A2%98%2F</url>
    <content type="text"><![CDATA[智力题:如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？https://www.zhihu.com/question/38331955]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>智力题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch 基础]]></title>
    <url>%2Fz_post%2FPyTorch-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[PyTorch主要提供以下两大特色: 支持强力GPU加速的Tensor计算能力 基于tape的具有自动微分求导能力的深度神经网络框架 PyTorch 主要包含以下组成要素: 组成要素 描述说明 torch 一个类似于numpy的tensor哭, 提供强力的GPU支持 torch.autograd 一个基于tape的具有自动微分求导能力的库, 可以支持几乎所有的tesnor operatioin torch.nn 一个神经网络库, 与autograd深度整合, 可以提供最大限度的灵活性 torch.multiprocessing Python的多线程处理, 可以提供torch Tensors之间的内存共享, 对于加载数据和Hogwild training来说十分有用 torch.utils 一些功能类和函数, 如DataLoader, Trainer等等 torch.legacy(.nn/.optim) 为了兼容性而存在的一些代码和实现 Pytorch通常可以作为以下用途使用: 为了使用GPUs性能的numpy替代品 可以提供强大灵活力和速度优势的深度学习平台. torchtorch.cat()1torch.cat(seq, dim=0, out=None) # 返回连接后的tensor 将给定的 tensor 序列 seq 按照维度连接起来. 默认维度为0, 说明会将其在第 0 个维度上进行拼接.(最后的结果是第 0 维度增大, 例如三个2行3列的 tensor 按照第0维度拼接, 最后得到的 tensor 维度为6行3列) clamp()/clamp_()1torch.clamp(input, min, max, out=None) -&gt; Tensor 将input里面元素全部划分到[min,max]区间内, 小于min的置为min, 大于max的置为max. 如果不指定min或者max,则认为无下界或上界 其他调用形式:1torch.Tensor(min, max) # 调用tensor为input, 返回值为out device()1device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") gather()1torch.gather(input, dim, index, out=None) -&gt; Tensor 沿着dim指定的轴按着index指定的值重新组合成一个新的tensor. 123out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 即假设input是一个 n 维的tensor, 其 size 为 $(x_0, x_1, …, x_{i-1}, x_i, x_{i+1},…, x_{n-1})$, 若dim=i, 则 index 必须也是一个 n 维的tensor, 其 size 为 $(x_0, x_1, …, x_{i-1}, y, x_{i+1},…, x_{n-1})$, 其中 $y\geq 1$, 而返回的 tensor out 的 size 和 index 的 size 相同.一句来说 gather 的作用就是, 在指定的维度上筛选给给定下标index指示的值, 其他值舍弃. 一个例子说明:scores是一个计算出来的分数，类型为[torch.FloatTensor of size 5x1000]而y_var是正确分数的索引，类型为[torch.LongTensor of size 5]容易知道，这里有1000个类别，有5个输入图像，每个图像得出的分数中只有一个是正确的，正确的索引就在y_var中，这里要做的是将正确分数根据索引标号提取出来。12scores = model(X_var) # 分数scores = scores.gather(1, y_var.view(-1, 1)).squeeze() #进行提取 提取后的scores格式也为[torch.FloatTensor of size 5]这里讲一下变化过程：1、首先要知道之前的scores的size为[5,1000]，而y_var的size为[5]，scores为2维，y_var为1维不匹配，所以先用view将其展开为[5,1]的size，这样维数n就与scroes匹配了。2、接下来进行gather，gather函数中第一个参数为1，意思是在第二维进行汇聚，也就是说通过y_var中的五个值来在scroes中第二维的5个1000中进行一一挑选，挑选出来后的size也为[5,1]，然后再通过squeeze将那个一维去掉，最后结果为[5]. Tensor形式:1torch.Tensor.gather(dim, index) -&gt; Tensor torch.ge()torch.gt()1torch.gt(input, other, out=None) # -&gt; Tensor 根据 input 和 other 的值返回一个二值 tensor, 如果满足大于条件则为1, 不满足则为0.other 可以是能够转换成 input size 的tensor, 也可以是一个 float 标量. torch.index_select()1torch.index_select(input, dim, index, out=None) # -&gt; Tensor 返回在 dim 维度上的 index 指明的下标组成的 tensor.返回的 tensor 的维度的数量和 input 是相同的, 但是第 dim 维度的 size 会和 index size大小相同. 其他维度的 size 保持不变. torch.le()1torch.le(input, other, out=None) # -&gt;Tensor 按元素计算 $input \leq other$. max()123torch.max(input) # 返回一个Tensor, 代表所有元素中的最大值torch.max(input,dim,keepdim=False,out=None) # 返回一个元组:(Tensor, LongTensor) 第二种形式会返回一个元组, 元组内元素类型为: (Tensor, LongTensor), 其中, 前者代表对应 dim 上 reduce 后的最大值, 后者代表最大值在维度 dim 中对应的下标.如果keepdim=True, 则输出的 tensor 的 size 会和输入的相同, 只不过对应 dim 维度上的size为1. 否则, 对应 dim 维度会被 squeeze/reduce, 使得输出的维度比输入的维度少1.12345678&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]])&gt;&gt;&gt; torch.max(a, 1)(tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) mm()注意, 没有torch.mm_版本1torch.mm(mat1, mat2, out=None) # 返回值为Tensor, 也可以使用out记录返回值 两矩阵相乘, 矩阵的size需要满足乘法规则 其他调用形式:1torch.Tensor(mat2) # 调用者为mat1 norm()返回输入tensor的p-norm标量1torch.norm(input, p=2) # 返回一个标量tensor numel()1torch.numel(input) #返回一个int值 返回 inpput tensor 中的元素的总个数12a = torch.randn(1,2,3,4,5)print(torch.numel(a)) # 120 ones()randn()标准正太分布随机基础, 传入参数为维度信息 torch.sort()1torch.sort(input, dim=None, descending=False, out=None) # 返回 (Tensor, LongTensor) 如果没有给定维度 dim, 则会默认选择最后一个维度. sum()1234torch.sum(input, dtype=None) # 返回求和后的Tensor(只有一个元素)torch.sum(input, dim, keepdim=False, dtype=None) # 返回在dim上reduce的sum和, 如果dim包含多个维度, 则都进行reduce求和.# reduce这个词很形象, 因为返回的Tensor的维度刚好没有了dim指示的那些维度 其他形式:1torch.Tensor.sum() torch.t()1torch.t(input) # 返回转置后的Tensor 其他形式:1torch.Tensor.t() unsqueeze()在指定维度上插入一个 singleton 维度(一般用于将单一数据处理用 batch 的形式)1torch.unsqueeze(input, dim, out=None) # -&gt; Tensor 返回的tensor与input tensor 共享数据 dim 的取值范围在 [-input.dim()-1, input.dim()+1] 之间, 如果为负值, 则相当于 dim = dim + input.dim() + 1. zeros()torch.Tensortorch.Tensor 是默认类型 torch.FloatTensor 的别名, 使用 torch.Tenosr 的构造函数创建 tensor 变量时, 传入的是维度信息(注意与 torch.tensor() 的区别):12t = torch.Tensor(2,3,4) # 里面的数值未初始化, 是随机的print(t.size()) # torch.Size([2,3,4]) torch.LongTesnor 使用方法相似, 只不过数据类型是长整型. troch.tensor()创建tensor1torch.tensor(data, dtype=None, device=None, requires_grad=False) 可以利用torch.tensor从python的list数据或者其他序列数据中创建tensor对象12torch.tensor([[1,-1],[1,-1]])torch.tensor(np.array([[1,2,3],[4,5,6]])) 注意, torch.tensor()函数总是会对数据进行复制操作, 因此, 如果你仅仅是想将数据的requires_grad标志改变, 那么就应该使用required_grad_()或者detach()函数来避免复制. 同时, 对numpy数组使用torch.as_tensor()将其转换成tensor而无需复制 cpu12torch.Tensor.cpu()z = x.cpu() 将tensor移动到cpu上, 注意返回值z是cpu上的数据, tensorx本身的device属性不变 cuda12torch.Tensor.cuda()z = x.cuda() dim()1torch.Tensor.dim() -&gt; int 返回 tensor 的维度的个数. max()1torch.Tensor.max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) 详情见 torch.max() numel()1torch.Tensor.numel() 详见 torch.numel() to1torch.Tensor.to(*args, *kwargs) 返回一个转移后的tensor, 而自身维持不变1234t = torch.randn(2,3)t.to(torch.float64)t.to(device)t.to("cuda:0") 将tensor移动到gpu上, 注意返回值z是gpu上的数据, tensorx本身的device属性不变 tensor与numpy数组的转换123torch.Tensor.numpy() # 返回tensor对应的numpy数组torch.from_numpy(ndarray) # 将numpy数组ndarray转换成对应的tensor并返回. torch.Tensor 实际上是 torch.FloatFensor 的别名 permute()重新排列tensor的维度1torch.Tensor.permute(*dims) # 返回一个重新排列维度后的 tensor unsqueeze()详细可见torch.unsqueeze expand()1torch.Tensor.expand(*sizes) # 返回 tensor 将 tensor 中的 singleton 维度扩展到一个更大的 size.参数 -1 意味着不改变原始的维度新增的维度的元素被被添加到前头, size不能设置为-1.expand 并没有申请新的内存, 而仅仅是在当前已经存在的 tensor 上面创建了新的视图(view), 使得 singleton 维度被扩展成了一个更大的尺寸.Any dimension of size 1 can be expanded to an arbitrary value without new memory.1234x = torch.tensor([1],[2],[3])print(x.size()) # torch.Size([3,1])print(x.expand(3,4)) # torch.Size([3,4]) # 将维度为1的扩展到任意尺寸print(x.expand(-1,4)) # torch.Size([3,4]) # -1 代表不改变维度 注意, 只能对 singleton 的维度进行扩展, 如果强行对其他维度扩展, 则会报错. expand_as()1torch.Tensor.expand_as(other) # 返回 tensor 将当前 tensor 扩展到和 other 一样的size.self.expand_as(other) 与 self.expand(other.size()) 等价. index_fill_()1torch.Tensor.index_fill_(dim, index, val) # 返回tensor 在给定的维度 dim 上, 用 val 将该维度上的 index 坐标的值填充.1234567x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)index = torch.tensor([0, 2])x.index_fill_(1, index, -1)print(x)#tensor([[-1., 2., -1.],# [-1., 5., -1.],# [-1., 8., -1.]]) contiguous()返回一个连续的tensor, 数据内容不变1torch.Tensor.contiguous() # 如果tensor本身就是连续的, 那么就会返回tensor本身 这里的 contiguous 指的是内存上的连续, 由于在 PyTorch 中, view 只能用在 contiguous 的 tensor 上面, 而如果在 view 之前使用了 transpose, permute等操作后, 就需要使用 contiguous 来返回一个 contiguous tensor. 在 PyTorch 0.4 版本以后, 增加了 torch.reshape(), 这与 numpy.reshape() 的功能类似, 它大致相当于 tensor.contiguous().view() ? item()当Tensor中只包含一个元素时, 可以利用该函数返回这个元素的标量 tolist()可以将Tensor转换成列表 zero_()1torch.Tensor.zero_() 将当前的 tensor 变量全部置为0(原地) torch.autogradset_grad_enabled()1class torch.autograd.set_grad_enabled(mode) 用来控制梯度计算的开关(依据bool类型参数mode决定), 可以当做上下文管理器使用, 也可以当做函数使用1234567891011# 当做上下文管理器with torch.set_grad_enabled(is_train): # 注意, 这里省略了autograd loss.backward() optimizer.step()# 当做函数使用w1 = torch.Tensor([1], requires=True)torch.set_grad_enabled(True)print(w1.requires_grad) # Truetorch.set_grad_enabled(False)print(w1.requires_grad) # False sort()1sort(dim=None, descending=False) # 默认为升序, 返回(Tensor, LongTensor) 详见 torch.sort() no_grad()1class torch.autograd.no_grad 用于禁用梯度计算的上下文管理器.在测试阶段, 当你确信你不会调用Tensor.backward()时,禁用梯度计算十分有用. 这会降低计算使用内存消耗.123456x = torch.tensor([1.0], requires_grad=True)with torch.no_grad(): # 省略了autograd print(x.requires_grad) # True, 虽然为True, 但在该上下文中, 会无视掉requires_grad参数, 一律做False处理 y = x*2 print(y.requires_grad) # False, 在当前上下文产生的tensor的requires_grad属性为Falseprint(x.requires_grad) # True torch.autograd.Function1class torch.autograd.Function 为可微分的 ops 记录 operation history, 同时定义计算公式. 每一个作用在 tensor 上的 operatin 都会创建一个新的 function 对象, 它会执行计算过程并记录相关信息. 这些信息可以从一个由 functions 组成的有向图中获得. 当 backward() 方法被调用时, 就会利用这些信息在 function 上进行反向传播, 并将梯度传给下一个 Funtion.通常情况下, 当用于需要自定义可自动求导的 ops 时, 可以实现一个 Function 的子类.123456789101112# Exampleclass Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output*result static forward(ctx, *args, kwargs):**定义前向计算的逻辑. static backward(ctx, *grad_outputs):定义反向传导的逻辑, 如果确定不会使用到反向传播, 则可以不实现该函数. torch.nnModule1class torch.nn.Module 所有神经网络Module的基类, 自定义的模型也应该是它的子类.Modules可以包含其他Module(如Linear, Conv2d等等). parameters()12for param in model.parameters(): print(param.data, param.size()) state_dict:1torch.nn.Module.state_dict(destination=None,prefix="",keep_vars=False) 以字典形式返回整个module的状态 train1torch.nn.Module.train(mode=True) 将module的模式设置为train, 这只对部分module有效, 如Dropout, BatchNorm等, 详细请查看官网.返回值: torch.nn.Module training1torch.nn.Module.training # 属性, 返回一个bool值, 指示当前的模式是否为train eval1torch.nn.Module.eval() # 注意, 和train不同, eval为无参函数 将module的mode设置为evaluation, 同样, 只对部分module起效. Linear1torch.nn.Linear(in_features, out_features, bias=True) 全连接层的实现. 输入的shape为 $(N, …, in_features)$, 输出的shape为 $(N,…, out_features)$, 可以看出, 除了最后一维不同外, 其他维度都相同. (通常在使用Linear之前, 会将输入变成二维的矩阵, 其中第一维为batch size, 第二维为特征向量). in_features 和 out_features 可以当做属性用.来获取. Conv2d1class torch.nn.Conv2的(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int): out_channels(int): kernel_size(intortuple): stride(intortuple, optional): MaxPool2dSoftmax()1class torch.nn.Softmax(dim=None) dim指明了需要进行 softmax 的维度, 在这个维度上的值, 加起来和为1. ReLU1torch.nn.ReLU(inplace=False) 输入输出的shape是相同的, 执行relu函数 torch.nn.Sequential1class torch.nn.Sequential(*args) torch.nn.MSELoss1class torch.nn.MSELoss(size_average=None, reduce=None, reduction="elementwise_mean") size_average(bool, optional): 弃用(见reduction参数). 默认情况下, loss会计算在每个样本上的平均误差. 如果将size_average置为False, 则计算平方误差总和. 当reduce参数为False时, 忽视该参数 reduce(bool, optional): 弃用(见reduction参数). reduce参数顾名思义, 就是是否让MSELoss函数返回值的维度减少, 默认为True, 即会将任意维度的输入计算loss后, 返回一个标量(平均or总和取决于size_average), 如果为False, 则说明返回值维度不应该发生变化, 故而返回值就是对每个元素单独进行平方损失计算. 12345678910y = torch.tensor([1,2,3,4], dtype=torch.float)pred_y = torch.tensor([1,1,1,1], dtype=torch.float)loss_fn1 = torch.nn.MSELoss()loss1 = loss_fn1(y, pred_y)loss_fn2 = torch.nn.MSELoss(size_average=False)loss2 = loss_fn2(y, pred_y)loss_fn3 = torch.nn.MSELoss(reduce=False)loss3 = loss_fn3(y, pred_y)print(loss1,loss2,loss3)# tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.]) reduction(string, optional): 用字符串来替代上面两个参数的作用: “elementwise_mean”(默认) | “sum” | “none” (不进行reduce). torch.nn.functionalconv1d()conv2d()relu()1torch.nn.functional.relu(input, inplace=True) # 返回 一个 Tenosr relu_()1torch.nn.functional.relu_(input) # relu() 的原地版本 torch.optimlr_schedulerStepLR1class torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) 每经过step_size次epoch之后, lr就会衰减gamma倍(new_lr=lr×gamma), 初始的lr来自于optimizer中的lr参数.12345# Observe that all parameters are being optimizedoptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) ExponentialLR1class torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1) CosineAnnealingLR12345```## Adam```pyclass torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False) conv2dtorch.utils.dataDataLoader1class torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=&lt;function default_collate&gt;,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None) 数据加载器, 将数据集和采样器结合起来, 并且提供单/多线程的迭代器. dataset(utils.data.Dataset): batch_size(int,optional): batch中的样本个数 shuffle(bool,optional) num_worker(int,optional): 加载数据的线程个数, 0意味着只有一个主线程. 方法: __iter__(self): 可以当做迭代器使用, 如inputs,class_ids=next(iter(dataloaders)), 其中, input的shape为 $(N, C, H, W)$, class_ids的shape为 $(N)$. __len__(self): 返回数据集的类别数目 torchvisiontorchvision.utilsmake_grid1torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0) 制作一个关于image的grid, 返回值依然是一个tensor, 只不过尺度变成了3D, 相当于把多个图片拼接在一起了, 直接通过plt.imshow(grid)即可输出网格化以后的图片. tensor(Tensor/list): 4D的 mini-batch Tensor, Shape为 $(N×C×H×W)$, 或者是同维度的list. torchvision.transformstorchvision.transforms.Compose1234567class torchvision.transforms.Compose(transforms)# 使用trans.Compose([ transforms.CenterCrop(10), transforms.ToTensor(),]) 将多个transforms操作组合起来, 注意参数是列表形式 Transforms on PIL Image123# cv2 image to PIL Image# skimage to PIL Image 注意, 以下操作作用在PIL Image上的 CenterCrop1class torchvision.transform.CenterCrop(size) size参数表示输出的图谱的大小, 如果只传入了一个数字, 则该数字既表示高度, 又表示宽度. Resize1class torchvision.transforms.Resize(size, interpolation=2) size: 期望的输出size. interpolation: 插值方法, 默认为双线性插值 ToTensor1class torchvision.transforms.ToTensor 将一个PIL Image或者numpy.ndarray (H×W×C,[0, 255])转换成torch.FloatTensor (C×H×W, [0.0, 1.0]). RandomHorizontalFlip1transforms.RandomHorizontalFlip(p=0.5) 在给定概率下对PIL Image随机执行水平翻转操作 RandomResizedCrop1torch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2) 对PIL Image随机执行剪裁操作(按照scale和ratio的区间剪裁), 然后将剪裁后的图片放缩都期望的尺寸(默认插值为双线性插值) size: 期望得到的尺寸 scale: 剪裁的面积比例(相对于原始图) ratio: 剪裁的宽高比 interpolation: 默认为:PIL.Image.BILINEAR Transforms on torch.*Tensor注意, 以下操作是作用在tensor上的 Normalize1class torchvision.transforms.Normalize(mean, std) 将图片tensor按照均值mean和标准差std进行归一化, 对于n个channels, 有 mean=(M1, …, Mn), std=(S1,…,Sn).注意, 这个归一化操作是原地进行的 torchvision.datasetsImageFolder1class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=&lt;function default_loader&gt;) 一个一般化的数据加载器, 主要针对如下数据排列格式:1234567root/dog/x.pngroot/dog/y.pngroot/dog/z.png...root/cat/123.pngroot/cat/nsdf3.pngroot/cat/asd932_.png root: 根目录路径 transform(callable,optional): 对图片要做的变换操作 target_transform(callable,optional): 对target要做的变换操作 loader: 用于加载给定路径图片的函数 属性: classes(list): 返回类别的名字列表 class_names class_to_idx(dict): 以字典的形式返回(class_name, class_index) imgs(list): 返回元组列表: (image path, class_index) 方法: getitem(index): 根据index返回(sample,target)元组. 可以使用 len(imagefolder) 返回类别数量]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化方法整理总结]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概览 名称 公式 优化方法简述 SGD $g_t = \nabla_{\theta_{t-1}} f(\theta_{t-1})$ $\delta \theta_t = - \eta * g_t$ 每一次都计算mini-batch的梯度, 然后对参数进行更新 Momentum $m_t = \mu m_{t-1} + g_t$ $\delta \theta_t = -\eta m_t$ 公式中的 $\mu$ 为动量因子, 借助于物理学里面动量的概念, 通过动量的积累来在相关方向上加速SGD优化速度, 同时有助于跳出局部最优, 进而加快收敛 Nesterov $\delta_t = -\eta \mu m_{t-1} - \eta * g_t$ Nesterov在梯度更新时做一个矫正, 避免前进太快, 同时提高灵敏度, 从公式可以看出, 动量项 $m_{t-1}$ 没有改变当前的梯度 $g_t$, 而是利用之前的动量来影响当前的动量 Adagrad $n_t = n_{t-1} + g^2_t$ $\delta \theta_t = -\frac{\eta}{\sqrt{n_t + \epsilon}} * g_t$ Adagrad相当于在学习率前面乘了一个约束项 $\frac{\eta}{n_t + \epsilon}, 使得 $g_t$ 较小的时候, 能够放大梯度, 反之, 在 $g_t$ 较大的时候, 能够约束梯度 Adadelta … Adadelta是对Adagrad的扩展, 核心思想依然是对学习率乘上一个约束项, 但是进行了计算上的简化 RMSprop … RMSprop可以算作是Adadelta的一个特例 Adam $m_t = \mu m_{t-1} + (1-\mu) g_t$ $n_t = \nun_{t-1} + (1 - \nu) g^2_t$ $\hat m_t = \frac{m_t}{1-\mu_t}$ $\hat n_t = \frac{n_t}{1- \nu_t}$ $\delta \theta_t = -\frac{\hat m_t}{\sqrt{\hat n_t} + \epsilon} * \eta$ Adam本质上是带有动量项的RMSprop, 它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率. 公式中, $m_t, n_t$ 分别是对梯度的一阶矩估计和二阶矩估计, 可以看作对期望 $E g_t , E g^2_t $ 的估计, $\hat m_t, \hat n_t$ 是对 $m_t$, $n_t$ 的校正, 这样可以近似为对期望的无偏估计. Adamax … … Nadam … … 损失平面等高线: 鞍点处的比较: 损失函数选择经验 对于稀疏数据, 尽量使用学习率可自适应的优化方法, 因为自适应的优化方法对不同的参数会赋予不同的更新步长 // TODO 以下四点未确认正确性 SGD通常训练时间更长, 但是在好的初始化和学习率调度方案的情况下, 结果更可靠 如果在意更快的收敛, 并且需要训练较深较复杂的网络时, 推荐使用学习率自适应的优化方法 Adadelta, RMSprop, Adam是比较相近的算法, 在相似情况下表现差不多 在想使用带动量的RMSprop, 或者Adam的地方, 大多可以使用Nadam取得更好的效果 深入解析各个损失函数的优缺点SGD缺点因为要兼顾整个神经网络的训练效果, 因此通常学习率的选择比较困难. 并且训练过程中, 无法调节学习率. 某些情况下容易被困在鞍点, 需要使用合适的初始化和步长, 才能跳出鞍点 Momentum区别于联系超参数learning rate: 学习率决定了权值更新的速度, 设置的太大会使结果超过最优值, 太小会使下降速度过慢, 仅靠认为干预调整参数需要不断修改学习率, 十分不合理. weight decay: 就是正则项, L1或L2正则项 momentum: learning rate decay: 每经过一段迭代次数以后, 就会减小learning rate的大小.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习/深度学习面试问题汇总]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统面试问题汇总]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++面试总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-Cpp%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[[ToC] https://zhuanlan.zhihu.com/p/46237848?utm_source=wechat_session&amp;utm_medium=social vector 和 list 的区别new, delete, malloc, free的关系delete会调用对象的析构函数, 和new对应 free只会释放内存, 和malloc. malloc/free是C++/C 语言的标准库函数, new/deletec是C++语言的运算符. 它们都可用于申请动态内存和释放内存, 对于非内部数据类型的对象而言, 光用malloc/free无法满足动态对象的要求, 对象在创建的同时要自动执行构造函数, 在消亡之时要自动执行析构函数, 由于malloc/free是库函数而不是运算符, 因此不在编译器控制权限之内, 不能够把执行构造函数和析构函数的任务强加于malloc/free. 因此C++语言需要一个能完成动态内存分配和初始化工作的运算符new, 以及一个能完成清理与释放内存工作的运算符delete. new, delete, malloc, free的内部实现delete 和 delete []的区别delete只会调用一次析构函数, 而delete[]会调用数组内每一个成员的析构函数. 在More Effective C++中有更为详细的解释: 当delete操作符用于数组时, 它为每个数组元素调用析构函数, 然后调用operator delete来释放内存 在对 内建数据类型 使用时, delete和delete[]是等价的, 因此delete[]会调用数组元素的析构函数, 但是内部数据类型没有析构函数, 所以可以直接使用. 子类析构时要调用父类的析构函数吗?调用 析构函数调用的次序是先调用派生类的析构, 后调用基类的析构, 也就是说在基类的析构函数调用的时候, 派生类的信息已经全部销毁了. 定义一个对象时先调用基类的构造函数, 然后调用派生类的构造函数, 析构的时候恰好相反, 先调用派生类的析构函数, 然后调用基类的析构函数 什么是引用? 声明和使用引用时要注意哪些问题?引用就是某个变量的”别名”(alias). 声明一个引用的时候, 切记要对其进行初始化. 声明完毕后, 相当于目标变量具有了两个名称, 即原名称和引用名, 引用名与该变量绑定, 不能再把该引用名作为其他变量名的别名. 对引用求地址, 就是对目标变量取地址, 即我们常说引用名是目标变量名的一个别名. 注意, 引用是占据空间的, 编译器一般将引用实现为const指针, 即指向位置不可变的指针. 所以实际上引用与一般指针同样占用内存 不能建立引用的数组. 因为数组是一个由若干个元素所组成的集合, 所以无法建立一个由引用组成的集合. 但是可以建立数组的引用(即数组的别名) 12345int x=2,y=3,z=4;int&amp; ref[3] = &#123;&amp;x, &amp;y, &amp;z&#125;; //非法!int arr[3]&#123;1,2,3&#125;;int(&amp;ref) [3] = arr; //合法, ref是一个引用, 指向一个包含3个元素的一维数组 引用和指针的关系与区别?关系: 编译器一般会将引用实现为const指针, 所以可以将引用看做是一种特殊的指针 区别: 引用不能为空, 即引用必须连接到一块合法的内存 int &amp;a; 错误 一旦引用被初始化为一个对象, 就不能再被指向另一个对象(因为引用时const指针) 引用必须在创建的同时被初始化, 而指针可以在任何时间被初始化 将引用作为函数参数有哪些特点? 由于引用可以看做是const指针, 因此传递引用给函数和传递指针给函数的效果是一样的, 这时, 被调函数的形参就成为原来主调函数中的实参变量或对象的一个别名来使用, 所以在被调函数中对形参变量的操作就是对其相应的目标对象的操作. 使用引用传递参数时, 在内存中并不会产生实参的副本, 因此, 当参数传递的数据较大时, 用引用的空间利用率高 将引用作为函数返回值类型的优势和注意事项?优势:在内存中不会产生被返回值的副本 ( 注意: 正是因为这个原因, 所以返回一个局部变量的引用是不可取的. ) 注意事项: 不能返回局部变量的引用: 因为在内存中不会产生被返回值的副本, 随着该局部变量生存期的结束, 引用指向的变量就失效了, 此时会产生runtime error错误. 不要返回函数内部new分配的内存的引用: 虽然不存在局部变量的自动销毁问题, 但是对于这种情况, 又是会面临其他尴尬局面. 例如, 被函数返回的引用只是作为一个临时变量出现, 而没有被赋予一个实际的变量, 那么这个引用所指向的空间(由new分配)就无法释放 很容易造成内存泄漏. 可以返回对象成员的引用, 但最好是const. 主要原因是当对象的属性与某种业务规则相关联的时候, 其赋值常常与某些其他属性或者对象的状态有关, 因此有必要将赋值操作封装在一个业务规则中. 如果其他对象可以获得该属性的非常量引用(或指针), 那么对该属性的单纯赋值会破坏业务规则的完整性. 引用和流操作符的重载, 因为这两个操作符常常希望被连续使用, 因此这两个操作符重载时的返回值应该是一个仍然支持操作符特性的流引用 http://wyude.lofter.com/post/1cb19406_68f16ad 在什么时候需要使用常引用?如果既要利用引用提高程序的效率, 又要保护传递给函数的数据不在函数中被改变, 就应该使用常引用, 同时如果传入的实参是const类型的变量, 则形参必须也声明为const. 通常, 如果引用型参数在能够被定义为const的情况下, 优先定义为const. 什么是函数指针?结构体和联合体有什么区别? 结构体和联合体都是由多个不同的数据类型成员组成, 但在任何同一时刻, 联合体中只存放了一个被选中的成员(所有成员共用一块地址空间), 而结构体的所有成员都存在(不同成员的存放地址不同) 对于联合体的不同成员赋值, 将会对覆盖其他成员, 原来成员的值就不存在了, 而对于结构的不同成员赋值是互不影响的 重载(overload)和重写/覆盖(override)的区别?从定义上来说: 重载: 是指允许存在多个同名函数, 但是这些函数的参数签名不同(参数个数, 参数类型, const) 重写: 是在继承时体现的, 是指子类重新定义父类虚函数的方法, 其中父类函数必须有virtual关键字, 且不能有static, 子类函数与父类函数签名相同, 且返回值也要相同(或者 返回值协变 ), 访问权限修饰符可以不同. 从实现原理上来说: 重载: 编译器会根据函数不同的函数签名, 对这些同名函数的名称做一些修饰, 然后这些同名函数就成了不同的函数(至少对编译器来说是这样的), 这些修饰后的同名函数的调用, 在编译期间就已经确定了, 因此它们的地址也已经确定了, 因此, 重载与多态无关 重写: 重写与多态息息相关. 当子类重新定义了父类的虚函数以后, 父类指针会根据赋给它的不同的子类指针, 动态 的调用属于子类的该函数, 这样的函数调用在编译期间是无法确定的(无法给出子类的虚函数的地址). 只有在执行阶段, 子类的函数地址才能够确定. C++中的协变与逆变https://www.jianshu.com/p/db76a8b08694 哪些情况下只能用初始化列表(initialization list) 而不能用赋值 (assignment)?Cpp-初始化列表 C++是不是类型安全的?不是, 因为两个不同类型的指针之间可以强制转换. 描述内存分配方式以及它们之间的区别? 从静态存储区域分配: 内存在程序 编译 时就已经分配好, 这块内存在程序的整个运行期间都一直存在, 例如全局变量, static变量等. 从栈中分配: 函数内的局部变量的存储单元都会在栈上创建, 函数执行结束时这些存储单元会被自动释放 从堆中分配: 也称为动态内存. 程序在运行的时候用malloc或new申请任意大小的内存, 程序员自己负责在何时使用和释放这些内存. 动态内存的生存期由程序员自己决定, 使用非常灵活, 但相关的内存泄漏问题也尝尝发生. float与0比较时需要注意什么?需要注意精度表示的问题, 不能使用f == 0 而应使用f&lt;0.00001 &amp;&amp; f&gt;0.00001类似的语句. const和#define相比, 有何优点? const常量具有数据类型, 而宏常量没有数据类型. 编译器可以对前者进行类型安全检查, 而对后者只会进行字符替换, 没有类型安全检查, 容易发生意想不到的错误. 有些集成化的调试工具可以对const常量进行调试, 但是不能对宏常量进行调试 什么情况下会发生运行时错误(runtime error)?数组越界访问, 除数为0 , 堆栈溢出 数组和指向数组名的指针有什么区别?数组的内存空间要么在静态存储区中(全局数组), 要么在栈中. 而指针可以随时指向任意类型的内存块. 在使用sizeof运算符时, 数组返回的是整个数组所占的字节数, 指针返回的是指针变量本身的字节数. C++/C 语言没有办法知道指针所指的内存容量, 除非在申请内存时记住它, 注意当数组作为函数的参数进行传递时, 该数组名就会自动退化为同类型的指针, 也就是说此时再使用sizeof时, 返回的是指针变量的大小, 而不是数组大小 int (* a[10])(int) 表示的是什么?a是一个函数指针数组, 数组中的每个元素都是函数指针, 指向参数和返回值为int的函数. 栈内存与文字常量区this指针 this指针是一个 隐含于 每一个非静态成员函数中的特殊指针. 它指向正在被该成员函数操作的那个对象 当一个对象调用其成员函数时, 编译程序会先将对象的地址赋给this指针, 然后调用成员函数, 每次成员函数存取数据成员时, 会隐含使用this指针. 当一个成员函数被调用时, 自动向它传递一个隐含的参数, 该参数是一个指向这个成员函数所在的对象的指针 this指针被隐含的声明为: ClassName *const this, 这意味着不能给this指针不能再指向其他对象, 在ClassName类的const成员函数中, this指针的类型为const ClassName* const, 这说明也不能对this指针所指向的这种对象进行赋值操作. this并不是一个常规变量, 而是一个 右值, 所以不能取得this的地址.(不能&amp;this, 左值右值的区别就在于是否可以取地址) 在以下场景中, 经常需要显式使用this指针: 为实现对象的链式引用 为避免对同一对象进行赋值操作(this.obj = obj, 在构造函数中这种很常用) 为实现一些数据结构时, 如list inline内联函数在C/C++中, 如果有一些函数被频繁调用, 那么就会不断的进行函数入栈出栈的操作, 这会造成栈空间以及程序运行时间的消耗. 栈空间就是指放置程序的局部数据以及函数内数据的内存空间, 在正常情况下, 栈空间都是有限的, 如果频繁大量的使用就会造成栈空间不足的问题, 函数的死循环递归调用的最终结果就是导致占内存空间的枯竭 特征: 相当于把内联函数里面的内容直接写在了调用内联函数处 (类似于#define或者typedef的那种替换操作) 相当于不用执行进入函数的步骤, 直接执行函数体 相当于 宏, 却比宏多了类型检查, 真正具有函数特性(这也是与#define的区别所在) 在类声明中定义的函数, 除了虚函数的其他函数都会自动隐式的成为内联函数 关键字inline必须与函数定义体放在一起才能使函数成为内联, 仅将inline放在函数声明前不起任何作用 与非内联函数不同的是, inline函数必须在调用该函数的每个文本文件中定义. 当然, 对于同一程序的不同文件, 如果inline函数出现的话,其定义必须相同. 优点: 内敛函数会像宏函数一样在被调用处展开, 省去了参数压栈, 栈帧开辟, 结构返回等步骤, 从而提高了程序的运行速度 内联函数相比宏函数来说, 在代码展开处会做安全检查和类型转换(同普通函数), 而宏定义函数则不会 在类声明中同时定义的成员函数, 会自动转化为内联函数, 因此 内联函数可以访问类的成员变量, 宏定义则不能. 内联函数在运行时可以调试, 而宏定义则不行(因为宏定义是被预定义处理的, 所以不会有人黑的编译符号和调试信息, 调试的时候基本只能用肉眼去看) 缺点: 代码膨胀. 内联函数是以代码膨胀(复制)为代价, 消除函数调用带来的开销. 如果执行函数体内代码的时间, 相比于函数调用的开销较大, 那么效率的收获就会很少. 另一方面, 每一处内联函数的调用都要复制代码, 将使程序的总代码量增大, 消耗更多的内存空间. inline函数无法随着函数库升级而升级. inline函数的改变需要重新编译, 不像非内联函数那样可以直接链接 是否内联, 程序员不可控, 内联函数只是对编译器的建议, 对于最终实现的决定权在于编译器. 虚函数可以是内联函数吗 虚函数在语法上可以是内联函数, 内联是可以修饰虚函数的, 但是当虚函数表现多态性的时候是不能内联的. (在具体实现时, 到底是否内联, 是由编译器决定的), 如下: 1234567891011121314#include &lt;cstdio&gt;struct Base &#123; virtual ~Base() &#123;&#125; virtual void Foo() &#123; printf("Base::Foo()\n"); &#125;&#125;;struct Derived : Base &#123; virtual void Foo() &#123; printf("Derived::Foo()\n"); &#125;&#125;;Base* b = new Derived; // 非 static 令编译器不能在编译期知道 b 指向那个类型的对像int main() &#123; b-&gt;Foo(); // 不可能内联 b-&gt;Base::Foo(); // 非多态调用，可以内联（但具体是否内联由编译器决定） delete b;&#125; 内联实际上是在建议编译器内联, 而虚函数的多态性需要在运行期起作用, 编译器无法知道运行期具体调用哪个代码, 因此虚函数表现为多态性时, 不可以内联 inline virtual 唯一可以内联的时候是: 编译器知道所调用的对象是哪个类(如 Base::who()), 这只有在编译器具有 实际对象而不是对象的指针或引用时才会发生. explicit 关键字用explicit关键字修饰的构造函数可以用来防止函数参数的隐式转换 explicit关键字只能用于类内部的构造函数的声明上, 而不能用在类外部的函数定义上 Effective C++建议, 除非有一个好的理由允许构造函数被用于隐式类型转换, 否则应该将其声明为explicit. 友元类和友元函数 能访问私有成员 破坏封装性 友元关系不可传递 友元关系的单向性 友元声明的形式及数量不受限制 :: 范围解析运算符 全局作用域符(::name): 用于名称(类, 类成员, 成员函数, 变量等)前, 表示作用域为全局命名空间 类作用域符(class::name): 用于表示指定类型的作用域范围是具体某个类的 命名空间作用域符(namespace::name): 用于表示指定类型的作用域范围是具体某个命名空间的 1234567int count = 10; //全局(::)的countint main()&#123; int count = 20; //局部的count std::cout&lt;&lt;::count; // 输出 10, std为命名空间 std::cout&lt;&lt;count; // 输出 20&#125;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode算法题(Medium)]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98-LeetCode-2%2F</url>
    <content type="text"><![CDATA[002. Add Two NumbersDescriptionYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example: Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 解法一: 顺序相加, 注意进位从链表的第一个节点开始, 将两个节点的值和进位位想加, 如果大于10, 则当前结果节点的值对10取余, 同时将进位位置1, 如果小于10, 则直接赋值给当前结果节点, 同时将进位位置0. 特别注意l1和l2的长度问题, 当二者节点遇到nullptr时, 将较长的剩余部分重新赋给l1, 并继续判断 最后, 需要注意是否有进位位, 如果有, 则要申请一个新节点, 并将其置为1 时间复杂度: $O(\max(m,n))$ 空间复杂度: $O(1)$ (这种做法会破坏原有链的结构) 空间复杂度: $O(\max(m,n))$ (这种做法需要额外申请空间, 但不会破坏原有链的结构) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; int carry = 0; ListNode* head = new ListNode(0); //创建指向最终结果的头指针 if(l1!=nullptr) head-&gt;next = l1; // 虽然题目指明为非空链表, 但是最好还是做一下判断 else head-&gt;next = l2; ListNode* pre=head; // pre用于保存l1的上一个指针 while(l1!=nullptr &amp;&amp; l2!=nullptr)&#123; l1-&gt;val = l1-&gt;val + l2-&gt;val + carry; if(l1-&gt;val &gt; 9)&#123; l1-&gt;val %= 10; carry = 1; &#125;else&#123; carry = 0; &#125; pre = l1; l1 = l1-&gt;next; l2 = l2-&gt;next; &#125; if(l2!=nullptr)&#123; // 此时说明l2比l1长, 用l1的上一个指针指向当前l2剩余的部分, l1 = pre; l1-&gt;next = l2; l1 = l1-&gt;next; &#125; while(l1!=nullptr)&#123; // 此时l1为剩余(l1或l2) 的部分, 只需要考虑是否有进位即可 l1-&gt;val = l1-&gt;val + carry; if(l1-&gt;val &gt; 9)&#123; l1-&gt;val %= 10; carry = 1; &#125;else&#123; carry = 0; // 如果没有进位, 一定要将此处置0, 否则会引起错误 break; &#125; pre = l1; l1 = l1-&gt;next; &#125; if(carry == 1)&#123; // 对应 999 + 001 的特殊情况, 此时进位会不断传递, 最终数字位数加1, 最高位为1 ListNode* newnode = new ListNode(1); l1 = pre; l1-&gt;next = newnode; &#125; return head-&gt;next; &#125;&#125;; 扩展问题What if the the digits in the linked list are stored in non-reversed order? For example: $(3 \to 4 \to 2) + (4 \to 6 \to 5) = 8 \to 0 \to 7 (3→4→2)+(4→6→5)=8→0→7$ 思路: 先将链表转置 , 再用上面的方法求解 转置时间复杂度: $O(n)$转置空间复杂度: $O(1)$ 003. Longest Substring Without Repeating CharactersDescriptionGiven a string, find the length of the longest substring without repeating characters. Example 1: Input: “abcabcbb”Output: 3Explanation: The answer is “abc”, with the length of 3.Example 2: Input: “bbbbb”Output: 1Explanation: The answer is “b”, with the length of 1.Example 3: Input: “pwwkew”Output: 3Explanation: The answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. 解法一:暴力时间复杂度: $O(n^3)$ 对于每一个字符, 子串的添加以及查重过程时间复杂度为 $O(n^2)$ , 总共n个字符, 所以为 $O(n^3)$ 时间复杂度: $O(min(n,m))$ 需要将当前子串存在起来以便查询是否相等, n为字符串length, m为字符集size 解法二: 前后两个指示变量思路: 首先构造一个哈希表, 用来存储当前子串中出现的字符, 这样, 新来的字符可以直接查询哈希表来判断字符是否存在, 构建哈希表空间复杂度为 O(min(n,m)) (m为字符集合的大小,一般为26(字母), 128(ASCII), 256(ASCII)) 然后, 使用两个指示变量, 分别指向当前未重复子串的首字符, 和超尾字符, 进行如下几个判断: 如果超尾字符与当前子串中的字符不重复, 那么将超尾字符加入到当前子串中,并将length加1 如果超尾字符与当前子串中的字符重复, 利用哈希表查的重复字符的所在位置, 将当前子串的首字符直接跳向该重复字符的下一个位置( 这样可以保证只遍历一遍 ), 并将包括重复字符在内的之前所有字符都从哈希表中删除(之前的字符不再可能组成更长的子串了), 同时将超尾字符加入, length赋予新值: 超尾位置-重复位置-1; 判断首字符与超尾字符是否相等, 如果相等, 将超尾字符加1, 并将length置为1 看当前length是否比maxlength大, 并重复以上过程,直到超尾字符超出size 时间复杂度: $O(n)$空间复杂度: $O(min(n,m))$ 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: int lengthOfLongestSubstring(string s) &#123; unordered_map&lt;char,int&gt; ch_exists; if(s.size() &lt;= 0) return 0 ; int max_length = 1; int length = 1; ch_exists.insert(&#123;s[0], 0&#125;); for(int i =0,j = 1 ; j &lt; s.size() ; )&#123; if(ch_exists.count(s[j]) == 0 )&#123; // j对应的字母未重复, length增加 length++; ch_exists.insert(&#123;s[j], j&#125;); j++; &#125;else&#123; // j对应的字母重复, 将i置于重复字母的下一个(因为之前都都不可能产生更长的未重复子串了) int index = ch_exists[s[j]]; for(int k = i; k &lt;= index; k++) ch_exists.erase(s[k]); i=index + 1; ch_exists.insert(&#123;s[i], i&#125;); length = j - index - 1; &#125; if(i==j)&#123; //这里如果写在开头, j++之后有可能超过size, 导致有问题, //所以写在后面, 以便在for循环中检查j是否超过size j++; length = 1; &#125; if(length &gt; max_length) max_length = length; &#125; return max_length; &#125;&#125;; 解法二的另一种写法该写法核心思路与上面的一样, 所以时间复杂度和空间复杂度也一样, 唯一不同的是hash表, 前面的会将i与重复字符之间的都删除, 这里不删除, 利用max控制, 使i永远不会倒退 这里写法更加简洁, 应值得学习 123456789101112131415161718class Solution &#123;public: int lengthOfLongestSubstring(string s) &#123; unordered_map&lt;char, int&gt; s_hash; int max_length = 0; for(int i = 0 ,j=0 ; j&lt; s.size() ; j++)&#123; if(s_hash.count(s[j]))&#123; i = max(i,s_hash[s[j]]+1); //如果遇到重复的, 就将当前的i指向重复的下一个 // (这里用max的原因是, 没有删除当前i到重复字符之间的其他字符, 这些字符 // 后续还可能被检测到, 所以这里只取max的, 也就是i不会倒退) s_hash.erase(s[j]); // 将重复的删除, 以便赋予新的值 &#125; s_hash.insert(&#123;s[j], j&#125;); max_length = max_length &gt; (j-i+1) ? max_length : (j-i+1); &#125; return max_length; &#125;&#125;; 005. Longest Palindromic Substring(最大回文子串)DescriptionGiven a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: “babad”Output: “bab”Note: “aba” is also a valid answer.Example 2: Input: “cbbd”Output: “bb” 解法一：最长公共子串$O(n^2)$ $O(n)$ 解法二： 穷举$O(n^3)$ $O(1)$ 解法三： 动态规划？ 解法三： 扩展中心法以每一个字符为中心， 向两边扩展， 将当前能够扩展的长度 len 和最大扩展长度 max_len 作比较, 记录较大者, 同时记录较大者的所对应的重心字符的下标 max_index. 最后, 根据最大扩展的长度max_len 和中心字符的下标 max_index 计算最大回文子串的开始位置和总长度 此处注意, 回文子串有奇偶两种情况, 可采用以下举措之一解决: 分别检查奇数和偶数的情况 向字符内插入特殊符号 ‘#’, 这样不管偶数奇数, 都可以当做奇数处理, 缺点是占用了额外的 $O(n)$ 空间 时间复杂度: $O(n^2)$ 空间复杂度: $O(1)$ 或者 $O(n)$ 注意: 既然已经使用了空间复杂度为 $O(n)$ 的方法, 实际上更应该将其该写成马拉车算法 12345678910111213141516171819202122232425262728293031// 空间复杂度 $O(1)$class Solution &#123;public: string longestPalindrome(string s) &#123; int max_len = 0; int start = 0; for(int i=0; i &lt; s.size(); i++)&#123; int len1=0,len2=0; int left=i, right = i; //通过left和right , 是的对奇偶的分别处理更方便 while( left &gt;=0 &amp;&amp; right&lt;s.size() &amp;&amp; s[left] == s[right])&#123; left--; right++; &#125; len1 = right-left-1; // 注意, 这里一定是-1, 而不是+1 left=i; right=i+1; while( left&gt;=0 &amp;&amp; right&lt;s.size() &amp;&amp; s[left] == s[right])&#123; left--; right++; &#125; len2 = right-left-1; int len = max(len1, len2); if(len&gt;max_len)&#123; max_len = len; start = i- (len-1)/2; &#125; &#125; return s.substr(start, max_len); &#125;&#125;; 1234567891011121314151617181920212223242526272829303132// 空间复杂度 $O(n)$class Solution &#123;public: string longestPalindrome(string s) &#123; char* cs = new char[s.size() * 2+1]; cs[0]='#'; for(int i=0; i&lt;s.size() ; i++)&#123; //插入 '#' cs[i*2+1] = s[i]; cs[i*2+2] = '#'; &#125; int max_len=0; int max_index = 0; for(int i =0; i&lt;s.size()*2+1 ; i++)&#123; int len=0; //记录当前扩展长度len for(int j=1; i-j&gt;=0 &amp;&amp; i+j&lt;s.size()*2+1 ;j++)&#123; if(cs[i-j] == cs[i+j])&#123; //两边字符若相等, 则len长度增1 len++; &#125;else break; &#125; if(len &gt; max_len)&#123; max_len = len; max_index = i; &#125; &#125; int start = (max_index - max_len)/2; //根据maxlen和index 计算回文子串开始坐标 int len = max_len; delete cs; return s.substr(start, len); &#125;&#125;; 解法五: 马拉车(Manacher) 算法时间复杂度: $O(n)$空间复杂度: $O(n)$ There is even an O(n)O(n) algorithm called Manacher’s algorithm, explained here in detail. However, it is a non-trivial algorithm, and no one expects you to come up with this algorithm in a 45 minutes coding session. But, please go ahead and understand it, I promise it will be a lot of fun 马拉车算法的核心思想还是从中心扩展发出发, 不过他必须使用 ‘#’ 字符先对原始字符串插入, 如下所示: 接下来, 在每一次for循环当中, 都需要保存这么几个值(命名是个人习惯, 可以用其他名字代替): P: P为最大右边界下标值, 对应的是所有已检测的回文子串中, 右边界下标最大的那个 P_center: 该值是P对应的回文子串的中心下标 max_len: 对应当前最大回文子串的半径(aba的半径为1, a的半径为0) max_index: 对应当前最大回文子串的中心下标 然后, 还需要构建一个和插入’#’后的字符串长度相关的数组p_len, 里面存放着对应位置的回文串半径, 用以后续的计算, 这一步是关键, 有了这个数组 ,才能实现利用之前计算结果 接下来, 遍历 “新字符串”(即插入’#’之后的字符串) 的每一个字符, 设当前下标为 i, 则有如下情况, 分别处理: P&gt;i, 说明 i 在 P 的范围内, 可以利用前面的计算结果 P&lt;=i, 说明i不在 P 的范围内, 无法利用前面的计算结果, 只能逐个判断 对上面两种情况具体分析如下: 第一种情况: P&gt;i 找到i相对于 P_center 的对称位置，设为j，那么如果Len[j]&lt;P-i, 如下图所示: 则以i为中心的回文串的长度至少和以j为中心的回文串一样 , 即Len [i]&gt;=Len[j] , 因此可以直接从Len[j]+1开始判断回文 如果Len[j]&gt;=P-i, 如下图所示: 由对称性，说明以i为中心的回文串可能会延伸到P之外，而大于P的部分我们还没有进行匹配，所以要从P+1位置开始一个一个进行匹配，直到发生失配 第二种情况: P&lt;=i 如果i比P还要大，说明对于中点为i的回文串还一点都没有匹配，这个时候，就只能老老实实地一个一个匹配了 在这一次循环完成之前, 更新上面提及的四个变量 循环结束后, 根据 max_index 和 max_len 的值返回最长回文子串 时间复杂度分析: 对于每一个字符, 由于如果之间比较过, 那么就可以利用之前比较的结果直接判断, 所以每个字符都只进行了一次比较, 故而时间复杂度为 $O(n)$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123;public: string longestPalindrome(string s) &#123; int cs_size = s.size()*2+1; char* cs = new char[cs_size]; cs[0] = '#'; for(int i = 0;i&lt;s.size(); i++)&#123; cs[i*2 + 1] = s[i]; cs[i*2 + 2] = '#'; &#125; int P = 0; int P_center = 0; int max_index = 0; int max_len = 0; int* p_len = new int[cs_size]; for(int i =0; i&lt;cs_size; i ++)&#123; if( i &lt; P)&#123; // 如果i&lt;P, 说明可以复用前面的计算结果 int j = P_center*2 - i; // j对i关于P_center的对称点 if(P-i &gt; p_len[j])&#123; // 如果i与P之间的距离比 j 的回文串长度还大, //说明可以直接从p_len[j] + 1开始比较, 之前的子串一定是回文串 int k = p_len[j] + 1; while(i-k&gt;=0 &amp;&amp; i+k&lt;cs_size &amp;&amp; cs[i-k] == cs[i+k])&#123; k++; &#125; p_len[i] = k-1; &#125;else&#123; // 如果距离没有p_len[j] + 1大, 则从超出P的部分开始比较 int k = P - i; while(i-k&gt;=0 &amp;&amp; i+k&lt;cs_size &amp;&amp; cs[i-k] == cs[i+k])&#123; k++; &#125; p_len[i] = k-1; &#125; &#125;else&#123; //如果i不在P范围内, 则必须从1开始逐个比较, 无法利用之前的计算结果 int k = 1; while(i-k&gt;=0 &amp;&amp; i+k&lt;cs_size &amp;&amp; cs[i-k] == cs[i+k])&#123; k++; &#125; p_len[i] = k-1; &#125; if(p_len[i] &gt; max_len)&#123; max_len = p_len[i]; max_index = i; &#125; if(i+p_len[i] &gt; P)&#123; P = i+p_len[i]; P_center = i; &#125; &#125; delete cs; delete p_len; int start = (max_index - max_len)/2; int len = max_len; return s.substr(start, len); &#125;&#125;; 008.Description解法一:此题时间复杂度为 $O(n)$ , 重点考察是否考虑的全面, 主要有以下几种情况, 缺一不可: +123 dd // 返回123 +123d // 返回123 d-123 // 返回0 -123+ //返回-123 -123+4 // 返回-123 323123423423423 // 返回INT_MAX -1231238923894234 // 返回INT_MIN 1234-5 // 返回1234 123456789101112131415161718192021222324252627282930class Solution &#123;public: int myAtoi(string str) &#123; int sign =1; bool is_first = true; //记录当前非数字字符是否是第一个非空格字符, 如果是, 返回0 bool has_sign = false; // 记录正负号的出现次数, 出现多于1次的, 返回0 long res = 0; //记录当前的int值, 要出现int范围, 返回对应的INT for(int i =0 ; i&lt;str.size(); i++)&#123; if(str[i] == ' ' &amp;&amp; is_first) continue; // 空格, 且没有出现任何非空格字符(如出现了, 则空格也会跟着变成循环停止的标志) else if( !has_sign &amp;&amp; (str[i] == '+' || str[i] == '-') )&#123; // 判断符号 has_sign = true; is_first = false; sign = str[i]=='+' ? 1:-1; &#125;else if(str[i] &lt;= '9' &amp;&amp; str[i] &gt;= '0')&#123; has_sign = true; is_first = false; res = res*10 + int(str[i] - '0') * sign; // 数字累加, 注意这里使用了sign, 因此无需在后面判断正负, 直接加就可以 if (res &gt; INT_MAX) return INT_MAX; // 超限 else if(res &lt; INT_MIN) return INT_MIN; &#125;else if(is_first)&#123; //首字符为非法字符, 返回0 return 0; &#125;else&#123; break; &#125; &#125; return int(res); &#125;&#125;; 011. Container With Most WaterDescriptionGiven n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. Note: You may not slant the container and n is at least 2. The below vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. 解法一: 暴力时间复杂度: $O(n^2)$ 用max_area标记当前最大容器的取值, 然后两个for循环遍历所有容器的可能取值 1234567891011121314class Solution &#123;public: int maxArea(vector&lt;int&gt;&amp; height) &#123; int max_area = 0; for(int i=0; i&lt;height.size(); i++)&#123; for(int j = i+1; j &lt; height.size(); j++)&#123; if(max_area &lt; min( height[i],height[j] ) * (j-i))&#123; max_area = min( height[i],height[j] ) * (j-i); &#125; &#125; &#125; return max_area; &#125;&#125;; 解法二: 用两个指针时间复杂度: $O(n)$ 分别用两个指针指向数组的第一个元素和最后一个元素, 并计算当前的area, 然后移动指针元素值较小的一方, 移动过程中更新max_area的值 原理: 首先假设容器可以具有最大长度的宽, 也就是分别指向首尾元素, 这时候 , 我们想查看是否还有比当前最大容积更大的容器, 那么, 我们必须维持较高的垂直边不动, 而将较低的垂直边移动, 因为只有这样, 我们才 有可能 (注意不是一定)获得比当前容积更大的容器, 这个时候虽然宽变小了, 但是高度却可能增加(因为新增的边有可能大于当前较低边的高). 如果移动较高的边, 那么新增的边由于受到当前较低边的作用, 只有可能减小容器的面积 123456789101112131415161718class Solution &#123;public: int maxArea(vector&lt;int&gt;&amp; height) &#123; int low = 0, high = height.size()-1; int max_area = 0; while(low&lt;high)&#123; int area = min( height[low], height[high] ) * (high-low); if(max_area &lt; area)&#123; max_area = area; &#125; if(height[low] &lt; height[high]) low++; else high--; &#125; return max_area; &#125;&#125;; 015. 3SumDescription解法一: 固定一个数, 按照two sum的方式来解因为不能包含重复的元祖, 因此不能直接固定然后用two sum 的方式求解, 首先要对数组进行排序, 然后从第一个开始逐个判断, 期间要去除重复的元组 时间复杂度: $O(n^2)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution &#123;public: int partition(vector&lt;int&gt;&amp; nums, int low, int high)&#123; if(nums[low] != nums[(low+high)/2])&#123; // 注意这里用异或交换的陷阱 nums[low] = nums[low] + nums[(low+high)/2]; nums[(low+high)/2] = nums[low] - nums[(low+high)/2]; nums[low] = nums[low] - nums[(low+high)/2]; &#125; // 主要是将中将的数字和首位交换, 个人觉得可有可无, 因为时间复杂度是一样的 int P = nums[low]; while(low &lt; high)&#123; while(low&lt;high &amp;&amp; P&lt;=nums[high]) high--; nums[low] = nums[high]; while(low&lt;high &amp;&amp; P&gt;=nums[low]) low++; nums[high] = nums[low]; &#125; nums[low] = P; return low; &#125; void quickSort(vector&lt;int&gt;&amp; nums, int low, int high)&#123; int mid = partition(nums, low, high); if(low&lt;mid ) quickSort(nums, low, mid-1); if(mid&lt;high) quickSort(nums, mid+1, high); &#125; vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt; &gt; res; if(nums.size()&lt;3) return res; quickSort(nums, 0, nums.size()-1); for(int i =0; i&lt;nums.size(); i++)&#123; if(nums[i]&gt; 0) break; //剪枝, 如果当前数字为正, 那么后面就不可能再有符合条件的三元组, 可以提前退出 if(i&gt;0 &amp;&amp; nums[i] == nums[i-1] ) continue; //去除重复, 遇到除第一个外相同的三元组最小的数字, 则跳过 int low = i+1, high = nums.size()-1; while(low &lt; high)&#123; if(low&gt;i+1 &amp;&amp; nums[low] == nums[low-1])&#123; // 仍然是去除重复, low++; continue; &#125; int sum = nums[low] + nums[i] + nums[high]; if(sum&gt;0) high--; else if(sum&lt;0) low++; else&#123; vector&lt;int&gt; tmp&#123;nums[low], nums[i], nums[high]&#125;; res.push_back(tmp); low++; // 这一点千万别漏了, 要继续判断, 因为以当前数字开始的三元组可能不止一个 &#125; &#125; &#125; return res; &#125;&#125;; 更好的写法:12345678910111213141516171819202122232425vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; result; if(nums.size()&lt;=2)return result; sort(nums.begin(), nums.end()); for (int i = 0; i &lt; nums.size() - 2; i++) &#123; int a = nums[i]; if(a &gt; 0) break; if (i &gt; 0 &amp;&amp; a == nums[i - 1]) continue; for (long j = i + 1, k = nums.size() - 1; j &lt; k;) &#123; int b = nums[j]; int c = nums[k]; int value = a + b + c; if (value == 0) &#123; result.push_back(vector&lt;int&gt;(&#123;a, b, c&#125;)); while (j&lt;k &amp;&amp; b == nums[++j]); // 主要是这里的写法很优雅, 其他地方和上面差不多 while (j &lt; k &amp;&amp;c == nums[--k]); &#125; else if (value &gt; 0) &#123; k--; &#125; else &#123; j++; &#125; &#125; &#125; return result; &#125; 017. Letter Combinations of a Phone NumberDescriptionC++解法一: 递归时间复杂度: $O(n*4^n)$ 空间复杂度: $O(4^n)$ 1234567891011121314151617181920212223242526class Solution &#123;public: void back_tracking(vector&lt;string&gt;&amp; res, const vector&lt;string&gt;&amp; digit_letters, string&amp; tmp,string digits, int index)&#123; if(index == digits.size())&#123; res.push_back(tmp); &#125; else &#123; for(int i=0; i&lt;digit_letters[digits[index]-'0'].size(); i++)&#123; tmp.push_back(digit_letters[digits[index]-'0'][i]); back_tracking(res, digit_letters, tmp, digits, index+1); tmp.pop_back();// 移除当前末尾元素, 以便可以加下一个 &#125; &#125; &#125; vector&lt;string&gt; letterCombinations(string digits) &#123; vector&lt;string&gt; res; if(digits.size() &lt;=0) return res; //res.push_back(""); //对res向量初始化,以便开始, 如果不初始化, 则size为0,后面的循环无法进行 const vector&lt;string&gt; digit_letters&#123;"","","abc","def","ghi","jkl", "mno","pqrs","tuv","wxyz"&#125;; string tmp=""; back_tracking(res, digit_letters, tmp, digits, 0); return res; &#125;&#125;; 解法二: 非递归时间复杂度: $O(n*4^n)$ 空间复杂度: $O(4^n)$ 123456789101112131415161718192021222324class Solution &#123;public: vector&lt;string&gt; letterCombinations(string digits) &#123; vector&lt;string&gt; res; if(digits.size() &lt;=0) return res; res.push_back(""); //对res向量初始化,以便开始, 如果不初始化, 则size为0,后面的循环无法进行 const vector&lt;string&gt; digit_letters&#123;"","","abc","def","ghi","jkl", "mno","pqrs","tuv","wxyz"&#125;; for(int i =0 ;i&lt;digits.size(); i++)&#123; int num = digits[i] - '0'; if(digit_letters[num] == "") continue; vector&lt;string&gt; tmp; // 申请一个临时vector, 用于存放加上当前数字字符的string集合 for(int k = 0; k &lt; digit_letters[num].size(); k++)&#123; for(int l =0; l &lt; res.size(); l++)&#123; tmp.push_back(res[l]+digit_letters[num][k]); &#125; &#125; res.swap(tmp); // 将res于tmp交换, swap仅仅是改变指针, 比'='更快, 因为'='包含了复制 &#125; return res; &#125;&#125;; Python解法一: 利用reduce实现123456789101112131415class Solution: def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ if digits=="": return [] digit_letters = &#123;'0':"", '1':"", '2':"abc", '3':"def", '4':"ghi", '5':"jkl", '6':"mno", '7':"pqrs", '8':"tuv", '9':"wxyz"&#125; from functools import reduce # 在python3中, reduce()函数已经从全局命名空间移除, 现在存在于functools模块中,使用时需要导入 return reduce(lambda res,digit:[x+y for x in res for y in digit_letters[digit]], digits, [""]) 019. Remove Nth Node From End of ListDescriptionGiven a linked list, remove the n-th node from the end of list and return its head. Example: Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2. After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. 解法一: 遍历两次第一次遍历求出链表长度, 第二次遍历在对应位置删除节点 只遍历一次时间复杂度: $O(n)$ 且只遍历一次 空间复杂度: $O(1)$ 维护两个指针, 两指针之间的距离刚好相差n, 当第二个指针到达链表尾部时, 第一个指针刚好指向倒数第n个节点, 直接删除该节点即可. 12345678910111213141516171819202122232425262728293031/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* removeNthFromEnd(ListNode* head, int n) &#123; if(head == nullptr || n &lt;= 0) return head; //链表为空, 或者n&lt;=0时, 直接返回head ListNode* first = head; ListNode* second = head; for(int i = 0; i &lt; n ; i++)&#123; second = second-&gt;next; if(second == nullptr)&#123; //对于 n&gt;=链表长度时的特殊判断和处理 if(i==n-1) return head-&gt;next; else return head; &#125; &#125; while(second-&gt;next!=nullptr)&#123; first = first-&gt;next; second = second-&gt;next; &#125; first-&gt;next = first-&gt;next-&gt;next; return head; &#125;&#125;; 下面是有一种写法, 新申请了一个节点空间, 用于指向head节点, 可以使代码看起来更容易理解, 对边界条件的判断也更加方便 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* removeNthFromEnd(ListNode* head, int n) &#123; if(head == nullptr || n &lt;= 0) return head; //链表为空, 或者n&lt;=0时, 直接返回head ListNode* dummy = new ListNode(0); dummy-&gt;next = head; ListNode* first = dummy; ListNode* second = dummy; for(int i = 0; i &lt; n ; i++)&#123; second = second-&gt;next; if(second == nullptr) return dummy-&gt;next;// n超出了链表的长度 &#125; while(second-&gt;next!=nullptr)&#123; first = first-&gt;next; second = second-&gt;next; &#125; first-&gt;next = first-&gt;next-&gt;next; return dummy-&gt;next; &#125;&#125;; 022. Generate ParenthesesDescription解法一: 暴力先求出所有可能性, 然后验证每一种可能性是否正确 解法二: 递归有关递归的时间空间复杂度分析起来都不太容易, 这里只上答案(//TODO 具体怎么来没搞懂) 时间复杂度: $O(\frac{4^n}{\sqrt n}$ 空间复杂度: $O(\frac{4^n}{\sqrt n}$ 以及 $O(n)$ 的空间来存储组合序列 考虑合法括号组合的规律: 必须首先出现左括号, 然后才能出现右括号, 如果当前的string里面的右括号数量大于左括号数量, 那么就一定会出现)(这种不匹配的情况. 核心思路: 从头开始构建组合, 每次接入一个字符, 接入的字符只有两种可能性, 即左括号或者右括号, 而一旦接入的字符使得当前字符中右括号数量大于左括号, 就会变得不合法组合,其它均为合法. 根据此性质, 进行如下递归: 维护两个变量left_rest, right_rest分别代表 剩余 可以添加的括号的 数量. 采用递归算法, 每次添加一个 ‘(‘ 或者一个 ‘)’, 添加时需要考虑下面几种情况: 为了保证当前string内左括号数量多于右括号数量, left_rest一定要小于right_rest 如果left_rest = right_rest = 0, 则说明此时没有可以添加的括号了, 1234567891011121314151617class Solution &#123;public: vector&lt;string&gt; generateParenthesis(int n) &#123; vector&lt;string&gt; res; helper(res, "", n, n); return res; &#125; void helper(vector&lt;string&gt; &amp;res, string out, int left_rest, int right_rest)&#123; //if(left_rest &gt; right_rest) return; if(left_rest == 0 &amp;&amp; right_rest ==0) res.push_back(out); else&#123; if(left_rest&gt;0) helper(res, out+'(', left_rest-1, right_rest); if(right_rest&gt;0 &amp;&amp; right_rest &gt; left_rest) helper(res, out+')', left_rest, right_rest-1); &#125; &#125;&#125;; 解法三: 用栈来模拟递归首先是最厚的括号包裹状态，即一开始左边是连续的左括号，右边是连续的右括号，然后执行以下逻辑：1、右括号不能比左括号多；2、弹出右括号，直到遇到第一个左括号，如果左括号改成右括号仍然合法，则把它改成右括号；否则，左括号继续弹出；3、改完之后一个劲加左括号，直到所有可以用的左括号都加完为止。4、循环一直执行到不能弹出括号为止, 即直到栈为空。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123;public: vector&lt;string&gt; generateParenthesis(int n) &#123; vector&lt;string&gt; res; string par_s; int left_rest = n, right_rest = n; //注意, 这里如果把'left_rest--' 写在while判断里 //会导致循环结束后的left_rest值为-1, 后面的同理 //while(left_rest--) par_s.push_back('('); while(left_rest)&#123; par_s.push_back('('); left_rest--; &#125; while(right_rest) &#123; par_s.push_back(')'); right_rest--; &#125; res.push_back(par_s); while(par_s.size()&gt;0)&#123; char cur_c = par_s.back(); par_s.pop_back(); if(cur_c == ')')&#123; right_rest++; continue; &#125; left_rest++; if(left_rest &lt; right_rest)&#123; par_s.push_back(')'); right_rest--; while(left_rest)&#123; par_s.push_back('('); left_rest--; &#125; while(right_rest) &#123; par_s.push_back(')'); right_rest--; &#125; res.push_back(par_s); &#125; &#125; return res; &#125;&#125;; 029. Divide Two IntegersDescription解法一: 循环加法时间复杂度: $O(dividend)$ 这种方法很容易时间超限: 当被除数很大(INT_MAX), 除数很小(1), 则需要循环INT_MAX次才能完成计算. 解法二: 左移法时间复杂度: $O(log(dividend))$ 对除数进行左移, 相当于每次乘以2, 直到左移后大于被除数, 用被除数减去左移后的数字, 记录左移对应除数的倍数, 然后再次将从除数开始左移, 直到被除数小于除数. 123456789101112131415161718192021222324class Solution &#123;public: int divide(int dividend, int divisor) &#123; if(divisor==0 || (dividend==INT_MIN&amp;&amp;divisor==-1)) return INT_MAX; int res=0; int sign = ((dividend&lt;0) ^ (divisor&lt;0)) ? -1:1;// 用异或来获取符号 long long did = labs(dividend); // long与int在有些环境中字节中一样, 所以最好用long long long long dis = labs(divisor); while(did &gt;= dis)&#123; long long temp = dis, multiple = 1; while( did &gt;= temp&lt;&lt;1 )&#123; temp = temp&lt;&lt;1; multiple = multiple&lt;&lt;1; &#125; did -= temp; res+= multiple; &#125; if(sign == 1) return res; else return -res; &#125;&#125;; 扩展: 这道题如果不允许使用long 或者long long 怎么解?033. Search in Rotated Sorted ArrayDescriptionSuppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]). You are given a target value to search. If found in the array return its index, otherwise return -1. You may assume no duplicate exists in the array. Your algorithm’s runtime complexity must be in the order of O(log n). Example 1: Input: nums = [4,5,6,7,0,1,2], target = 0Output: 4Example 2: Input: nums = [4,5,6,7,0,1,2], target = 3Output: -1 解法一: 二分查找对于数组[4,5,6,7,0,1,2], 可以将其看成是两段: [4,5,6,7] 和 [0,1,2], 可以看出, 前一段中的任意一个数字都大于后一段中的数字, 于是, 令low=0, high=size()-1, 进行二分查找, 其中 mid 对应的数字要么落在前半段(nums[low] &lt;= nums[mid]), 要么落在后半段. 如果落在的前半段, 则看 target 的值是否在 low与mid之间. 是则 high = mid-1, 否则 low = mid+1 反之, 如果落在后半段, 则看 target 的值是否在 mid 与 high 之间, 是则 low=mid+1 , 否则high = mid-1 1234567891011121314151617181920212223242526class Solution &#123;public: int search(vector&lt;int&gt;&amp; nums, int target) &#123; int low = 0; int high = nums.size()-1; //数组前半段的数字永远大于后半段的数字 while(low&lt;=high)&#123; //当low==high时, mid=low=high, 如果不等于target, 则之后会退出循环 int mid = (low+high)/2; if(target == nums[mid]) return mid; if(nums[low] &lt;= nums[mid])&#123; //说明当前mid落在数组的前半段(), 这里等于号必须带, 否则会漏解 //判断target是否在low与mid之间, 这里low需要带等于号, //因为target有可能=nums[low], mid无需带等于号 if(target &gt;= nums[low] &amp;&amp; target &lt; nums[mid]) high = mid-1; else low = mid+1; &#125;else&#123; // 只有当nums[low]完全小于nums[mid]时, mid才落在后半段 if(target &gt; nums[mid] &amp;&amp; target &lt;= nums[high]) low = mid+1; else high = mid-1; &#125; &#125; return -1; &#125;&#125;; 解法二: 二分查找该方法同样是二分查找, 只不过与上面有一点不同, 对于数组nums=[4,5,6,7,0,1,2]来说, 如果 target &lt; nums[0], 说明 target 位于数组的后半段, 那么可以将数组看做是nums=[INT_MIN,INT_MIN,INT_MIN,INT_MIN,0,1,2] , 这样一来, 就变成了最常规的有序数组, 反之, 如果 target 位于数组的前半段, 那么可以将数组看做是nums=[4,5,6,7,INT_MAX,INT_MAX,INT_MAX]. 注意, 这里并不会改变数组内部的值, 我们只是利用一个临时变量num来代替当前的nums[mid]的值, 然后利用 num 与 target 比较进行二分查找. 1234567891011121314151617181920212223class Solution &#123;public: int search(vector&lt;int&gt;&amp; nums, int target) &#123; int low = 0; int high = nums.size()-1; while(low&lt;=high)&#123; int mid = (low+high)/2; int num; if(target &lt; nums[0])&#123; //target在后半段, 所以将前半段都看做INT_MIN if(nums[mid] &lt; nums[0]) num = nums[mid]; // nums[mid]在后半段 else num = INT_MIN; // nums[mid]在前半段, &#125;else&#123; //target在前半段, 所以将后半段都看作是INT_MAX if(nums[mid] &lt; nums[0]) num = INT_MAX; // nums[mid]在后半段 else num = nums[mid]; // nums[mid]在前半段 &#125; if(num == target) return mid; else if(target &lt; num) high = mid-1; else low = mid+1; &#125; return -1; &#125;&#125;; 034. Find First and Last Position of Element in Sorted ArrayDescriptionGiven an array of integers nums sorted in ascending order, find the starting and ending position of a given target value. Your algorithm’s runtime complexity must be in the order of O(log n). If the target is not found in the array, return [-1, -1]. Example 1: Input: nums = [5,7,7,8,8,10], target = 8Output: [3,4]Example 2: Input: nums = [5,7,7,8,8,10], target = 6Output: [-1,-1] 解法一:时间复杂度: $O(logn)$空间复杂度: $O(1)$ 先用常规的二分查找找到target, 然后分别用二分查找找到最左边的target和最右边的target下标. 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; int low = 0; int high = nums.size() - 1; vector&lt;int&gt; res&#123;-1,-1&#125;; int mid=-1; while(low &lt;= high)&#123; //正常的二分查找, 先找到target mid = (low+high)/2; if(nums[mid] == target) break; else if(nums[mid] &lt; target) low = mid+1; else high = mid-1; &#125; if(mid==-1 || nums[mid] != target) return res; // 数组为空或者数组内没有target //以mid为中心, 分别查找下标最小的target和下标最大的target int llow=low, lhigh=mid; // 左边的二分查找low,high初始化 int rlow=mid, rhigh=high; // 右边的二分查找low,high初始化 while(llow&lt;=lhigh)&#123; int mid = (llow+lhigh)/2; if(nums[mid] == target)&#123; if(mid==llow || nums[mid-1] != target)&#123; //关键: 只有当等于target并且左边没有元素或者左边元素不等于target时, 当前mid才是最左边的target res[0] = mid; break; &#125;else lhigh = mid-1; &#125;else if(nums[mid] &lt; target) llow = mid+1; else lhigh = mid-1; &#125; while(rlow&lt;=rhigh)&#123; int mid = (rlow+rhigh)/2; if(nums[mid] == target)&#123; if(mid==rhigh || nums[mid+1] != target)&#123; //同理, 找最右边的target res[1] = mid; break; &#125;else rlow = mid+1; &#125;else if(nums[mid] &lt; target) rlow = mid+1; else rhigh = mid-1; &#125; return res; &#125;&#125;; 解法二: 二分查找同样是二分查找, 更加精炼, 先找到最左边的target, 然后以最左边为low, 开始找最右边的target, 需要注意的是不能在nums[mid] == target时就退出循环. 1234567891011121314151617181920212223class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; int low = 0; int high = nums.size()-1; vector&lt;int&gt; res&#123;-1, -1&#125;; while(low &lt; high)&#123; int mid = (low+high)/2; //偏向左边, 很重要, 否则会死循环 if(nums[mid] &lt; target) low = mid+1; else high = mid; //注意, 这里不是mid-1, 因为现在是在找最左边的target, 故不能在=target时退出, 因此也不能直接令high=mid-1, 否则会丢失mid=target的情况 &#125; if(nums.size()==0 || nums[low] != target) return res; res[0]=low; high = nums.size()-1; while(low &lt; high)&#123; int mid = (low+high+1)/2; //使mid偏向右边, 这很重要 if(nums[mid] &gt; target) high = mid-1; else low = mid; &#125; res[1]=high; return res; &#125;&#125;; 036 Valid Sudoku验证一个矩阵是否是数独数据 (注意, 验证和) DescriptionDetermine if a 9x9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules: Each row must contain the digits 1-9 without repetition.Each column must contain the digits 1-9 without repetition.Each of the 9 3x3 sub-boxes of the grid must contain the digits 1-9 without repetition. 解法一: 利用flag数组存储判断矩阵时间复杂度: $O(9^2)$ 空间复杂度: $O(3*9^2)$ 虽然要申请三个二维数组, 但都是常数级. 用三个 9×9 大小的矩阵, 分别储存每一行上, 每一列上, 每一个子块上1-9数字是否出现.12345678910111213141516171819class Solution &#123;public: bool isValidSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; // 下面三个矩阵分别存储了 行上1-9是否出现, 列上1-9是否出现, sub-box上1-9是否出现的bool值 // 如果row_flag[1][3] 为真, 则说明第1行(从第0行算起)上已经具有数字4(数字比下标大1)了 bool row_flag[9][9] &#123;0&#125;, col_flag[9][9] &#123;0&#125;, sub_flag[9][9] &#123;0&#125;; for(int i = 0 ; i&lt;board.size(); i++)&#123; for(int j = 0; j&lt;board[i].size(); j++)&#123; if(board[i][j] == '.') continue; // 如果为 '.' 则可以直接跳过此次判断 int num = board[i][j] - '0' - 1; //这里-1主要是为了能够直接将num作为下标使用 int k = i/3*3 + j/3; if(row_flag[i][num] || col_flag[j][num] || sub_flag[k][num]) return false; row_flag[i][num]=col_flag[j][num]=sub_flag[k][num]=true; &#125; &#125; return true; &#125;&#125;; 046 Permutations全排列, 注意是distict的数字, 故而不需要进行重复检查 DescriptionGiven a collection of distinct integers, return all possible permutations. Example: Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 解法一: 递归时间复杂度: $O(A^n_n)$ , 每一种情况都是 $O(1)$ , 共有 $O(A^n_n)$ 种情况. (对吗?) 用一个变量pos指向nums的第一个位置, 然后将pos与后面所有位置上的数字交换(包括自己), 最终会得到n种可能性, 这n种可能性就是出现在第一位置上的所有可能字符的情况集合, 然后将第一位固定, 并将pos指向下一位, 此时问题转换成了n-1个字符的全排列, 按照这种想法一致递归下去, 就可以找到所有位置上的所有组合情况(用pos==nums.size()判断) 123456789101112131415161718192021class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(nums.size()==0) return res; permute_helper(res, 0, nums); return res; &#125; void permute_helper(vector&lt;vector&lt;int&gt; &gt; &amp;res, int pos, vector&lt;int&gt; &amp;nums)&#123; if(pos == nums.size()) res.push_back(nums); // 当pos走到最后时, 说明一种情况诞生, 将其添加到res中 else&#123; for(int i = pos; i&lt;nums.size(); i++)&#123; std::swap(nums[pos], nums[i]); permute_helper(res, pos+1, nums); std::swap(nums[pos], nums[i]); // 能够去掉这句话的前提是对res内的字符串进行重复检查, 具体可看牛客分析 //在面对含有重复字符的情况时, 最好加上这句话 &#125; &#125; &#125;&#125;; 解法二: 迭代时间复杂度: $O(n^3)$空间复杂度: $O(A_n^n)$ 全排列的size 对于n个数的全排列问题, 可以想象成已经获得了n-1个数的全排列, 然后将第n个数插入到n-1个数的n个空位上( 如将3插入到12的空位上分别为: 312,132,123). 123456789101112131415161718class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; permute(vector&lt;int&gt; &amp;num) &#123; vector&lt;vector&lt;int&gt;&gt; res(1,vector&lt;int&gt;()); for(int i=0; i&lt;num.size(); i++)&#123; vector&lt;vector&lt;int&gt;&gt; tmp_res(std::move(res)); // move之后, res内部会自动被清空, 而且move的效率较高 for(int j=0; j&lt;tmp_res.size(); j++)&#123; for(int k=0; k&lt;=tmp_res[0].size(); k++)&#123; // 注意这里是&lt;=, 因为还要往尾部插 vector&lt;int&gt; tmp(tmp_res[j]); tmp.insert(tmp.begin()+k, num[i]); res.push_back(tmp); &#125; &#125; &#125; return res; &#125;&#125;; 解法三: 利用C++的内置函数123456789101112class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; permute(vector&lt;int&gt; &amp;num) &#123; vector&lt;vector&lt;int&gt; &gt; ans; sort(num.begin(), num.end()); ans.push_back(num); while(std::next_permutation(num.begin(), num.end())) ans.push_back(num); return ans; &#125;&#125;; 048. Rotate Image将二维矩阵顺时针旋转90度. DescriptionYou are given an n x n 2D matrix representing an image. Rotate the image by 90 degrees (clockwise). Note: You have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation. Example 1: Given input matrix =[ [1,2,3], [4,5,6], [7,8,9]], rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]]Example 2: Given input matrix =[ [ 5, 1, 9,11], [ 2, 4, 8,10], [13, 3, 6, 7], [15,14,12,16]], rotate the input matrix in-place such that it becomes:[ [15,13, 2, 5], [14, 3, 4, 1], [12, 6, 8, 9], [16, 7,10,11]] 解法一: 逆置+转置对行向量使用逆置, 然后对整个矩阵转置 clockwise rotatefirst reverse up to down, then swap the symmetry1 2 3 7 8 9 7 4 14 5 6 =&gt; 4 5 6 =&gt; 8 5 27 8 9 1 2 3 9 6 3 12345678910class Solution &#123;public: void rotate(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; std::reverse(matrix.begin(), matrix.end()); //逆置 for(int i = 0; i&lt;matrix.size(); i++)&#123; for(int j=i+1; j&lt;matrix[i].size();j++) // 转置, 注意j=i+1 std::swap(matrix[i][j], matrix[j][i]); &#125; &#125;&#125;; 解法二: 转置+列向量逆置先求转置, 再对行向量逆置: 1234567891011class Solution &#123;public: void rotate(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; for(int i = 0; i&lt;matrix.size(); i++)&#123; for(int j=i+1; j&lt;matrix[i].size();j++) std::swap(matrix[i][j], matrix[j][i]); &#125; for(auto &amp;vec_i : matrix) std::reverse(vec_i.begin(), vec_i.end()); &#125;&#125;; 补充: 逆时针旋转90度对列向量使用逆置(reverse), 然后对矩阵使用转置 1234567void anti_rotate(vector&lt;vector&lt;int&gt;&gt; &amp;matrix)&#123; for(auto &amp;vec_i:matrix) std::swap(vec_i.begin(), vec_i.end()); for(int i = 0; i&lt;matrix.size(); i++)&#123; for(int j = i+1; i&lt;matrix[i].size(); j++) std::swap(matrix[i][j], matrix[j][i]); &#125;&#125; 049. Group Anagrams找到具有相同字符的各个字符串, 并按字符分组输出 DescriptionGiven an array of strings, group anagrams together. Example: Input: [“eat”, “tea”, “tan”, “ate”, “nat”, “bat”],Output:[ [“ate”,”eat”,”tea”], [“nat”,”tan”], [“bat”]]Note: All inputs will be in lowercase.The order of your output does not matter. 解法一: 哈希表+sort用哈希表来存, 键为有序的字符序列, 值为string数组, 里面存着各个与有序字符序列包含字符相同的其他序列 时间复杂度: $O(nmlogm)$ , 其中, n为输入字符串数组的长度, m为每个字符串的长度, 对于n个字符串, 要进行n次哈希表的插入, 同时每次插入时, 需要对字符串进行排序, 排序复杂度为 $O(mlogm)$. 空间复杂度: $O(n)$ 123456789101112131415class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; std::unordered_map&lt;string,vector&lt;string&gt;&gt; res_map; for(auto str: strs)&#123; string str_value = str; std::sort(str.begin(), str.end()); res_map[str].push_back(str_value); //key 为字母有序string, value为含有这些字母的序列 &#125; vector&lt;vector&lt;string&gt;&gt; res_vec; for(auto str : res_map) res_vec.push_back(str.second); //将map中的所有的string转移到vec返回结果中 return res_vec; &#125;&#125;; 解法二: 哈希表(不使用sort)时间复杂度: $O(nm)$ , 其中, n为string个数, m为每个string的字母数. 由于上面的解法二需要使用排序, 故而时间上不够优化, 因此, 这里我们可以设计新的键来代替sort, 基本思想是对26个字母, 分别赋予一个素数值, 然后, 计算键的时候, 将对应字母的素数相乘即可, 这样一来, 每一种字符串的key都是唯一的(因为最终的乘积可以唯一的表示成素数相乘的序列). 空间复杂度: $O(n)$ 123456789101112131415161718192021class Solution &#123;public: int primer[26] = &#123;2, 3, 5, 7, 11 ,13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101&#125;; int get_sum_id(string str)&#123; int sum = 1; for(auto c : str)&#123; sum * = primer[(int)(c-'a')]; &#125; return sum; &#125; vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; std::unordered_map&lt;int,vector&lt;string&gt;&gt; res_map; for(auto str: strs)&#123; res_map[get_sum_id(str)].push_back(str); //key 为字母有序string, value为含有这些字母的序列 &#125; vector&lt;vector&lt;string&gt;&gt; res_vec; for(auto str : res_map) res_vec.push_back(str.second); //将map中的所有的string转移到vec返回结果中 return res_vec; &#125;&#125;; 050. Pow(x, n)实现幂乘操作 DescriptinImplement pow(x, n), which calculates x raised to the power n (x^n). Example 1: Input: 2.00000, 10Output: 1024.00000Example 2: Input: 2.10000, 3Output: 9.26100Example 3: Input: 2.00000, -2Output: 0.25000Explanation: 2-2 = 1/22 = 1/4 = 0.25Note: -100.0 &lt; x &lt; 100.0n is a 32-bit signed integer, within the range $[−2^{31}, 2^{31} − 1]$ 解法一: 递归当n为偶数时: $x^n = x^{n/2} \times x^{n/2}$当n为奇数时: $x^n = x\times x^{n/2} \times x^{n/2}$ 12345678910class Solution &#123;public: double myPow(double x, int n) &#123; if(n==0) return 1.0; unsigned int un = abs(n); //注意这里必须是 unsigned类型, 就算是long , 也必须带unsigned, 主要是因为abs(INT_MIN)仍为负数INT_MIN! if(n &lt; 0) x = 1/x; return (un%2==0) ? myPow(x*x, un/2) : x*myPow(x*x, un/2); &#125;&#125;; 解法二: 非递归n要么为偶数, 要么为奇数, 就算为奇数, 也可以拆分成 $x\times x^{n-1}$ 的形式, 对于偶数n, 可以写成 $x^{n/2} \times x{n/2}$ 的形式, 对于 $x^{n/2}$, 可以继续按奇数偶数进行拆分. 举例来说, 对于x=2, n=10 , 可以写成 $2^{10} = 2^{5} \times 2^{5}$ 对于 $2^5$ , 可以写成, $2 \times 2^2 \times 2^2$, 可以看出, x每次与自身相乘后, n的次数就会变成原来二分之一, 这样, 可以用循环实现幂乘的操作, 如下所示. 123456789101112131415161718class Solution &#123;public: double myPow(double x, int n) &#123; if(n==0) return 1.0; unsigned int un = abs(n); //注意这里必须是 unsigned类型, 就算是long , 也必须带unsigned, 主要是因为abs(INT_MIN)仍为负数INT_MIN! if(n &lt; 0) x = 1/x; double res =1.0; while(un&gt;0)&#123; if(un%2 == 1)&#123; res * = x; &#125; x * =x; un /= 2; &#125; return res; &#125;&#125;; 054. Spiral Matrix以顺时针螺旋顺序返回矩阵元素, 顺时针打印矩阵 DescriptionGiven a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order. Example 1: Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5]Example 2: Input:[ [1, 2, 3, 4], [5, 6, 7, 8], [9,10,11,12]]Output: [1,2,3,4,8,12,11,10,9,5,6,7] 解法: 按层次输出(由外而内)时间复杂度: $O(n)$空间复杂度: $O(n)$ 输出形式如下(按层次编码, 以4×6的矩阵为例), 需要注意边界控制条件: \begin{matrix} 1_{top}&1_{top}&1_{top}&1_{top}&1_{top}&1_{top} \\ 1_{left}&2_{top}&2_{top}&2_{top}&2_{top}&1_{right} \\ 1_{left}&2_{bottom}&2_{bottom}&2_{bottom}&2_{bottom}&1_{right} \\ 1_{bottom}&1_{bottom}&1_{bottom}&1_{bottom}&1_{bottom}&1_{bottom} \end{matrix}12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;int&gt; spiralOrder(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; res; if(matrix.size()==0 || matrix[0].size() ==0) return res; int row_layer = (matrix.size()+1)/2; int col_layer = (matrix[0].size()+1)/2; int layer = min( row_layer, col_layer); // 计算总共的层数 int cur_layer =0; // 用于记录当前所处哪一层 int len_row = matrix.size(); int len_col = matrix[0].size(); //分别为行和列的size while(cur_layer &lt; layer)&#123; //top 输出上边 for(int j =cur_layer; j&lt;len_col-cur_layer; j++) res.push_back(matrix[cur_layer][j]); //right 输出右边 for(int i = cur_layer+1; i&lt;len_row-1-cur_layer; i++) res.push_back(matrix[i][len_col - 1 - cur_layer]); //bottom 输出下边, 这里注意为了防止重复输出, 需要确保上边和下边的行数不同,即: // cur_layer!=len_row-1-cur_layer for(int j= len_col - 1 -cur_layer; cur_layer!=len_row-1-cur_layer &amp;&amp; j &gt;=cur_layer ;j--) res.push_back(matrix[len_row - 1 -cur_layer][j]); //left 输出左边, 同样, 要确保左边和右边的列数不同, 即: cur_layer!=len_col-1-cur_layer for(int i = len_row-2-cur_layer; cur_layer!=len_col-1-cur_layer &amp;&amp; i&gt;cur_layer; i--) res.push_back(matrix[i][cur_layer]); cur_layer++; &#125; return res; &#125;&#125;; 055. Jump Game数组的数字为最大的跳跃步数, 根据数组判断是否能跳到最后一位上 DescriptionGiven an array of non-negative integers, you are initially positioned at the first index of the array. Each element in the array represents your maximum jump length at that position. Determine if you are able to reach the last index. Example 1: Input: [2,3,1,1,4]Output: trueExplanation: Jump 1 step from index 0 to 1, then 3 steps to the last index.Example 2: Input: [3,2,1,0,4]Output: falseExplanation: You will always arrive at index 3 no matter what. Its maximum jump length is 0, which makes it impossible to reach the last index. 解法一: 回溯时间复杂度: $O(2^n)$ 总共有 $2^n$ 种跳法来跳到最后一个位置上(对于任意一个位置, 有经过和不经过两个种可能性)空间复杂度: $O(n)$ 试遍所有的可能性, 正常来说会超时, 并且也肯定不是最佳答案 123456789101112131415161718class Solution &#123;public: bool canJump(vector&lt;int&gt;&amp; nums) &#123; return helper(nums, 0); &#125; bool helper(vector&lt;int&gt; &amp;nums, int position)&#123; int final_position = nums.size()-1; if(position == final_position) return true; int furthest = std::min(position+nums[position], final_position); for(int i = position+1; i&lt;=furthest; i++)&#123; //这里有个小小的优化, 就是令i从最大步长开始, i--, 这种优化虽然最坏情况时一样的 //但在实际使用中, 会比从position+1开始要快一点(但是依然超时) if(helper(nums, i)) return true; &#125; return false; &#125;&#125;; 解法二: top-down 动态规划(递归)时间复杂度: $O(n^2)$ , 对于每个点来说, 都是要找到下一个good_position, 则需要进行 $(O)$ 的查找, 又因为总共有 $O(n)$个元素, 所以复杂度为 $O(n^2)$.空间复杂度: $O(2n)$, 递归需要 $O(n)$ , memo需要 $O(n)$. 设计一个数组, 用来记录当前下标对应位置是否可又达到终点, 如果能, 则该位置为good position, 如果不能, 则为bad position, 刚开始的时候都是unknown position(除了最后一个位置为good). 123456789101112131415161718192021222324class Solution &#123;public: enum class Status&#123;GOOD, BAD, UNKNOWN&#125;; bool canJump(vector&lt;int&gt;&amp; nums) &#123; vector&lt;Status&gt; memo; for(int i=0; i&lt;nums.size()-1; i++) memo.push_back(Status::UNKNOWN); memo.push_back(Status::GOOD); return helper(nums, memo, 0); &#125; bool helper(vector&lt;int&gt; &amp;nums, vector&lt;Status&gt; &amp;memo, int position)&#123; int final_position = nums.size()-1; if(memo[position] != Status::UNKNOWN) return memo[position]==Status::GOOD ? true : false; int furthest = std::min(position+nums[position], final_position); for(int i = furthest; i&gt;position; i--)&#123; if(helper(nums, memo, i))&#123; memo[position] = Status::GOOD; //注意是position, 不是i return true; &#125; &#125; memo[position] = Status::BAD; return false; &#125;&#125;; 解法三: down-top 动态规划(非递归)时间复杂度: $O(n^2)$ , 对于每个点来说, 都是要找到下一个good_position, 则需要进行 $(O)$ 的查找, 又因为总共有 $O(n)$个元素, 所以复杂度为 $O(n^2)$.空间复杂度: $O(n)$, 无需递归 , 只需要memo, $O(n)$. 动态规划的非递归版本. 1234567891011121314151617181920212223class Solution &#123;public: enum class Status&#123;GOOD, BAD, UNKNOWN&#125;; bool canJump(vector&lt;int&gt;&amp; nums) &#123; //if(nums.size() ==0) return false; vector&lt;Status&gt; memo; for(int i=0; i&lt;nums.size()-1; i++) memo.push_back(Status::UNKNOWN); memo.push_back(Status::GOOD); int final_position = nums.size()-1; for(int i=nums.size()-2; i&gt;=0; i--)&#123; int furthest = std::min(i+nums[i], final_position); //for(int j = i+1; j&lt;=furthest; j++)&#123; for(int j = furthest; j&gt;i;j--)&#123; if(memo[j] == Status::GOOD)&#123; // 只要有一个GOOD, 当前i位置就为GOOD, 而无需考虑BAD的情况 memo[i] = memo[j]; break; &#125; &#125; &#125; return memo[0] == Status::GOOD ? true : false; &#125;&#125;; 解法四: 贪心时间复杂度: $O(n)$空间复杂度: $O(1)$ 由上面的down-top递归可以看出, 当前下标位置的点是否为good点, 实际上只取决于当前点是否能够达到右边坐标中(从右往左走)最左边的good(可以看上面的break语句), 如果能够达到, 则当前点一定为good点, 因此, 我们只需要用一个变量left_most_good来维护当前点右边的最左good点下标即可, 无需任何其他空间和操作.(速度极快) 123456789101112class Solution &#123;public: bool canJump(vector&lt;int&gt;&amp; nums) &#123; int left_most_good = nums.size()-1; for(int i = nums.size()-2; i&gt;=0; i--)&#123; if(i+nums[i] &gt;= left_most_good)&#123; left_most_good = i; &#125; &#125; return left_most_good==0; &#125;&#125;; 另一种贪心的形式: 记录当前能够达到的最大位置 123456789class Solution &#123;public: bool canJump(vector&lt;int&gt;&amp; nums) &#123; int i =0; for(int reach=0; i&lt;nums.size() &amp;&amp; i&lt;=reach; i++ ) reach = max(i+nums[i], reach); return i==nums.size(); &#125;&#125;; 056. Merge Intervals融合区间 DescriptionGiven a collection of intervals, merge all overlapping intervals. Example 1: Input: [[1,3],[2,6],[8,10],[15,18]]Output: [[1,6],[8,10],[15,18]]Explanation: Since intervals [1,3] and [2,6] overlaps, merge them into [1,6].Example 2: Input: [[1,4],[4,5]]Output: [[1,5]]Explanation: Intervals [1,4] and [4,5] are considerred overlapping. 解法一: sort+O(n)时间复杂度: $O(nlogn)$, 主要是排序空间复杂度: $O(n)$ 最简单的实现方法, 先按照interval.start用sort排序, 排好序以后, 能够融合的interval都会聚到一起, 这个时候, 因为start是呈递增的, 只需要看end的大小关系就可以. 最简单的实现方法就是sort之后, 通过额外申请空间来存储融合后的interval, 最后返回 1234567891011121314class Solution &#123;public: vector&lt;Interval&gt; merge(vector&lt;Interval&gt;&amp; intervals) &#123; if(intervals.size()==0) return vector&lt;Interval&gt;&#123;&#125;; vector&lt;Interval&gt; res; std::sort(intervals.begin(), intervals.end(), [](Interval a, Interval b)&#123;return a.start &lt; b.start;&#125;); res.push_back(intervals[0]); for(auto iv : intervals)&#123; if(res.back().end &lt; iv.start) res.push_back(iv); else res.back().end = std::max(res.back().end, iv.end); &#125; return res; &#125;&#125;; 解法二: sort+O(1)时间复杂度: $O(nlogn)$ , 主要是排序空间复杂度: $O(1)$ 上面的方法在逻辑上不够好, 因为既然已经申请了额外的内存来存储放回结果, 说明我们不希望改变原vector内部的数据, 但是sort之后, 数据顺序已经被破坏了, 既然已经破坏了, 那最好就是直接使用原地融合的办法, 来减少内存的开销1234567891011121314151617181920class Solution &#123;public: vector&lt;Interval&gt; merge(vector&lt;Interval&gt;&amp; intervals) &#123; if(intervals.size()==0) return vector&lt;Interval&gt;&#123;&#125;; //vector&lt;Interval&gt; res; 既然决定使用sort, 就说明已经改变了intervals, 此时不应该在额外申请空间, 而应该进行原地融合. std::sort(intervals.begin(), intervals.end(), [](Interval a, Interval b)&#123;return a.start &lt; b.start;&#125;); auto cur_iv = intervals.begin(); auto next_iv = intervals.begin()+1; for(; next_iv!=intervals.end(); next_iv++)&#123; if( (*cur_iv).end &lt; (*next_iv).start )&#123; cur_iv++; (*cur_iv) = (*next_iv); &#125;else&#123; (*cur_iv).end = std::max( (*cur_iv).end, (*next_iv).end ); &#125; &#125; intervals.erase(cur_iv+1, intervals.end()); return intervals; &#125;&#125;; 解法三: 不使用sort有时, 我们要求不能改变原向量intervals的内容, 此时, 就不能使用sort (除非牺牲大量空间留副本,但单肯定不推荐). //TODO, 未细看, 但时间复杂度应该会高于 O(nlogn)https://leetcode.com/problems/merge-intervals/discuss/153979/Elegant-c++-solutions.-One-without-modifying-intervals-and-one-inplace123456789101112131415161718192021222324252627Without modifying intervalsSince we can't sort interval, we want to instead ensure our destination vector is sorted. A insertion sort is required then. Insertion should be done as follows;Find first destination interval that ends after the incoming interval starts. Called itIf no such interval is found or the incoming interval end is less than found intervals start then we can just insert and be done.Otherwise there must be an overlap, but it could be more than one. Do another search, this time for the first interval whose start is greater than incoming interval end. Called lastEverything from [it, last) can be merged together with incoming interval into a single interval vector&lt;Interval&gt; merge(vector&lt;Interval&gt;&amp; intervals) &#123; std::vector&lt;Interval&gt; ret; for (auto&amp; interval : intervals) &#123; auto it = std::lower_bound(ret.begin(), ret.end(), interval.start, [](const Interval&amp; l, int r) &#123; return l.end &lt; r; &#125;); if (it == ret.end() || interval.end &lt; it-&gt;start) // No overlap, insert as is ret.insert(it, interval); else &#123; // There is an overlap, there might be more, so find the upper bound too it-&gt;start = std::min(it-&gt;start, interval.start); auto last = std::upper_bound(it, ret.end(), interval.end, [](int l, const Interval&amp; r) &#123; return l &lt; r.start; &#125;); it-&gt;end = std::max((last - 1)-&gt;end, interval.end); ret.erase(it + 1, last); &#125; &#125; return ret; &#125; 062. Unique PathsDescriptionA robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below). The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below). How many possible unique paths are there? 解法一: DP时间复杂度: $O(mn)$空间复杂度: $O(mn)$ 这是一道经典的DP问题, 当机器人处于某一点时, 它只能从上面或者左边到达该点, 因此很容易得出path[i][j] = path[i-1][j] + path[i][j-1];, 其中 path[i][j]指到达 $(i,j)$ 点的可能路径数量. 123456789101112class Solution &#123;public: int uniquePaths(int m, int n) &#123; vector&lt;vector&lt;int&gt;&gt; path(m, vector&lt;int&gt;(n,1)); for(int i = 1 ;i&lt;m; i++)&#123; for(int j=1 ; j&lt;n; j++)&#123; path[i][j] = path[i-1][j] + path[i][j-1]; &#125; &#125; return path[m-1][n-1]; &#125;&#125;; 解法二: 优化的DP时间复杂度: $O(mn)$空间复杂度: $O(n)$ 通过分析指导, 当前点的可能路径数量只与上面点和左边点的值有关, 在上面的方法中, 我们用一个 $m\times n$ 的数组来存储当前点上面和左边的值, 实际上, 我们只需要用一行数组就可以完成这个功能, 首先, 求出第一行的所有点的值, 这里只会用每个点左边的值, 然后, 对于第二行的第一个点来说, 它只会用到上面的值, 也就是第一行的第一个值, 因此可以通过行数组直接得到, 然后, 对于第二行的第二个值, 它可以从第二行的第一个值, 以及第二行的第二个值得到, 这些值都是已知的, 所以可以直接求的, 由于在求得以后, 我们就再也不需要第一行的第二个值了, 所以我们可以用这个存储空间来存储第二行的第二个值, 如此递归执行, 我们只需要 $O(n)$ 的空间即可. 123456789101112class Solution &#123;public: int uniquePaths(int m, int n) &#123; vector&lt;int&gt; path(n,1); for(int i = 1; i&lt;m; i++)&#123; for(int j = 1; j&lt;n; j++)&#123; path[j] = path[j] + path[j-1]; &#125; &#125; return path[n-1]; &#125;&#125;; 解法三: 排列组合(最优)时间复杂度: $O(n)$空间复杂度: $O(1)$ 实际上, 仔细分析该问题, 可以把该问题看成是一个典型的排列组合问题. 首先, 将机器人向右走记为 1, 将机器人向下走记为 0. 题目问有多少种不同的走法, 实际上就是在问1/0序列的不同排列有多少种, 并且, 1/0 的长度必须为 $(m -1 + n - 1)$. 因此, 这个问题可以看做是从 $(m-1+n-1)$ 个空槽位上选择 $(m-1)$ 个槽位, 将其置为1, 并将剩余的 $n-1$ 个槽位置为0, 故而就是组合问题: $C_{m-1+n-1}^{m-1}$ . 又因为 $C_{m-1+n-1}^{m-1} = C_{m-1+n-1}^{n-1}$ , 所以为了防止溢出, 我们可以选择小的进行计算 12345678910class Solution &#123;public: int uniquePaths(int m, int n) &#123; long res = 1; //需要注意的是, 由于下面的计算操作是会有先乘一个数, 再初以一个数的操作, 因此很有可能乘完后超过int上限, 所以需要声明为long整型 for(int i = 1; i&lt; std::min(m,n); i++)&#123; res = res * (m-1+n-1 - i+1) / i; &#125; return res; &#125;&#125;; 073. Set Matrix ZeroesDescriptionGiven a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in-place. Example 1: Input:[ [1,1,1], [1,0,1], [1,1,1]]Output:[ [1,0,1], [0,0,0], [1,0,1]]Example 2: Input:[ [0,1,2,0], [3,4,5,2], [1,3,1,5]]Output:[ [0,0,0,0], [0,4,5,0], [0,3,1,0]]Follow up: A straight forward solution using O(mn) space is probably a bad idea.A simple improvement uses O(m + n) space, but still not the best solution.Could you devise a constant space solution? 解法一: 穷举时间复杂度: $O(nm)$空间复杂度: $O(nm)$ 记录所有出现0的位置, 然后根据这些位置坐标将对应的行和列上的值置为0. 12345678910111213141516171819202122232425class Solution &#123;public: void setZeroes(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; rows; vector&lt;int&gt; cols; for(int i=0; i&lt;matrix.size(); i++)&#123; for(int j=0; j&lt;matrix[0].size(); j++)&#123; if(matrix[i][j] == 0)&#123; rows.push_back(i); cols.push_back(j); &#125; &#125; &#125; for(auto i:rows)&#123; for(int j=0; j&lt;matrix[0].size(); j++)&#123; matrix[i][j] = 0; &#125; &#125; for(auto j:cols)&#123; for(int i=0; i&lt;matrix.size(); i++)&#123; matrix[i][j] = 0; &#125; &#125; &#125;&#125;; 解法二: 穷举(减少空间复杂度)时间复杂度: $O(nm)$空间复杂度: $O(n+m)$ 上面在记录位置坐标时没有进行重复检查, 实际上, 对于已经记录过的行或列, 可以不用再记录, 此时, 空间复杂度可以降为 $O(m+n)$. 1234567891011121314151617181920212223242526class Solution &#123;public: void setZeroes(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; rows; vector&lt;int&gt; cols; for(int i=0; i&lt;matrix.size(); i++)&#123; for(int j=0; j&lt;matrix[0].size(); j++)&#123; if(matrix[i][j] == 0)&#123; // 记录行或列坐标之前先进行重复检查 if(std::count(rows.begin(), rows.end(), i)==0) rows.push_back(i); if(std::count(cols.begin(), cols.end(), j)==0) cols.push_back(j); &#125; &#125; &#125; for(auto i:rows)&#123; for(int j=0; j&lt;matrix[0].size(); j++)&#123; matrix[i][j] = 0; &#125; &#125; for(auto j:cols)&#123; for(int i=0; i&lt;matrix.size(); i++)&#123; matrix[i][j] = 0; &#125; &#125; &#125;&#125;; 解法三: 穷举(空间复杂度 $O(1)$ )时间复杂度: $O(nm\times (m+n))$空间复杂度: $O(1)$ 遍历矩阵时, 如果遇到 $(i,j)$ 上的值为0, 那么就将对应的行和列上的所有非0值全部置为一个矩阵范围外的值NAN(解答里面用的是-100000, 实际上这种解法存在问题, 因为理论上矩阵中的元素可以是表示范围内的任何值). 之后将所有的NAN值置为0, 就可以完成置0任务, 并且没有使用额外的空间. 由于每次找到一个0时, 都要遍历这个位置上的行和列, 因此时间复杂度较高 解法四: 用第一行和第一列记录时间复杂度: $O(nm)$空间复杂度: $O(1)$ 用第一行和第一列的值记录是否应该将对应的行和列置为0, 此时由于第一行和第一列被用作了标记数组, 因此第一行和第一列的0不能用来判断是否应该置为全0, 所以需要额外设置两个变量记录.1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: void setZeroes(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; bool is_row=false, is_col = false; // 用第一行和第一列的值来做标记, 因此需要额外的记录第一行和第一列本身是有应该全0 for(int i=0; i&lt;matrix.size(); i++)&#123; for(int j=0; j&lt;matrix[0].size(); j++)&#123; if(matrix[i][j] == 0)&#123; if(i==0) is_row=true; if(j==0) is_col=true; matrix[i][0] = 0; matrix[0][j] = 0; &#125; &#125; &#125; for(int i=1; i&lt;matrix.size(); i++)&#123; if(matrix[i][0]!=0) continue; for(int j=0; j&lt;matrix[0].size(); j++)&#123; matrix[i][j] = 0; &#125; &#125; for(int j=1; j&lt;matrix[0].size(); j++)&#123; if(matrix[0][j]!=0) continue; for(int i=0; i&lt;matrix.size(); i++)&#123; matrix[i][j] = 0; &#125; &#125; if(is_row)&#123; //需要特别判断第一行和第一列是否应该置为0 for(int j=0; j &lt;matrix[0].size();j++) matrix[0][j]=0; &#125; if(is_col)&#123; for(int i=0; i&lt; matrix.size(); i++) matrix[i][0]=0; &#125; &#125;&#125;; 075. Sort Colors对0,1,2 (颜色: RGB) 进行排序 DescriptionHere, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Note: You are not suppose to use the library’s sort function for this problem. Example: Input: [2,0,2,1,1,0]Output: [0,0,1,1,2,2]Follow up: A rather straight forward solution is a two-pass algorithm using counting sort.First, iterate the array counting number of 0’s, 1’s, and 2’s, then overwrite array with total number of 0’s, then 1’s and followed by 2’s.Could you come up with a one-pass algorithm using only constant space? 解法一: 两次遍历时间复杂度: $O(n)$空间复杂度: $O(1)$ 第一次遍历统计0,1,2的个数, 第二次遍历根据0,1,2的个数覆盖数组原有值 解法二: 一次遍历时间复杂度: 大于 $O(n)$空间复杂度: $O(1)$ 设置mid, low, high三个指示变量, 如果mid==0, 则将其与low交换, 如果mid==2, 则将其与high交换, 直到mid&gt;high为止. 1234567891011121314class Solution &#123;public: void sortColors(vector&lt;int&gt;&amp; nums) &#123; int low=0, mid=0, high=nums.size()-1; while(mid&lt;=high)&#123; if(nums[mid]==2) std::swap(nums[mid], nums[high--]); else if(nums[mid]==0) std::swap(nums[mid++], nums[low++]); else mid++; &#125; &#125;&#125;; 078. Subsets返回给定数字序列的子集, 序列中每个元素都不同(这是一个很重要的条件!!) DescriptionGiven a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: Input: nums = [1,2,3]Output:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] 解法一: 迭代直接求出子集时间复杂度: $O(2^n)$ , 对于任意一个元素, 有包含和不包含两种情况空间复杂度: $O(2^n)$ 由于序列中的每个元素都不同, 因此, 对于任意一个元素, 只需要将其添加都前面序列所组成的子集的每一个子序列的末尾即可, 无需考虑是否包含重复元素的情况. 123456789101112131415161718class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res &#123;vector&lt;int&gt;&#123;&#125;&#125;; for(auto n : nums)&#123; int len = res.size(); for(int i=0; i&lt;len; i++)&#123; vector&lt;int&gt; sub_item = res[i]; // c++中, =为复制赋值, move函数为移动赋值 sub_item.push_back(n); res.push_back(sub_item); &#125; &#125; return res; &#125;&#125;; 解法二: 回溯https://leetcode.com/problems/subsets/discuss/27281/A-general-approach-to-backtracking-questions-in-Java-(Subsets-Permutations-Combination-Sum-Palindrome-Partitioning)回溯法可以解决一系列相关问题, 先看Subsets的求解 123456789101112131415161718class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; sub_item; back_track(res, sub_item, 0, nums); return res; &#125; void back_track(vector&lt;vector&lt;int&gt;&gt; &amp;res, vector&lt;int&gt; sub_item, int start, vector&lt;int&gt; &amp;nums)&#123; res.push_back(sub_item); for(int i=start; i&lt;nums.size(); i++)&#123; sub_item.push_back(nums[i]); back_track(res, sub_item, i+1, nums); sub_item.pop_back(); &#125; &#125;&#125;; 其他问题: Subsets II (contains duplicates) : https://leetcode.com/problems/subsets-ii/悠悠 11:05:53Permutations : https://leetcode.com/problems/permutations/悠悠 11:06:01Permutations II (contains duplicates) : https://leetcode.com/problems/permutations-ii/悠悠 11:06:09Combination Sum : https://leetcode.com/problems/combination-sum/悠悠 11:06:16Combination Sum II (can’t reuse same element) : https://leetcode.com/problems/combination-sum-ii/悠悠 11:06:23Palindrome Partitioning : https://leetcode.com/problems/palindrome-partitioning/ 解法三: bit控制时间复杂度: $O(n\times 2^n)$ , 最慢的方法.空间复杂度: $O(2^n)$因为对于任意一个数只有两种可能性, 出现在子序列中, 或者不出现在子序列中, 因此对于长度为 n 的(无相同元素的)序列来说, 共有 $2^n$ 个子序列, 我们先为这些子序列申请空间, 然后根据位操作(刚好有0,1两种情况)来决定对应位置上的字符出现还是不出现. 在实现时, 观察到, 第一个元素每隔两个子序列出现一次, 第二个元素每隔四个子序列出现两次, 第三个元素每隔八个子序列出现四次… 依次类推, 我们可以根据当前元素的位置来决定当前元素是否出现(间隔的前一半出现, 后一半不出现) 123456789101112131415class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; int len_subsets = std::pow(2,nums.size()); vector&lt;vector&lt;int&gt;&gt; res(len_subsets, vector&lt;int&gt;&#123;&#125;); for(int i =0; i&lt;nums.size(); i++)&#123; for(int j=0; j&lt;len_subsets; j++)&#123; if(j&gt;&gt;i &amp; 1 == 1)&#123; res[j].push_back(nums[i]); &#125; &#125; &#125; return res; &#125;&#125;; 079. Word Search判断指定单词是否存在于字符数组中(可以通过上下左右邻接字符相连的才算是一个单词) DescriptionGiven a 2D board and a word, find if the word exists in the grid. The word can be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once. Example: board =[ [‘A’,’B’,’C’,’E’], [‘S’,’F’,’C’,’S’], [‘A’,’D’,’E’,’E’]] Given word = “ABCCED”, return true.Given word = “SEE”, return true.Given word = “ABCB”, return false. 解法一: dfs+回溯时间复杂度: 暴力空间复杂度: $O(1)$ , 回溯时, 用#来记录已经遍历过的点, 无需申请额外空间来记录 123456789101112131415161718192021222324252627class Solution &#123;public: bool exist(vector&lt;vector&lt;char&gt;&gt;&amp; board, string word) &#123; if(board.size()==0 || board[0].size()==0) return false; for(int i=0; i&lt;board.size(); i++)&#123; for(int j=0; j&lt;board[0].size(); j++)&#123; if(dfs(board, word, 0, i, j)) return true; &#125; &#125; return false; &#125; bool dfs(vector&lt;vector&lt;char&gt;&gt; &amp;board, string word, int start, int x, int y)&#123; char cur_c = board[x][y]; if(cur_c != word[start]) return false; if(start == word.size()-1) return true; board[x][y]='#'; bool res=false, b_down=false, b_left=false, b_right=false; if(x&gt;0) res = dfs(board, word, start+1, x-1, y); if(!res &amp;&amp; x&lt;board.size()-1) res = dfs(board, word, start+1, x+1, y); if(!res &amp;&amp; y&gt;0) res = dfs(board, word, start+1, x, y-1); if(!res &amp;&amp; y&lt;board[0].size()-1) res = dfs(board, word, start+1, x, y+1); board[x][y]=cur_c; return res; &#125;&#125;; 091. Decode WaysDescriptionA message containing letters from A-Z is being encoded to numbers using the following mapping: ‘A’ -&gt; 1‘B’ -&gt; 2…‘Z’ -&gt; 26Given a non-empty string containing only digits, determine the total number of ways to decode it. Example 1: Input: “12”Output: 2Explanation: It could be decoded as “AB” (1 2) or “L” (12).Example 2: Input: “226”Output: 3Explanation: It could be decoded as “BZ” (2 26), “VF” (22 6), or “BBF” (2 2 6). 解法一(最优): DP constant space时间复杂度: $O(n)$空间复杂度: $O(1)$ 存在问题: 下面的程序在面对测例:230001或230时, 输出的不是0. 但是仍然能通过OJ, 需要注意. 1234567891011121314151617class Solution &#123;public: int numDecodings(string s) &#123; if(s.size()==0 || s.front()=="0") return 0; // 注意, 不能用s.front() == "0" int f1=1, f2=1; for(int i=1; i&lt;s.size(); i++)&#123; if(s[i]=='0') f1=0; //注意, 不能用s[i] == "0" if(s[i-1]=='1' || (s[i-1]=='2' &amp;&amp; s[i]&lt;='6'))&#123; f1 = f1+f2; // 令f1为前i-1字符的可能组合+前i-2字符的可能组合 f2 = f1-f2; // 令f2为前i-1字符的可能组合, 也就是对于下一个i来说的前i-2的可能组合 &#125; else f2 = f1; // 如果当前字符不能与前一个字符组合, 则当前字符f1不变, 而f2有变为下一个i的前i-2的可能组合, 即让新f2等于旧的f1 &#125; return f1; &#125;&#125;; 解法二: 递归时间复杂度: $O(n^2)$ 1234567891011121314151617class Solution &#123;public: int numDecodings(string s) &#123; if(s.size()==0) return 0; return recurve(0,s); &#125; int recurve(int pos, string &amp;s)&#123; if(pos==s.size()) return 1; if(s[pos]=='0') return 0; int tmp_res = recurve(pos+1, s); if(pos&lt;s.size()-1 &amp;&amp; (s[pos]=='1' || (s[pos]=='2'&amp;&amp;s[pos+1]&lt;='6'))) tmp_res += recurve(pos+2, s); return tmp_res; &#125;&#125;; 094. Binary Tree Inorder Traversal中序遍历二叉树 DescriptionGiven a binary tree, return the inorder traversal of its nodes’ values. Example: Input: [1,null,2,3] 1 \ 2 / 3 Output: [1,3,2]Follow up: Recursive solution is trivial, could you do it iteratively? 解法一: 递归/** Definition for a binary tree node. struct TreeNode { int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) {} };/class Solution {public: vector inorderTraversal(TreeNode root) { vector&lt;int&gt; res; if(root==nullptr) return res; inorder(root, res); return res; } void inorder(TreeNode* root, vector &amp;res){ if(root-&gt;left!=nullptr) inorder(root-&gt;left, res); res.push_back(root-&gt;val); if(root-&gt;right!=nullptr) inorder(root-&gt;right, res); }}; 解法二: 非递归标准的中序非递归遍历算法 1234567891011121314151617181920class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; if(root==nullptr) return res; std::stack&lt;TreeNode*&gt; s_tree; while(!s_tree.empty() || root!=nullptr)&#123; while(root!=nullptr)&#123; s_tree.push(root); root= root-&gt;left; &#125; if(!s_tree.empty())&#123; root = s_tree.top(); s_tree.pop(); res.push_back(root-&gt;val); root = root-&gt;right; &#125; &#125; return res; &#125;&#125;; 098. Validate Binary Search TreeDescriptionGiven a binary tree, determine if it is a valid binary search tree (BST). Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key.The right subtree of a node contains only nodes with keys greater than the node’s key.Both the left and right subtrees must also be binary search trees.Example 1: Input: 2 / \ 1 3Output: trueExample 2: 5 / \ 1 4 / \ 3 6Output: falseExplanation: The input is: [5,1,4,null,null,3,6]. The root node’s value is 5 but its right child’s value is 4. 解法一: 递归用一个指针来指向当前节点在顺序上的前一个节点, 判断是否为BST 123456789101112131415class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; TreeNode* pre_node = nullptr; return isBST(root, pre_node); &#125; bool isBST(TreeNode* root, TreeNode * &amp;pre_node)&#123; // 注意!!! 要维持递归时的pred_node, 因此必须使用 * &amp;, 否则每次的pre_node = root;实际上只是改变了pred_node的副本 if(root==nullptr) return true; if(isBST(root-&gt;left, pre_node) == false) return false; if(pre_node!=nullptr &amp;&amp; pre_node-&gt;val &gt;= root-&gt;val) return false; pre_node = root; if(isBST(root-&gt;right, pre_node)==false) return false; return true; &#125;&#125;; 典型错误解法: 只考虑了左子树根节点值要小于当前节点值, 没有考虑应该是左子树所有的节点都应该小于当前节点的值. 1234567891011121314151617181920212223242526/*Input[10,5,15,null,null,6,20]OutputtrueExpectedfalse*/class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; if(root==nullptr) return true; bool b=true; if(root-&gt;left!=nullptr)&#123; if(root-&gt;left-&gt;val &gt;= root-&gt;val) return false; b = isValidBST(root-&gt;left); &#125; if(b==false) return b; if(root-&gt;right!=nullptr)&#123; if(root-&gt;right-&gt;val &lt;= root-&gt;val) return false; b = isValidBST(root-&gt;right); &#125; return b; &#125;&#125;; 102. Binary Tree Level Order Traversal按层次输出二叉树节点的值(每层的值要分开) Description解法一: 层次遍历时间复杂度: $O(n)$ , 每个节点遍历一次空间复杂度: $O(n)$ , 存储了n个节点的值 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(root == nullptr) return res; std::queue&lt;TreeNode*&gt; q; TreeNode * cur_node; q.push(root); while(!q.empty())&#123; int len = q.size(); vector&lt;int&gt; layer; for(int i=0; i&lt;len; i++)&#123; cur_node = q.front(); q.pop(); layer.push_back(cur_node-&gt;val); if(cur_node-&gt;left != nullptr) q.push(cur_node-&gt;left); if(cur_node-&gt;right != nullptr) q.push(cur_node-&gt;right); &#125; res.push_back(layer); &#125; return res; &#125;&#125;; 103. Binary Tree Zigzag Level Order Traversal按之字形打印二叉树 DescriptionGiven a binary tree, return the zigzag level order traversal of its nodes’ values. (ie, from left to right, then right to left for the next level and alternate between). For example:Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7return its zigzag level order traversal as:[ [3], [20,9], [15,7]] 解法一：利用reverse时间复杂度为 $O(n^2)$ 空间复杂度为 $O(n)$ 然后每次访问节点时，都判断当前节点的层数，如果为奇数层，则将该层直接push back到结果向量中，如果为偶数，则将该层数据进行reverse后再push back到结果向量中。通过while里面内置for循环，来保证每次for循环都会将一整层的节点放进队列中，无需额外的数组来存储depth信息1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(pRoot == NULL) return res; queue&lt;TreeNode*&gt; que; que.push(pRoot); bool even = false; while(!que.empty())&#123; vector&lt;int&gt; vec; //将vec声明在内部，省去每次的clear操作，clear操作需要对vector进行遍历，并将每个元素置为null？ const int size = que.size(); //当前存的节点数目就是这一层所有的节点，之前层的到已经被取出, 并且这一层的子节点还没有开始入队列 for(int i=0; i&lt;size; ++i)&#123; //将该层所有节点的子节点入队列，同时当到达该层最后一个节点时终止 TreeNode* tmp = que.front(); que.pop(); vec.push_back(tmp-&gt;val); if(tmp-&gt;left != NULL) que.push(tmp-&gt;left); if(tmp-&gt;right != NULL) que.push(tmp-&gt;right); &#125; if(even) //根据奇偶标识判断是否需要reverse std::reverse(vec.begin(), vec.end()); res.push_back(vec); even = !even; &#125; return res; &#125;&#125;; 解法二: 最优(不用reverse)时间复杂度: $O(n)$空间复杂度: $O(n)$ 在解法二中, 复杂度高的原因是因每次遇到偶数层的时候都要进行 reverse, 实际上, 当我们知道了该层的节点个数以后, 我们可以直接开辟一个指定大小的 vector, 然后根据下标随机访问来填入该层的节点值, 这样一来就不用进行 reverse, 并且空间复杂度与解法二相同 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(root == nullptr) return res; std::queue&lt;TreeNode*&gt; q; q.push(root); bool is_odd = true; TreeNode* cur_node; while(!q.empty())&#123; int layer_len = q.size(); vector&lt;int&gt; cur_layer(layer_len); for(int i=0; i&lt;layer_len; i++)&#123; cur_node = q.front(); q.pop(); if(is_odd==true) cur_layer[i] = cur_node-&gt;val; else cur_layer[layer_len-1-i ] = cur_node-&gt;val; if(cur_node-&gt;left!=nullptr) q.push(cur_node-&gt;left); if(cur_node-&gt;right!=nullptr) q.push(cur_node-&gt;right); &#125; res.push_back(cur_layer); is_odd = !is_odd; &#125; return res; &#125;&#125;; 116. Populating Next Right Pointers in Each Node令每个节点中的 next 指针指向它的右兄弟, 如果没有右兄弟, 那么就置为 nullptr, 注意, 题目给定的树是满二叉树 DescriptionGiven a binary tree struct TreeLinkNode { TreeLinkNode left; TreeLinkNode right; TreeLinkNode * next;}Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Initially, all next pointers are set to NULL. Note: You may only use constant extra space.Recursive approach is fine, implicit stack space does not count as extra space for this problem.You may assume that it is a perfect binary tree (ie, all leaves are at the same level, and every parent has two children).Example: Given the following perfect binary tree, 1 / \ 2 3 / \ / \4 5 6 7After calling your function, the tree should look like: 1 -&gt; NULL / \ 2 -&gt; 3 -&gt; NULL / \ / \4-&gt;5-&gt;6-&gt;7 -&gt; NULL 解法一: 层次遍历时间复杂度: $O(n)$空间复杂度: $O(n)$ 显而易见可以用层次遍历, 只需额外设置一个节点指针来维护当前节点的前一个节点(左兄弟节点). 但是, 题目中要求只能使用 $O(n)$ 的空间, 因此该解法不是最优解. 123456789101112131415161718192021222324252627282930/** * Definition for binary tree with next pointer. * struct TreeLinkNode &#123; * int val; * TreeLinkNode *left, *right, *next; * TreeLinkNode(int x) : val(x), left(NULL), right(NULL), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: void connect(TreeLinkNode * root) &#123; if(root == nullptr) return; std::queue&lt;TreeLinkNode*&gt; q; q.push(root); TreeLinkNode * pre, * cur; while(!q.empty())&#123; int layer_len = q.size(); pre = nullptr; for(int i=0; i&lt;layer_len; i++)&#123; cur = q.front(); q.pop(); if(pre!=nullptr)&#123; pre-&gt;next = cur; &#125; pre = cur; if(cur-&gt;left!=nullptr) q.push(cur-&gt;left); if(cur-&gt;right!=nullptr) q.push(cur-&gt;right); &#125; &#125; &#125;&#125;; 解法二: 利用 next 指针的特性时间复杂度: $O(n)$, 每个节点都要访问一次(仅访问一次)空间复杂度: $O(1)$ 由于是满二叉树, 因此我们可以轻易的利用next指针自身的特性来实现层次遍历. 123456789101112131415161718class Solution &#123;public: void connect(TreeLinkNode *root) &#123; if(root==nullptr) return; TreeLinkNode* cur_first = root; while(cur_first-&gt;left!=nullptr)&#123; TreeLinkNode* cur_node = cur_first; while(cur_node != nullptr)&#123; cur_node-&gt;left-&gt;next = cur_node-&gt;right; if(cur_node-&gt;next!=nullptr) cur_node-&gt;right-&gt;next = cur_node-&gt;next-&gt;left; cur_node = cur_node-&gt;next; &#125; cur_first = cur_first-&gt;left; &#125; &#125;&#125;; 127. Word Ladder实际上是图的BFS(广度优先搜索) DescriptionGiven two words (beginWord and endWord), and a dictionary’s word list, find the length of shortest transformation sequence from beginWord to endWord, such that: Only one letter can be changed at a time.Each transformed word must exist in the word list. Note that beginWord is not a transformed word.Note: Return 0 if there is no such transformation sequence.All words have the same length.All words contain only lowercase alphabetic characters.You may assume no duplicates in the word list.You may assume beginWord and endWord are non-empty and are not the same.Example 1: Input:beginWord = “hit”,endWord = “cog”,wordList = [“hot”,”dot”,”dog”,”lot”,”log”,”cog”] Output: 5 Explanation: As one shortest transformation is “hit” -&gt; “hot” -&gt; “dot” -&gt; “dog” -&gt; “cog”,return its length 5.Example 2: Input:beginWord = “hit”endWord = “cog”wordList = [“hot”,”dot”,”dog”,”lot”,”log”] Output: 0 Explanation: The endWord “cog” is not in wordList, therefore no possible transformation. 解法一: BFS时间复杂度: $O(nl)$, 其中, l 为单词的长度, 因为广度优先遍历会对每个节点遍历一次, 而每个节点计算邻居时, 需要对 l 个字母进行替换(替换26种, 常数级别), 另外, unordered_set 的 find 复杂度也为常数.空间复杂度: $O(n)$ 需要额外借助队列进行广度优先遍历, 另外还使用了 unordered_set 来存储单词表 我们可以将此题看做是图的广度优先搜索, 首先, 以 beginWord 为图的起始节点, 然后, 那些所有与 beginWord 只有一个字母不相同的单词都可以看做是 beginWord 的邻居节点, 依次类推, 直到找到一个单词, 与 endWord 相同为止, 此时, 返回当前 endWord 与 beginWord 的距离. (距离的记录方式和二叉树层次遍历时的方式差不多, 都是利用当前队列中的元素大小来控制深度的). 需要注意的地方有以下几点: 这里的图和树不太一样, 这里图没有链表指针来指示, 因此, 在每次将某一个单词入队列以后, 都需要在单词列表中删除掉这个单词(或者额外设置标记也行), 以防止重复搜索 题目给的是没有重复单词的单词表, 因此推荐使用 set 结构来进行删除 (erase) 操作, vector 结构的删除 (erase) 操作的时间复杂度较高. 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: int ladderLength(string beginWord, string endWord, vector&lt;string&gt;&amp; wordList) &#123; std::unordered_set&lt;string&gt; word_dict; for(auto word : wordList) word_dict.insert(word); std::queue&lt;string&gt; to_visit; word_dict.erase(beginWord); to_visit.push(beginWord); int dist = 1; while(!to_visit.empty())&#123; int len = to_visit.size(); for(int i =0; i&lt;len; i++)&#123; string word = to_visit.front(); to_visit.pop(); if(word == endWord) return dist; add_next_word(word, word_dict, to_visit); &#125; dist++; &#125; return 0; &#125; void add_next_word(string &amp;word, std::unordered_set&lt;string&gt; &amp;word_dict, std::queue&lt;string&gt; &amp;to_visit)&#123; // word_dict.erase(word); for(int i=0; i&lt;word.size(); i++)&#123; char letter = word[i]; for(int k=0; k&lt;26; k++)&#123; word[i] = 'a'+k; if(word_dict.find(word) != word_dict.end())&#123; to_visit.push(word); word_dict.erase(word); &#125; &#125; word[i] = letter; &#125; &#125;&#125;; 130. Surrounded Regions类似于围棋, 将被包裹住(4连通)的字符 O 全部转换成字符 X. DescriptioinGiven a 2D board containing ‘X’ and ‘O’ (the letter O), capture all regions surrounded by ‘X’. A region is captured by flipping all ‘O’s into ‘X’s in that surrounded region. Example: X X X XX O O XX X O XX O X XAfter running your function, the board should be: X X X XX X X XX X X XX O X XExplanation: Surrounded regions shouldn’t be on the border, which means that any ‘O’ on the border of the board are not flipped to ‘X’. Any ‘O’ that is not on the border and it is not connected to an ‘O’ on the border will be flipped to ‘X’. Two cells are connected if they are adjacent cells connected horizontally or vertically. 解法一: 递归时间复杂度: $O(n)$, n 为 board 中的元素个数空间复杂度: $O(n)$, 递归深度优先遍历的递归次数最坏情况下为 n 次. 根据题目的要求, 我们可以从 board 的四个边界开始, 每遇到一次 O 就执行深度优先遍历, 将其相邻的所有 O 都变成另一个字符(如 #). 然后, 在顺序遍历整个 board, 将 board 中所有的 O 变成 X, 将所有的 # 变成 O, 即得解. 123456789101112131415161718192021222324252627282930class Solution &#123;public: void solve(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; if(board.size()==0 || board[0].size()==0) return; for(int i=0, j=0; j&lt;board[i].size(); j++) //上边界 if(board[i][j]=='O') dfs_helper(i,j,board); for(int i=1, j=board[i].size()-1; i&lt;board.size()-1; i++) //右边界 if(board[i][j]=='O') dfs_helper(i,j,board); for(int i=board.size()-1, j=0; j&lt;board[i].size(); j++) //下边界 if(board[i][j]=='O') dfs_helper(i,j,board); for(int i=1, j=0; i&lt;board.size()-1; i++) //左边界 if(board[i][j]=='O') dfs_helper(i,j,board); for(int i=0; i&lt;board.size(); i++)&#123; for(int j=0; j&lt;board[i].size(); j++)&#123; if(board[i][j]=='O') board[i][j]='X'; else if(board[i][j]=='#') board[i][j]='O'; &#125; &#125; &#125; void dfs_helper(int i, int j, vector&lt;vector&lt;char&gt;&gt; &amp;board)&#123; board[i][j]='#'; if(i&gt;0 &amp;&amp; board[i-1][j]=='O') dfs_helper(i-1, j, board); if(j&gt;0 &amp;&amp; board[i][j-1]=='O') dfs_helper(i, j-1, board); if(i&lt;board.size()-1 &amp;&amp; board[i+1][j]=='O') dfs_helper(i+1, j, board); if(j&lt;board[i].size()-1 &amp;&amp; board[i][j+1]=='O') dfs_helper(i, j+1, board); //注意是 j&lt;board[i].size()-1, 不是 board.size()-1 &#125;&#125;; 解法二: 迭代时间复杂度: $O(n)$, n 为 board 中的元素个数空间复杂度: $O(n)$, 额外申请队列的大小为 n 思想和解法一相同, 不过采用 BFS 迭代实现, 利用一个队列来实现 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: void solve(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; if(board.size()==0 || board[0].size()==0) return; for(int i=0, j=0; j&lt;board[i].size(); j++) //上边界 if(board[i][j]=='O') bfs_helper(i,j,board); for(int i=1, j=board[i].size()-1; i&lt;board.size()-1; i++) //右边界 if(board[i][j]=='O') bfs_helper(i,j,board); for(int i=board.size()-1, j=0; j&lt;board[i].size(); j++) //下边界 if(board[i][j]=='O') bfs_helper(i,j,board); for(int i=1, j=0; i&lt;board.size()-1; i++) //左边界 if(board[i][j]=='O') bfs_helper(i,j,board); for(int i=0; i&lt;board.size(); i++)&#123; for(int j=0; j&lt;board[i].size(); j++)&#123; if(board[i][j]=='O') board[i][j]='X'; else if(board[i][j]=='#') board[i][j]='O'; &#125; &#125; &#125; void bfs_helper(int i, int j, vector&lt;vector&lt;char&gt;&gt; &amp;board)&#123; std::queue&lt;int&gt; bfs_q; int len = board[i].size(); bfs_q.push(i*len +j); board[i][j]='#'; while(!bfs_q.empty())&#123; i = bfs_q.front()/len; j = bfs_q.front()%len; bfs_q.pop(); if(i&gt;0 &amp;&amp; board[i-1][j]=='O')&#123; board[i-1][j]='#';bfs_q.push( (i-1)*len+j); &#125; //注意这里一定要更改了字符以后再存入队列, 负责可能引起字符重复入队列, 最终内存超限 if(j&gt;0 &amp;&amp; board[i][j-1]=='O') &#123; board[i][j-1]='#'; bfs_q.push( i*len+j-1); &#125; if(i&lt;board.size()-1 &amp;&amp; board[i+1][j]=='O') &#123; board[i+1][j]='#'; bfs_q.push( (i+1)*len + j );&#125; if(j&lt;board[i].size()-1 &amp;&amp; board[i][j+1]=='O') &#123; board[i][j+1]='#'; bfs_q.push( i*len + j+1); &#125; &#125; &#125;&#125;; 131. Palindrome Partitioning划分回文子串 Description解法一: 回溯+验证回文子串时间复杂度: $O(n\times 2^n)$, 其中, 可能的 partition 情况最多有 $2^n$ 种, 而对于每一种都要进行复杂度为 $O(n)$ 的回文子串检查空间复杂度: $O(n\times 2^n)$ ? 数组 res 的大小最坏情况下可达 $(n\times 2^n)$. 12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; partition(string s) &#123; vector&lt;vector&lt;string&gt;&gt; res; vector&lt;string&gt; part_res; dfs(s, 0, part_res, res); return res; &#125; void dfs(string s, int start, vector&lt;string&gt; &amp;part_res, vector&lt;vector&lt;string&gt;&gt; &amp;res)&#123; if(start == s.size())&#123; res.push_back(part_res); &#125; for(int i=start; i&lt;s.size(); i++)&#123; if(is_palin(start, i, s))&#123; part_res.push_back(s.substr(start, i-start+1)); dfs(s, i+1, part_res, res); part_res.pop_back(); &#125; &#125; &#125; bool is_palin(int start, int end, string s)&#123; while(start &lt; end)&#123; if(s[start]!=s[end]) return false; start++;end--; &#125; return true; &#125;&#125;; 解法二: 回溯+DP时间复杂度: $O(2^n)$, 利用 DP 建立一个 $n\times n$ 的 bool 数组, 其中 dp[i][j] 代表字符串从第 i 个字符开始, 到第 j 个字符组成的子串是否为回文串. 因此, 检查回文串时无需执行 $O(n)$ 的检查.空间复杂度: $O(n\times 2^n + n^2)$, 需要额外的数组空间来实现 DP. 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; partition(string s) &#123; vector&lt;vector&lt;string&gt;&gt; res; vector&lt;string&gt; part_res; vector&lt;vector&lt;bool&gt;&gt; dp(s.size(), vector&lt;bool&gt;(s.size(), false)); for(int j=0; j&lt;s.size(); j++)&#123; for(int i=0; i&lt;=j; i++)&#123; // 注意这两个for循环的顺序和控制条件, dp算法一定要保证在计算当前元素时, 之前的元素已经计算完成并且存入到了数组当中, 否则建立出的dp数组会出现漏解 if(s[i]==s[j] &amp;&amp; (j-i&lt;=2 || dp[i+1][j-1]==true)) dp[i][j]=true; &#125; &#125; dfs(s, 0, part_res, res, dp); return res; &#125; void dfs(string s, int start, vector&lt;string&gt; &amp;part_res, vector&lt;vector&lt;string&gt;&gt; &amp;res, vector&lt;vector&lt;bool&gt;&gt; &amp;dp )&#123; if(start == s.size())&#123; res.push_back(part_res); &#125; for(int i=start; i&lt;s.size(); i++)&#123; if(dp[start][i]==true)&#123; part_res.push_back(s.substr(start, i-start+1)); dfs(s, i+1, part_res, res, dp); part_res.pop_back(); &#125; &#125; &#125; bool is_palin(int start, int end, string s)&#123; while(start &lt; end)&#123; if(s[start]!=s[end]) return false; start++;end--; &#125; return true; &#125;&#125;; 134. Gas Station加油站问题, 根据油量和消耗量判断是否能走完一圈 DescriptionThere are N gas stations along a circular route, where the amount of gas at station i is gas[i]. You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations. Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Note: If there exists a solution, it is guaranteed to be unique.Both input arrays are non-empty and have the same length.Each element in the input arrays is a non-negative integer.Example 1: Input:gas = [1,2,3,4,5]cost = [3,4,5,1,2] Output: 3 Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index.Example 2: Input:gas = [2,3,4]cost = [3,4,3] Output: -1 Explanation:You can’t start at station 0 or 1, as there is not enough gas to travel to the next station.Let’s start at station 2 and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 0. Your tank = 4 - 3 + 2 = 3Travel to station 1. Your tank = 3 - 3 + 3 = 3You cannot travel back to station 2, as it requires 4 unit of gas but you only have 3.Therefore, you can’t travel around the circuit once no matter where you start. 解法: 最优时间复杂度: $O(n)$空间复杂度: $O(1)$ 首先要知道, 如果总油量大于总消耗量, 那么就一定存在一个起始点, 使得可以走完全程. 因此, 设置两个变量 total_left 和 cur_left, 前者存储从0点开始的总的剩余量, 后者存储从起点 start 开始的剩余量. 当 cur_left&lt;=0 时, 说明从 start 开始一直到当前位置之间的任何一个加油站都不能够成为起点, 因此将 start 置为下一个位置, 重新开始, 并令 cur_left=0. 在遍历完所有加油站以后, 如果总的剩余量不小于0, 则此时 start 所指的位置就一定是解.(由题意知, 该解是唯一解). 1234567891011121314151617class Solution &#123;public: int canCompleteCircuit(vector&lt;int&gt;&amp; gas, vector&lt;int&gt;&amp; cost) &#123; int total_left = 0; int cur_left =0; int start=0; for(int i=0; i&lt;gas.size(); i++)&#123; total_left += gas[i]-cost[i]; cur_left += gas[i]-cost[i]; if(cur_left&lt;0)&#123; start = i+1; cur_left=0; &#125; &#125; return total_left &lt; 0 ? -1:start; &#125;&#125;; 138. Copy List with Random Pointer复杂链表的复制, 复制带有随机指针的链表 DescriptionA linked list is given such that each node contains an additional random pointer which could point to any node in the list or null. Return a deep copy of the list. 解法一: 复制+拆分时间复杂度: $O(n)$, 遍历三次链表空间复杂度: $O(1)$, 不包括复制链表占用的空间 先将每个节点复制到对应节点的后面, 然后给随机指针进行赋值 123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for singly-linked list with a random pointer. * struct RandomListNode &#123; * int label; * RandomListNode *next, *random; * RandomListNode(int x) : label(x), next(NULL), random(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: RandomListNode *copyRandomList(RandomListNode *head) &#123; if(head==nullptr) return nullptr; RandomListNode *cur_node = head; RandomListNode *copy_node; while(cur_node!=nullptr)&#123; copy_node = new RandomListNode(cur_node-&gt;label); copy_node-&gt;next = cur_node-&gt;next; cur_node-&gt;next = copy_node; cur_node = cur_node-&gt;next-&gt;next; &#125; cur_node = head; while(cur_node != nullptr)&#123; copy_node = cur_node-&gt;next; if(cur_node-&gt;random!=nullptr) copy_node-&gt;random = cur_node-&gt;random-&gt;next; cur_node = cur_node-&gt;next-&gt;next; &#125; cur_node = head; copy_node = head-&gt;next; RandomListNode *copy_head = copy_node; while(cur_node!=nullptr)&#123; cur_node-&gt;next = copy_node-&gt;next; cur_node = cur_node-&gt;next; if(cur_node==nullptr) break; // 一定不要忘了尾部的空指针判断, 不要想空指针进行赋值操作. copy_node-&gt;next = cur_node-&gt;next; copy_node = copy_node-&gt;next; &#125; return copy_head; &#125;&#125;; 解法二: 一次遍历时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(n)$, 需要申请链表长度的哈希表 利用一个哈希表来存储已经访问过的节点, 哈希表的键值为: {cur_node, copy_node}, 其中, cur_node 代表旧链表中的节点, copy_node 代表新链表中的节点. 顺序遍历旧链表, 对于旧链表中的每一个节点, 查看其 next 节点是否存在于哈希表 visit 中, 如果存在, 则将 copy_node 的 next 指针指向该节点(键)对应的复制节点(值). 对于 random 指针也是同理 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: RandomListNode *copyRandomList(RandomListNode *head) &#123; if(head==nullptr) return nullptr; RandomListNode *cur_node = head; RandomListNode *copy_node = new RandomListNode(head-&gt;label); unordered_map&lt;RandomListNode *, RandomListNode *&gt; visit; // key: old_node, value: copy_node visit.insert(&#123;cur_node, copy_node&#125;); //注意不要少了花括号 while(cur_node!=nullptr)&#123; RandomListNode *next_node=nullptr; if(cur_node-&gt;next==nullptr) copy_node-&gt;next = nullptr; else if(visit.find(cur_node-&gt;next)==visit.end())&#123; next_node = new RandomListNode(cur_node-&gt;next-&gt;label); copy_node-&gt;next = next_node; visit.insert(&#123;cur_node-&gt;next, next_node&#125;); &#125;else copy_node-&gt;next = visit[cur_node-&gt;next]; RandomListNode *random_node=nullptr; if(cur_node-&gt;random==nullptr) copy_node-&gt;random = nullptr; else if(visit.find(cur_node-&gt;random) == visit.end())&#123; random_node = new RandomListNode(cur_node-&gt;random-&gt;label); copy_node-&gt;random = random_node; visit.insert(&#123;cur_node-&gt;random, random_node&#125;); &#125;else copy_node-&gt;random = visit[cur_node-&gt;random]; cur_node = cur_node-&gt;next; copy_node = copy_node-&gt;next; &#125; return visit[head]; &#125;&#125;; 解法三: 递归时间复杂度: $O(n)$空间复杂度: $O(n)$, 除了哈希表所占空间外, 递归还需额外空间, 但是可以近似看做是 $O(n)$ 123456789101112131415class Solution &#123; unordered_map&lt;RandomListNode *, RandomListNode *&gt; visit;public: RandomListNode *copyRandomList(RandomListNode *head) &#123; if(head==nullptr) return nullptr; if(visit.find(head)!=visit.end()) return visit[head]; RandomListNode *node = new RandomListNode(head-&gt;label); visit.insert(&#123;head, node&#125;); node-&gt;next = copyRandomList(head-&gt;next); node-&gt;random = copyRandomList(head-&gt;random); return node; &#125;&#125;; 139. Word Break判断字符串是否可以划分成字典里面的单词 DescriptionGiven a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words. Note: The same word in the dictionary may be reused multiple times in the segmentation.You may assume the dictionary does not contain duplicate words.Example 1: Input: s = “leetcode”, wordDict = [“leet”, “code”]Output: trueExplanation: Return true because “leetcode” can be segmented as “leet code”.Example 2: Input: s = “applepenapple”, wordDict = [“apple”, “pen”]Output: trueExplanation: Return true because “applepenapple” can be segmented as “apple pen apple”. Note that you are allowed to reuse a dictionary word.Example 3: Input: s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]Output: false 解法一: 回溯时间复杂度: 超时空间复杂度: $O(1)$ 123456789101112131415161718192021class Solution &#123;public: bool wordBreak(string s, vector&lt;string&gt;&amp; wordDict) &#123; // 纯回溯实现, 复杂度很高, 很容易超时 unordered_set&lt;string&gt; word_dict(wordDict.begin(), wordDict.end()); return helper(s,-1,word_dict); &#125; bool helper(string &amp;s, int seg, unordered_set&lt;string&gt; &amp;word_dict)&#123; if(seg==s.size()-1) return true; string temp=""; for(int i=seg+1; i&lt;s.size(); i++)&#123; temp+=s[i]; if(word_dict.find(temp) != word_dict.end() &amp;&amp; helper(s, i, word_dict)==true)&#123; return true; &#125; &#125; return false; &#125;&#125;; 解法二: DP时间复杂度: $O(n^2)$, $n$ 为字符串的长度空间复杂度: $O(n)$, dp 数组额外空间, unordered_set 额外空间 1234567891011121314151617181920class Solution &#123;public: bool wordBreak(string s, vector&lt;string&gt;&amp; wordDict) &#123; unordered_set&lt;string&gt; word_dict(wordDict.begin(), wordDict.end()); if(wordDict.size()==0) return false; vector&lt;bool&gt; dp(s.size(), false); for(int i=0; i&lt;s.size(); i++)&#123; for(int j=i; j&gt;=0; j--)&#123; if(j-1&lt;0 || dp[j-1]==true)&#123; string temp = s.substr(j, i-j+1); if(word_dict.find(temp) != word_dict.end())&#123; dp[i]=true; break; // break to next i &#125; &#125; &#125; &#125; return dp.back(); &#125;&#125;; 解法三: DP时间复杂度: $O(nm)$, $n$ 为字符串的长度, $m$ 为字典的 size空间复杂度: $O(n)$, dp 数组额外空间 12345678910111213141516171819class Solution &#123;public: bool wordBreak(string s, vector&lt;string&gt;&amp; wordDict) &#123; unordered_set&lt;string&gt; word_dict(wordDict.begin(), wordDict.end()); if(wordDict.size()==0) return false; vector&lt;bool&gt; dp(s.size(), false); for(int i=0; i&lt;s.size(); i++)&#123; for(int j=0; j&lt;wordDict.size(); j++)&#123; if(i&gt;=wordDict[j].size()-1)&#123; int len = wordDict[j].size(); string temp= s.substr(i-len+1, len); if(temp == wordDict[j] &amp;&amp; ((i-len)&lt;0 || dp[i-len]==true))// 这里注意, .size() 返回的类型并不是int, 如果使用i-wordDict[j].size() &lt;0, 就会造成runtime error, 正确做法是进行强制的类型转换, 或者用一个int变量代表之. dp[i]=true; &#125; &#125; &#125; return dp.back(); &#125;&#125;; 148. Sort List对链表进行排序, 要求时间复杂度为 $O(nlogn)$, 空间复杂度为常数 DescriptionSort a linked list in O(n log n) time using constant space complexity. Example 1: Input: 4-&gt;2-&gt;1-&gt;3Output: 1-&gt;2-&gt;3-&gt;4Example 2: Input: -1-&gt;5-&gt;3-&gt;4-&gt;0Output: -1-&gt;0-&gt;3-&gt;4-&gt;5 解法一: 递归时间复杂度: $O(nlogn)$空间复杂度: $O(logn)$ 首先对于链表的排序最先想到的就是归并排序, 因为题目的要求是空间复杂度为常数, 因为不能使用递归实现(递归会占用额外空间), 但是, 递归是一种很好理解的排序方法, 因此, 这里我们先给链表归并排序的递归实现. 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* sortList(ListNode* head) &#123; if(head==nullptr || head-&gt;next==nullptr) return head; //链表中至少应有两个元素, 否则不能进行融合, 会产生运行时错误 ListNode *slow=head, *fast=head, *pre=head; // 两指针, 找到最中间的元素, 用slow指向 while(fast!=nullptr &amp;&amp; fast-&gt;next!=nullptr)&#123; pre = slow; slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; &#125; pre-&gt;next = nullptr; // 将前后两个链断开 ListNode* sort1 = sortList(head); // 将前一半排序 ListNode* sort2 = sortList(slow); // 将后一半排序 return merge_sort(sort1, sort2); // 融合两个有序链表 &#125; ListNode* merge_sort(ListNode* l1, ListNode* l2)&#123; ListNode* dummy = new ListNode(0); ListNode* cur = dummy; while(l1!=nullptr &amp;&amp; l2!=nullptr)&#123; if(l1-&gt;val &lt; l2-&gt;val)&#123; cur-&gt;next = l1; l1 = l1-&gt;next; &#125;else&#123; cur-&gt;next = l2; l2 = l2-&gt;next; &#125; cur = cur-&gt;next; &#125; if(l1!=nullptr) cur-&gt;next = l1; if(l2!=nullptr) cur-&gt;next = l2; // 将最后的一个非空元素加入排序链表 return dummy-&gt;next; &#125;&#125;; 解法二: 迭代时间复杂度: $O(nlogn)$空间复杂度: $O(1)$ 接下来, 我们考虑如何实现归并排序的迭代算法, 代码如下: TODO 150. Evaluate Reverse Polish Notation计算逆波兰表达式 DescriptionEvaluate the value of an arithmetic expression in Reverse Polish Notation. Valid operators are +, -, *, /. Each operand may be an integer or another expression. Note: Division between two integers should truncate toward zero.The given RPN expression is always valid. That means the expression would always evaluate to a result and there won’t be any divide by zero operation.Example 1: Input: [“2”, “1”, “+”, “3”, ““]Output: 9Explanation: ((2 + 1) 3) = 9Example 2: Input: [“4”, “13”, “5”, “/“, “+”]Output: 6Explanation: (4 + (13 / 5)) = 6Example 3: Input: [“10”, “6”, “9”, “3”, “+”, “-11”, ““, “/“, ““, “17”, “+”, “5”, “+”]Output: 22Explanation: ((10 (6 / ((9 + 3) -11))) + 17) + 5= ((10 (6 / (12 -11))) + 17) + 5= ((10 (6 / -132)) + 17) + 5= ((10 0) + 17) + 5= (0 + 17) + 5= 17 + 5= 22 解法一: 栈时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(n)$, 需要一个额外的栈来存储中间结果 用栈来实现, 从到开始扫描字符串vector, 如果当前字符串不为运算符, 则直接入栈, 如果为运算符 , 则取栈顶两个元素进行运算然后将计算结果入栈. 最终, 栈中只剩一个结果值 需要注意的是: 首先要确保输入的逆波兰表达式是没有问题的, 其次还有要进行零除判断, 这几点本题没有考查, 但仍需注意 1234567891011121314151617181920212223class Solution &#123;public: int evalRPN(vector&lt;string&gt;&amp; tokens) &#123; stack&lt;int&gt; polish; for(auto token : tokens)&#123; int a,b,c; if(token.back()=='+' || token.back()=='-' || token.back()=='*' || token.back()=='/')&#123; // 用back的原因是数字有可能是 -13 这种形式 b = polish.top(); polish.pop(); a = polish.top(); polish.pop(); &#125; switch(token.back())&#123; case '+': c=a+b; break; case '-': c=a-b; break; case '*': c=a*b; break; case '/': c= (b==0) ? 0 : a/b; break; default: c = c=std::stoi(token); &#125; polish.push(c); &#125; return polish.top(); &#125;&#125;; 解法二: 栈+异常解法与上面相同, 不同借助了异常, 显得更加简洁 12345678910111213141516171819202122232425262728class Solution &#123;public: int evalRPN(vector&lt;string&gt; &amp;tokens) &#123; stack&lt;int&gt; rpn; for(int i =0; i&lt;tokens.size(); i++)&#123; try&#123; rpn.push(stoi(tokens[i])); &#125; catch (exception e)&#123; int num1 = rpn.top(); rpn.pop(); int num2 = rpn.top(); rpn.pop(); switch(tokens[i][0])&#123; case '+': rpn.push(num2+num1);break; case '-': rpn.push(num2-num1);break; case '* ': rpn.push(num2*num1);break; case '/': rpn.push(num2/num1);break; &#125; &#125; &#125; if(rpn.size()==1) return rpn.top(); else return 0; &#125;&#125;; 解法三: 栈+lambda思路与解法一一直, 另一种写法: 借助哈希表和lambda表达式, 使程序更加整洁 12345678910111213141516171819202122class Solution &#123;public: int evalRPN(vector&lt;string&gt;&amp; tokens) &#123; unordered_map&lt;string, function&lt;int(int, int)&gt;&gt; op_map=&#123; &#123;"+", [](int a, int b)&#123;return a+b;&#125;&#125;, //注意要用双引号, 因为token是stirng类型, 而不是char类型 &#123;"-", [](int a, int b)&#123;return a-b;&#125;&#125;, &#123;"*", [](int a, int b)&#123;return a*b;&#125;&#125;, &#123;"/", [](int a, int b)&#123;return (b==0) ? 0 : a/b;&#125;&#125; &#125;; stack&lt;int&gt; polish; for(auto token : tokens)&#123; if(!op_map.count(token)) polish.push(std::stoi(token)); else&#123; int b = polish.top(); polish.pop(); int a = polish.top(); polish.pop(); polish.push(op_map[token](a, b)); &#125; &#125; return polish.top(); &#125;&#125;; 152. Maximum Product Subarray求连续子序列的最大乘积 DescriptionGiven an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Example 1: Input: [2,3,-2,4]Output: 6Explanation: [2,3] has the largest product 6.Example 2: Input: [-2,0,-1]Output: 0Explanation: The result cannot be 2, because [-2,-1] is not a subarray. 解法一: 递归时间复杂度: $O(n)$, 遍历一次空间复杂度: $O(n)$, 递归 $n$ 次 这道题和连续子序列的最大和比较相似, 但是更难一些, 我们需要考虑负负得正这种情况, 因此, 我们不仅仅要维护最大值, 还要维护最小值. 考虑利用递归的方法来实现, 假设我们现在已经知道了以第 i-1 个数为结尾的连续子序列的最大乘积值max和最小乘积值min, 那么如果数组中新来一个数 nums[i], 则以第 i 个数为结尾的连续子序列的最大乘积就一定是max * nums[i], min*nums[i], nums[i]之中的最大者, 最小值为这三者的最小者. 由于我们还不知道最终的连续子序列是以第几个字符为结尾的, 因此我们利用一个变量res来维护当前找到的最大的子序列乘积, 并且随着循环的进行不断更新这个值, 最终, res的值就是我们要求的解, 代码如下: 123456789101112131415161718192021class Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0 ) return 0; int res=nums[0]; helper(nums, nums.size()-1, res); return res; &#125; pair&lt;int, int&gt; helper(vector&lt;int&gt; &amp;nums, int index, int &amp;res)&#123; //注意这里要设置一个引用res来不断更新最大值 if(index == 0) return make_pair(nums[0], nums[0]); pair&lt;int, int&gt; max_min = helper(nums, index-1, res); int a = max_min.first * nums[index]; int b = max_min.second * nums[index]; int c = nums[index]; max_min.first = max(a, max(b,c)); max_min.second = min(a, min(b,c)); res = max(res, max_min.first); return max_min; &#125;&#125;; 解法二: DP 迭代上面的递归写法, 可以转换成DP迭代, 为此需要两个dp数组, 一个用来保存以第i个元素为结尾的连续子序列的最大值, 另一个保存最小值. 代码如下: 写法一: new数组12345678910111213141516171819202122class Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0 ) return 0; int res=nums[0]; int *dp_max = new int[nums.size()](); int *dp_min = new int[nums.size()](); dp_max[0] = nums[0]; dp_min[0] = nums[0]; for(int i = 1; i&lt;nums.size(); i++)&#123; int a = dp_max[i-1]*nums[i]; int b = dp_min[i-1]*nums[i]; int c = nums[i]; dp_max[i] = max(a, max(b,c)); dp_min[i] = min(a, min(b,c)); res = max(res, dp_max[i]); &#125; delete[] dp_max; delete[] dp_min; return res; &#125;&#125;; 写法二: vector数组:1234567891011121314151617181920CCclass Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0 ) return 0; int res=nums[0]; vector&lt;int&gt; dp_max(nums.size(), 0); vector&lt;int&gt; dp_min(nums.size(), 0); dp_max[0] = nums[0]; dp_min[0] = nums[0]; for(int i = 1; i&lt;nums.size(); i++)&#123; int a = dp_max[i-1]*nums[i]; int b = dp_min[i-1]*nums[i]; int c = nums[i]; dp_max[i] = max(a, max(b,c)); dp_min[i] = min(a, min(b,c)); res = max(res, dp_max[i]); &#125; return res; &#125;&#125;; 162. Find Peak ElementDescription: 局部最大值A peak element is an element that is greater than its neighbors. Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. The array may contain multiple peaks, in that case return the index to any one of the peaks is fine. You may imagine that nums[-1] = nums[n] = -∞. Example 1: Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2.Example 2: Input: nums = [1,2,1,3,5,6,4]Output: 1 or 5Explanation: Your function can return either index number 1 where the peak element is 2, or index number 5 where the peak element is 6. 解法一: $O(n)$ 复杂度$O(n)$ 的时间复杂度, 不合符题目要求, 仅仅记录一下. 12345678910111213class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() ==0) return -1; if(nums.size() ==1 || nums[0] &gt; nums[1]) return 0; for(int i=1; i&lt;nums.size()-1; i++)&#123; if(nums[i] &gt; nums[i-1] &amp;&amp; nums[i] &gt; nums[i+1]) return i; &#125; if(nums[nums.size()-2] &lt; nums[nums.size()-1]) return nums.size()-1; &#125;&#125;; 解法二: $O(logn)$ 复杂度二分查找, 分为以下几种情况: If num[i-1] &lt; num[i] &gt; num[i+1], then num[i] is peak If num[i-1] &lt; num[i] &lt; num[i+1], then num[i+1…n-1] must contains a peak If num[i-1] &gt; num[i] &gt; num[i+1], then num[0…i-1] must contains a peak If num[i-1] &gt; num[i] &lt; num[i+1], then both sides have peak 12345678910111213141516class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() ==0) return -1; int low = 0; int high = nums.size()-1; int mid; while(low &lt; high-1)&#123; //避免low和high相邻, 使得mid-1或mid+1可能非法 mid = (low+high)/2; if(nums[mid-1] &lt; nums[mid] &amp;&amp; nums[mid] &gt; nums[mid+1]) return mid; else if(nums[mid] &lt; nums[mid+1]) low = mid+1; else high = mid-1; &#125; return nums[low]&gt;nums[high] ? low : high; // 当low或high相邻时, 即为两端时的情况 &#125;&#125;; 166. Fraction to Recurring DecimalDescription: 无限循环小数Given two integers representing the numerator and denominator of a fraction, return the fraction in string format. If the fractional part is repeating, enclose the repeating part in parentheses. Example 1: Input: numerator = 1, denominator = 2Output: “0.5”Example 2: Input: numerator = 2, denominator = 1Output: “2”Example 3: Input: numerator = 2, denominator = 3Output: “0.(6)” 解法一: 用余数作为哈希表的key时间复杂度: $O(logn)$, 每次都会乘以10再取余数空间复杂度: $O(logn)$, 余数的哈希表 首先, 获取最终浮点数的符号和整数部分, 此处由于可能出现分子为-2147483648, 而分母为-1的情况, 为此, 建议使用long长整型来避免溢出.在计算小数部分时, 将余数作为key, 小数当前位置作为value存入哈希表中, 然后将余数乘以10, 再计算当前小数位的值, 并将取余得到新的余数.题目指明浮点数是无限循环小数, 则如果小数部分没有循环, 那么一定会出现余数为0的情况, 此时, 返回当前的res即可. 如果小数存在循环, 那么循环一定出现在余数相同的时刻, 此时, 将添加后扩号, 并根据哈希表中的value添加前括号. 123456789101112131415161718192021222324252627282930class Solution &#123;public: string fractionToDecimal(int numerator, int denominator) &#123; if(numerator == 0 || denominator == 0) return "0"; string res; if(numerator&lt;0 ^ denominator&lt;0) res+="-"; long numer = (numerator &lt; 0) ? (long)(numerator)*-1 : (long)numerator; // 注意, 不能写成 (long)(numerator*-1) long denom = (denominator &lt; 0) ? (long)(denominator)*-1 : (long)denominator; long integral = numer/denom; res += std::to_string(integral); // 添加整数部分 long rmd = numer % denom; if(rmd!=0) res += "."; // 存在小数 unordered_map &lt;long, long&gt; hash; while(rmd!=0)&#123; if(hash.find(rmd) != hash.end())&#123; // 判断余数 res.insert(hash[rmd], "("); res += ")"; break; &#125; hash[rmd] = res.size(); rmd = rmd*10; long quotient = rmd/denom; res += std::to_string(quotient); rmd = rmd%denom; &#125; return res; &#125;&#125;; 179. Largest NumberDescription: 排列数字使其字符串形式的数字为最大Given a list of non negative integers, arrange them such that they form the largest number. Example 1: Input: [10,2]Output: “210”Example 2: Input: [3,30,34,5,9]Output: “9534330” 解法一: 构造比较函数, 快排排序时间复杂度: $O(nlogn)$, 快排时间复杂度空间复杂度: $O(logn)$, 快排空间复杂度, 如果使用其他排序算法, 可将空间复杂度降为 $O(1)$ 我们可以构造一个新的比较函数来决定两个元素的先后关系, 对于任意两个元素 a 和 b, 首先将其转换成字符串形式 s_a 和 s_b, 我们知道, 若整形 a&gt;b, 则一定有 s_a &gt; s_b, 因此我们可以比较 s_a+s_b 和 s_b+s_a 的大小关系, 根据题目要求, 我们要进行递减排序. 得到比较函数以后, 利用快排排序即可. 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: string largestNumber(vector&lt;int&gt;&amp; nums) &#123; q_sort(nums, 0, nums.size()-1); if(nums.size()!=0 &amp;&amp; nums[0] == 0) return "0"; // 对于输入[0, 0, 0] 应该返回 "0", 而不是"000", 必须要放在排序后, nums[0] == 0 说明所有元素均为0 string res; for(auto num: nums)&#123; res += std::to_string(num); &#125; return res; &#125; bool str_geq(int a, int b)&#123; string s_a = std::to_string(a); string s_b = std::to_string(b); if(s_a+s_b &gt;= s_b+s_a) return true; //注意是递减排序, 所以为 &gt;= else return false; &#125; int partition(vector&lt;int&gt; &amp;nums, int low, int high)&#123; int P = nums[low]; while(low &lt; high)&#123; while(low&lt;high &amp;&amp; str_geq(P, nums[high])) high--; nums[low] = nums[high]; while(low&lt;high &amp;&amp; str_geq(nums[low], P)) low++; nums[high] = nums[low]; &#125; nums[low] = P; return low; &#125; void q_sort(vector&lt;int&gt; &amp;nums, int low, int high)&#123; int mid = partition(nums, low, high); if(mid&gt;low) q_sort(nums, low, mid-1); if(mid&lt;high) q_sort(nums, mid+1, high); &#125;&#125;; 解法二: 利用 STL sort() 函数时间复杂度: $O(nlogn)$, 快排时间复杂度空间复杂度: $O(logn)$, 快排空间复杂度, 如果使用其他排序算法, 可将空间复杂度降为 $O(1)$ 思路与解法一一致, 只不过省略了排序算法的实现, 使用了 STL 的 sort 函数. 需要注意, 在 C++ STL 的 sort 函数中, bool 返回真的时候, 必须是绝对大于或者绝对小于, 对于等于的情况, 只能返回 false(因为当返回 true 时, 元素会继续下一个, 这样对于极端情况, 如所有元素都一样时, 会出现越界, 从而导致段错误) 123456789101112131415161718bool str_geq(int a, int b)&#123; string s_a = std::to_string(a); string s_b = std::to_string(b); if(s_a+s_b &gt; s_b+s_a) return true; // 这里用 &gt;= 会产生运行时错误, 用 &gt; 则可以通过, 为什么? else return false;&#125;class Solution &#123;public: string largestNumber(vector&lt;int&gt;&amp; nums) &#123; std::sort(nums.begin(), nums.end(), str_geq); if(nums.size()!=0 &amp;&amp; nums[0] == 0) return "0"; // 对于输入[0, 0, 0] 应该返回 "0", 而不是"000", 必须要放在排序后, nums[0] == 0 说明所有元素均为0 string res; for(auto num: nums)&#123; res += std::to_string(num); &#125; return res; &#125; &#125;;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>算法刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode算法题(Easy)]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98-LeetCode-1%2F</url>
    <content type="text"><![CDATA[001. Two Sum题目描述Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 解法一: 穷举时间复杂度: $O(n^2)$空间复杂度: $O(1)$ 123456789101112131415class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; for(int i = 0; i&lt;nums.size(); i++)&#123; for(int j = i+1; j&lt;nums.size(); j++)&#123; if(nums[i] + nums[j] == target)&#123; vector&lt;int&gt; res =&#123;i,j&#125;; return res; &#125; &#125; &#125; &#125;&#125;; 解法二 : 哈希表, 两次遍历这里要特别注意: 同一个元素不能使用两次, 但是数组中的元素是可以重复的, 重复的元素看作是两个元素. hash表中最终存储的将会是重复元素的最后一个下标, 因此, 在进行比较时, 使用 i!= nums_map[target-nums[i]] 来判断它们是否为同一个元素, 而不能使用nums_map[nums[i]] != nums_map[target-nums[i]] 时间复杂度: $O(n)$ 遍历两次空间复杂度: $O(n)$ 123456789101112131415class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; unordered_map&lt;int, int&gt; nums_map; for(int i = 0 ;i&lt;nums.size(); i++)&#123; nums_map.insert(&#123;nums[i], i&#125;); &#125; for(int i = 0 ; i&lt;nums.size(); i++)&#123; if(nums_map.count(target-nums[i]) == 1 &amp;&amp; i!= nums_map[target-nums[i]])&#123; vector&lt;int&gt; res = &#123;i, nums_map[target-nums[i]]&#125;; //这里一定要用i,而不能用nums_map[nums[i]] , 上面也同理 return res; &#125; &#125; &#125;&#125;; 解法三: 哈希表 一次遍历事实上, 可以将hash表的插入和查找对应元素的操作放在 一个循环里, 这样就只需要进行一次遍历 时间复杂度: $O(n)$ 遍历一次空间复杂度: $O(n)$ 1234567891011121314class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; unordered_map&lt;int, int&gt; nums_map; for(int i = 0 ;i&lt;nums.size(); i++)&#123; if(nums_map.count(target-nums[i]) == 1 &amp;&amp; i!= nums_map[target-nums[i]])&#123; vector&lt;int&gt; res = &#123;i, nums_map[target-nums[i]]&#125;; return res; &#125; nums_map.insert(&#123;nums[i], i&#125;); &#125; &#125;&#125;; 扩展问题How would you approach the problem if the input array is very large (but limited range) and cannot fit in the memory ? This is a follow-up question for this problem. 007.解法一: 取余数这道题本身不难, 只要不断对x的绝对值取余数, 就可以得到反转的整数, 但是, 该题的核心考察点在于边界条件的判断, 稍不注意, 很容易漏解(如果不进行边界判断, 即使写出了解决方法, 面试官也很不满意) x为0 x反转后的值,超过了int型数据的表示范围, 检查方法是先用long存储, 然后看 1234567891011121314151617class Solution &#123;public: int reverse(int x) &#123; if(x==0) return x; int abs_x = abs(x); int sign_x = x&gt;0? 1:-1; long res = 0; // 为了看int是否越界,特意将res声明为long型 while( abs_x!=0 )&#123; res = res*10 + abs_x%10; if(res &gt; INT_MAX || res &lt; INT_MIN) return 0; //这一句就是最主要的考察点,看int是否越界 abs_x = abs_x/10 ; &#125; if(sign_x ==-1) return 0-res; return res; &#125;&#125;; 013. Roman to IntegerDescriptionRoman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Symbol ValueI 1V 5X 10L 50C 100D 500M 1000For example, two is written as II in Roman numeral, just two one’s added together. Twelve is written as, XII, which is simply X + II. The number twenty seven is written as XXVII, which is XX + V + II. Roman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used: I can be placed before V (5) and X (10) to make 4 and 9.X can be placed before L (50) and C (100) to make 40 and 90.C can be placed before D (500) and M (1000) to make 400 and 900.Given a roman numeral, convert it to an integer. Input is guaranteed to be within the range from 1 to 3999. Example 1: Input: “III”Output: 3Example 2: Input: “IV”Output: 4Example 3: Input: “IX”Output: 9Example 4: Input: “LVIII”Output: 58Explanation: L = 50, V= 5, III = 3.Example 5: Input: “MCMXCIV”Output: 1994Explanation: M = 1000, CM = 900, XC = 90 and IV = 4. 解法一: 顺序扫描时间复杂度: $O(n)$ 顺序扫描, 如果当前字符比下一个字符小, 说明是 ‘4’ 或 ‘9’ 的情况, 用下一个字符的值减去当前字符的值 12345678910111213141516171819202122232425class Solution &#123;public: int romanToInt(string s) &#123; map&lt;char, int&gt; roman_char; roman_char['I'] = 1; roman_char['V'] = 5; roman_char['X'] = 10; roman_char['L'] = 50; roman_char['C'] = 100; roman_char['D'] = 500; roman_char['M'] = 1000; int res = 0; for(int i =0; i&lt;s.size() ; i++)&#123; if( i&lt;s.size()-1 &amp;&amp; roman_char[s[i]] &lt; roman_char[s[i+1]])&#123; res += roman_char[s[i+1]]-roman_char[s[i]]; i++; &#125; else res += roman_char[s[i]]; &#125; return res; &#125;&#125;; 扩展问题: 异常检测上面的解法虽然可以通过OJ, 但是此题还需要进行特别的异常诊断, 即要能够判断出当前输入的罗马输出是否合法! 如 “IVIV” 就是典型的不合法输入, 对于此输入, 上面的程序会输出 , 这显然不正确 014.Description解法一: 顺序比较顺序比较所有字符串的值, 直到遇到第一次不相等的位置, 然后输出前面的公共前缀, 需要额外注意处理以下几种特殊情况:输入 输入为: [] 或 [“”] 应该直接返回”” 输入为: [“abc”] 应该直接返回”abc” 123456789101112131415161718class Solution &#123;public: string longestCommonPrefix(vector&lt;string&gt;&amp; strs) &#123; if(strs.size() ==0 || strs[0]=="") return ""; if(strs.size() ==1 ) return strs[0]; for(int i =0 ;; i++)&#123; for(int k = 1; k&lt;strs.size(); k++)&#123; if(strs[0][i] != strs[k][i])&#123; if(i&gt;0) return strs[0].substr(0,i); else return ""; &#125; &#125; &#125; return ""; &#125;&#125;; 020. Valid ParenthesesDescriptionGiven a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. An input string is valid if: Open brackets must be closed by the same type of brackets.Open brackets must be closed in the correct order.Note that an empty string is also considered valid. Example 1: Input: “()”Output: trueExample 2: Input: “()[]{}”Output: trueExample 3: Input: “(]”Output: falseExample 4: Input: “([)]”Output: falseExample 5: Input: “{[]}”Output: true 解法一: 栈时间复杂度: $O(n)$空间复杂度: $O(n)$ 123456789101112131415161718class Solution &#123;public: bool isValid(string s) &#123; stack&lt;char&gt; s_brack; for(int i = 0; i&lt;s.size(); i++)&#123; char c='\0'; if(s[i] == ')') c='('; else if(s[i] == ']') c='['; else if(s[i] == '&#125;') c='&#123;'; if(!s_brack.empty() &amp;&amp; c == s_brack.top()) s_brack.pop(); else s_brack.push(s[i]); &#125; if(!s_brack.empty()) return false; return true; &#125;&#125;; 021. Merge Two Sorted ListsDescriptionMerge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Example: Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 解法一: 遍历融合时间复杂度: $O(min(m,n))$ 空间复杂度: $O(1)$ 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if(l1 == nullptr) return l2; if(l2 == nullptr) return l1; ListNode* head=nullptr; if(l1-&gt;val &lt; l2-&gt;val) &#123; head = l1; l1 = l1-&gt;next; &#125; else&#123; head = l2; l2 = l2-&gt;next; &#125; ListNode* cur = head; while(l1!=nullptr &amp;&amp; l2!=nullptr)&#123; if(l1-&gt;val &lt; l2-&gt;val)&#123; cur-&gt;next = l1; cur= cur-&gt;next; l1 = l1-&gt;next; &#125;else&#123; cur-&gt;next = l2; cur = cur-&gt;next; l2 = l2-&gt;next; &#125; &#125; if(l2==nullptr) cur-&gt;next = l1; else if(l1 == nullptr) cur-&gt;next = l2; return head; &#125;&#125;; 上面开关头结点的过程过于复杂, 可以用dummy指针简化这个过程 1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if(l1 == nullptr) return l2; if(l2 == nullptr) return l1; ListNode* dummy=new ListNode(0); ListNode* cur = dummy; while(l1!=nullptr &amp;&amp; l2!=nullptr)&#123; if(l1-&gt;val &lt; l2-&gt;val)&#123; cur-&gt;next = l1; cur = cur-&gt;next; l1 = l1-&gt;next; &#125;else&#123; cur-&gt;next = l2; cur = cur-&gt;next; l2 = l2-&gt;next; &#125; &#125; if(l2==nullptr) cur-&gt;next = l1; else if(l1 == nullptr) cur-&gt;next = l2; return dummy-&gt;next; &#125;&#125;; 026. Remove Duplicates from Sorted ArrayDescriptionGiven a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. 解法一:遍历, 两种写法, 后者相当精简 123456789101112131415161718192021class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if(nums.size()==0) return 0; int same = nums[0]; int length = 1; for(int i=1; i&lt;nums.size(); i++)&#123; if(nums[i] == same)&#123; nums.erase(nums.begin()+i); i--; continue; &#125; else&#123; same = nums[i]; length++; &#125; &#125; return length; &#125;&#125;; 1234567891011class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; int length = !nums.empty(); for(auto n : nums)&#123; if(n &gt; nums[length-1]) nums[length++] = n; &#125; return length; &#125;&#125;; 28. Implement strStr()模式匹配, 判断是否为子串 descriptionReturn the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack. Example 1: Input: haystack = “hello”, needle = “ll”Output: 2Example 2: Input: haystack = “aaaaa”, needle = “bba”Output: -1Clarification: What should we return when needle is an empty string? This is a great question to ask during an interview. For the purpose of this problem, we will return 0 when needle is an empty string. This is consistent to C’s strstr() and Java’s indexOf(). 解法一: 暴力解法二: KMP求解next数组: 求解某个位置 $k$ 的next数组值是一个循环的过程, 需要不断检查以 位置 $k-1$ 的next值 为下标的元素的 下一位元素 与 当前位置 $k$ 元素 是否相等, 如果相等, 则 next[k] = next[k-1]+1, 如果不相等, 则 038. Count and SayDescriptionThe count-and-say sequence is the sequence of integers with the first five terms as following: 1 11 21 1211 1112211 is read off as “one 1” or 11.11 is read off as “two 1s” or 21.21 is read off as “one 2, then one 1” or 1211. Given an integer n where 1 ≤ n ≤ 30, generate the nth term of the count-and-say sequence. Note: Each term of the sequence of integers will be represented as a string. 解法一: 依次查看上一次的数字时间复杂度: $O(nm)$ m为数字字符串的长度空间复杂度: $O(m)$ 每次根据上一次的数字更新当前的数字字符串, 如此迭代直到达到指定次数 123456789101112131415161718192021222324252627282930class Solution &#123;public: string countAndSay(int n) &#123; string res=""; string temp=""; for(int i = 0; i&lt; n; i++)&#123; if(i==0) res="1"; else&#123; temp =""; char cur=res[0]; int count = 0; for(int k=0; k &lt; res.size(); k++)&#123; if( cur == res[k]) count++; else&#123; temp = temp + (to_string(count)); temp.push_back(cur); cur = res[k]; count = 1; //重新计数, 当前已经有一个cur了, 所以是1 &#125; &#125; temp = temp + (to_string(count)); // 由于最后的一部分相同字符串没有加进来, 所以这里额外加一下 temp.push_back(cur); res.swap(temp); &#125; &#125; return res; &#125;&#125;; 053. Maximum Subarray连续子数组的最大和 DescriptionGiven an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: Input: [-2,1,-3,4,-1,2,1,-5,4],Output: 6Explanation: [4,-1,2,1] has the largest sum = 6.Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. 解法: 记录当前最大值时间复杂度: $O(n)$根据数组性质，设置两个变量，一个记录当前的最大值，一个记录当前的子序列之和。首先，如果当前子序列之和为负，那么就是说，从当前位置开始的子序列，比从之前位置开始的子序列大，那么就可以不考虑从之前位置开始的子序列，之前累计的和也被抛弃 1234567891011121314class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int sum = 0; int max_sum = INT_MIN; //数组有可能全负, 所以不能赋值为0 for(auto num : nums)&#123; if(num &gt; max_sum) max_sum = num; //主要是为了预防数组中全是负数的情况 sum += num; if(sum!=0 &amp;&amp; sum&gt;max_sum) max_sum = sum; // sum!=0 , 为了预防数组全负时, 0一定大于sum, 造成的错解 if(sum &lt;0) sum =0; &#125; return max_sum; &#125;&#125;; 更简洁的写法: (貌似用max要比用if语句判断快一点???)1234567891011121314class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int sum = 0; int max_sum = nums[0]; for(auto num : nums)&#123; max_sum = max(max_sum, num); sum += num; max_sum = max(max_sum, sum); sum = max(sum, 0); &#125; return max_sum; &#125;&#125;; 066. Plus One数组代表一个整数, 模拟整数的加法 DescriptionGiven a non-empty array of digits representing a non-negative integer, plus one to the integer. The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit. You may assume the integer does not contain any leading zero, except the number 0 itself. Example 1: Input: [1,2,3]Output: [1,2,4]Explanation: The array represents the integer 123.Example 2: Input: [4,3,2,1]Output: [4,3,2,2]Explanation: The array represents the integer 4321. 解法一: 直接模拟时间复杂度: $O(n)$空间复杂度: $O(1)$ 1234567891011121314151617181920class Solution &#123;public: vector&lt;int&gt; plusOne(vector&lt;int&gt;&amp; digits) &#123; int carry = 0, last_i = digits.size()-1; digits[last_i] += 1; if(digits[last_i] &gt; 9) &#123; digits[last_i] = 0; carry=1; &#125; for(int i = last_i-1; i&gt;=0 &amp;&amp; carry ; i--)&#123; digits[i] += carry; if(digits[i] &gt; 9) digits[i] = 0; else carry = 0; &#125; if(carry == 1) digits.insert(digits.begin(), 1); return digits; &#125;&#125;; 解法二: 不使用加法(更快更简单, 击败100%)123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; plusOne(vector&lt;int&gt; &amp;digits) &#123; //未考虑前缀0的情况 for(int i = digits.size() - 1; i &gt;= 0; i--) &#123; if(digits[i] != 9) &#123; digits[i] ++; break; &#125; digits[i] = 0; &#125; if(digits[0] == 0) digits.insert(digits.begin(), 1); return digits; &#125;&#125;; 069. Sqrt(x)实现开方算法 DescriptionImplement int sqrt(int x). Compute and return the square root of x, where x is guaranteed to be a non-negative integer. Since the return type is an integer, the decimal digits are truncated and only the integer part of the result is returned. Example 1: Input: 4Output: 2Example 2: Input: 8Output: 2Explanation: The square root of 8 is 2.82842…, and since the decimal part is truncated, 2 is returned. 解法一: 二分法时间复杂度: $O(logn)$空间复杂度: $O(1)$ 1234567891011121314151617class Solution &#123;public: int mySqrt(int x) &#123; double low=0, high=x; double res = high; while( std::abs(res*res - x) &gt; 0.000001 )&#123; if(res*res &gt; x)&#123; high = res; res = (low+high)/2; &#125;else&#123; low = res; res = (low+high)/2; &#125; &#125; return int(res); &#125;&#125;; 解法二: 牛顿迭代法时间复杂度: $O(logn)$空间复杂度: $O(1)$ 相当于求解 $f(res)=res^2 - x = 0$ 中 $res$ 的解. 则对于任意一点 $(res, f(res))$, 都有切线方程: f(res) - 0 = f'(res)(res-res')其中, $res’$ 是该直线与 $x$ 轴的交点. 令新的 $res$ 为该值, 就可以不断逼近 $f(res)$ 的零点, $res’$ 的值为: res' = res- \frac{f(res)}{f'(res)} = res- \frac{res^2-x}{2\times res} = \frac{res^2 + x}{2\times res}12345678910class Solution &#123;public: int mySqrt(int x) &#123; double res = x; while( std::abs(res*res - x) &gt; 0.000001 )&#123; res = (res*res+x) / (2*res); &#125; return int(res); &#125;&#125;; 解法三: 按位检索时间复杂度: $O(1)$空间复杂度: $O(1)$ 由于本题要返回的是整数, 而上面的两种方法都是针对double类型的精确开根方法, 时间复杂度为 $O(logn)$, 实际上, 当只需要返回整数时, 我们可以按整数的位进行检索, 而整数总共只有32位(传入的x位int型, 所以开根后不可能超过int), 因此时间复杂度只有 $O(32)$ , 也就是 $O(1)$. 注意: 由于该方法是首先找到比 x 大的那一位, 因此有可能超过int上限, 所以要换成long整型 12345678910111213141516class Solution &#123;public: int mySqrt(int x) &#123; long res=0; int h=0; while( long(1&lt;&lt;h) * long(1&lt;&lt;h) &lt;= x) h++; long b = 1&lt;&lt;(h-1); while( b &gt; 0)&#123; if( (res+b) * (res+b) &lt;= x) res += b; b = b/2; &#125; return res; &#125;&#125;; 070. Climbing Stairs实际上就是斐波那契数列, 更具体分析可看牛客的跳台阶 DescriptionYou are climbing a stair case. It takes n steps to reach to the top. Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top? Note: Given n will be a positive integer. Example 1: Input: 2Output: 2Explanation: There are two ways to climb to the top. 1 step + 1 step 2 stepsExample 2: Input: 3Output: 3Explanation: There are three ways to climb to the top. 1 step + 1 step + 1 step 1 step + 2 steps 2 steps + 1 step 解法一: 递归解法二: 迭代123456789101112131415class Solution &#123;public: int climbStairs(int n) &#123; if(n==0) return 0; if(n==1) return 1; int n1 = 1; int n2 = 2; for(int i=3; i&lt;=n; i++)&#123; int temp = n2; n2 = n1+n2; n1 = temp; &#125; return n2; &#125;&#125;; 088. Merge Sorted Array融合两个有序数组, 其中第一个数组的元素长度为n, 第二个为m, 题目假设第一个数组的空间为n+m. Description解法一: 后移+插入融合1234567891011121314151617181920212223class Solution &#123;public: void merge(vector&lt;int&gt;&amp; nums1, int m, vector&lt;int&gt;&amp; nums2, int n) &#123; for(int i =n+m-1; i&gt;=n; i--) nums1[i]=nums1[i-n]; //for(int i =n; i&lt;n+m; i++) 注意, 这样写是有问题的, 例如对于 [1,2,3,4,0], 这种情况, 从前往后的复制方法会造成元素覆盖 // nums1[i]=nums1[i-n]; int i =n, j=0, k=0; while(i&lt;n+m &amp;&amp; j&lt;n)&#123; if(nums1[i] &lt; nums2[j])&#123; nums1[k] = nums1[i]; k++; i++; &#125;else&#123; nums1[k] = nums2[j]; k++; j++; &#125; &#125; while(i&lt;n+m) nums1[k++] = nums1[i++]; while(j&lt;n) nums1[k++] = nums2[j++]; &#125;&#125;; 101. Symmetric Tree判断一个二叉树是否为对称的.(与自身镜像相等) DescriptionGiven a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). For example, this binary tree [1,2,2,3,4,4,3] is symmetric: 1 / \ 2 2 / \ / \3 4 4 3But the following [1,2,2,null,3,null,3] is not: 1 / \ 2 2 \ \ 3 3Note:Bonus points if you could solve it both recursively and iteratively. 解法一: 递归时间复杂度: $O(n)$ , 遍历了整个树中的每个节点一次空间复杂度: $O(n)$ , 调用递归的次数与树的高度有关, 在最差的情况下, 树的高度为n. 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool isSymmetric(TreeNode* root) &#123; if(root==nullptr) return true; return isSymHelper(root-&gt;left, root-&gt;right); &#125; bool isSymHelper(TreeNode* subRoot1, TreeNode* subRoot2)&#123; if(subRoot1 == nullptr &amp;&amp; subRoot2 == nullptr) return true; if(subRoot1 == nullptr || subRoot2 == nullptr) return false; if(subRoot1-&gt;val != subRoot2-&gt;val) return false; bool b1 = isSymHelper(subRoot1-&gt;left, subRoot2-&gt;right); bool b2 = isSymHelper(subRoot1-&gt;right, subRoot2-&gt;left); return b1&amp;&amp;b2; &#125;&#125;; 解法二: 迭代时间复杂度: $O(n)$ , 遍历了整个树中的每个节点一次空间复杂度: $O(n)$ , 层次遍历创建了两个队列, 其大小总和刚好为n. (有一种说法是: 层次遍历我们最多只会同时保存两层的节点数, 而最后一层的节点数最多为 $logn$, 所以空间复杂度实际上是 $O(logn)$ (常数项被约掉), 这种说法对吗??) 层次遍历, 注意不应该左子树和右子树做非空检查, 因此判断是否对称时, 需要包含节点为空的情况.(因为不需要知道当前的深度, 所以也不用维护深度信息) 123456789101112131415161718192021class Solution &#123;public: bool isSymmetric(TreeNode* root) &#123; if(root==nullptr) return true; queue&lt;TreeNode*&gt; q1; queue&lt;TreeNode*&gt; q2; q1.push(root-&gt;left); q2.push(root-&gt;right); TreeNode * cur1, * cur2; while(!q1.empty() &amp;&amp; !q2.empty())&#123; cur1 = q1.front(); q1.pop(); cur2 = q2.front(); q2.pop(); if(cur1==nullptr &amp;&amp; cur2 ==nullptr) continue; if(cur1==nullptr || cur2 == nullptr) return false; if(cur1-&gt;val != cur2-&gt;val) return false; q1.push(cur1-&gt;left); q1.push(cur1-&gt;right); q2.push(cur2-&gt;right); q2.push(cur2-&gt;left); &#125; return true; &#125;&#125;; 104. Maximum Depth of Binary Tree求二叉树的最大深度(树的深度) DescriptionGiven a binary tree, find its maximum depth. The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node. Note: A leaf is a node with no children. Example: Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7return its depth = 3. 解法一: 层次遍历时间复杂度: $O(n)$空间复杂度: $O(1)$ 1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: int maxDepth(TreeNode* root) &#123; if(root == nullptr) return 0; std::queue&lt;TreeNode*&gt; q; q.push(root); int depth = 0; TreeNode* cur_node; while(!q.empty())&#123; int layer_len = q.size(); depth++; for(int i=0; i&lt;layer_len; i++)&#123; cur_node = q.front(); q.pop(); if(cur_node-&gt;left!=nullptr) q.push(cur_node-&gt;left); if(cur_node-&gt;right!=nullptr) q.push(cur_node-&gt;right); &#125; &#125; return depth; &#125;&#125;; # 根据 有序数组 构造平衡二叉搜索树(不唯一, 只要符合规则即可) DescriptionGiven an array where elements are sorted in ascending order, convert it to a height balanced BST. For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Example: Given the sorted array: [-10,-3,0,5,9], One possible answer is: [0,-3,9,-10,null,5], which represents the following height balanced BST: 0 / \ -3 9 / / -10 5 解法一: 递归构造时间复杂度: $O(n)$空间复杂度: $O(n)$, 递归了n次(每个节点都被访问了一次) 由于题目给的条件是 有序数组 , 因此大大降低了了构造难度, 可以每次将数组的中间位置作为根节点, 然后分别将两边的数组作为一个新的子数组进行构造, 无需考虑插入新节点引起的二叉搜索树不平衡的问题.12345678910111213141516171819202122232425/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* sortedArrayToBST(vector&lt;int&gt;&amp; nums) &#123; return construct_BST(nums, 0, nums.size()-1); &#125; TreeNode* construct_BST(vector&lt;int&gt;&amp; nums, int low, int high)&#123; if(low&gt;high) return nullptr; int mid = (low+high)/2; TreeNode* root = new TreeNode(nums[mid]); root-&gt;left = construct_BST(nums, low, mid-1); root-&gt;right = construct_BST(nums, mid+1, high); return root; &#125;&#125;; 解法二: 迭代时间复杂度: $O(n)$, 只不过需要遍历两次树的size空间复杂度: $O(n)$, 层次遍历的队列和中根遍历的栈 先用层次遍历构造一个完全二叉树(以却确保树是平衡的), 然后在利用中根遍历对树中的每个元素进行赋值. 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123;public: TreeNode* sortedArrayToBST(vector&lt;int&gt;&amp; nums) &#123; int tree_len = nums.size(); if(tree_len == 0) return nullptr; std::queue&lt;TreeNode*&gt; q; TreeNode* root = new TreeNode(0); q.push(root); tree_len--; TreeNode* cur_node; int layer_len = 1; while(tree_len&gt;0)&#123; layer_len *= 2; for(int i=0; i&lt;layer_len &amp;&amp; tree_len&gt;0; i++)&#123; cur_node = q.front(); q.pop(); TreeNode* left = new TreeNode(0); cur_node-&gt;left = left; q.push(cur_node-&gt;left); tree_len--; if(tree_len&gt;0)&#123; TreeNode *right = new TreeNode(0); cur_node-&gt;right = right; q.push(cur_node-&gt;right); tree_len--; &#125; &#125; &#125; std::stack&lt;TreeNode*&gt; s; cur_node = root; int i = 0; while(!s.empty() || cur_node!=nullptr)&#123; while(cur_node!=nullptr)&#123; s.push(cur_node); cur_node = cur_node-&gt;left; &#125; if(!s.empty())&#123; cur_node = s.top(); s.pop(); cur_node-&gt;val =nums[i++]; cur_node = cur_node-&gt;right; &#125; &#125; return root; &#125;&#125;; 解法三: 迭代(只中根遍历一次)【链接】Loading…https://leetcode.com/problems/convert-sorted-array-to-binary-search-tree/discuss/35218/Java-Iterative-Solution 111. minimum depth of binary tree 题目描述Given a binary tree, find its minimum depth. The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node. Note: A leaf is a node with no children. Example: Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7 解法一:层次优先遍历,遇到的首个叶子结点(左右子树为空)即为最短的深度 注意: 利用while内嵌for循环的方式, 可以省去对每个结点depth的维护, 只需要每次进入for循环之前, depth++即可(因为一个for循环会将当前层所有的结点都入队列, for循环结束后, 意味着进入了下一层, 所以depth++即可) 123456789101112131415161718192021class Solution &#123;public: int run(TreeNode *root) &#123; queue&lt;TreeNode*&gt; q_node; if(root==nullptr) return 0; q_node.push(root); int depth = 0; while(!q_node.empty())&#123; const int size = q_node.size(); depth++; for(int i = 0; i&lt; size; i++)&#123; TreeNode* node = q_node.front(); q_node.pop(); if(node-&gt;left!=nullptr) q_node.push(node-&gt;left); if(node-&gt;right!=nullptr) q_node.push(node-&gt;right); if(node-&gt;left==nullptr &amp;&amp; node-&gt;right == nullptr) return depth; &#125; &#125; return -1; &#125;&#125;; 解法二(递归):让当前结点为空, 则当前结点深度为0, 若当前结点左子树为空, 则当前结点深度等于左子树深度, 反之 ,等于右子树深度. 若当前结点左右子树均不为空, 则当前结点的 最小深度 等于左右子树深度 较小者 加1 123456789101112131415class Solution &#123;public: int run(TreeNode* root) &#123; if(root== nullptr) return 0; if(root-&gt;left==nullptr) return run(root-&gt;right) + 1; else if(root-&gt;right ==nullptr) return run(root-&gt;left) + 1; else&#123; int depth1=run(root-&gt;left); int depth2=run(root-&gt;right); return depth1&lt;depth2 ? depth1+1 : depth2+1; &#125; &#125;&#125;; 118. Pascal’s TrianglePascal 三角形 DescriptionGiven a non-negative integer numRows, generate the first numRows of Pascal’s triangle. Example: Input: 5Output:[ [1], [1,1], [1,2,1], [1,3,3,1], [1,4,6,4,1]] 解法一: 按照三角形的性质进行赋值赋值时, 每一行的两端都是1, 无需重复赋值, 注意控制好边界条件 1234567891011121314class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; generate(int numRows) &#123; vector&lt;vector&lt;int&gt;&gt; res; for(int i =0; i&lt;numRows; i++)&#123; vector&lt;int&gt; temp(i+1, 1); for(int j=1; j&lt;i; j++)&#123; // 两边默认为1, 无需重复赋值 temp[j] = res[i-1][j-1]+res[i-1][j];// i和j的值只有在大于1时才会进入循环, 所以无需担心i-1或j-1&lt;0 &#125; res.push_back(temp); &#125; return res; &#125;&#125;; 121. Best Time to Buy and Sell Stock获取最大的股票利润 DescriptionSay you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. Example 1: Input: [7,1,5,3,6,4]Output: 5Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Not 7-1 = 6, as selling price needs to be larger than buying price.Example 2: Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. 解法一: 穷举计算所有可能性, $O(n^2)$ 解法二: 一次遍历时间复杂度: $O(n^2)$空间复杂度: $O(1)$ 维护两个变量 min_price 和 max_profit, 每次检查元素, 一方面如果当前价格更低, 则更改 min_price 变量, 另一方面如果当前利润超过 max_profit, 则更新之. 123456789101112131415161718192021222324252627Vclass Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.size() == 0) return 0; int min_price=prices[0], max_profit=0; for(int i=0; i&lt;prices.size(); i++)&#123; if(prices[i] &lt;= min_price)&#123; min_price = prices[i]; &#125; if(prices[i]-min_price &gt; max_profit) max_profit = prices[i]-min_price; &#125; return max_profit; &#125;&#125;;class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.size() == 0) return 0; int min_price=prices[0], max_profit=0; for(int i=0; i&lt;prices.size(); i++)&#123; if(prices[i] &lt;= min_price)&#123; min_price = prices[i]; &#125; if(prices[i]-min_price &gt; max_profit) max_profit = prices[i]-min_price; &#125; return max_profit; &#125;&#125;; 122. Best Time to Buy and Sell Stock II可以多次交易, 统计最大利润和 DescriptionSay you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example 1: Input: [7,1,5,3,6,4]Output: 7Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.Example 2: Input: [1,2,3,4,5]Output: 4Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4. Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are engaging multiple transactions at the same time. You must sell before buying again.Example 3: Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. 解法一: 用变量维护最低价格时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(1)$ 寻找递增序列, 一旦出现递减的情况, 则说明应该及时卖出, 并将 min_price 重新赋值. 因为最后一个元素后面没有值来判断是否递减, 因此需要对最后一个元素进行单独判断12345678910111213141516171819class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.size() ==0) return 0 ; int min_price = prices[0]; int sum_profit = 0, pre_price=prices[0]; for(int i=1; i&lt;prices.size(); i++)&#123; if(prices[i] &lt; pre_price)&#123; //如果小于之前的price, 则说明此时应该卖出 sum_profit += pre_price-min_price; //计算卖出利润 min_price = prices[i]; &#125; pre_price = prices[i]; if(i==prices.size()-1 &amp;&amp; prices[i] &gt; min_price) //到了最后一个元素, 查看是否应该卖出 sum_profit += prices[i] - min_price; &#125; return sum_profit; &#125;&#125;; 解法二: 每两个相邻数字当做一次交易时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(1)$ 实际上和解法一本质相同, 只不过在累加利润上有一点小区别.该解法是将每两个相邻数字看做是一次交易, 如果后者大于前者, 说明应该执行交易, 并累加交易所的利润.12345678910Cclass Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int sum_profit = 0; for(int i =1 ; i&lt;prices.size(); i++)&#123; if(prices[i] &gt; prices[i-1]) sum_profit += prices[i] - prices[i-1]; &#125; return sum_profit; &#125;&#125;; 125 Valid Palindrome判断是否为回文子串 DescriptionGiven a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases. Note: For the purpose of this problem, we define empty string as valid palindrome. Example 1: Input: “A man, a plan, a canal: Panama”Output: trueExample 2: Input: “race a car”Output: false 解法一: 前后两个指示变量, 向中间遍历判断时间复杂度: $O(n)$, 遍历一次空间复杂度: $O(1)$, 只额外用了两个变量 需要注意的是将大小写字母转换成同大写或者同小写的形式再进行判断 写法一:123456789101112131415161718192021222324class Solution &#123;public: bool isPalindrome(string s) &#123; for(int i=0, j=s.size()-1; i&lt;j; )&#123; if(is_alphanumeric(s[i]) == false)&#123; i++; continue; &#125; if(is_alphanumeric(s[j]) == false)&#123; j--; continue; &#125; if(std::tolower(s[i]) != std::tolower(s[j])) return false; i++; j--; &#125; return true; &#125; bool is_alphanumeric(char c)&#123; if(c&gt;='0' &amp;&amp; c&lt;='9') return true; else if(c&gt;='a' &amp;&amp; c&lt;='z') return true; else if(c&gt;='A' &amp;&amp; c&lt;='Z') return true; else return false; &#125;&#125;; 写法二: 12345678910111213141516171819class Solution &#123;public: bool isPalindrome(string s) &#123; for(int i=0, j=s.size()-1; i&lt;=j;i++,j-- )&#123; while(i&lt;s.size() &amp;&amp; is_alphanumeric(s[i]) == false) i++; while(j&gt;=0 &amp;&amp; is_alphanumeric(s[j]) == false) j--; if(std::tolower(s[i]) != std::tolower(s[j])) return false; &#125; return true; &#125; bool is_alphanumeric(char c)&#123; if(c&gt;='0' &amp;&amp; c&lt;='9') return true; else if(c&gt;='a' &amp;&amp; c&lt;='z') return true; else if(c&gt;='A' &amp;&amp; c&lt;='Z') return true; else return false; &#125;&#125;; 136. Single Number数组中有一个数字出现了1次(奇数次), 其他均出现了2次(偶数次), 找到出现1次(奇数次)的数字. DescriptionGiven a non-empty array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Example 1: Input: [2,2,1]Output: 1Example 2: Input: [4,1,2,1,2]Output: 4 解法一: 哈希时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(n)$, 哈希表额外空间 遍历数组, 对于每一个数, 如果当前的数存在于hash表中, 则将表中哈希删除, 如果不存在, 则添加到哈希表中, 最终, 哈希表中存在的值就是只出现一次的值 解法二: 数学公式2\times (a + b + c) - (a+b+a+b+c) = c. 将数组中的元素转换为 set(无重复元素), 然后利用上面的公式纠结时间复杂度: $O(n + n)=O(n)$, 转换为 set 需要 $O(n), 公式求解遍历也需要 $O(n)$空间复杂度: $O(n)$. set 所占额外空间 解法三: 异或任何数和0异或不变, 和自身异或变为0123456789class Solution &#123;public: int singleNumber(vector&lt;int&gt;&amp; nums) &#123; int res=0; for(auto num : nums) res ^= num; return res; &#125;&#125;; 其他更多扩展问题可看牛客第40题. 141. Linked List CycleDescriptionGiven a linked list, determine if it has a cycle in it. Follow up:Can you solve it without using extra space? 解法一:时间复杂度: $O(n+k)$, 可以认为是$O(n)$, $n$ 为链表长度, $k$ 为环长空间复杂度: $O(1)$ 从头结点开始，slow每次走一步，fast每次走两步，那么只要有环，slow和fast就一定会在环中的某个节点处相遇，如果无环，则fast一定先到达空指针 1234567891011121314151617181920212223/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool hasCycle(ListNode *head) &#123; if(head==nullptr) return false; ListNode* slow=head, *fast=head-&gt;next; while(fast!=nullptr &amp;&amp; slow != fast)&#123; slow= slow-&gt;next; if(fast-&gt;next == nullptr) return false; fast = fast-&gt;next-&gt;next; &#125; if(fast==nullptr) return false; return true; &#125;&#125;; 更多扩展见牛客第55题, 链表中环的入口节点 155. Min Stack获取栈中最小的元素 DescriptionDesign a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) — Push element x onto stack.pop() — Removes the element on top of the stack.top() — Get the top element.getMin() — Retrieve the minimum element in the stack.Example:MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.getMin(); —&gt; Returns -3.minStack.pop();minStack.top(); —&gt; Returns 0.minStack.getMin(); —&gt; Returns -2. 解法一: 两个栈时间复杂度: $O(1)$空间复杂度: $O(n)$, 两个栈 申请两个栈, 一个栈正常操作, 另一个栈只有当当前元素小于或等于栈顶元素时才入栈 1234567891011121314151617181920212223242526272829303132333435363738class MinStack &#123;private: stack&lt;int&gt; s1; stack&lt;int&gt; s2;public: /** initialize your data structure here. */ MinStack()&#123; &#125; void push(int x) &#123; s1.push(x); if(s2.empty() || x &lt;= s2.top()) s2.push(x); &#125; void pop() &#123; if(s1.top() == s2.top()) s2.pop(); s1.pop(); &#125; int top() &#123; return s1.top(); &#125; int getMin() &#123; return s2.top(); &#125;&#125;;/** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */ 160. Intersection of Two Linked Lists两个链表的第一个公共节点 DescriptionWrite a program to find the node at which the intersection of two singly linked lists begins. For example, the following two linked lists:12345A: a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3 begin to intersect at node c1. Notes: If the two linked lists have no intersection at all, return null.The linked lists must retain their original structure after the function returns.You may assume there are no cycles anywhere in the entire linked structure.Your code should preferably run in O(n) time and use only O(1) memory. 解法一：栈时间复杂度: $O(m+n)$, 遍历两个链表空间复杂度: $O(m+n)$, 两个栈 分析公共子节点的特点，首先，是单向链表，因此，从第一个公共子节点开始，后面的都是一样的，所以最好是能从链表的最后一项还是比较。但由于是单向链表，因此只能从头访问，从能访问最后的节点。 就像是先进先出一样 因此，考虑用两个辅助栈来帮助实现～ 1234567891011121314151617181920212223242526272829303132/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123; stack&lt;ListNode*&gt; s1; stack&lt;ListNode*&gt; s2; for(ListNode* cur = pHead1; cur!=nullptr; cur = cur-&gt;next)&#123; s1.push(cur); &#125; for(ListNode* cur = pHead2; cur!=nullptr; cur = cur-&gt;next)&#123; s2.push(cur); &#125; ListNode* firstCN = nullptr; while(!s1.empty() &amp;&amp; !s2.empty())&#123; if(s1.top() == s2.top())&#123; firstCN = s1.top(); s1.pop(); s2.pop(); &#125;else break; &#125; return firstCN; &#125;&#125;; 解法二: 常数空间复杂度时间复杂度: $O(m+n)$, 遍历两次空间复杂度: $O(1)$, 不使用额外空间 首先遍历得到两个链表的长度, 然后先让长链表前进长度差个节点, 接着两个链表共同向前遍历, 当相遇时即为第一个公共节点. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123; if(headA==nullptr || headB==nullptr) return nullptr; int lenA = 0; ListNode *curA = headA; while(curA !=nullptr)&#123; lenA++; curA = curA-&gt;next; &#125; int lenB = 0; ListNode *curB = headB; while(curB != nullptr)&#123; lenB++; curB = curB-&gt;next; &#125; if(lenA &gt; lenB)&#123; int len = lenA-lenB; curA = headA; while(len--)&#123; curA = curA-&gt;next; &#125; curB = headB; while(curA!=nullptr &amp;&amp; curB!=nullptr)&#123; if(curA == curB) return curA; curA = curA-&gt;next; curB = curB-&gt;next; &#125; return nullptr; &#125;else&#123; int len = lenB-lenA; curB = headB; while(len--)&#123; curB = curB-&gt;next; &#125; curA = headA; while(curA!=nullptr &amp;&amp; curB!=nullptr)&#123; if(curA == curB) return curA; curA = curA-&gt;next; curB = curB-&gt;next; &#125; return nullptr; &#125; &#125;&#125;; 169 Majority ElementDescription: 找出数组中超过一半的数字Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1: Input: [3,2,3]Output: 3Example 2: Input: [2,2,1,1,1,2,2]Output: 2 题目中指明了该数字一定存在, 所以无需进行count检查, 如果该数字有可能不存在, 则根据情况需要进行 $O(n)$ 复杂度的count检查(即检查当前的数字是否出现了大于 n/2 次). 解法一: 排序时间复杂度: $O(nlogn)$空间复杂度: $O(1)$ 先排序, 然后取中间元素, 即为 majority element.(如有需要可进行count检查, $O(n)$) 解法二: 哈希时间复杂度: $O(n)$空间复杂度: $O(n)$ 每个元素的值为哈希的 key, 每个元素出现的次数为哈希的 value, 如果某个 key 的 value 大于 n/2, 则该元素即为 majority element.哈希法记录的元素的出现次数, 所以无需进行 count 检查. 12345678910class Solution &#123;public: int majorityElement(vector&lt;int&gt;&amp; nums) &#123; unordered_map&lt; int, int&gt; hash; for(auto num: nums)&#123; hash[num]++; if(hash[num] &gt; nums.size()/2) return num; &#125; &#125;&#125;; 解法三: 同增异减如果数组中存在这样一个数，那么这个数的出现次数一定大于其他所有数的出现次数总和，因此，设置两个变量，一个 cur_num 用来存储当前数组中的可能解，另一个 count 为统计差值. 即每遇到一个和可能解相同的元素, 就 count++, 否则, count—. 如果 count=0, 则说明当前的可能解已经注定不是最终的解, 则令新的元素为可能解.最终, 对可能解进行 $O(n)$ 的 count 检查, 判断是否存在 majority element (题目假设一定存在, 所以可以不做此检查). 12345678910111213141516171819class Solution &#123;public: int majorityElement(vector&lt;int&gt;&amp; nums) &#123; if(nums.size()==0) return 0; int cur_num = nums[0]-1; //确保cur_num不和第一个数相等 int count = 0; for(auto num : nums)&#123; if(num == cur_num) count++; else if(count&gt;0)&#123; count--; &#125;else&#123; cur_num = num; count=1; &#125; &#125; return cur_num; &#125;&#125;; 解法四: 随机如果确定数组中存在 majority element 的话, 则我们可以从数组中随机选取一个元素, 并判断这个元素是否为 majority element. 这种解法依赖于统计学的概率知识, 实际的时间复杂度与数组的组成规律有关. 171. Excel Sheet Column NumberDescription: Excel列表数字Given a column title as appear in an Excel sheet, return its corresponding column number. For example: A -&gt; 1 B -&gt; 2 C -&gt; 3 ... Z -&gt; 26 AA -&gt; 27 AB -&gt; 28 ... Example 1: Input: “A”Output: 1Example 2: Input: “AB”Output: 28Example 3: Input: “ZY”Output: 701 解法一: 遍历字符串时间复杂度: $O(n)$空间复杂度: $O(1)$ 12345678910class Solution &#123;public: int titleToNumber(string s) &#123; int res=0; for(auto c : s)&#123; res += res*25 + int(c-'A') + 1; &#125; return res; &#125;&#125;; 172. Factorial Trailing ZeroesDescription: 阶乘的尾部含有0的个数解法一: 统计5的个数首先, 求出阶乘值在取余求0个数的方法肯定不可以, 阶乘会轻松溢出(n=13时就已经 int 溢出了) 时间复杂度: $O(logn)$, 以5位基数空间复杂度: $O(1)$ 因为尾部的0只可能来自于 $2\times 5$ 这样的数, 对于 $n$ 的阶乘 $1\times 2\times 3\times, …, n$ 来说, $2$ 一定是充足的, 所以我们只需要统计 $5$ 的个数就可以.统计时, 每个5个数字会出现一次5, 每隔25个数字会额外出现一次5, 每个125个数字又会额外出现一次5…, 如此循环下去, 最终5的个数就是尾部0的个数. 1234567891011class Solution &#123;public: int trailingZeroes(int n) &#123; int res = 0; for(long i =5; n/i &gt;0; i*=5)&#123; //注意这里的i的字节数一定要大于n, 因为n有可能为INT_MAX, 而 n/i &gt;0 时, i必须&gt;n res += n/i; &#125; return res; &#125;&#125;; 解法二: 另一个角度时间复杂度: $O(logn)$, 以5位基数空间复杂度: $O(1)$ (迭代), $O(logn)$ (递归需额外空间) 核心思想是相同的, 同样是统计5的出现个数, 只不过这里我们是先求出 n 中 5 的倍数, 然后再求 n/5 中 5 的倍数, 实际上这里就是相当于求 n 中 25 的倍数. 因此, 和解法一是相同的, 只不过解法二因为是通过减小 n, 而不是增大 i (5,25,125,..)的方式来统计 5 个数, 因此解法二有个好处就是可以不使用 long 类型的变量, 下面分别是该方法的递归实现和迭代实现. 递归:123456class Solution &#123;public: int trailingZeroes(int n) &#123; return n &lt; 5 ? 0 : n / 5 + trailingZeroes(n / 5); &#125;&#125;; 迭代:1234567891011class Solution &#123;public: int trailingZeroes(int n) &#123; int res=0; while(n&gt;=5)&#123; res += n/5; n /= 5; &#125; return res; &#125;&#125;; 189. Rotate ArrayDescription: 循环右移数组Given an array, rotate the array to the right by k steps, where k is non-negative. Example 1: Input: [1,2,3,4,5,6,7] and k = 3Output: [5,6,7,1,2,3,4]Explanation:rotate 1 steps to the right: [7,1,2,3,4,5,6]rotate 2 steps to the right: [6,7,1,2,3,4,5]rotate 3 steps to the right: [5,6,7,1,2,3,4] Example 2: Input: [-1,-100,3,99] and k = 2Output: [3,99,-1,-100]Explanation:rotate 1 steps to the right: [99,-1,-100,3]rotate 2 steps to the right: [3,99,-1,-100] Note:Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem.Could you do it in-place with O(1) extra space? 解法一: 暴力时间复杂度: $O(nk)$空间复杂度: $O(1)$ 所有的数字每次移动一步, 攻移动 k 次. 超时 解法二: 使用额外数组时间复杂度: $O(n)$空间复杂度: $O(n)$ 申请一个长度相等的数组, 复制原数组中的 $i$ 号元素到新数组中的 $i+k$ 号位置. 解法三: 循环置换时间复杂度: $O(n)$, 遍历一次空间复杂度: $O(1)$ 每次直接将元素放置在正确的位置, 放置前, 需要用一个临时变量将被放置的元素保存起来以防止覆盖, 然后将临时变量的元素再直接放到正确的位置, 循环进行, 知道临时变量指向了最开始的变量, 然后再继续从下一个元素开始这个过程. 在代码中设置一个 count 变量, 用来统计放置的次数, 当次数等于数组长度时, 说明已经完成移动. 123456789101112131415161718class Solution &#123;public: void rotate(vector&lt;int&gt;&amp; nums, int k) &#123; int count=0; for(int start=0; count&lt;nums.size(); start++)&#123; int cur_pos = start; int cur_val = nums[start]; do&#123; int next_pos = (cur_pos + k) % nums.size(); int temp = nums[next_pos]; nums[next_pos] = cur_val; cur_pos = next_pos; cur_val = temp; count++; &#125;while(start!=cur_pos); &#125; &#125;&#125;; 解法四: reverse时间复杂度: $O(n)$, 调用扫除 reverse 函数空间复杂度: $O(1)$ 12345678class Solution &#123;public: void rotate(vector&lt;int&gt;&amp; nums, int k) &#123; std::reverse(nums.begin(), nums.end()-k); std::reverse(nums.end()-k, nums.end()); std::reverse(nums.begin(), nums.end()); &#125;&#125;; 190.Description: 按位逆置Reverse bits of a given 32 bits unsigned integer. Example: Input: 43261596Output: 964176192Explanation: 43261596 represented in binary as 00000010100101000001111010011100, return 964176192 represented in binary as 00111001011110000010100101000000.Follow up:If this function is called many times, how would you optimize it? 解法一: 按位进行32次操作每次取 n 的最后一位, 如果为 1, 则令res左移一位并加一, 如果为0, 则只左移一位. 进行32次(n的32位). 12345678910class Solution &#123;public: uint32_t reverseBits(uint32_t n) &#123; uint32_t res= 0; for(int i=0; i&lt;32; i++)&#123; res = (res&lt;&lt;1) | ((n&gt;&gt;i)&amp;1); //res = (res&lt;&lt;1) | (n&amp;1); n = (n&gt;&gt;1); &#125; return res; &#125;&#125;; 解法二: 按位二分进行5次操作先将前16位和后16位交换(利用位移和位操作实现)然后再将16位中的前8位和后8位交换然后再将8位中的前4位和后4位交换然后再将4位中的前2位和后2位交换最后将2位中的前1位和后1位交换. 上述交换全部采用位操作实现, 因此, 速度上有所优化. 1234567891011class Solution &#123;public: uint32_t reverseBits(uint32_t n) &#123; n = (n&gt;&gt;16) | (n&lt;&lt;16); n = ( ((n &amp; 0xff00ff00)&gt;&gt;8) | ((n &amp; 0x00ff00ff)&lt;&lt;8) ); n = ( ((n &amp; 0xf0f0f0f0)&gt;&gt;4) | ((n &amp; 0x0f0f0f0f)&lt;&lt;4) ); n = ( ((n &amp; 0xcccccccc)&gt;&gt;2) | ((n &amp; 0x33333333)&lt;&lt;2) ); n = ( ((n &amp; 0xaaaaaaaa)&gt;&gt;1) | ((n &amp; 0x55555555)&lt;&lt;1) ); return n; &#125;&#125;; 191. Number of 1 BitsDescription: 统计二进制中1的个数Write a function that takes an unsigned integer and returns the number of ‘1’ bits it has (also known as the Hamming weight). Example 1: Input: 11Output: 3Explanation: Integer 11 has binary representation 00000000000000000000000000001011Example 2: Input: 128Output: 1Explanation: Integer 128 has binary representation 00000000000000000000000010000000 解法一: 逐位统计时间复杂度: $O(1)$, 循环32次空间复杂度: $O(1)$ 查看每一位上的二进制是否为1, 若为1, 则count++ 12345678910class Solution &#123;public: int hammingWeight(uint32_t n) &#123; int count=0; for(int i=0; i&lt;32; i++)&#123; if( (n &amp; (1&lt;&lt;i)) != 0) count++; &#125; return count; &#125;&#125;; 解法二: 和 $n-1$ 按位与时间复杂度: $O(1)$, 循环次数为二进制中1的个数.空间复杂度: $O(1)$ 1234567891011class Solution &#123;public: int hammingWeight(uint32_t n) &#123; int count=0; while(n!=0)&#123; count++; n = n&amp;(n-1); &#125; return count; &#125;&#125;; 198. House RobberDescription: 房屋小偷获取最大收益You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night. Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police. Example 1: Input: [1,2,3,1]Output: 4Explanation:Rob house 1 (money = 1) and then rob house 3 (money = 3).Total amount you can rob = 1 + 3 = 4.Example 2: Input: [2,7,9,3,1]Output: 12Explanation:Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1).Total amount you can rob = 2 + 9 + 1 = 12. 解法一: DP时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(1)$ 依据 DP 的思想, 对于一个任意价格的房子, 我们有两种选择: 偷或不偷. 如果选择不偷, 那么前 $(i+1)$ 个房子的最大收益, 就应该是前 $i$ 个房子的最大收益(偷或者不偷第 $i$ 个房子收益中的较大者), 如果选择偷, 那么就不能偷第 $i$ 个房子.根据上面的描述, 我们可以维护两个变量 cur_rob 和 cur_nrob, 前者代表偷第 $i$ 个房子的收益, 后者代表不偷第 $i$ 个房子的收益, 则最大收益就应该为二者中的较大者. 详细代码如下: 12345678910111213class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; int cur_rob=0; int cur_nrob=0; for(int i =0; i&lt;nums.size(); i++)&#123; int temp = cur_nrob; cur_nrob = std::max(cur_rob, cur_nrob); cur_rob = temp+nums[i]; &#125; return std::max(cur_rob, cur_nrob); &#125;&#125;; 解法二: 根据房屋的编号奇偶性时间复杂度: $O(n)$, 一次遍历空间复杂度: $O(1)$ 因为偷取的房屋不能相邻, 因此我们可以维护两个变量, even 是前偶数个房屋的最大收益, odd 是前奇数个房屋的最大收益, 对于任意的一个新来的房屋, 如果该新房屋的编号为奇数, 那么它的最大收益就是 odd+new 和 even 当中的较大者(因为不能相邻, 所以只能令 odd+new). 对于偶数的情况同理. 最终返回 odd 和 even 的较大者.(因为有可能包含最后一个元素, 也有可能不包含) 代码如下: 123456789101112class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; int odd=0; int even=0; for(int i=0; i&lt;nums.size(); i++)&#123; if(i%2==0) even = std::max(odd, even+nums[i]); else odd = std::max(odd+nums[i], even); &#125; return std::max(odd, even); &#125;&#125;;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>算法刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习经典算法之SVM深入解析]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E4%B9%8BSVM%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言起初让我最头疼的是拉格朗日对偶和SMO，后来逐渐明白拉格朗日对偶的重要作用是将w的计算提前并消除w，使得优化函数变为拉格朗日乘子的单一参数优化问题。而SMO里面迭代公式的推导也着实让我花费了不少时间。 对比这么复杂的推导过程，SVM的思想确实那么简单。它不再像logistic回归一样企图去拟合样本点（中间加了一层sigmoid函数变换），而是就在样本中去找分隔线，为了评判哪条分界线更好，引入了几何间隔最大化的目标。 之后所有的推导都是去解决目标函数的最优化上了。在解决最优化的过程中，发现了w可以由特征向量内积来表示，进而发现了核函数，仅需要调整核函数就可以将特征进行低维到高维的变换，在低维上进行计算，实质结果表现在高维上。由于并不是所有的样本都可分，为了保证SVM的通用性，进行了软间隔的处理，导致的结果就是将优化问题变得更加复杂，然而惊奇的是松弛变量没有出现在最后的目标函数中。最后的优化求解问题，也被拉格朗日对偶和SMO算法化解，使SVM趋向于完美。 1. 支持向量机基本概念及原理1.1 间隔与支持向量给定训练样本集 $D = {(\vec x^{(1)}, y^{(1)}), (\vec x^{(2)},y^{(2)}),..,(\vec x^{(m)},y^{(m)})}, y_i \in \\{-1, +1\\} (二分类问题) , \vec x^{(i)} =(x^{(i)}_1;x^{(i)}_2;…;x^{(i)}_d )$ (注意,这里用的是分号, 表示这是一个列向量), SVM做的事情就是试图把一根”木棍”放在最佳位置, 好让”木棍”的两边都有尽可能大的”间隔”. 这个”木棍”就叫做”划分超平面”, 可以用下面的线性方程来描述: \vec w^T\vec x + b = 0, 其中 $\vec w =(w_1; w_2;…; w_d)$ 为 $d$ 维法向量(注意,这里用的是分号, 表示这是一个列向量), $\vec x$ 为”木棍”上的点的坐标, $b$ 为位移项. 根据点到”直线”的距离公式,我们可以得到样本空间中任意点 $\vec x$ 到超平面 $(\vec w,b)$ 的距离为: r = \frac{|\vec w^T\vec x+b|}{\|\vec w \|}$|\vec w| = \sqrt{w_1^2 + w_2^2 + … + w_d^2}$ 为向量长度(也即向量的L2范式) 首先假设 当前的超平面可以将所有的训练样本正确分类, 那么就有如下式子: \begin{cases} \vec w^T\vec x^{(i)} + b \geq 0, & y^{(i)} = +1 \\ \vec w^T\vec x^{(i)} + b \leq 0, & y_{(i)} = -1 \end{cases}上式可以统一写成如下的约束不等式:() y^{(i)}(\vec w^T\vec x^{(i)} + b) \geq 0上面的式子其实是冗余的, 因为假设样本点不在超平面上, 所以不可能出现等于0的情况, 又因为超平面方程两边都乘一个不等于0的数,还是 同一个超平面, 因此为了简化问题的表述, 我们对 $\vec w$ 和 $b$ 加上如下约束(这里的1没有什么特别的含义, 可以是任意的常数, 因为这里的点 $\vec x^{(i)}$ 不是超平面上的点, 所以所得值不为0): \min_i|\vec w^T\vec x^{(i)} +b| = 1即离超平面最近的正, 负样本距离超平面的距离为: $\frac{1}{|\vec w|}$ , 我们将这些距离超平面最近的几个训练样本点为定义”支持向量”, 那么, 两个异类支持向量到超平面的距离之和就为 $\gamma = \frac{2}{|\vec w|}$ , 我们将这称为”间隔”. 同时, 根据此约束, 我们可以消除超分类平面约束的冗余, 得到新的超分类平面约束如下: y^{(i)}(\vec w^T\vec x^{(i)} + b) \geq 1SVM的目的就是找到具有”最大间隔”的划分超平面, 也就是要找到满足约束$y^{(i)}(\vec w^T\vec x^{(i)} + b) \geq 1$中的参数 $\vec w, b$ , 使得其具有最大的间隔 $\gamma$ , 也就: \arg\max_{\vec w,b}\frac{2}{\|\vec w\|}s.t. y^{(i)}(\vec w^T \vec x{(i)} +b) \geq 1, i=1,...,m显然, 为了最大化间隔 $\gamma$ , 我们仅需要最大化 $|\vec w|^{-1}$ , 这就等于最小化 $|\vec w|^2$, 于是上式等价为: \arg\min_{\vec w,b} \frac{1}{2}\|\vec w\|^2 = \arg\min_{\vec w,b} \frac{1}{2}\vec w^T\vec w \tag 1s.t. y^{(i)}(\vec w^T \vec x{(i)} +b) \geq 1, i=1,...,m下图即为SVM示意图, 注意,图中的1可以被任意常数替换(只要前面乘上对应的系数即可, =0说明在超分类平面上, !=0说明在两侧) 以上就是线性可分时的SVM基本型(现实中大多数问题是线性不可分的, 所以线性可分的SVM没有太多实用价值) 1.2 对偶问题求解 $\vec w$ 和 $b$1.2.1 问题说明凸二次规划问题(convex quadratix programming): 目标函数是变量的二次函数, 约束条件是变量的线性不等式 对偶问题(dual problem):在求出一个问题解的同时, 也给出了另一个问题的解 我们希望通过求解式(1)来得到具有最大间隔的划分超平面的模型参数,由于该式是一个凸二次规划问题 因此,对该式使用拉格朗日乘子法得到其”对偶问题” 对于式(1)的每条样本点约束添加拉格朗日乘子 $\alpha^{(i)} \geq 0$, 则该问题的拉格朗日函数为: L(\vec w,b,\alpha) = \frac{1}{2}\|\vec w\|^2 +\sum_{i=1}^{m}\alpha^{(i)} (1-y^{(i)}(\vec w^T \vec x^{(i)} +b))\tag 2其中, $\vec \alpha = (\alpha^{(1)}, \alpha^{(2)},…,\alpha^{(m)}) ,每一个\alpha^{(i)}均为标量$ .接着对 $L(\vec w,b,\vec \alpha)$ 对 $\vec w$ 和 $b$ 求偏导, 并令其为0, 可得: \frac{\partial L(\vec w,b,\vec \alpha)}{\partial \vec w} = \vec w - \sum_{i=1}^{m} \alpha^{(i)}y^{(i)}\vec x^{(i)} = 0 \tag 3\frac{\partial L(\vec w,b,\vec \alpha)}{\partial b} = -\sum_{i=1}^{m}\alpha^{(i)} y^{(i)} = 0 \tag 4将(3)和(4)代入(2)式中, 消去 $\vec w$ 和 $b$ ( 注意, 这里 $\sum_{i=1}^{m}\alpha^{(i)} y^{(i)} = 0$, 但是不代表 $\alpha^{(i)} y^{(i)} = 0$ ), 可得: L(\vec w, b, \vec \alpha) = \frac{1}{2}\bigg( \sum_{i=1}^{m}\alpha^{(i)} y^{(i)}\vec x^{(i)} \bigg)^2 + \sum_{i=1}^{m} \alpha^{(i)} - \sum_{i=1}^{m}\alpha^{(i)} y^{(i)} \Big( \sum_{j=1}^{m}\alpha^{(j)} y^{(j)} \vec x^{(j)} \Big)^T \vec x^{(i)} - \sum_{i=1}^{m} \alpha^{(i)} y^{(i)}b= \sum_{i=1}^{m}\alpha^{(i)} - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha^{(i)} y^{(i)} \alpha^{(i)} y^{(j)} \vec x^{(i)T} \vec x^{(j)}这里 $\vec x^{(i)},\vec x^{(j)}$ 位置可互换, 为了好看,我将 $\vec x^{(i)}$ 写在了前面. 到此, 我们就得到了式(2)的对偶问题: \arg\max_{\vec \alpha} \bigg( \sum_{i=1}^{m}\alpha^{(i)} - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha^{(i)} \alpha^{(j)} y^{(i)} y^{(j)} \vec x^{(i)T} \vec x^{(j)} \bigg) \tag 5s.t. \sum_{i=1}^{m} \alpha^{(i)} y{(i)} = 0, 其中 \alpha^{(i)} \geq 0为了满足原始问题(1) 和对偶问题(5)之间的充分必要条件, 上述推导过程还需要满足KKT(Karush-Kuhn-Tucker)条件(其中前两条已经在上述推导过程中满足) , 即要求: \begin{cases} \alpha^{(i)} \geq 0 ; \\ y^{(i)} f(\vec x^{(i)}) - 1 \geq 0 ; \\ \alpha^{(i)}(y^{(i)} f(\vec x^{(i)}) - 1 ) = 0. \end{cases}当我们解出上式得到 $\vec \alpha$ 后, 就可以通过求得 $\vec w$ 和 $b$ 的值, 进而可得到划分超平面对应的模型: f(\vec x) = \vec w ^T \vec x +b = \sum_{i=1}^{m} \alpha^{(i)} y^{(i)} \vec x^{(i)T} \vec x +b根据KKT条件我们可以轻易得出, 对任意的训练样本 $(\vec x^{(i)} , y^{(i)})$ , 总有 $\alpha^{(i)} = 0$ 或 $y^{(i)} f(\vec x^{(i)}) = 1$ . 若 $\alpha^{(i)} = 0$ , 则该项对应的样本不会出现在求和项中 ; 若 $\alpha^{(i)} &gt; 0$ , 则必有 $y^{(i)} f(\vec x^{(i)}) = 1$ , 这说明该样本点出现在最大间隔边界上, 是一个支持向量. 这显示出支持向量机的一个重要性质: 训练完成后, 大部分的训练样本都不需要保留(该样本对应的系数 $\alpha^{(i)}=0$ ), 最终模型仅与支持向量有关. 使用SMO算法求对偶问题的解从(5)式可以看出, 这仍是一个二次规划问题, 可以使用通用的二次规划法来求解, 但是, 该问题的规模正比于训练样本数量, 在实际任务中使用通用解法会造成很大的开销, 因此, 需要使用更高效的算法—-SMO(Sequential Minimal Optimization, 序列最小算法) SMO的基本思路: 先固定 $\alpha^{(i)}$ 之外的所有参数, 然后求 $\alpha^{(i)}$ 上的极值. 但是这里由于 $\alpha^{(i)}$ 之间不是互相独立的, 需要满足约束 $\sum_{i=1}^{m} \alpha^{(i)} y^{(i)} = 0$ , 即一个分量改变, 另一个也要随之改变,因此每次在优化变量中选取两个分量 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ ,并将其他参数固定, 然后在参数初始化后, 不断执行如下两个步骤直至收敛: 选取一对需要更新的变量 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ 固定 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ 以外的参数, 求解(5)式更新后的 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ 具体的求解过程如下: 首先假设需要优化的参数是 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ , 于是我们将剩下的分量 $\sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)}$ 固定, 作为常数处理, 可得下式: \alpha^{(i)} y^{(i)} + \alpha^{(j)} y^{(j)} = -\sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)} = C对上式两边同乘以 $y^{(j)}$ ,由于 $y^{(j)}\times y^{(j)} = 1$ 可得: \alpha^{(j)} = Cy^{(j)} - \alpha^{(i)} y^{(i)} y^{(j)} = y^{(j)}(C - \alpha^{(i)} y^{(i)})将上式代入(5)式, 消去变量 $\alpha^{(j)}$ , 得到一个关于 $\alpha^{(i)}$ 的单变量二次规划问题, 所有的常数项用 $C$ 表示, (5)式被转换成如下,: F(\alpha^{(i)}) = \alpha^{(i)} + \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) - \frac{1}{2}\alpha^{(i)} \alpha^{(i)} y^{(i)}y^{(i)}\vec x^{(i)T}\vec x^{(i)} - \frac{1}{2}\Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big)^2y^{(j)}y^{(j)}\vec x^{(j)T}\vec x^{(j)}- \alpha^{(i)} \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) y^{(i)}y^{(j)}\vec x^{(i)T} \vec x^{(j)}- \alpha^{(i)}y^{(i)}\sum_{k=1,k\neq i,j}^{m}\alpha^{(k)}y^{(k)}\vec x^{(i)T} \vec x^{(k)} - \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) y^{(j)}\sum_{k=1,k\neq i,j}^{m}\alpha^{(k)}y^{(k)}\vec x^{(j)T}\vec x^{(k)}= \alpha^{(i)} + \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) - \frac{1}{2}(\alpha^{(i)})^2\vec x^{(i)T}\vec x^{(i)} - \frac{1}{2} \big( C - \alpha^{(i)}y^{(i)} \big)^2 \vec x^{(j)T}\vec x^{(j)} - \alpha^{(i)} \Big( (C - \alpha^{(i)} y^{(i)}) \Big) y^{(i)}\vec x^{(i)T} \vec x^{(j)} - \alpha^{(i)}y^{(i)}v^{(i)} - \big(C- \alpha^{(i)}y^{(i)} \big)v^{(j)} + C= \alpha^{(i)} + \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) - \frac{1}{2}(\alpha^{(i)})^2K_{i,i} - \frac{1}{2} \big( C - \alpha^{(i)}y^{(i)} \big)^2 K_{j,j} - \alpha^{(i)} \Big( (C - \alpha^{(i)} y^{(i)}) \Big) y^{(i)}K_{i,j} - \alpha^{(i)}y^{(i)}v^{(i)} - \big(C- \alpha^{(i)}y^{(i)} \big)v^{(j)} + C上式为了简便, 将 $\vec x^{(i)T}\vec x^{(j)}$ 简记为 $K_{i,j}$ (后文会用K代表核函数, 这里姑且认为此时的核函数 $K$ 为恒等映射),将上式对 $\alpha^{(i)}$ 求导, 并令其等于0, 可得: \frac{\partial F(\alpha^{(i)})}{\partial \alpha^{(i)}} = 1 - y^{(i)}y^{(j)} - \alpha^{(i)}K_{i,i} + y^{(i)}(C-\alpha^{(i)} y^{(i)})K_{j,j} - \Big( C-\alpha^{(i)}y^{(i)} - \alpha^{(i)} y^{(i)} \Big)y^{(i)}K_{i,j} - y^{(i)}v^{(i)} + y^{(i)}v^{(j)}= 1-y^{(i)}y^{(j)} -\alpha^{(i)} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big) + Cy^{(i)}K_{j,j} - Cy^{(i)}K_{i,j} - y^{(i)}\big(v^{(i)} -v^{(j)} \big) = 0下面对上式进行变形, 使得可以用 $\alpha_{old}^{(i)}$ 来更新 $\alpha_{new}^{(i)}$ . 因为SVM对数据点的预测值为: $f(\vec x) = \sum_{i=1}^{m}\alpha^{(i)} y^{(i)} K(\vec x^{(i)},\vec x) + b$, 则 $v^{(i)}$ 以及 $v^{(j)}$ 的值可以表示成: v^{(i)} = \sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)} K_{i,k} = f(x^{(i)}) - \alpha^{(i)} y^{(i)} K_{i,i} - \alpha^{(j)} y^{(j)} K_{i,j} + bv^{(j)} = \sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)} K_{j,k} = f(x^{(j)}) - \alpha^{(j)} y^{(j)} K_{j,j} - \alpha^{(i)} y^{(i)} K_{j,i} + b将 $\alpha^{(j)} = y^{(j)}(C - \alpha^{(i)} y^{(i)})$ 带到上式, 可得到 $v^{(i)} - v^{(j)}$ 的表达式为: v^{(i)} - v^{(j)} = f(x^{(i)}) - f(x^{(j)}) - \alpha^{(i)} y^{(i)} K_{i,i} + \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) y^{(j)} K_{j,j} - \Big( y^{(j)}(C - \alpha^{(i)} y^{(i)}) \Big) y^{(j)}K_{i,j} + \alpha^{(i)}y^{(i)}K_{j,i}= f(x^{(i)}) - f(x^{(j)}) - \alpha^{(i)}y^{(i)}K_{i,i} + CK_{j,j} - \alpha^{(i)}y^{(i)}K_{j,j} - CK_{i,j} + 2\alpha^{(i)}y^{(i)}K_{i,j}= f(x^{(i)}) - f(x^{(j)}) - \alpha^{(i)}y^{(i)} \Big( K_{i,i} + K_{j,j} -2K_{i,j} \Big)+ CK_{j,j} - CK_{i,j}注意 $v^{(i)} - v^{(j)}$ 中 $\alpha^{(i)}$ 是更新前初始化的值, 我们将其记作 $\alpha^{(i)}_{old}$ ,以便与我们期望获得的更新后的分量 $\alpha^{(i)}_{new}$ 相区分 , 将 $v^{(i)} - v^{(j)}$ 的表达式代入 $\frac{\partial F(\alpha^{(i)})}{\partial \alpha^{(i)}_{new}}$ 中 , 可得到: \frac{\partial F(\alpha^{(i)}_{new})}{\partial \alpha^{(i)}_{new}} = 1-y^{(i)}y^{(j)} -\alpha^{(i)}_{new} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big) + Cy^{(i)}K_{j,j} - Cy^{(i)}K_{i,j} - y^{(i)}\bigg (f(x^{(i)}) - f(x^{(j)}) - \alpha^{(i)}_{old}y^{(i)} \Big( K_{i,i} + K_{j,j} -2K_{i,j} \Big)+ CK_{j,j} - CK_{i,j} \bigg)= \big( y^{(i)} \big)^2 -y^{(i)}y^{(j)} - y^{(i)}f(x^{(i)}) + y^{(i)}f(x^{(j)}) - \alpha^{(i)}_{new} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big) + \alpha^{(i)}_{old} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big)= f(x^{(j)}) - y^{(j)} - \big( f(x^{(i)}) -y^{(i)} \big) - \alpha^{(i)}_{new} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big) + \alpha^{(i)}_{old} \Big( K_{i,i} + K_{j,j} - 2K_{i,j}\Big)我们记 $E^{(i)}$ 为SVM预测值与真实值的误差: $E^{(i)} = f(x^{(i)}) - y^{(i)}$ . 并令 $\eta = K_{i,i} + K_{j,j} - 2K_{i,j}$ , 则最终的一阶导数表达式可以简化为: \frac{\partial F(\alpha^{(i)}_{new})}{\partial \alpha^{(i)}_{new}} = -\eta \alpha^{(i)}_{new} + \eta \alpha^{(i)}_{old} + y^{(i)}\big(E^{(j)} - E^{(i)} \big) = 0由此, 我们可以根据当前的参数值, 直接得到更新后的参数值: \alpha^{(i)}_{new} = \alpha^{(i)}_{old} + \frac{y^{(i)}\big(E^{(j)} - E^{(i)} \big)}{\eta} => \alpha^{(i)}_{new, unclipped} \tag 6这里注意, (6)式的推导过程并未考虑下面的约束, 因此, 我们暂且将(6)式中的 $\alpha^{(i)}_{new}$ 记作 $\alpha^{(i)}_{new, unclipped}$, 然后考虑如下约束: \alpha^{(i)} y^{(i)} + \alpha^{(j)} y^{(j)} = -\sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)} = C0 \leq \alpha^{(i)} , \alpha^{(j)} \leq C我们分别以 $\alpha^{(i)}, \alpha^{(j)}$ 为坐标轴, 于是上述约束可以看作是一个方形约束(Bosk constraint), 在二维平面中我们可以看到这是个限制在方形区域中的直线, 如下图所示, 直线在方形区域内滑动(对应不同的截距), 同时 $\alpha^{(i)}_{new}$ 的上下边界也在改变: 当 $y^{(i)} \neq y^{(j)}$ 时(如左图), 限制条件可以写成 $\alpha^{(i)} - \alpha^{(j)} = \xi$ ,根据 $\xi$ 的正负可以得到不同的上下界, 因此 $\alpha^{(i)}_{new}$ 的上下界可以统一表示成: 下界: $L = \max(0, \alpha^{(i)}_{old} - \alpha^{(j)}_{old})$ 上界: $H = \min(C, C + \alpha^{(i)}_{old} - \alpha^{(j)}_{old})$ 当 $y^{(i)} = y^{(j)}$ 时(如右图), 限制条件可以写成 $\alpha^{(i)} + \alpha^{(j)} = \xi$ , 于是 $\alpha^{(i)}_{new}$ 的上下界为: 下界: $L = \max(0,\alpha^{(i)}_{old} + \alpha^{(j)}_{old} - C)$ 上界: $H = \min(C, \alpha^{(i)}_{old} + \alpha^{(j)}_{old})$ 根据得到的上下界, 我们可以得到”修剪”后的 $\alpha^{(i)}_{new,clipped}$ : \alpha^{(i)}_{new,clipped} = \begin{cases} H & \alpha^{(i)}_{new,unclipped} > H \\ \alpha^{(i)}_{new,unclipped} & L \leq \alpha^{(i)}_{new,unclipped} \leq H \\ L & \alpha^{(i)}_{new,unclipped} < L \end{cases} \tag 7得到了 $\alpha^{(i)}_{new,clipped}$ 以后, 便可以根据 $\alpha^{(i)}_{old} y^{(i)} + \alpha^{(j)}_{old} y^{(j)}= \alpha^{(i)}_{new}y^{(i)} + \alpha^{(j)}_{new}y^{(j)}$ 得到 $\alpha^{(j)}_{new}$ : \alpha^{(j)}_{new,clipped} = \alpha^{(j)}_{old} + y^{(i)}y^{(j)}\big( \alpha^{(i)}_{old} - \alpha^{(i)}_{new,clipped} \big) \tag 8通过(7)(8)式, 我们便可以高效的计算出更新后的 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ . 当更新了一对 $\alpha^{(i)}$ 和 $\alpha^{(j)}$ 之后, 我们需要计算偏移项 $b$ 注意到, 对于任意支持向量 $(\vec x^{(s)} , y^{(s)})$ , 都有 $y^{(s)} f(x^{(s)}) = 1$ , 即: y^{(s)} \Big( \sum_{i \in S} \alpha^{(i)} y^{(i)} \vec x^{(i)T} \vec x^{(s)} + b\Big) = 1式中 $S$ 为所有支持向量的下标集. 理论上, 可以选取任意支持向量来获得 $b$ , 但现实中我们采取更加鲁棒的做法: 使用所有支持向量求解的平均值(式中所有量均已知, $\vec \alpha$ 使用的是支持向量对应的系数): b = \frac{1}{|S|} \sum_{s\in S} \bigg( \frac{1}{y^{(s)}} - \sum_{i \in S} \alpha^{(i)} y^{(i)}\vec x^{(i)T} \vec x^{(s)} \bigg)还有另一种更新 $b$ 的方式是, 只使用当前更新的变量 $\alpha^{(i)}_{new}$ 和 $\alpha^{(j)}_{new}$ 来对 $b$ 进行更新,如此一来, 为了满足KKT条件, 就有以下几种情况: 如果 $\alpha^{(i)}_{new}$ 在界内(即此时 $0 &lt; \alpha^{(i)}_{new} &lt; C$ , 当前对应样本为支持向量), 则 $b = b^{(i)}_{new}$ 如果 $\alpha^{(j)}_{new}$ 在界内(即此时 $0 &lt; \alpha^{(j)}_{new} &lt; C$ , 当前对应样本为支持向量), 则 $b = b^{(j)}_{new}$ 如果 $\alpha^{(i)}_{new}$ 和 $\alpha^{(j)}_{new}$ 都在界上,且 $L \neq H$时, 则 $b^{(i)}_{new}$ 和 $b^{(j)}_{new}$ 之间的所有的值都符合KKT条件, SMO一般选择终点作为新的偏移量: $b_{new} = \frac{b^{(i)}_{new} + b^{(j)}_{new}}{2}$ 以上讨论中, $b^{(i)}_{new}$ 的推导过程为, 当 $\alpha^{(i)}_{new}$ 在界内时, 对应的样本为支持向量 (根据KKT条件得出) , 此时 $y^{(i)}(\vec w^T \vec x^{(i)} +b) = 1$ , 两边同时乘上 $y^{(i)}$ ,得到 $\sum_{k=1}^{m}\alpha^{(k)}y^{(k)}K_{k,i} + b = y^{(i)}$, 将该式展开, 得到: b^{(i)}_{new} = y^{(i)} - \sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)}K_{k,i} - \alpha^{(i)}_{new}y^{(i)}K_{i,i} - \alpha^{(j)}_{new}y^{(j)}K_{j,i}其中前两项可以写成: y^{(i)} - \sum_{k=1,k\neq i,j}^{m} \alpha^{(k)} y^{(k)}K_{k,i} = -E^{(i)} + \alpha^{(i)}_{old}y^{(i)}K_{i,i} + \alpha^{(j)}_{old}y^{(j)}K_{j,i} + b_{old}于是有: b^{(i)}_{new} = -E^{(i)} - \big( \alpha^{(i)}_{new} - \alpha^{(i)}_{old} \big)y^{(i)} K_{i,i} - \big(\alpha^{(j)}_{new} - \alpha^{(j)}_{old} \big)y^{(j)}K_{j,i} + b_{old}同理有: b^{(j)}_{new} = -E^{(j)} - \big( \alpha^{(j)}_{new} - \alpha^{(j)}_{old} \big)y^{(j)} K_{j,j} - \big(\alpha^{(i)}_{new} - \alpha^{(i)}_{old} \big)y^{(j)}K_{i,j} + b_{old}如何恰当的选取需要更新的变量 $\alpha^{(i)}$ 和 $\alpha^{(j)}$采用启发式的规则来选取, 直觉上我们知道, 我们应该首先优化那些违反KKT条件最严重的样本, 因此我们首先首先遍历所有满足约束条件 $0 &lt; \alpha^{(i)} &lt; C$ 的样本点, 即位于间隔边界上的支持向量点(直觉上也能发现这些点最有可能分类错误), 检验它们是否满足KKT条件. 如果这些样本都满足KKT条件，则遍历整个训练样本集，判断它们是否满足KKT条件，直到找到一个违反KKT条件的变量 $\alpha^{(i)}$ (即使 $\alpha^{(i)}$ 位于边界上,也有可能违反KKT条件). 当找到了第一个分量 $\alpha^{(i)}$ 后, 接下来寻找第二个分类 $\alpha^{(j)}$, 而选取的标准是使得它有足够大的变化, 也就是说使选取的两变量所对应的样本之间的间隔最大, 一种直观的解释是, 这样的两个变量有很大的差别, 与对两个相似的变量进行更新相比(相似说明有可能属于同一类, 更新意义不大), 对它们进行更新会带给目标函数值更大的变化. 第二个乘子的迭代步长正比于 $|E^{(i)} - E^{(j)}|$ , 因此, 我们希望选择的乘子能够具有最大的 $|E^{(i)} - E^{(j)}|$. 即当 $E^{(i)}$ 为正时选择绝对值最大的赋值 $E^{(j)}$ , 反之, 选择正值最大的 $E^{(i)}$ 1.3 核函数在之前的讨论中,我们假设 训练样本 是线性可分的, 然而在现实任务中, 原始样本空间内也许并不存在一个能正确划分两类样本的超平面, 对于这样的问题, 可将一样本从原始空间映射到一个更高维的特征空间, 使得样本在这个特征空间内线性可分 . 需要知道, 如果原始空间是有限维, 即属性数有限, 那么一定存在一个高维特征空间使样本可分 令 $\phi(\vec x)$ 表示将 $\vec x$ 映射后的特征向量, 于是, 在特征空间中划分超平面所对应的模型可表示为: f(\vec x) = \vec w^T \phi(\vec x) + b类似式(1), 有: \arg\min_{\vec w,b} \frac{1}{2} \|w\|^2s.t. y^{(i)}\big( \vec w^T \phi (\vec x^{(i)}) + b \big), i=1,2,..,m其对偶问题为: \arg\max_{\vec \alpha} = \sum_{i=1}^{m}\alpha^{(i)} - \frac{1}{2}\sum_{i=1}^{m} \sum_{j=1}^{m} \alpha^{(i)} \alpha^{(j)} y^{(i)} y^{(j)} \phi(\vec x^{(i)})^T \phi(\vec x^{(j)}) \tag 9s.t. \sum_{i=1}^{m} \alpha^{(i)} y^{(i)} = 0, \alpha^{(i)} \geq 0 , i = 1,2,...,m求解上式涉及到计算 $\phi(\vec x^{(i)})^T \phi(\vec x^{(j)}$ , 这是样本 $\vec x^{(i)}$ 与 $\vec x^{(j)}$ 映射到特征空间之后的内积, 由于特征空间维数可能很高, 甚至是无穷维, 因此直接计算 $\phi(\vec x^{(i)})^T \phi(\vec x^{(j)}$ 是很困难的, 为了避开这个障碍, 可以设想这样一个函数: K \big(\vec x^{(i)}, \vec x^{(j)} \big) = \phi(\vec x^{(i)})^T \phi(\vec x^{(j)}即 $x^{(i)}$ 与 $x^{(j)}$ 在特征空间的内积等于它们在原始样本空间中通过函数 $K(\cdot, \cdot)$ 计算的结果. (有可能是先内积再函数映射, 也有可能是求范式再函数映射). 于是(9)式可重写为: \arg\max_{\vec \alpha} \sum_{i=1}^{m}\alpha^{(i)} - \frac{1}{2}\sum_{i=1}^{m} \sum_{j=1}^{m}\alpha^{(i)} \alpha^{(j)} y^{(i)} y^{(j)} K\big(\vec x^{(i)}, \vec x^{(j)} \big)s.t. \sum_{i=1}^{m} \alpha^{(i)} y^{(i)} = 0\alpha^{(i)} \geq 0, i=1,2,...,m注意, 前面几个小节的推导过程也用了符号 $K$ , 但是就像前面所说的, 前几个小节的 $K$ 是为了方便书写而使用的, 你可以把它看作是一个恒等映射的核函数 当我们解出上式得到 $\vec \alpha$ 后, 就可以得到划分超平面对应的模型(式中 $\vec x$ 为样本点, $f(\vec x)$ 为该样本点的预测结果): f(\vec x) = \vec w ^T \vec x +b = \sum_{i=1}^{m} \alpha^{(i)} y^{(i)} K\big(\vec x, \vec x^{(j)} \big) +b核函数定理: 令 $\chi$ 为输入空间 $K(\cdot, \cdot)$ 是定义在 $\chi \times \chi$ 上的对称函数, 则 $K(\cdot, \cdot)$ 是核函数 当且仅当 对于任意数据 $D = \\{\vec x^{(1)}, \vec x^{(2)},…,\vec x ^{(m)} \\}$ , 核矩阵 $K$ 总是半正定的 从以上分析可知, 核函数的选择决定了特征空间的好坏, 因此, 一个合适的核函数,就成为了支持向量机的最大变数. 下面是几种常用的核函数: 名称 表达式 参数 线性核 高斯核 拉普拉斯核 Sigoid核 此外,还可以通过函数组合得到: 若 $K_1$ 和 $K_2$ 都是核函数 ,则对任意的正数 $\gamma_1, \gamma_2$ , 其线性组合 $\gamma_1 K_1 + \gamma_2 K_2$ 也是核函数 若 $K_1$ 和 $K_2$ 为核函数, 则函数的直积 $K_1 \otimes K_2 (\vec x , \vec z) = K_1(\vec x, \vec z) K_2(\vec x, \vec z)$ 若 $K_1$ 是核函数, 则对任意函数 $g(\vec x)$, $K(\vec x, \vec z) = g(\vec x) K_1(\vec x, \vec z) g(\vec z)$ 也是核函数 1.4 软间隔与正则化在实现任务中, 往往很难确定合适的核函数, 使得训练样本在特征空间中线性可分, 即便是找到了, 也无法断定是否是由于过拟合造成的 , 因此, 我们需要 允许支持向量机在一些样本上出错 , 以缓解上面的问题. 硬间隔(hard margin)与软间隔(soft margin)的区分: 硬间隔: 所有样本都必须分类正确 软间隔: 允许某些样本不满足约束(11)式(即,预测结果和真实结果符号相反,分类错误,或预测结果绝对值小于1,相当于越过了支持向量划定的边界) 我们要在最大化间隔的同时, 使得不满足约束的样本应尽可能的少, 于是, 优化目标可写为: \min_{\vec w,b} \frac{1}{2} \|w\|^2 + C\sum_{i=1}^{m} l_{0/1} \big( y^{(i)} (\vec w^T x^{(i)}+b) - 1\big) \tag {10}y^{(i)} (\vec w^T \vec x^{(i)} +b) \geq 1 \tag {11}其中, $C&gt;0$ 是一个常数(注意与前几节推导SVM时的常数区分), $l_{0/1}$ 是 “0/1 损失函数”: l_{0/1} (z) = \begin{cases} 1, & \text{if } z < 0 ; \\ 0, & \text{otherwise}. \end{cases}当C无穷大时, (10)式就会迫使所有样本均满足约束, 也就是令所有训练样本都分类正确(容易产生过拟合), 当C取有限值时, 则允许有一些样本不满足约束(11)式. 但是, $l_{0/1}$ 非凸, 不连续, 数学性质不好, 因此, 通常使用其他函数来替代, 称为” 替代损失”, 下面为三种常用的替代损失: hinge损失: $l_{hinge}(z) = max(0,1-z)$ 指数损失(exponential loss): $l_{exp}(z) = exp(-z)$ 对率损失(logistic loss): $l_{log}(z) = log(1+ exp(-z))$ 假设采用hinge损失损失, 然后可以引入”松弛变量”(slack variables) $\xi^{(i)} \geq 0$ ,每一个样本都有一个对应的松弛变量, 用以表征该样本不满足约束(11)的程度 则可将(10)式重写为: \min_{\vec w, b, \xi^{(i)}} \frac{1}{2} \|\vec w\|^2 + C \sum_{i=1}^{m} \xi^{(i)} \tag {12}s.t. y^{(i)} (\vec w^T x^{(i)} + b) \geq 1- \xi ^{(i)}\xi^{(i)} \geq , i=1,2,...,m.可以看出, 上式是与之前推导相似的二次规划问题, 只不过是约束条件变的宽松了(为了允许一些样本犯错), 因此,同样利用拉格朗日乘子法求解, 首先得到上式的拉格朗日函数: L(\vec w, b, \vec \alpha, \vec \xi, \vec \mu) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{m} \xi^{(i)} + \sum_{i=1}^{m}\alpha^{(i)}\big(1- \xi^{(i)} - y^{(i)}(\vec w^T\vec x^{(i)} +b) \big) - \sum_{i=1}^{m} \mu^{(i)} \xi^{(i)}其中, $\alpha^{(i)} \geq 0, \mu^{(i)} \geq 0$ 是拉格朗日乘子, 令 $L(\vec w, b, \vec \alpha, \vec \xi, \vec \mu)$ 对 $\vec w, b, \vec \alpha, \vec \xi$ 求偏导, 并令其为0 , 可得: \vec w =\sum_{i=1}^{m} \alpha^{(i)} y^{(i)} \vec x^{(i)}0 = \sum_{i=1}^{m} \alpha^{(i)} y^{(i)}C = \alpha^{(i)} + \mu^{(i)} 得到(12)式对应的对偶问题如下: \max_{\alpha} \sum_{i=1}^{m} \alpha^{(i)} - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha^{(i)} \alpha^{(j)} y^{(i)} y^{(j)} K_{i,j}s.t. \sum_{i=1}^{m} \alpha^{(i)} y^{(i)} = 00 \leq \alpha^{(i)} \leq C , i=1,2,...,m可以看到, 此时, $\alpha^{(i)}$ 的约束条件变成了 $0 \leq \alpha^{(i)} \leq C$ , 上式的KKT条件要求为: \begin{cases} \alpha^{(i)} \geq 0, \mu^{(i)} \geq 0 \\ y^{(i)}f(\vec x^{(i)}) -1 +\xi^{(i)} \geq 0, \\ \alpha^{(i)} \big( y^{(i)}f(\vec x^{(i)}) - 1 + \xi^{(i)} \big) = 0, \\ \xi^{(i)} \geq 0, \mu^{(i)} \xi^{(i)} = 0 \end{cases}于是, 从KKT条件中我们可以看出, 对任意的训练样本 $(\vec x^{(i)}, y^{(i)})$, 总有 $\alpha^{(i)} = 0$ 或 $y^{(i)} f(\vec x^{(i)}) = 1 - \xi^{(i)}$. 若 $\alpha^{(i)} = 0$, 则该样本不会对 $f(\vec x)$ 产生影响. 若 $\alpha^{(i)} &gt; 0$, 则必有 $y^{(i)} f(\vec x^{(i)}) = 1 - \xi^{(i)}$, 即该样本是支持向量 因为 $C = \alpha^{(i)} + \mu^{(i)}$ , 所以, 若 $\alpha^{(i)} &lt; C$ , 则有 $\mu^{(i)} &gt; 0$ , 进而有 $\xi^{(i)} = 0$, 即该样本在最大间隔边界上(是否也就是支持向量?) 若 $\alpha^{(i)} = C$ , 则有 $\mu^{(i)} = 0$, 此时若 $\xi^{(i)} \leq 1$, 则该样本落在最大间隔内部, 若 $\xi^{(i)} &gt; 1$, 则该样本被错误分类. 以上讨论, 我们可以看出, 最终的模型依然只与支持向量有关, 保持了稀疏性(hinge损失有一块平坦的零区域,这使得SVM的解具有稀疏性) 以上是对使用hinge损失时讨论的情况, 还可以将其替换成别的损失函数以得到其他学习模型, 这些模型的性质与所用的替代函数直接相关, 但它们具有一个共性: 优化目标中的第一项用来描述划分超平面的”间隔”大小, 另一项用来表示训练集上的误差, 可写为更一般的形式: \min_{f} \Omega(f) + C\sum_{i=1}^{m} l(f(\vec x^{(i)}) , y^{(i)})其中, $\Omega(f)$ 称为”结构风险”(structural risk), 用于描述模型 $f$ 自身的性质; 第二项 $C\sum_{i=1}^{m} l(f(\vec x^{(i)})$ 称为”经验风险”(empirical risk), 用于描述模型与训练数据的契合程度. $C$ 用于对二者进行折衷. 从预测误差的角度来看, 第二项相当于模型误差, 第一项相当于正则化项, 表述了模型本身的性质, 一方面, 这为引入领域知识和用户意图提供了途径, 另一方面, 该信息有助于消减假设空间, 降低过拟合风险 3. 问答为什么SVM的分类结果仅依赖于支持向量?百机p53 核函数中不同参数的影响https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247484495&amp;idx=1&amp;sn=4f3a6ce21cdd1a048e402ed05c9ead91&amp;chksm=fdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9&amp;scene=21#wechat_redirect 既然深度学习技术性能表现以及全面超越SVM, SVM还有存在的必要吗?6.Reference[1] https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247483937&amp;idx=1&amp;sn=84a5acf12e96727b13fd7d456c414c12&amp;chksm=fdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329&amp;scene=21#wechat_redirect [2] 西瓜书 [3] http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html https://zhuanlan.zhihu.com/p/29212107]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>核函数</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播算法完整推导]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%AE%8C%E6%95%B4%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[链式法则神经网络计算过程对于神经网络中的单个神经元来说, 若输入信号为向量 $\vec x=(x_1, x_2, x_3, x_4, x_5)$ , 该层的权重为 $\vec w = (w_1, w_2, w_3, w_4, w_5)$, 偏置项为 $b$ , 那么该层的输出就为(其中$f$为激活函数): y= f(\sum_{i=1}^{n}w_i x_i +b)化简成向量形式($\vec w , \vec x$均为列向量)为: y = f(\vec w ^T \vec x +b)对于多层网络来说, 如下图所示 第一层为输入层, 神经元数量对应原始数据维数, 这一层不对数据进行输出, 直接输出 第二层为隐藏层, 可以有多个隐藏层, 每层神经元数量为中间特征维数(一般自定), 每层都具有一个权重矩阵, 将输入信号与权重矩阵做点乘运算, 加上偏置量以后按激活函数输出 第三层为输出层, 同样有一个权重矩阵, 若用于分类, 则神经元数量等于要分类的类别数 如果激活函数使用sigmoid函数, 则第二层和第三层的输出分别为(第一层输出为原始数据): 符号说明: $x$ : 向量 $x$ $x_i$ : 向量 $x$ 的第 $i$ 项 $x^{(i)}$ : 第 $i$ 个样本向量 $x_j^{(i)} : 第 $i$ 个样本向量中的 第 $j$ 项 几个重要结论 条件说明 说明及问题 结论 给定如下线性映射函数: $u = W x$ 其中 $x$ 是 $n$ 维向量, $W$ 是 $m\times n$ 的矩阵, $u$ 是 $m$维向量, 假设存在函数 $f(u)$ , 试求 $\nabla _w f$ 及 $\nabla _x f$ 因为 $w_{ij}$ 只与 $u_i, x_j$ 有关(画出矩阵相乘示意图即可), 所以有: $\frac{\partial f}{\partial w_{ij}} = \sum_{k=1}^{m} \frac{\partial f}{\partial u_k} \frac{\partial u_k}{\partial w_{ij}} = \sum_{k=1}^{m} \Big( \frac{\partial f}{\partial u_k} \frac{\partial \sum_{l=1}^{n} (w_{kl} x_l)}{\partial w_{ij}}\Big) = \frac{\partial f}{\partial u_i} \frac{\partial \sum_{l=1}^{n}(w_{il} x_l)} {\partial w_{ij}} = \frac {\partial f}{\partial u_i} x_j$ 上式写成矩阵形式为: $\nabla _W f = (\nabla _u f) x^T$ 因为 $x_i$与每一个 $u_k$ 都有关, 所以可得: $\frac{\partial f}{x_i} = \sum_{k=1}^{m} \frac{\partial f}{\partial u_k} \frac{\partial u_k}{\partial x_i} = \sum_{k=1}^{m} \Big( \frac{\partial f}{u_k} \frac{\partial \sum_{l=1}^{n} w_{kl} x_l}{\partial x_i} \Big)= \sum_{k=1}^{m} \Big( \frac{\partial f}{u_k} w_{ki} \Big) = [w_{1i}, w_{2i},…, w_{mi}]\left[ \begin{matrix} \frac{\partial f}{u_1} \\ \frac{\partial f}{u_2} \\ … \\ \frac{\partial f}{u_2} \end{matrix} \right]$ 上式写成矩阵形式为: $\nabla _x f = W^T \nabla _u f$ 给定如下向量到向量的映射: $z=g(u)$ 写成分量形式为: $z_i = g(u_i)$ 在这里, 每个 $z_i$ 只和 $x_i$ 有关, 且每个分量采用了相同的映射函数$g$ 假设存在函数 $f(z)$, 试求 $\nabla _u f$ $\frac{\partial f}{\partial u_i} = \frac{\partial f}{\partial z_i} \frac{\partial z_i}{\partial u_i} = \frac{\partial f}{\partial z_i} g’(u_i)$ $\nabla _u f = \nabla _z f \odot g’(u)$ 给定下面的复合函数 # 推导过程说明 详细推导 简洁推导 反向传播中的一些特殊环节RuLe激活函数的导数ReLU 激活函数的公式定义如下: ReLu(x) = \begin{cases} x, & x > 0 \\ 0, & x\le 0 \end{cases}可以看出, RuLu函数在 $x=0$ 处是不可微的, 为了解决这个问题, 在深度学习框架中, 往往会将其在 $x=0$ 处的导数置为0, 如下所示: ReLu'(x) = \begin{cases} 1, & x > 0 \\ 0, & x\le 0 \end{cases}Pooling池化层的反向梯度传播CNN网络中另外一个不可导的环节就是Pooling层的池化操作, 因为Pooling操作会使得feature map的尺寸发生变化. 解决这个问题的方法就是把一个该层某个位置的梯度反向传播到前一层所有与这个位置相关联的位置. 这是需要 保证传递的梯度总和不变. max pooling mean pooling Referencehttps://zhuanlan.zhihu.com/p/39195266 https://zhuanlan.zhihu.com/pytlab]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[正则化技巧总结]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[为了解决过拟合问题，通常有两种办法，第一是减少样本的特征（即维度），第二就是我们这里要说的”正则化”. 下面是一些可以帮助缓解过拟合现象的正则化技巧 使用正则项(Regularization)L1 RegularizationL1范数为: \|w\|_1 = |w_1| + |w_2| + ... + |w_n|L1正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, $m$ 则代表训练batch的样本大小 : L = L_0 + \lambda\|w\|_1 = L_0 + \lambda \sum_{w}|w|将上式对参数 $w$ 求导如下(由于正则项与 $b$ 无关, 因此参数 $b$ 的导数不变): \frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda sign(w)上式中 $sign(w)$ 表示 $w$ 的符号, 当 $w&gt;0$ 时, $sign(w)=1$ , 当 $w&lt;0$ 时, $sign(w)=-1$, 为了实现方便, 我们特意规定, 当 $w=0$ 时, $sign(w) = 0$ , 相当于去掉了正则项. 因此, 权重 $w$ 的更新表达式可如下表示: w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta \lambda sign(w)L1正则化使模型参数稀疏的原理是什么?角度一: 解空间性状“百面机器学习” 角度二: 函数叠加(梯度下降更新公式)从以上的更新表达式我们可以看出, 当 $w$ 为正时, L1正则化会将更新后的 $w$ 变的再小一点, 而当 $w$ 为负时, L1正则化会将其变的更大一点—-因此L1的正则化效果就是让 $w$ 尽可能的向 $0$ 靠近, 即最终的 $w$ 参数矩阵会变的更加稀疏 角度三: 贝叶斯先验“百面机器学习” 补充: 为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?为什么权重矩阵稀疏可以防止过拟合?可以从两个方面来理解: 1）特征选择(Feature Selection)： 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 2）可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型： $y=w1x1+w2x2+…+w1000*x1000+b$ （当然了，为了让 $y$ 限定在 $[0,1]$ 的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的 $w$ 就只有很少的非零元素，例如只有 5 个非零的 $wi$ ，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果 1000 个 $wi$ 都非 0，医生面对这 1000 种因素，累觉不爱. $L0$ 范式和 $L1$ 范式都能实现稀疏, 为什么不选择用 $L0$ 而要用 $L1$?一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解 L2 Regulation(权重衰减/岭回归)L2范数为: \|w\|_1 = \sqrt {w_1^2 + w_2^2 + ... + w_n^2 }L2正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, 式中的系数 $\frac{1}{2}$ 主要是为了消去求导后产生的常数 $2$, 方便表示 (因为可以根据 $\lambda$ 的值来替代这些常数): L = L_0 + \lambda\|w\|^2_2 =L_0 + \lambda \sum_{w}w^2将上式对参数 $w$ 求导如下: \frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + 2\lambda w则, 权重 $w$ 的更新表达式可如下表示: w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta 2\lambda w由于, $\eta, \lambda, m$ 三个值都是正的, 因此, 加上 $L2$ 正则化以后, 权重整体减小了, 这也是”权重衰减”的由来. 为何权重参数 $w$ 减小就可以防止过拟合?直观解释: 更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果 “数学一点”的解释: 过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大. 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。 L2 范式的好处是什么?防止过拟合: 最基本的好处是可以提高模型泛化能力, 防止过拟合 优化计算: 从优化或者数值计算的角度来说, L2正则化有利于提高模型训练速度, 加快计算 原因: https://www.cnblogs.com/callyblog/p/8094745.html L1 和 L2 的区别1. L1相对于L2能够产生更加稀疏的模型:原因见上面L1稀疏性的原理 2. 二者梯度下降速度不同:根据L1和L2的函数图像可以看出, L1是按照线性函数进行梯度下降的, 而L2则是按照二次函数, 因此, L1在下降时的速度是恒定的, 在接近于0的时候会很快就将参数更新成0 , 而L2在接近于0 时, 权重的更新速度放缓, 使得不那么容易更新为0 : 3. 二者解空间性状不同:这一点也可以解释为什么L1相比于L2更加稀疏的原因 数据增广(Data Augmentation)水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转等等, 也可利用GAN辅助生成(不常用) DropoutDropout是指在深度网络的训练中, 以一定的概率随机的”临时丢弃”一部分神经元节点. 具体来讲, Dropout作用于每份小批量训练数据, 由于其随机丢弃部分神经元的机制, 相当于每次迭代都在训练不同结构的神经网络, 可以被认为是一种实用的大规模深度神经网络的模型继承算法. 对于包含 $N$ 个神经元节点的网络, 在Dropout的作用下可以看作为 $2^N$ 个模型的集成, 这 $2^N$ 个模型可认为是原始网络的子网络, 它们共享部分权值, 并且拥有相同的网络层数, 而模型整个的参数数目不变, 大大简化了运算. 对于任意神经元来说, 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化, 这个过程会减弱全体神经元之间的联合适应性, 减少过拟合的风险, 增强泛化能力. 工作原理和实现: 应用Dropout包括训练和预测两个阶段, 在训练阶段中, 每个神经元节点需要增加一个概率系数, 在前向传播时, 会以这个概率选择是否丢弃当前的神经元 在测试阶段的前向传播计算时, 每个神经元的参数都会预先乘以概率系数p, 以恢复在训练中该神经元只有p的概率被用于整个神经网络的前向传播计算 Drop ConnectDrop Connect 是另一种减少算法过拟合的正则化策略，是 Dropout 的一般化。在 Drop Connect 的过程中需要将网络架构权重的一个随机选择子集设置为零，取代了在 Dropout 中对每个层随机选择激活函数的子集设置为零的做法。由于每个单元接收来自过去层单元的随机子集的输入，Drop Connect 和 Dropout 都可以获得有限的泛化性能 [22]。Drop Connect 和 Dropout 相似的地方在于它涉及在模型中引入稀疏性，不同之处在于它引入的是权重的稀疏性而不是层的输出向量的稀疏性。 早停早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。 Referencehttps://www.cnblogs.com/callyblog/p/8094745.html]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种损失函数深入解析]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E5%90%84%E7%A7%8D%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[常用损失函数及其形式 损失函数 形式 评分损失 各个损失函数详细解析绝对值损失别名 $L_1$ 损失 平方损失:平方损失的别名是 $L_2$ 损失 平方损失函数是由线性模型引出的. 对于最简单的线性模型, 可以用房屋面积和房屋价格来举例子, 假设我们已经知道了一些面积和价格的数据: 将其描绘出来如下图所示: 那么, 如果新来了一个面积, 我们能否根据已有的数据来预测它的价格, 这就是线性回归问题. 我们利用一条直线来拟合这些数据, 从而可以得到预测的价格, 如下图所示: 将这种最简单的线性回归一般化, 使其成为具有多个变量的线性模型, 就可以用向量的形式来表达, 如下所示: h_\theta (x) = \theta ^Tx对于上面的公式, 我们就可以求出多个不同的 $\theta$, 来得到不同的模型, 但是我们需要知道到底哪些模型是好的, 哪些是不好的, 因此, 就需要引入了评价机制来判断当前的参数 $\theta$ 是好还是坏, 这就引出了平方误差损失函数, 如下所示: J(\theta) = \frac{1}{2} \sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}这个公式本身非常好理解, 就是希望我们当前模型的参数 $\theta$ 可以让模型的输出结果与真实结果无限逼近. 但是问题是: 为什么是平方形式? 对此,数学解释如下: 一句话说明: 平方损失函数就是对theta的极大似然估计 首先, 预测结果和真实结果之间肯定是有误差的, 我们假设这个误差是 $\epsilon ^{(i)}$ , 那么就有如下公式: y^{(i)} = \theta ^T x^{(i)} + \epsilon ^{(i)}而一般在实际使用中, 我们的训练数据是海量的, 根据中心极限定力, 我们可以假定误差满足均值为0, 方差为 $\sigma ^2$ 的正态分布, 即 $\epsilon^{(i)} \sim N(0, \sigma ^2)$ : p(\epsilon^{(i)}) = \frac{1}{\sqrt {2 \pi } \sigma }exp(-\frac{(\epsilon^{(i)})^2}{2 \sigma ^2}) 这也就是说: p(y^{(i)} | x^{(i)};\theta) = \frac{1}{\sqrt {2 \pi } \sigma }exp(-\frac{(y^{(i)} - \theta ^T x^{(i)})^2}{2 \sigma ^2}) $p(y^{(i)} | x^{(i)};\theta)$ 代表在给定的 $x^{(i)}$ 和参数 $\theta$ 下, $y^{(i)}$的分布概率, 这可以看做是在给定的 $\theta$ 一个关于 $y$ 和$x$ 的函数. 与之相对的,我们也可以将其视为是关于参数 $\theta$ 的函数,如下所示: L(\theta) = L(\theta ; X, \vec y) = p(\vec y | X; \theta)注意到, $\epsilon^{(i)} , y^{(i)} , x^{(i)}$ 都是独立同分布的, 因此, 根据极大似然定理, 我们希望下面的式子能够取得最大值(也就是在给定数据的情况下, 我们希望找到一组参数 $\theta$ , 来使这些数据出现的概率最大, 也就是概率积最大) L(\theta) = \prod_{i=1}^{m}{p(y^{(i)} | x{(i)} ; \theta)} = \prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi} \sigma} exp\Big( -\frac{ (y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big)}为方便计算, 对上式取对数, 可得: l(\theta) = log L(\theta) = log\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi} \sigma} exp\Big( -\frac{ (y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big)} = \sum_{i=1}^{m} log \frac{1}{sqrt{2\pi} \sigma} exp\Big( -\frac{(y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big) = mlog\frac{1}{\sqrt{2\pi} \sigma} - \frac{1}{\sigma ^2}\times \frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta ^T x^{(i)})^2为了让上面的式子取值最大, 那我们就只需要令下面的式子取值最小即可: \frac{1}{2} \sum_{i=1}^{m} ( y^{(i)} - \theta ^T x^{(i)}) ^2上面的形式恰好就是我们的平方误差损失函数(通常还需要对上面的损失函数做归一化, 也就是乘上 $\frac{1}{m}$ ), 这也是平方误差损失函数的来源. (但实际上, 要知道, 基于概率假设来说, 不一定非要是平方项, 另外, 无需在意 $\sigma$ 的具体值是什么) softmax 交叉熵损失y_i = softmax(z_j) = \frac{e^{z_j}}{\sum_j e^{z_j}}E(t,y) = -\sum_j t_j log y_j上式中, $t$ 和 $y$ 分别表示神经网络的真实标签和预测输出, 第一个公式代表 softmax 激活函数. 交叉熵损失首先定义符号说明: $p^{(i)}$: 第i个样本类别为1的真实概率(如第i个样本真实类别为1, 则概率为1, 否则为0) $o^{(i)}$: 第i个样本预测类别为1的概率 $p_k^{(i)}$: 第i个样本类别为k的真实概率(如第i个样本真实类别为k, 则概率为1, 否则为0) $o_k^{(i)}$: 第i个样本预测类别为k的概率 面对二分类问题, 损失函数形式为: J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}{m}\big(y^{(i)}logo^{(i)} + (1-y^{(i)})log(1-o^{(i)}) \big) \Big]面对多分类问题, 损失函数形式为: J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{n} y_k^{(i)} log o_k^{(i)} \Big]交叉熵衡量了两个分布之间的差异性, 当概率相等时, 交叉熵最大, 则损失函数达到最小(因为加了负号) 损失函数之间的区别和联系为什么分类问题要使用交叉熵损失而不用平方损失?]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ResNet (CVPR, 2016)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ResNet-CVPR2016%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/malefactor/article/details/67637785 文章: Deep Residual Learning for Image Recognition作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun备注: MSRA, Best Paper 核心亮点本文突破了传统的卷积神经网络结构, 首次提出了残差网络, 并成功的将网络的深度提升到了一个很高的层级上, 同时解决了深层网络的模型退化问题, 对整个深度学习领域产生了重大影响. 提出动机首先文章提出了一个假设:有一个L层的深度神经网络, 如果我们在上面加入一层, 直观来讲得到的L+1层深度神经网络的效果应该至少不会比L层的差. 因为可以简单的学习出最后一层为前一层的恒等映射, 并且其它层参数设置不变.(说明是这种更深的网络是存在是的性能不下降的解的)但是, 通过实验发现, 当网络层数加深时, 网络的性能会下降(说明后面几层网络层没有学习到恒等映射这个解), 也就是所谓的”模型退化”问题, 如图1所示. 观察上述现象后, 作者认为产生模型退化的根本原因很大程度上也许不在于过拟合, 而在于梯度消失问题. 为了解决模型退化问题, 作者基于以上假设, 提出了深度残差学习框架, 没有直接堆叠网络层来 fit 期望的映射函数, 而是选择让这些网络层来 fit 一个残差映射. 也就是说, 如果我们期望得到的映射函数为 $H(x)$, 那么我们不是通过堆叠网络来直接学习这个映射函数, 而是学习对应的残差函数: $F(x):=H(x)-x$. 那么, 原始的映射函数就可以通过 $F(x)+x$ 得到(如图2所示). 我们假设这个残差映射比原始的映射函数更容易学习和优化. 极端情况下, 如果一个恒等映射是最优的, 那么相对于使得网络层学习到 $H(x)=x$ 这个映射关系, 它应该更加容易使得残差部分 $F(x) \rightarrow 0$.(原因可以看后文) 残差学习(Residual Learning)首先, 我们假设 $H(x)$ 就是几层网络层堆叠后希望学习到的映射函数(underlying mapping), 而 $x$ 代表了这几层网络的输入. 在神经网络中, 我们通常假设几层非线性的网络相堆叠可以渐进的拟合一个复杂函数, 那我们也可以等价的假设这些网络层可以渐进的拟合对应的残差函数: $H(x) - x$(姑且假设 $H(x)$ 和 $x$ 维度相同). 因此, 我们不需要令网络层来近似函数 $H(x)$, 相反, 我们希望这些网络层能够近似函数 $F(x):=H(x) - x$. 原始的映射函数也可以通过 $F(x)+x$ 得到.这种残差定义方式是收到了图1中的违反直觉的现象的启发而得出的. 正如我们之前所说的, 如果我们仅仅在模型中添加了一些恒等连接层, 那么得到的新的更深的模型的精度应该至少不会比之前的差, 但是模型还是出现了退化问题, 这说明很有可能是模型在学习的时候, 很难直接通过多层的非线性网络层学习到这种恒等映射. 然而, 通过本文的残差学习定义, 如果恒等连接层是最优的, 那么模型在学习时可以简单的令非线性的网络层函数 $F(x)$ 为0, 以此来使模型学习到恒等映射.在实际情况中, 往往不太可能使恒等映射是最优的, 但是本文提出的残差方法可以帮助模型提前为解决问题提供便利(precondition the problem). 核心思想为: 如果最优的映射函数相对于 zero mapping 更接近恒等映射, 那么相对于学习一个新的映射函数, 应该更容易的找到与恒等映射相关的扰动. 在实验中(图7), 我们发现这些学习后的残差函数大多具有很小的响应(标准差 standard deviations), 这说明本文的恒等映射提供了一种合理的先验条件(preconditioning). 恒等短接(identity shortcut connection)ResNet提出的恒等短接用于直接跳过一个或多个层, 以便让离输入层近的网络更加靠近输出层, 残差块的结构如下图所示: 在本文中, 我们可以将一个 building block 定义成下面的形式: y = F(x, \{W_i\}) + x \tag 1上式中, $x,y$ 分别代表着这个 block 的输入和输出, 而函数 $F(x, \{W_i\})$ 代表着需要学习的残差映射. 以图2为例, 该残差块具有两个网络层, 因此 $F=W_2 \sigma (W_1 x)$, 其中, $\sigma$ 代表 ReLU, 同时为了简化表示, 忽略了偏向量. 操作 $F + x$ 是通过 element-wise addition 的短接实现的. 我们采用 $\sigma(y)$ 作为本残差模块的输出. 可以看出, 恒等短接的方式有一个好处就是既不会引入任何额外参数, 也不会带来计算成本. 这样一来我们就可以很公平的与其他卷积网络模型在各种参数上进行对比.注意, 公式(1)中 $x$ 和 $F$ 的维度必须相同. 如果不同的话(即 changing the input/output channels), 我们可以利用一个线性投影矩阵 $W_s$ 来让维度匹配: y = F(x, \{W_i\}) + W_s x \tag 2我们也在可以在公式(1)用添加方阵 $W_s$(不改变维度). 但是通过实验我们发现, 当维度相同时, 直接相加就已经足够了, 因此我们只会在维度不同时才使用矩阵 $W_s$.函数 $F$ 的形式是灵活的, 本文中包含了两种形式(如图5, 分别为两层和三层). 如果只使用一层的残差模块, 这近乎于是普通的线性层了, 貌似并不能获得什么提升.上面的讨论为了方便我们使用的是全连接层, 但是残差模块同样可以用于卷积层, 在两个 feature maps 之间 channel by channel 的执行 element-wise addition. 网络结构(Network Architectures)我们通过实验验证了多种不同的 plain/residual 网络, 并且观察到了相同的现象, 下面我们介绍两种网络以供讨论. Plain Network如图3中间所示, 我们对 VGG-19 进行扩展, 得到了 plain baseline. 图中的卷积层大多为 3×3 大小, 并且遵守两条设计规则: 1), 对于输入和输出的 feature map 具有相同的 size 时, 卷积层也和设定为相同数量的卷积核(即当输出不改变尺寸时, 也不应改变通道数); 2), 如果 feature map size 减半, 那么卷积核的数量会变为双倍, 以此来保持每一层的时间复杂度. 我们在执行 downsampling 时, 是通过利用 stride=2 的卷积层实现的(即没有用 max pooling 层). 网络的最后会接一个全局平均池化层和一个1000路的 softmax 全连接层. 图3中的网络总共的层数为34层.值得注意的是: ResNet 虽然比 VGGNet 的层深更深, 但是却拥有更低的复杂度, VGG-19 的 FLOPs (multiply-adds) 次数约为 19.6 billion, ResNet 的 FLOPs 如表1所示.(复杂度低的原因主要是去掉了两次全连接层) 残差网络(Residual Network)基于上面的 Plain 网络, 我们向其中添加 shortcut connections(如图3右侧所示), 如此, 便可以将网络转换成残差网络(residual network). 当残差模块的输入和输出的维度相同时(实线), 就是可直接使用公式(1)来建立短接. 当输入和输出的维度不同时(虚线), 我们考虑了两中方法: (A), shorcut 仍然通过恒等连接来实现, 对于升高的那些维度, 直接用0填充, 这个方法不会引入额外的参数; (B), 利用公式(2)的矩阵来使维度匹配(利用1×1卷积实现). 另外, 对于输入输出的特征图谱 size 不同的情况, 我们通过将卷积层的 stride 设置为2来实现. 实现细节(Implementation)training: scale augmentation(image 的最短边被随机放缩到256或480) horizaontal flip 224 random crop per-pixel mean subtracted 标准color augmentation 在每一个卷积层之后, 激活层之前, 都是用了BN 使用了msra初始化方法. 训练时没有使用预训练模型(train from scratch) SGD batch size = 256 lr 从0.1开始,每当 error 停滞(plateaus)时, 缩小1/10 总训练迭代次数为 $60\times 10^4$ weight decay 为 0.0001 momentum 为 0.9 没有使用dropout testing: 10-crop multi-scales: {224, 256, 384, 480, 640} 实验(Experiments)图像分类(Image Classification)不同层的模型结构和参数如表1所示(both plain and residual). 表2的数据显示出较深的34层的 plain 网络相比于它的 residual 版本, 具有更高的错误率. Plain Network为了揭示其中的原因, 我们比较了这两个网络在训练/验证过程中错误率如图4所示. 我们观察到, 对于 plain 版本的网络, 18层的网络的解空间只是34层网络的解空间的一个子集, 但是更深的34层网络却发生了模型退化的问题. 我们认为造成优化困难的原因不太可能是因为梯度消失问题而产生的. 因为这些 plain network 在每一个卷积层之后的应用了 BN, 这就保证了在前向传播过程中的信号具有非零的方差(which ensures forward propagated signals to have non-zero variances). 同时, 我们也验证了在反向传播过程中梯度值保持着健康的归一化. 所以不论是前向传播过程还是反向传播过程, 都没有出现信号消失的现象.因此, 具体是什么原因导致了 plain network 难以优化还有待讨论(The reason for such opti- mization difficulties will be studied in the future). Residual Network接下来我们评估了 ResNet-18 和 ResNet-34 两个网络, 基本的网络结构和对应的 Plain-18 和 Plain-34 相同, 只是在每一对 3×3 的卷积层之间添加了 shortcut connection. 在第一次对比当中(表2和图4右侧), 我们采用了恒等连接和零填充的短接方式, 因此相对于 plain 版本的网络并没有引入新的参数.从表2和图4中, 我们观察到了三个结论: ResNet-34 比 ResNet-18 的错误率更低(说明找到了解空间中另一个更优的解, 而此时 ResNet-34 的解空间和 Plain-34 的解空间是完全相同的). 更重要的是, ResNet-34 不仅在训练数据集上错误率更低, 在验证集上的错误率也更低, 说明确实找到了一个泛化能力更好的解, 而不是因为过拟合. 图4中的 ResNet 相比于 PlainNet, 错误率更低, 说明了 ResNet 的有效性. 对于错误率相当的 Plain-18 和 ResNet-18, ResNet 的收敛速度更快, 说明残差模块的存在确实可以加快模型的训练速度. 恒等连接与映射连接(Identity / Projection Shortcuts)上面我们讨论了一种 parameter-free 的恒等短接的方式, 接下来我们将研究一下引入参数的映射短接(Projection Shortcuts). 在表3中我们给出了三个选项: (A). 使用恒等映射, 如果需要改变输出维度时, 对增加的维度用0来填充, 不会增加任何参数.(这种就是之前讨论的 parameter-free 的恒等短接) (B). 在输入输出维度一致时使用恒等映射, 不一致时使用矩阵映射以保证维度一致, 增加部分参数. (C). 对所有的block均使用矩阵映射, 大量增加参数 如表3所示, 这三种方式相比于对应的 PlainNet 都可以取得较大的精度提升, 我们可以发现, 在效果上 C&gt;B&gt;A，我们认为这是因为在 A 中的 zero-padded dimensions 实际上并没有进行残差学习. 由于 A/B/C 之间的差距比较小，而线性变换需要引进额外的参数, 因此这是一个可以根据实际问题进行权衡的事情.(通常不要用C, 因为增加的参数较多, 且性能提升并不是很明显). Deeper Bottleneck Architectures 接下来, 我们讨论一下本文在 ImageNet 中使用的更深的网络. 为了取得更快的训练速度, 我们将残差网络的 building block 修改成了 bottleneck building block.(如图5所示, 左右两种 block 的复杂度相同). 其中, 1×1 的卷积层负责降维和升维, 使得 3×3 的卷积层需要处理维度更小.对于 bottleneck 结构来说, parameter-free 的恒等短接尤其重要. 如果用矩阵映射替换了 bottleneck 中的恒等短接, 那么因为 shortcuts 需要处理的维度很高, 使得模型的 size 和时间复杂度都会加倍. 因此, 对于 bottlenect 来说, 选择恒等短接可以大大降低模型复杂度. ResNet-50:把 ResNet-34 中的每一个2层的 building block 换成3层的 bottlenect block. ResNet-101/152:在 conv4 阶段使用更多的 bottleneck block.(ResNet-152 在 conv3 也使用了更多的 bottleneck block). 在表4和表5中, 我们将本文的 ResNet 与目前最好的模型进行了对比. 结果显示本文的 ResNet 具有更高的精度. CIFAR-10 and Analysis表6 图6 图7 表7 表8 表9 表10 表11 表12 表13 表14 要点提问怎样理解所谓的残差 $F(x)$ 要比原始目标 $H(x)$ 更容易优化呢?假设我们要学习一种从输入x到输出H(x)的mapping, 最简单的例子, 假设解空间里的函数只有两个，就是在这两个可能的mapping 函数里面选择一个更好的。如果是非resnet的情况，那么给定 $H(5)＝5.1$ 和 $H(5)＝5.2$ 这两个函数映射, 其对应权重参数分别是 $H(x) = wx = \frac{5.1}{5} x$ 和 $H(x) =w x = \frac{5.2}{5} x$ ，这两个函数的w近似的都近似等于1, 或者说一个 $w$ 是另一个 $w$ 的1.04/1.02＝1.0196倍. 也就是说，如果用sgd来选择参数 $w$ 的话，是容易认为两个 $w$ 很像的(对数据不敏感, 导致训练慢，学错)。但是resnet就不同了，在resnet下，原输入输出数据相当于变成了 $H(5)=0.1$ 和 $H(5)=0.2$, 这两个对应的潜在函数变成了 $F(x)= wx = \frac{0.1}{5} x$ 和 $H(x) = wx = \frac{0.2}{5} x$ , 两个 $w$ 的关系变成了一个 $w$ 是另一个 $w$ 的0.2／0.1 ＝ 2倍，所以 $w$ 的选取对于数据集非常敏感了。 这是基于这个原因，resnet里面的参数 $w$ 会更加”准确”反映数据的细微变化。(因此也更容易学到不同数据的特征) 另一方面, 由于恒等连接的存在, 当我们令学得的 $F(x)=0$ 时, 那么就有 $H(x)=x$, 所以如果我们将残差模块拼接在普通的 vgg 网络之后, 最终的模型性能也不会比 vgg 差, 因为后面几层相当于是一种恒等短接, 也可以认为是为模型的性能做到了一种保底措施. 为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5?关于为什么是 $x$ 而不是 $\lambda_i x$,主要是因为如果是 $\lambda_i x$ 的话,梯度里面 就会有一项 $\prod_{i=1}^{L-1}{\lambda_i}$，就是从输出到当前层之间经过的 shortcut上的所有$\lambda_i$相乘，假如$\lambda_i$都大于1那经过多层之后就会爆炸，都小于1就会趋向0而引发梯度消失. 具体公式分析可见下面关于”用简单缩放来替代恒等连接”的讨论 ResNet 到底解决了一个什么问题?既然可以通过初试化和归一化（BN层）解决梯度弥散或爆炸的问题，那Resnet提出的那条通路是在解决什么问题呢？在He的原文中有提到是解决深层网络的一种模型退化问题，但并未明确说明是什么问题！ 今年2月份有篇文章，正好跟这个问题一样。The Shattered Gradients Problem: If resnets are the answer, then what is the question?大意是神经网络越来越深的时候，反传回来的梯度之间的相关性会越来越差，最后接近白噪声。因为我们知道图像是具备局部相关性的，那其实可以认为梯度也应该具备类似的相关性，这样更新的梯度才有意义，如果梯度接近白噪声，那梯度更新可能根本就是在做随机扰动。有了梯度相关性这个指标之后，作者分析了一系列的结构和激活函数，发现resnet在保持梯度相关性方面很优秀（相关性衰减从 到了 ）。这一点其实也很好理解，从梯度流来看，有一路梯度是保持原样不动地往回传，这部分的相关性是非常强的。 ResNet的其它变体ResNeXtDenseNet详见DenseNet]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>网络架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度消失和梯度爆炸问题详解]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1.为什么使用梯度下降来优化神经网络参数？反向传播（用于优化神网参数）：根据损失函数计算的误差通过反向传播的方式，指导深度网络参数的更新优化。 采取反向传播的原因：首先，深层网络由许多线性层和非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数$f(x)$(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。 我们最终的目的是希望这个非线性函数很好的完成输入到输出之间的映射，也就是找到让损失函数取得极小值。所以最终的问题就变成了一个寻找函数最小值的问题，在数学上，很自然的就会想到使用梯度下降来解决。 2.梯度消失、爆炸会带来哪些影响举个例子，对于一个含有三层隐藏层的简单神经网络来说，当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。 3.产生的原因以最简单的网络结构为例，加入有三个隐藏层，每层的神经元个数都是1，且对应的非线性函数为$y_i = \sigma(z_i)=\sigma(w_i x_i + b_i)$（其中 $\sigma$ 为某个激活函数）如下图： 现在假设我们需要更新参数 $b_1$ ，那么我们就要求出损失函数对参数 $b_1$ 的导数，根据链式法则，可以写成下面这样： 而对于激活函数，之前一直使用Sigmoid函数，其函数图像成一个S型，如下所示，它会将正无穷到负无穷的数映射到0~1之间： S(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x+1} 当我们对Sigmoid函数求导时，得到其结果如下： S(x) = S(x)(1-S(x))由此可以得到它Sigmoid函数图像，呈现一个驼峰状（很像高斯函数），从求导结果可以看出，Sigmoid导数的取值范围在0~0.25之间，而我们初始化的网络权值$|w|$通常都小于1，因此，当层数增多时，小于0的值不断相乘，最后就导致梯度消失的情况出现。同理，梯度爆炸的问题也就很明显了，就是当权值$|w|$过大时，导致 $|\sigma’(z)w| &gt; 1$，最后大于1的值不断相乘，就会产生梯度爆炸。 Sigmoid函数求导图像 4.解决办法梯度消失和梯度爆炸本质上是一样的，都是因为网络层数太深而引发的梯度反向传播中的连乘效应。 解决梯度消失、爆炸主要有以下几种方案： 4.1 换用Relu、LeakyRelu、Elu等激活函数ReLu：让激活函数的导数为1 LeakyReLu：包含了ReLu的几乎所有有点，同时解决了ReLu中0区间带来的影响 ELU：和LeakyReLu一样，都是为了解决0区间问题，相对于来，elu计算更耗时一些（为什么） 具体可以看关于各种激活函数的解析与讨论 4.2 BatchNormalizationBN本质上是解决传播过程中的梯度问题，具体待补充完善，查看BN 4.3 ResNet残差结构具体待补充完善，查看ResNet 4.4 LSTM结构LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）”，具体看LSTM基本原理解析 4.4 预训练加finetunning此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。 训练完成后，再对整个网络进行“微调（fine-tunning）”。 此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在基本都是直接拿imagenet的预训练模型直接进行finetunning。 4.5 梯度剪切、正则这个方案主要是针对梯度爆炸提出的，其思想是设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内。这样可以防止梯度爆炸。 另一种防止梯度爆炸的手段是采用权重正则化，正则化主要是通过对网络权重做正则来限制过拟合，但是根据正则项在损失函数中的形式： 可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种激活函数整理总结]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[常用激活函数及其导数 激活函数 形式 导数形式 Sigmoid $f(x) =\frac{1}{1+e^{-x}}$ $f’(x)(1-f(x))$ Tanh $f(x) = tanh(x)= \frac{e^x-e^{-x}}{e^x+e^{-x}}$ $f’(x) = 1-(f(z))^2$ ReLU $f(x)=max(0,x)=\begin{cases} 0 &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}$ $f’(x)=\begin{cases} 0 &amp; x\leq 0 \\ 1 &amp; x&gt;0 \end{cases}$ Leaky ReLU $f(x)=max(0.001 x,x)=\begin{cases} 0.001x &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}a$ $f(x)=max(0.001 x,x)=\begin{cases} 0.001 &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$ PReLU $f(x)=max(\alpha x,x)=\begin{cases} \alpha x &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}$ $f(x)=max(\alpha x,x)=\begin{cases} \alpha &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$ RReLU PReLU中的 $\alpha$ 随机取值 ELU $f(x) = \begin{cases} x &amp; x \geq 0 \\ \alpha(e^x - 1) &amp; x&lt;0 \end{cases}$ $f(x) = \begin{cases} 1 &amp; x \geq 0 \\ \alpha e^x &amp; x&lt;0 \end{cases}$ Maxout $f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$ $f(x) = max(w_1, w_2)$ 常用激活函数及其导数的图像Sigmoid Tanh ReLU LeakyReLU PReLU RReLU ELU 为什么需要激活函数标准说法这是由激活函数的性质所决定来, 一般来说, 激活函数都具有以下性质: 非线性: 首先,线性函数可以高效可靠对数据进行拟合, 但是现实生活中往往存在一些非线性的问题(如XOR), 这个时候, 我们就需要借助激活函数的非线性来对数据的分布进行重新映射, 从而获得更强大的拟合能力. (这个是最主要的原因, 其他还有下面这些性质也使得我们选择激活函数作为网络常用层) 可微性: 这一点有助于我们使用梯度下降发来对网络进行优化 单调性: 激活函数的单调性在可以使单层网络保证网络是凸优化的 $f(x) \approx x:$ 当激活满足这个性质的时候, 如果参数初值是很小的值, 那么神经网络的训练将会很高效(参考ResNet训练残差模块的恒等映射); 如果不满足这个性质, 那么就需要用心的设值初始值( 这一条有待商榷 ) 如果不使用激活函数, 多层线性网络的叠加就会退化成单层网络,因为经过多层神经网络的加权计算，都可以展开成一次的加权计算 更形象的解释对于一些线性不可分的情况, 比如XOR, 没有办法直接画出一条直线来将数据区分开, 这个时候, 一般有两个选择. 如果已知数据分布规律, 那么可以对数据做线性变换, 将其投影到合适的坐标轴上, 然后在新的坐标轴上进行线性分类 而另一种更常用的办法, 就是使用激活函数, 以XOR问题为例, XOR问题本身不是线性可分的, https://www.zhihu.com/question/22334626 用ReLU解决XOR问题.首先, XOR问题如下所示: $x_1$ $x_2$ y 1 0 1 0 1 1 1 1 0 0 0 0 首先构造一个简单的神经网络来尝试解决XOR问题, 网络结构如下图所示: 先来看看不使用激活函数时的情况, 当不使用激活函数时, 整个网络的函数表达式如下所示: y = f(x_1, x_2; W, c, w ,b) = [w_1, w_2] \bigg( \bigg[\begin{matrix} W_{11} & W_{12} \\ W_{21} & W_{22} \end{matrix} \bigg] \Big[\begin{matrix} x_1 \\ x_2 \end{matrix} \Big]+ \Big[\begin{matrix} c_1 \\ c_2 \end{matrix} \Big] \bigg) + b = (w^TW^T)x + (w^Tc+b) = w'^Tx+b'可以看到, 多层无激活函数的网络叠加, 首先是会退化成单层网络, 而对于单层网络, 求解出来的参数 $w’$ 和 $b’$ 无法对非线性的数据进行分类. 再来看看进入ReLU以后, 是如何解决XOR问题的, 首先, 引入后的公式如下所示: y = f(x_1, x_2; W, c, w ,b) = [w_1, w_2] max \bigg(0 , \bigg[\begin{matrix} W_{11} & W_{12} \\ W_{21} & W_{22} \end{matrix} \bigg] \Big[\begin{matrix} x_1 \\ x_2 \end{matrix} \Big]+ \Big[\begin{matrix} c_1 \\ c_2 \end{matrix} \Big] \bigg) + b可以看到, 此时函数是无法化简, 因为此时引入了非线性的ReLU函数, 于是 ,就可以求得一个参数组合${w,W,c,b}$ 使得对于特定的输入$x_1, x_2$ ,能够得到正确的分类结果 $y$. 至于这个参数组合具体是什么, 这是需要通过梯度下降来不断学习的, 假如我们现在找到了一组参数如下(当然不一定是最优的), 来看看这组参数具体是如何解决XOR问题的: W=\bigg[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \bigg]c =\Big[ \begin{matrix} 0 \\ -1 \end{matrix} \Big]w =\Big[ \begin{matrix} 1 \\ -1 \end{matrix} \Big]b = 0然后, 分别将4种 $x_1, x_2$的值代入上式, 可以得到, y的值如下: $x_1$ $x_2$ 计算过程 y 1 0 $[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 1 \\ 0 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$ 1 0 1 $[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 0 \\ 1 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$ 1 1 1 $[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 1 \\ 1 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$ 0 0 0 $[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 0 \\ 0 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$ 0 关于各个激活函数的比较和适用场景神经元饱和问题: 当输入值很大或者很小时, 其梯度值接近于0, 此时, 不管从深层网络中传来何种梯度值, 它向浅层网络中传过去的, 都是趋近于0的数, 进而引发梯度消失问题 zero-centered: 如果数据分布不是zero-centered的话就会导致后一层的神经元接受的输入永远为正或者永远为负, 因为 $\frac{\partial f}{\partial w} = x$ , 所以如果x的符号固定,那么 $\frac{\partial f}{\partial w}$ 的符号也就固定了, 这样在训练时, weight的更新只会沿着一个方向更新, 但是我们希望的是类似于zig-zag形式的更新路径 (关于非0均值问题, 由于通常训练时是按batch训练的, 所以每个batch会得到不同的信号, 这在一定程度上可以缓解非0均值问题带来的影响, 这也是ReLU虽然不是非0 均值, 但是却称为主流激活函数的原因之一) 激活函数 优势 劣势 适用场景 Sigmoid 可以将数据值压缩到[0,1]区间内 1. 神经元饱和问题 2.sigmoid的输出值域不是zero-centered的 3. 指数计算在计算机中相对来说比较复杂 在logistic回归中有重要地位 Tanh 1. zero-centered: 可以将 $(-\infty, +\infty)$ 的数据压缩到 $[-1,1]$ 区间内 2.完全可微分的，反对称，对称中心在原点 1. 神经元饱和问题 2. 计算复杂 在分类任务中，双曲正切函数（Tanh）逐渐取代 Sigmoid 函数作为标准的激活函数 ReLU 1. 在 $(0,+\infty)$ ,梯度始终为1, 没有神经元饱和问题 2. 不论是函数形式本身,还是其导数, 计算起来都十分高效 3. 可以让训练过程更快收敛(实验结果表明比sigmoid收敛速度快6倍) 4. 从生物神经理论角度来看, 比sigmoid更加合理 1. 非zero-centered 2. 如果输入值为负值, ReLU由于导数为0, 权重无法更新, 其学习速度可能会变的很慢,很容易就会”死”掉(为了克服这个问题, 在实际中, 人们常常在初始化ReLU神经元时, 会倾向于给它附加一个正数偏好,如0.01) 在卷积神经网络中比较主流 LeakyReLU 1. 没有神经元饱和问题 2. 计算高效 3. 收敛迅速(继承了ReLU的优点) 4. 神经元不会”死”掉(因为在负值时, 输出不为0, 而是x的系数0.001) PReLU 1. 没有神经元饱和问题 2. 计算高效 3. 收敛迅速(继承了ReLU的优点) 4. 神经元不会”死”掉(因为在负值时, 输出不为0, 而是x的系数 $\alpha$ ) 5. 相对于Leaky ReLU需要通过先验知识人工赋值, PReLU通过迭代优化来自动找到一个较好的值, 更加科学合理, 同时省去人工调参的麻烦 ELU 1. 拥有ReLU所有的优点 2. 形式上更接近于zero-centered 3. 在面对负值输入时,更加健壮 1. 引入了指数计算, 使计算变的复杂 Maxout 1. 跳出了点乘的基本形式 2. 可以看作是ReLU和Leaky ReLU 的一般化形式 3. linear Regime(啥意思?)! 4. 在所有输入范围上都没有神经元饱和问题 5. 神经元永远不会”死”掉 6. 拟合能力非常强，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个maxout节点就可以拟合任意的凸函数了(相减)，前提是”隐含层”节点的个数可以任意多 1. 使得神经元个数和参数个数加倍, 导致优化困难 其他要点sigmoid 和softmax区别sigoid是将一个正负无穷区间的值映射到(0,1)区间, 通常用作二分类问题,而softmax把一个k维的实值向量映射成一个$(b_1,b_2,…,b_k)$ ,其中$b_i$为一个0~1的常数, 且它们的和为1, 可以看作是属于每一类的概览,通常用作多分类问题. 在二分类问题中, sigmoid和softmax是差不多的, 都是求交叉熵损失函数, softmax可以看作是sigmoid的扩展, 当类别k为2时, 根据softmax回归参冗余的特点, 可以将softmax函数推导成sigmoid函数 https://www.jianshu.com/p/22d9720dbf1a 其他更多激活函数https://www.jiqizhixin.com/articles/2017-10-10-3 总结 优先使用ReLU, 同时要谨慎设置初值和学习率 ( 实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁 ) 尝试使用LeakyReLU/PReLU/Maxout/ELU等激活函数 可以试下tanh, 但是一般不会有太好的结果 不要使用sigmoid]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch-Normalization深入解析]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[BN:总的来说，BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance每次都不同，引入了随机噪声，在一定程度上对模型起到了正则化的效果。 Normalization字面上意思就是标准化,也就是对输入的数据做标准化,可以用下面的公式表示(这里 $x_i$ 代表输入数据, n代表训练集大小): \mu = \frac{1}{n}\sum_{i=1}^{n}{x_i}\sigma^2 = \frac{1}{n} \sum_{i=1}^{n}{(x_i - \mu)}\hat x_i = \frac{x_i - \mu}{\sqrt{\sigma^2}+\varepsilon}以上可以看出, 标准化以后的数据服从均值为0,方差为1的正态分布 为什么要进行Normalization?在介绍BN之前,先说说为什么要进行Normalization 在神经网络中, 数据分布对训练会产生影响. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了. 如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生. Normalization的效果: 如上图,当没有进行normalizatin时,数据的分布是任意的,那么就会有大量的数据处在激活函数的敏感区域外, 对这样的数据分布进行激活后, 大部分的值都会变成1或-1,造成激活后的数据分布不均衡,而如果进行了Normallizatin, 那么相对来说数据的分布比较均衡,如下图所示: 一句话总结就是: 通过Normalization让数据的分布始终处在激活函数敏感的区域 BN的提出背景https://zhuanlan.zhihu.com/p/34879333 Internal Covariate ShiftCovariate [kʌ’veərɪrt] 什么是Internal Covariate Shift: 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。 带来了什么问题: 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低 网络的训练过程容易陷入梯度饱和区,减缓网络收敛速度 如何减缓Internal Covariate Shift: (1)白化(PCA白化和ZCA白化): 使得输入特征分布具有相同的均值与方差 取出特征之间的相关性 通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛 白化缺点: 白化过程计算成本太高，并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作； 白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。 于是就提出了BN 什么是Batch Normalization传统的Normalization使用的均值和方差是整个训练集的均值和方差, 并且只对输入层的数据做归一化, 而Batch Normalization按字面意思就是对每一批数据进行归一化, 同时会对每一层输入做归一化, 所以, 首先要将传统的标准化中的n改为m, m表示一个batch的大小,如下所示: \mu = \frac{1}{m}\sum_{i=1}^{m}{x_i}\sigma^2 = \frac{1}{m} \sum_{i=1}^{m}{(x_i - \mu)}\hat x_i = \frac{x_i - \mu}{\sqrt{\sigma^2}+\varepsilon}传统的Normalization直接使用了减均值除方差的方式来进行标准化, 但是, 这样一概而全的方法未必对所有数据来说就是最优的, 比如数据本身就不对称, 或者激活函数未必对方差为1的数据有最好的效果, 所以, BN的想法是在传统标准化之后再加上一个线性变换,如下所示: \hat y_i = \gamma \hat x_i + \beta其中,$\gamma$ 和 $\beta$ 是两个需要学习的参数, 可以看出, BN的本质就是利用参数优化来改变一下数据分布的方差大小和均值的位置. BN的优点（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度 （2）BN使得模型对初始化方法和网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定 （3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题 （4）BN具有一定的正则化效果 原因如下: （1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度 BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，更加有利于优化的过程,提高整个神经网络的学习速度。 （2）BN使得模型对初始化方法和网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定 在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛…https://zhuanlan.zhihu.com/p/34879333 （3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题 在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 $\gamma$ 与 $\beta$ 又让数据保留更多的原始信息。 （4）BN具有一定的正则化效果 在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。 另外，原作者也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。 BN的具体实现及其反向传播https://www.jianshu.com/p/4270f5acc066https://zhuanlan.zhihu.com/p/27938792 在Caffe2实现中, BN层需要和Scale层配合使用, 其中, BN专门用于做归一化操作, 而后续的线性变换层, 会交给Scale层去做. 训练阶段:在训练时利用当前batch的mean和variance来进行BN处理, 同时使用滑动平均的方式不断的更新global 的mean和variance, 并将其存储起来. 测试阶段:在预测阶段, 直接使用模型存储好的均值和方差进行计算 使用BN时应注意的问题 训练/测试阶段的使用 在实际应用中, 均值和方差是通过滑动平均方法在训练数据集上得到的, 如果换了其他的任务或数据集, 建议先finetune之后再使用BN层存储的均值和方差. 同时, 注意训练时的均值和方差是来自于当前batch的. 隐藏层中BN的数据大小 在卷积网络隐藏层中, BN的大小不单单是batch, 而是batch和特征相应图大小的乘积. 也就是说, 在隐藏层, 层的输入是上一层的输出, 也就是上一层的神经元个数, 而对于上一层来说, 如果输出的特征相应图大小为 $w\times h$ , 那么上一层的神经元个数就应该是 $b\times w \times h$, 其中,b是指batch的大小]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种初始化方法整理总结]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E5%90%84%E7%A7%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[初始化方法概览 初始化方法 说明 均匀分布 高斯分布 初始化为服从 $N(\mu, \sigma ^2)$ 的高斯分布 xavier msra 双线性初始化 初始化方法详细说明Xavier(‘zeiviə’)核心理念是: 优秀的初始化方法应该使得各层的激活值和状态梯度在传播过程中的方差保持一致 也就是说, 假如网络某一层函数表示如下: z^{(i)} = W^{(i)} x^{(i-1)}x^{(i)} = f(z^{(i)}), i=1,2,..那么, 好的初始化应该可以满足下面的条件: \forall (i,j), Var(x^i) = Var(x^j)\forall (i,j), Var(\frac{\partial Loss}{\partial z^i}) = Var(\frac{\partial Loss}{\partial z^j})再继续推导之前, 需要先提出以下假设: 首先,输入数据来说,其均值和方差应满足: $E(x)=0, Var(x)=1$ (通过BN,较容易满足) 权重矩阵 $W$ 和 网络输入 $x$ 互相独立 每层输入的每个特征方差一样 激活函数对称: 这主要是为了满足均值为0的假设 激活函数是线性的, 也就是说其导数为1 初始时, 状态值落在激活函数的线性区域, 即此时导数为1 根据状态梯度和参数的梯度公式($l$ 代表层数): \frac{\partial Loss}{\partial z_k^i} = f'(z_k^i)(W_{.,k}^{i+1})^T \frac{\partial Loss}{\partial z^{i+1}}\frac{\partial Loss}{\partial w_{l,k}^i} = h_l^{i-1} \frac{\partial Loss}{\partial z_k^i}// TODO 推导之后, 有: 为了保证前向传播和反向传播时每一层的方差一致, 则有下面的公式成立: \forall i, n_i Var[W^i] = 1\forall i, n_{i+1} Var[W^i] =1式中, $n_i, n_{i+1}$ 分别为当前第i层和i+1层的输入节点个数, 但是, 在实际当中, 每一层的输入个数往往不相等, 于是为了均衡考量, 最终给出的权重方差为: \forall, Var[W^i] = \frac {2}{n_i + n_{i+1}}再由概率统计中均匀分布方差的性质反推,可以得到Xavier最终的初始化分布如下: W \sim U\Big[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}} \Big]Xavier在Caffe中的具体实现: 123456789101112131415161718192021222324template &lt;typename Dtype&gt;class XavierFiller : public Filler&lt;Dtype&gt; &#123; public: explicit XavierFiller(const FillerParameter&amp; param) : Filler&lt;Dtype&gt;(param) &#123;&#125; virtual void Fill(Blob&lt;Dtype&gt;* blob) &#123; CHECK(blob-&gt;count()); int fan_in = blob-&gt;count() / blob-&gt;num(); int fan_out = blob-&gt;count() / blob-&gt;channels(); Dtype n = fan_in; // default to fan_in if (this-&gt;filler_param_.variance_norm() == FillerParameter_VarianceNorm_AVERAGE) &#123; n = (fan_in + fan_out) / Dtype(2); &#125; else if (this-&gt;filler_param_.variance_norm() == FillerParameter_VarianceNorm_FAN_OUT) &#123; n = fan_out; &#125; Dtype scale = sqrt(Dtype(3) / n); caffe_rng_uniform&lt;Dtype&gt;(blob-&gt;count(), -scale, scale, blob-&gt;mutable_cpu_data()); CHECK_EQ(this-&gt;filler_param_.sparse(), -1) &lt;&lt; "Sparsity not supported by this Filler."; &#125;&#125;; 可以看出, Caffe的Xavier实现有三种选择: (1) 初始化方法讨论为什么不能全0初始化首先, 在神经网络中, 每一层中的任意神经元都是同构的, 它们拥有相同的输入, 如果再将参数全部初始化为同样的值(如0), 那么输出也就是相同的, 反过来它们的梯度也都是相同的. 那么无论是前向传播还是反向传播的取值都是完全相同的, 那么每一个神经元都是基于input做相同的事情, 这样一来, 不同的神经元根本无法学到不同的特征, 这样就失去网络学习特征的意义了 高斯 msra]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[被忽视的Patition算法]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E8%A2%AB%E5%BF%BD%E8%A7%86%E7%9A%84Patition%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[如果你学习过算法，那么肯定听说过快速排序的大名，但是对于快速排序中用到的 partition 算法，你了解的够多吗？或许是快速排序太过于光芒四射，使得我们往往会忽视掉同样重要的 partition 算法。 Partition 可不只用在快速排序中，还可以用于Selection algorithm（在无序数组中寻找第K大的值）中。 Partition实现用 Two Pointers 的思想，保持头尾两个指针向中间扫描，每次在头部找到大于pivot的值，同时在尾部找到小于pivot的值，然后将它们做一个交换，就可以一次把这两个数字放到最终的位置。一种比较明智的写法如下： 123456789101112int partition(vector&lt;int&gt; &amp; arr, int low, int high)&#123; int pivot = arr[low]; while(low&lt;high)&#123; //比较时如果少了等于号,就有可能会陷入死循环,两个重复的数不断交换 while(low&lt;high &amp;&amp; arr[low]&lt;=pivot) low++; arr[high] = arr[low]; while(low&lt;high &amp;&amp; arr[high]&gt;=pivot) high--; arr[low] = arr[high]; &#125; arr[low] = pivot; return low;&#125; 上面的算法虽然没有显式用的swap,但实际上也相当于进行了swap操作,如下图所示: Partition应用快排1234567void quick_sort(vector&lt;int&gt; &amp; arr, int low, int high)&#123; if(low &gt;= high) return; mid = partition(arr,low,high); if(mid&gt;low) quick_sort(arr, low,mid-1); if(mid&lt;high) quick_sort(arr,mid+1, high);&#125; 复杂度:$O(nlogn)$ 寻找无序数组中第K大的值首先用 partition 将数组分为两部分，得到分界点下标 pos，然后分三种情况： pos == k-1，则找到第 K 大的值，arr[pos]； pos &gt; k-1，则第 K 大的值在左边部分的数组。 pos &lt; k-1，则第 K 大的值在右边部分的数组。 123456789101112int find_kth_number(int k)&#123; int low = 0; int high = arr.size()-1; while(low&lt; high)&#123; pos = partition(arr,low,high); if(pos==k-1) return arr[k-1]; else if (pos &lt; k-1) low = pos+1; else high = pos-1; &#125;&#125; 时间复杂度 $O(n)$ 分析:考虑最坏情况下，每次 partition 将数组分为长度为 N-1 和 1 的两部分，然后在长的一边继续寻找第 K 大，此时时间复杂度为 O(N^2 )。不过如果在开始之前将数组进行随机打乱，那么可以尽量避免最坏情况的出现。而在最好情况下，每次将数组均分为长度相同的两半，运行时间 T(N) = N + T(N/2)，时间复杂度是 O(N)。 Partition 进阶接下来先考虑这样一个问题，给定红、白、蓝三种颜色的小球若干个，将其排成一列，使相同颜色的小球相邻，三种颜色先后顺序为红，白，蓝。这就是经典的 Dutch national flag problem。 我们可以针对红，蓝，白三种颜色的球分别计数，然后根据计数结果来重新放球。不过如果我们将问题进一步抽象，也就是说将一个数组按照某个target值分为三部分，使得左边部分的值小于 target，中间部分等于 target，右边部分大于 target，这样就不能再用简单的计数来确定排序后的结果。这时候，就可以用到另一种 partition 算法： three-way-partition 。它的思路稍微复杂一点，用三个指针将数组分为四个部分，通过一次扫描最终将数组分为 &lt;，=，&gt; 的三部分，如下图所示:]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深度学习轻松学》]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[第七章 网络结构7.1 关于网络结构，我们更关心什么模型结构的关注点有以下几个： 模型的总深度：代表了模型潜在的学习能力和模型的复杂度。 模型的参数总量：同样代表了模型的学习能力和复杂的复杂度。 模型前向计算所需的内存量：模型的大小 7.2 网络结构的演化7.2.1 VGG哲学AlexNet：采用类7*7的卷积核 VGGNet：用3个3×3个卷积核代替了7×7的卷积核，效果差不多，但是参数量降低了。同时增加了非线性层，过去一个卷积层加一个非线性，现在替换成了三个卷积核加三个非线性。另外，在VGG中，卷积层的操作不会改变输入数据的维度，通常会通过padding来维持输出的大小。而只通过pooling层来改变输出的大小。 提问： 卷积的时候为什么要进行padding？ 回答：对于一些通过卷积减小维度的模型来说，对于不同的输入尺度，卷积后的输出维度各不一样，所以模型不容易适配更多的场景，而如果只用pooling层改变场长宽维度，整体模型的维度计算就方便了许多。 7.2.2 GooLeNet：丰富模型层的内部结构NIN网络和Inception Module这类结构非常看中模型在局部区域的拟合能力。它们认为：一张图像通常具有总体特征和细节特征这两类特征，一般小卷积核能够更好的捕捉一些细节特征，随着深层网络的小卷积不断计算下去，总体特征也会慢慢的被提炼出来，但是这样存在一个问题，那就是在如果只采用小卷积，那么网络结构的前段一般只有细节特征，后段才慢慢有一些总体特征，而我们希望这两方面的特征总是能够一起发挥作用，因此，上面的两种模型考虑采用更多不同尺寸的卷积核来提取特征，并把这些特征连接起来，一起送到后面的网络中去计算，使得网络可以获取到更多的特征信息。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《百面机器学习》]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[第一章 特征工程1. 特征归一化7. 图像数据不足时的处理方法在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？一个模型的信息来源主要有两个方面： 训练数据中蕴含的信息 模型形成过程中（包括构造、学习、推理等），人们提供的先验信息 当训练数据不足时，说明模型从原始数据中获取的信息比较少，这种情况下要想保证模型的效果，就需要更多的先验信息 先验信息可以作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件 先验信息也可以直接施加在数据集上，让其展现处更多的、更有用的信息、以利于后续模型的训练和学习。 具体到图像分类任务上，训练数据不足带来的问题主要表现在 过拟合方面。所以，对应的处理方法大致分为两类： 基于模型的方法：采用降低过拟合风险的措施，包括简化模型（如将非线性简化成线性）、添加约束项以缩小假设空间（如L1和L2正则化）、集成学习、Dropout超参数等 基于数据的方法，主要通过数据扩充（Data Augmentation），即根据一些先验知识，在保持特定信息的前提下，对原始数据进行适合变换以达到扩充数据集的效果。 在图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换： 观察角度：一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等 噪声扰动：椒盐噪声、高斯白噪声 颜色变换：在RGB颜色空间上进行主成分分析 其他：亮度、清晰度、对比度、锐度 其他扩充数据方法：特征提取——&gt;在图像的特征空间内进行变换：数据扩充or上采样技术，如SMOTE（Synthetic Minority Over-sampling Technique)。 最后，迁移学习或者用GAN合成一些新样本也可帮助解决数据不足问题。 第三章 经典算法1. 支持向量机知识点: SVM模型推导, 核函数, SMO(Sequential Minimal Optimizaiton)算法 问题1: 在空间上线性可分的两类点, 分别向SVM分类的超平面上做投降, 这些点在超平面上的投影仍然是线性可分的吗首先明确下题目概念: 当前的空间已经是线性可分的空间,有可能是样本空间,也有可能是映射后的特征空间. 题目问的是将当前空间中的点全部投影都当前的分类超平面上, 提问在分类超平面上投影的点是否仍然线性可分. 先说结论: 对于任意线性可分的两组点, 它们在SVM分类的超平面上的投影都是线性不可分的 解释一(反证法直观推导): 首先根据拉格朗日对偶优化问题的公式和KKT条件要求,可以知道, SVM的分类结果仅仅依赖于支持向量, 那么此时我们假设存在一个SVM分类超平面使所有支持向量在该超平面上的投影依然线性可分, 那么根据初等几何只是我们可以知道, 两个类别中距离最近的两个点, 它们连线的中垂线所组成的新的超平面是相较于当前超平面更优的解. 这与我们的假设相矛盾, 故线性可分的两组点, 投影到超平面上以后是线性不可分的.(具体可画图或看p53) 解释二(超平面分离定理,Separatin Hyperplane Theorem, SHT): STH定理描述: 对于不想交的两个凸集, 存在一个超平面, 将两个凸集分离. 对于二维的情况, 两个凸集间的距离最短两点连线的中垂线就是一个将它们分离的超平面 根据此定理, 我们先对线性可分的这两组点求格子的凸包, 而SVM求得的超平面就是这两个凸包上距离最短的两点连线的中垂线,根据凸包的性质容易知道, 凸包上的点要么是样本点, 要么是两个样本点之间连线上的点, 那么, 两个凸包之间距离最短的两个点可以分为三个情况: 1)两边都是样本点, 2)两边都不是样本点, 3)一边是一边不是. 不论对于哪种情况, 当对中垂线(超平面)投影后, 两类点都是线性不可分的(具体可画图或者看p54) 问题2: 是否存在一组参数使SVM训练误差为0?问题详细描述: 一个使用高斯核( $K(x,z) = e^{-| x- z|^2/\gamma^2})$ 训练的SVM中, 试证明若给定训练集中 不存在两个点在同一个位置 (如果在同一个位置,则这两个点不可分), 则存在一组参数 $\vec \alpha$ 和参数 $\gamma$, 使得SVM的训练误差为0. 结论:存在(可想象成是过拟合) 公式证明: //TODO 以上推导证明可以看出, 对于任意样本的预测结果 $f(\vec x^{(i)})$, 与样本真实标签 $y^{(i)}$ 的距离都小于1 , 因此, 当训练样本为正例时, 由于 $y^{(i)}=1$, 则必有 $f(\vec x^{(i)}) &gt;0 $, 样本被预测为正例. 负例也是同理. 因此所有样本的类别都被正确预测, 训练误差为0. 问题3: 训练误差为0的SVM分类器一定存在吗问题详细描述: 虽然在问题2中找到了一组参数使得SVM的训练误差为0, 但是这组参数不一定是满足SVM条件的一个解, 在实际训练一个 不加入松弛变量 的SVM模型时, 是否能保证得到的SVM分类器满足训练误差为0呢? 结论: 存在 上一题找到了一组参数使得SVM的分类器训练误差为0, 但是训练误差为0的参数并不一定是SVM模型的一个解, 它还需要满足限制条件 $y^{(i)} f(x^{(i)}) \geq 1$ . 因为SVM模型中解的限制条件为 $y^{(i)} f(x^{(i)}) \geq 1$ (等号为支持向量样本点) , 而上题我们只得到了 $y^{(i)} f(x^{(i)}) &gt; 0$, 因此需要找到另一组参数满足更强的条件 // TODO公式推导 根据以上推导过程, 可以找到一个SVM的解, 同时一定使模型分类误差为0 问题4: 加入松弛变量的SVM的训练误差可以为0吗结论: 不一定能得到训练误差为0的模型(但不是一定不能, 具体看问题) 这是由于加入松弛变量后, 优化目标变成了 $\min_{\vec w, b, x_i^{(i)}} \frac{1}{2} |\vec w |^2_2 + C \sum_{i=1}^{m} x_i^{(i)}$ ,并不再是仅仅使训练误差最小, 同时还会考虑后面的惩罚项, 而当C的取值较小时, 一个带有训练误差, 但参数项较小的点将会称为更优的结果. 第七章 优化算法1. 有监督学习的损失函数损失函数定义了模型的评估指标. 不同的损失函数优化难度不同, 最终得到的模型参数也不同, 针对具体的问题需要选取合适的损失函数. 问题: 有监督学习涉及的损失函数有哪些? 请列举并简述它们的特点Hinge损失: L_{hinge}(f,y) = max{0, 1-fy}在 $fy=1$ 处不可导, 因此不能用梯度下降法优化 Logistic损失 平方损失 交叉熵损失 其他更多: 各种损失函数深入解析 2. 机器学习中的优化问题问题: 机器学习中的优化问题, 哪些是凸优化问题, 哪些是非凸优化问题? 请各举一个例子凸函数的定义: 函数 $L(\dot)$ 是凸函数当且仅当对定义域中的任意两点 $x,y$ 和任意实数 $\lambda \in [0,1]$ , 总有: L(\lambda x +(1-\lambda) y) \leq \lambda L(x) + (1-\lambda)L(y)上面不等式的一个直观解释是, 凸函数曲面上任意两点连接而成的线段, 其上的任意一点都不会处于该函数曲面的下方 凸优化问题举例: 逻辑回归问题 证明: … 非凸优化问题举例: 主成分分析 证明: … 3. 经典优化算法4. 梯度验证5. 随机梯度下降法经典的梯度下降法: 在每次迭代时需要使用所有的训练数据, 求完所有训练数据的损失后, 取平均值进行更新 问题: 当训练数据量特别大时, 经典的梯度下降法存在什么问题? 需要做如何改进?//TODO 补充缺少的公式 在机器学习中, 优化问题的目标函数通常可以表示成: 其中...//TODO 因此...//TODO 我们希望能够找到平均损失最小的模型参数, 也就是求解优化问题:经典的梯度下降法采用所有训练数据的平均损失来近似目标函数, 即 其中 $M$ 是训练样本的个数, 模型参数的更新公式为:因此, 经典的梯度下降法在每次对模型参数进行更新时, 需要遍历所有的训练数据. 当 $M$ 很大时, 这需要很大的计算量, 耗费很长的计算时间, 在实际应用中不可行 改进: 随机梯度下降法(SGD): 每次使用单个训练数据对模型进行更新 计算速度快, 内存开销小. 但由于每步接受的信息量有限, 随机梯度下降法对梯度的估计常常出现偏差, 造成目标函数曲线收敛的很不稳定, 伴有剧烈波动, 有时甚至出现不收敛的情况. 小批量梯度下降法(Mini-Batch GD): 每次选取 $m$ 个训练样本进行更新 对于小批量梯度下降法的使用, 有以下三点需要注意的地方: 如何选取参数 $m$ ? 一般挑选2的幂次, 以便充分利用矩阵运算操作 如何挑选这 $m$ 个训练数据? 为了避免数据的特定顺序给算法收敛带来的影响, 一般会在每次遍历训练数据之前, 先对所有的数据进行随机排序, 然后在每次迭代时按顺序挑选 $m$ 个训练数据直至遍历完所有的数据 如何选取学习率 $\alpha$ ? 为了加快收敛速率,, 同时提高求解精度, 通常会采用衰减学习速率的方案: 一开始算法采用较大的学习速率, 当误差曲线进入平台期后, 减小学习速率做更精细的调整 6. 随机梯度下降法的加速问题1: 为什么随机梯度下降法偶尔会失效, 无法给出满意的训练结果?对于深度学习中的优化问题来说, 当遇到类似于 “山谷” 和 “鞍点” 两种地形时, 随机梯度下降法往往无法取得好的效果. 具体来说, 山谷在三维空间中就像是狭长的山间小道, 左右两边是峭壁, 因此, 在这类地形中, 准确的梯度方向是沿着山道向下, 稍有偏离就会撞向山壁, 而一旦撞向山壁, 粗糙的梯度估计就会使得更新在两山壁之间来回反弹震荡, 不能沿着山道方向迅速下降, 导致收敛不稳定和收敛速度慢. 而在鞍点处, 三维性状看起来想一个马鞍, 一个方向上两头翘起, 另一个方向上两头垂下, 而中心区域是一片近乎水平的平地, 此处梯度为0, 随机梯度下降法无法准确察觉出梯度的微小变化, 结果就会导致更新停滞 问题2: 为了改进随机梯度下降法, 研究者都提出了哪些变种方法?// TODO 补充完善 动量方法: 前进步长 $\nu_t$ 有两部分组成, 一是学习速率 $\eta$ 乘以当前估计的梯度 $g_t$ ; 二是带衰减的前一次步伐 $\nu_{t-1}$. 这里, 惯性就体现在对前一次步伐信息的重利用上, 前一次步伐就好比前一时刻的速度, 当前步伐就好比当前时刻的速度. AdaGrad方法: AdaGrad是的梯度下降法就有了对周围环境感知的优点(没有动量项): 即根据之前的更新结果, 来动态决定当前每个的步长 对于深度学习任务, 数据的稀疏性会导致相应参数梯度的稀疏性, 不频繁出现的物体对应的特征参数的梯度在大多数情况下为0, 从而这些参数被更新的频率很低. 在实际中, 我们希望更新频率低的参数可以拥有较大的更新步长, 而更新频率高的参数拥有较小的更新步长. 采用 “历史梯度平方和” 来衡量不同参数的梯度的稀疏性, 取值越小表明越稀疏, 更新公式如下所示: 从上面公式可以看出, 当更新次数较多时, 历史梯度平方和值较大, 对应的 $g_{t,i}$ 前的约束项值越小, 更新步长就越小. 另外, 这里分母的求和形式还实现了退火过程, 意味着随着时间推移, 约束项肯定会越来越小, 从而保证了算法的最终收敛(更新停止) **Adam方法:** Adam方法将惯性保持和环境感知这两个优点集于一身. - 梯度的一阶矩: 记录了过往梯度与当前梯度的平均, 体现了 "惯性保持" - 梯度的二阶矩: 记录了过往梯度平方与当前梯度平方的平均, 类似于AdaGrad, 体现了 "环境感知", 为不同参数产生自适应的学习速率 一阶矩和二阶矩都采用滑动窗口内求平均的思想进行同和, 即当前梯度和近一段时间内梯度的平均值, 时间久远的梯度对当前平均值的贡献呈指数衰减. 具体来说, 一阶矩和二阶矩采用指数衰退平均技术, 计算公式为: 其中...//TODO **对一阶矩和二阶矩的理解:** 一阶矩相当于估计 $E[g_t]$ : 由于当前梯度 $g_t$ 是随机采样得到的估计结果, 因此更关注它在统计意义上的期望; 二阶矩相当于估计 $E[g_t^2]$, 这点与AdaGrad不同(后者计算的是 $g_t^2$ 的总和). **一阶矩和二阶矩的物理意义:** | --- | --- | --- | | 一阶矩的值 | 二阶矩的值 | 表示的含义 | | 大 | 大 | 说明此时的梯度值普遍较大且稳定, 相当于遇到了一个明显的大坡, 权重更新前进方向明确 | | 趋于0 | 大 | 说明梯度值极不稳定, 一会正一会负, 表明可能遇到了峡谷, 且处于反弹震荡状态 | | 大 | 趋于0 | 这种情况不可能出现 | | 趋于0 | 趋于0 | 此时说明梯度值普遍接近于0, 有可能进入了一个平原或者坡度极缓的地方 | 另外, Adam方法还考虑 $m_t,n_t$ 在领初始值情况下的偏执校正(即在分子中加了一个很小的非0项, 防止0除), Adam的更新公式如下:7. L1正则化与稀疏性此类的相关题目在面试中常被问道, 可以考察面试者对于机器学习模型各个相关环节的了解程度. 在回答时, 不能只给出一些大概的理解, 一定要深入且清晰的回答 稀疏性的概念: 说白了就是模型中的参数很多都是0, 这相当于对模型进行了一次特征选择, 只留下一些比较重要的特征, 提高模型的泛化能力.(以免学到过多的个性特征) 稀疏性的另一个重要性: 机器学习模型的输入动辄几百上千万维, 要想在线上系统中毫秒级响应要求下完成千万维特征的提取以及模型的预测, 还要在分布式环境下在内存中驻留这样一个大的模型, 往往很难做到. 问题 L1 正则化使得模型参数具有稀疏性的原理是什么?角度1: 解空间性状源自于西瓜书第11章, 比较权威且直观的解释, 大多数面试者都是从这个角度出发的. 首先, 假定输入想了 $\vec x$ 仅仅有两个属性, 那么 L1 和 L2 的 $\vec w$ 也只有两个分量, 分别是 $w_1, w_2$, 将这两个分量作为两个坐标轴, 分别绘制出 “无正则平方误差项” 和 “正则项” 的 “等值线”: 即在 $(w_1, w_2)$ 空间中取值相同的点. 首先, 根据引入L1和L2范式的损失函数的定义, 我们可以画出上面的解空间图, 其中, 坐标轴上面方形和分别是 L1 和 L2 范式的等值线, 而右上角的”年轮式”椭圆, 就是无正则误差项的等值线. 最终我们的模型需要在误差项和正则项之间折中, 体现在上图中就是误差项与正则项等值线相交的地方. 因此, 可以看到, 采用L1函数时, 误差项与正则项等值线的交点常常出现在坐标轴上, 而采用L2 范式时, 交点常常出现在某个象限中, 所以L1 比L2 范式更易于得到稀疏解. 扩展问题: 为什么加入正则项就是定义了一个解空间约束: 这个可以从优化的角度来理解, 首先把损失函数看做是一个优化问题, 而加入一个正则项就相当于是给该优化问题加入了一个约束, 即参数 $\vec w$ 的 Lp 范式不能大于m, m不是一个具体的值, 就代表了一种限制, 然后利用拉格朗日函数来解这个在范式约束条件下的最优解问题. 为什么L1和L2的解空间是不是同的: 他俩的参数运算公式不同, 解空间当然不同 角度2: 函数叠加(梯度下降更新公式)根据求导公式解释, L1求导区域将最终的权重向 0 靠拢 当 $w$ 为正时, L1求导为负, $w$ 为负时, L1求导为正 而L2求导, 无论 $w$ 的值是什么, 都为负, 因此L2只有减小 $w$ 绝对值的作用, 对解空间的稀疏性没有贡献. (即只会令 $w$ 减小, 但不要求其小到0, 在0附近时就可以) 公式求导过程见 正则化深入解析 角度3: 贝叶斯先验L1 的正则化相当于对模型参数 $w$ 引入了拉普拉斯先验, L2 相当于引入了高斯先验, 而拉普拉斯先验使参数为 0 的可能性更大 联想高斯分布的图像可知, 高斯分布在极值点(0点)附近取不通知的可能性是很接近的, 也就是高斯先验分布认为 $w$ 在极值点附近取不同值的可能性是接近的, 所有它只会要求 $w$ 接近于0, 而不会要求其等于0. 相反, 拉普拉斯分布曲线图在极值点(0dm)处是一个尖峰, 所以拉普拉斯先验分布中参数 $w$ 在极值点(0点)取值的可能性更高 至于为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验的证明, 可看正则化深入解析 第八章 采样1. 采样的作用第九章 前向神经网络1. 多层感知机与布尔函数问题1: 多层感知机表示异或逻辑时最少需要几个隐含层(仅考虑二元输入)?考虑零个隐藏层的情况(等同于逻辑回归): 逻辑回归公式如下: Z = sigmoid(AX+BY+C)具体的真值表为: // TODO表格 接着考虑下面的情况: //TODO 由上可知, 采用逻辑回归(即不带隐藏层的感知机)无法精确学习出一个输出为异或的模型表示 考虑具有一个隐藏层的情况: 如下图, 假设有两个隐藏单元, //TODO 问题2: 如果只使用一个隐层, 需要多少隐节点能够实现包含n元输入的任意布尔函数?//TODO 完善 结论: 在最差情况下, n元布尔函数的析取范式最多包含 $2^{(n-1)}$ 个不可规约的合取范式(异或), 因此对于单隐层的感知机, 需要 $2^{(n-1)}$ 个隐层节点实现 问题3: 考虑多隐层的情况, 实现包含n元输入的任意布尔函数最少需要多少个网络节点和网络层?// TODO 2. 深度神经网络中的激活函数为什么要使用激活函数: 在现实世界里, 往往会遇到线性不可分问题(如XOR异或函数), 需要非线性变换对数据的分布进行重新映射. 对于深度神经网络, 我们在每一层线性变换后叠加一个非线性激活函数, 以避免多层网络等效于单层线性函数, 从而获得更强大的学习和拟合能力 问题1: 写出常用激活函数及其导数 激活函数 形式 导数形式 Sigmoid $f(x) =\frac{1}{1+e^{-x}}$ $f’(x)(1-f(x))$ Tanh $f(x) = tanh(x)= \frac{e^x-e^{-x}}{e^x+e^{-x}}$ $f’(x) = 1-(f(z))^2$ ReLU $f(x)=max(0,x)=\begin{cases} 0 &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}$ $f’(x)=\begin{cases} 0 &amp; x\leq 0 \\ 1 &amp; x&gt;0 \end{cases}$ Leaky ReLU $f(x)=max(\alpha x,x)=\begin{cases} \alpha x &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases} \alpha 通常为0.1$ $f(x)=max(\alpha x,x)=\begin{cases} \alpha &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$ Maxout $f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$ $f(x) = max(w_1, w_2)$ ELU $f(x) = \begin{cases} x &amp; x \geq 0 \\ \alpha(e^x - 1) &amp; x&lt;0 \end{cases}$ $f(x) = \begin{cases} 1 &amp; x \geq 0 \\ \alpha e^x &amp; x&lt;0 \end{cases}$ 更多关于激活函数的讨论见[这里] 问题2: 为什么Sigmoid和Tanh激活函数会导致梯度消失的现象主要原因是它们在输入值过大或过小时都具有神经元饱和问题, 也就是说当输入很小或很大时, 其导数趋近于0, 又因为神经网络训练时依赖的是梯度下降法, 而根据链式法则的特点, 导数在趋近于0时就会出现梯度消失现象,导致浅层网络权重更新停滞 问题3: ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的有点是什么,它们有什么局限性?优点: 计算上,ReLU只需要一个阈值, 复杂度低 ReLU在正无穷范围内, 都具有非饱和性, 可以提供相对宽的激活边界, 缓解梯度消失问题 ReLU的负无穷总是输出0, 通过单侧一直提供了网络的稀疏表达能力, 经过ReLU激活的神经元输出中会有很多的0, 形成了稀疏矩阵, 通过实验发现, 这样的稀疏矩阵可以提高网络的性能. 局限性: 主要在于其训练过程中会导致神经元死亡. 这是由于ReLU在遇到负梯度时会将其置为0, 且在之后也不被任何数据激活,即流经该神经元的梯度永远为0, 不对任何数据产生相应. 在实际训练中, 如果学习率设置较大, 会导致超过一定比例的神经元不可逆死亡, 进而参数梯度无法更新, 整个训练过程失败 解决方法: Leaky ReLU. Leaky ReLU的参数 $\alpha$ 通常是一个很小的值, 也就是说可以实现一定程度上的单侧抑制,保证了网络的稀疏表达能力. 同时, 在输入为负时, 保留了部分信息, 避免了神经元死亡问题 但是LeakyReLU的缺点就是参数 $\alpha$ 的选择需要大量实验和先验知识, 这无疑给训练带来的麻烦, 基于此, PReLU选择将 $\alpha$ 作为网络中一个可学习的参数, 进行反向传播训练. 还有一个就是RReLU, 它令 $\alpha$的选择服从一定的分布, 并进行随机采样, 这在一定程度上能起到正则化的作用 3. 多层感知机的反向传播算法在网络训练中, 前向传播最终产生一个标量损失函数, 反向传播算法(Back Propagation)则将损失函数的信息沿网络层向后传播用以计算梯度, 打到优化网络参数的目的 问题1: 写出多层感知机的平方误差和交叉熵损失函数问题2: 根据问题1中定义的损失函数, 推导各层参数更新的梯度计算公式问题3: 平方误差损失函数和交叉熵损失函数分别适合什么场景?一般来说, 平方损失更适合输出为连续, 并且最后一层不含Sigmoid或者Softmax激活函数的神经网络; 而交叉熵则更适合二分类或多分类的场景. 为何平方损失函数不适合最后一层含有Sigmoid或Softmax激活函数的神经网络? 回顾平方损失函数相对于输出层的导数: \delta = - (y - a^{(L)})f'(z^{(L)})最后一项为激活函数的导数, 当激活函数为Sigmoid或者Softmax时, 其梯度很容易趋于饱和, 使得基于梯度的学习速度非常缓慢. 而交叉熵损失函数相对于输出层的导数是线性的, 因此不会存在学习速度过慢的问题 扩展问题: 多分类为什么选用交叉熵做损失函数, 而不用平方损失?Golik P, Doetsch P, Ney H. Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/ https://blog.csdn.net/u014313009/article/details/51043064 https://www.cnblogs.com/daniel-D/p/7931835.html#header-n1247 4. 神经网络训练技巧避免过拟合的办法: 数据增强, 正则化, 模型融合(其中dropout是模型融合方法中最高效和常用的技巧) 其他更多提升训练效果的还有如: 学习率, 权重衰减系数, dropout比例, 初始化等等 问题1: 神经网络训练时是否可以将全部参数初始化为0?首先, 在神经网络中, 每一层中的任意神经元都是同构的, 它们拥有相同的输入, 如果再将参数全部初始化为同样的值(如0), 那么输出也就是相同的, 反过来它们的梯度也都是相同的. 那么无论是前向传播还是反向传播的取值都是完全相同的, 那么每一个神经元都是基于input做相同的事情, 这样一来, 不同的神经元根本无法学到不同的特征, 这样就失去网络学习特征的意义了 不仅是初始化为0, 只要是初始化为同一个常数, 就会有这样的问题, 如果是常数, 虽然层与层之间的输出不一致 ,但是每一层内部的所有神经元结果一致, 仍然不是我们希望的那样#https://zhuanlan.zhihu.com/p/27190255 问题2: 为什么Dropout可以抑制过拟合? 它的工作原理和实现?Dropout是指在深度网络的训练中, 以一定的概率随机的”临时丢弃”一部分神经元节点. 具体来讲, Dropout作用于每份小批量训练数据, 由于其随机丢弃部分神经元的机制, 相当于每次迭代都在训练不同结构的神经网络, 可以被认为是一种实用的大规模深度神经网络的模型继承算法. 对于包含 $N$ 个神经元节点的网络, 在Dropout的作用下可以看作为 $2^N$ 个模型的集成, 这 $2^N$ 个模型可认为是原始网络的子网络, 它们共享部分权值, 并且拥有相同的网络层数, 而模型整个的参数数目不变, 大大简化了运算. 对于任意神经元来说, 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化, 这个过程会减弱全体神经元之间的联合适应性, 减少过拟合的风险, 增强泛化能力. 工作原理和实现: 应用Dropout包括训练和预测两个阶段, 在训练阶段中, 每个神经元节点需要增加一个概率系数, 在前向传播时, 会以这个概率选择是否丢弃当前的神经元 在测试阶段的前向传播计算时, 每个神经元的参数都会预先乘以概率系数p, 以恢复在训练中该神经元只有p的概率被用于整个神经网络的前向传播计算 问题3: 批量归一化的基本动机与原理是什么? 在卷积网络中如何使用?动机: 神经网络训练过程的本质是学习数据分布, 如果训练数据与测试数据的分布不同将大大降低网络的繁华能力, 因此需要在训练开始前对所有输入数据进行归一化处理. 然后随着网络训练的进行, 每个隐层的参数变化使得后一层的输入发生变化, 从而每一批训练数据的分布也随之改变, 致使网络在每次迭代中都需要拟合不同的数据分布, 增大训练的复杂度以及过拟合的风险. 原理: 批量归一化方法是针对每一批数据, 在网络的每一层输入之前增加归一化处理(均值为0, 标准差为1), 将所有批数据都强制统一到特定的分布下, 这样可以看作是在每一层输入和上一层输出之间加入了新的计算层, 对数据的分布进行了额外的约束,也就是隐含的增加了正则项, 从而增强模型的泛化能力. 但如果仅仅是这样, 同时也会降低模型的拟合能力, 归一化后的输入分布被强制为0均值和1标准差, 这在一定程度上掩盖了数据本身具有的一些特征, 因此, 为了恢复这些特征, 对归一化后的数据进行线性变换, 引入了两个新的参数 $\gamma$ 和 $\beta$ ,对于每一个卷积核都具有不同的 $\gamma$ 和 $\beta$, 在一定程度了可以保留原始数据中更多的信息, 从而更加有利于优化的过程, 提高模型的泛化能力 使用时需要注意的问题: 在卷积网络中应用时, 需要注意卷积神经网络的参数共享机制, 每一个卷积核的参数在不同的位置的神经元当中是共享的, 因此也应该一起被归一化, 如果有 $f$ 个卷积核, 就对应 $f$ 个特征图和 $f$ 组不同的BN参数. 5. 深度卷积神经网络卷积神经网络的特点是每层的神经元节点只响应前一层局部区域范围内的神经元(全连接会响应前一层的全部节点). 相较于其他网络模型, 卷积操作的参数共享特性使得需要优化的参数数目大大缩减, 提高了模型的训练效率以及可扩展性. 问题1: 卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用稀疏交互(Sparse Interaction): 对于全连接网络来说, 任意一个神经元都会与之前层的所有神经元产生交互, 形成稠密的连接结构, 而在卷积神经网络中, 卷积核尺寸远远小于输入数据的维度, 卷积核会在输入数据上进行类型窗口滑动似的扫描, 然后依次计算出输出神经元的值, 这样每个输出神经元仅仅与前一层特定局部区域内的神经元存在连接, 这种特性就成为稀疏交互. 稀疏交互的作用: 首先第一个很直观的作用就是大大降低了需要优化的参数数目, 是模型更加精简. 另外, 稀疏交互的实际意义还在于, 对于普通的图像, 文本, 语音等数据, 它们都具有较为明显的局部特征结构, 这样, 我们可以先让神经网络学习局部的特征, 再将局部的特征组合起来形成更复杂和抽象的特征, 进而达到识别目标的目的. 参数共享(Parameter Sharing): 参数共享是指在同一个模型的不同模块中使用相同的参数, 它是卷积运算的固有属性. 在卷积神经网络中, 卷积核中的每一个元素将作用于所有局部输入的特定位置上, 根据参数共享的思想, 我们队每个卷积核, 只需要学习一组参数, 而不需要针对不同的输入对每个局部位置的参数都进行优化, 这大大降低了模型的存储需求. 参数共享的作用: 参数共享的实际意义在于可以使得卷积层具有平移不变性. 对于图片来说, 不管图片中的物体出现在图片的哪个位置, 卷积层由于参数共享的作用, 它都会将这个物体看做是同一类, 也就是说, 对图片上的某个物体先进行卷积, 然后在将输出平移, 与先将图片中的物体平移, 再对平移后的物体进行卷积, 其输出结果是相等的 问题2: 常用的池化操作有哪些? 池化的作用是什么?常用的池化操作主要针对非重叠区域(即stride的值设置成大于等于pooling窗口的size), 包括均值池化(mean pooling), 最大池化(max pooling)等. 均值池化: 能够抑制由于邻域大小受限造成估计值方差增大的现象, 特点是对背景的保留效果更好 最大池化: 能够抑制网络参数误差造成估计值偏移的现象, 特点是更好地提取纹理信息 其他特殊的池化方式: 相邻重叠区域的池化: 采用比窗口宽度更小的步长, 使得窗口在每次滑动时存在重叠区域 空间金字塔池化: 主要考虑了多尺度信息的描述, 例如同时计算1×1, 2×2, 4×4 的矩阵池化, 并将结果拼接在一起作为一下网络层的输入 池化的本质是降采样. 除了能显著降低参数数量外, 还能保持对平移, 伸缩, 旋转操作的不变性. 平移不变性: 输入为(1,3,5) 和(5,1,3)时, 最大池化都将取值5 伸缩(尺度)不变性: 如果原先神经元在最大池化操作之后输出5, 那么经过伸缩(尺度变换)后, 最大池化操作的输出很大概率仍然是5(主要最大值不变) 旋转不变性: 对于任意角度的输入, 都会有对应的过滤器与之匹配并最终使神经元的输出引起大的激活, 无论哪个神经元获得了激活, 在经过最大池化操作后, 对于此局部特征, 输出都会具有大的激活 综上, 池化主要有以下作用: 降维, 减少模型参数, 防止过拟合, 提高模型泛化能力 实现非线性 扩大感受野 实现平移, 伸缩, 旋转不变性 下面的图与降采样有异曲同工之处, 可以帮助理解(https://www.zhihu.com/question/36686900): 问题3: 卷积神经网络如何用于文本分类任务?卷积神经网络的核心思想是捕捉局部特征, 对于文本数据来说, 局部特征就是由若干单词组成的滑动窗口, 类似于 N-gram. 下面分别从输入层, 卷积层, 池化层和输出层来说一下如何将卷积神经网络应用于文本数据. 输入层: 输入层通常是一个 $N\times K$ 的矩阵, 其中 $N$ 为单词总数, $K$ 是每个单词对应向量的维度. 每个单词的 $K$ 维向量可以是预先在其他语料库中训练好的, 也可以作为未知的参数由网络训练得到. 这两种方法各有优势: 预训练的词嵌入: 可以利用其他语料库得到更多的先验知识 作为未知参数训练: 能够更好的抓住与当前任务相关联的特征 通常, 输入层会采用两个通道的形式, 即有两个 $N\times K$ 的输入矩阵, 其中一个用预先训练好的词嵌入表达, 并且在训练过程中不再发生变化; 另外一个也由同样的方式初始化, 但是会作为参数, 随着网络的训练过程发生改变. 卷积层: 在输入的 $N\times K$ 维的矩阵上, 我们定义不同大小的滑动窗口进行卷积操作: c_i = f(w \dot x_{i:i+h-1} + b)其中 $x_{i:i+h-1}$ 代表由输入矩阵的第 $i$ 行到第 $i+h-1$ 行所组成的一个大小为 $h\times K$ 的滑动窗口, 假设 $h$ 为2, 则每次在 $2\times K$ 的滑动窗口上进行卷积, 并得到 $N-2$ 个结果, 再将这 $N-2$ 个结果拼接起来得到 $N-2$ 维的特征向量, 这样, 通过定义不同的滑动窗口, 就可以提取出不同的特征向量, 构成卷积层的输出 池化层: 池化层阶段从每个不同的特征向量筛选出 $K’$ (为了与上文的 $K$ 区分, 此处用 $K’$ ) 个最大的特征, 成为 K-MAX 池化 ( 也可以使用平均池化, 将每个特征向量平均), 最终达到的效果是将不同长度的句子通过池化得到一个定长的向量表示(长度为滑动窗口的个数) 输出层: 得到文本的向量表示后, 根据具体任务定义输出层的网络结构, 对于分类任务, 通常是用全连接层和Softmax激活函数输出每个类别的概率. 6. 深度残差网络问题: ResNet的提出背景和核心理论是什么?提出背景: 解决或缓解深层的神经网络训练中的梯度消失问题. ResNet中的一个重要假设: 假设有一个L层的深度神经网络, 如果我们在上面加入一层, 直观来讲得到的L+1层深度神经网络的效果应该至少不会比L层的差. 因为可以简单的设最后一层为前一层的拷贝(相当于恒等映射), 并且其它层参数设置不变. 但是最终实验发现: 层数更深的神经网络反而会具有更大的训练误差, 因此, 作者认为深层网络效果差的原因很大程度上也许不在于过拟合, 而在于梯度消失问题. ( 关于梯度消失问题的讨论可见这里) 解决办法: 根据梯度消失问题的根本原因所在(链式法则连乘), ResNet通过调整网络结构来解决上述问题. ResNet的结构设计思想为: 既然离输入近的神经网络层较难训练, 那么我们可以将它短接到更靠近输出的层 首先考虑两层神经网络的简单叠加, 在反向传播时, 有关ResNet更详细的讨论可以看这里]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的函数指针和函数对象]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%E5%92%8C%E5%87%BD%E6%95%B0%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[函数指针函数指针：指向函数地址的指针变量。 在C++编译时，每一个函数都有一个入口地址，该地址是存储函数机器语言代码的内存的开始地址。函数指针主要有两方面的用途：调用函数和用作函数参数。函数指针的声明方法如下所示： 123456789101112double pam(int);double (* pf)(int); //将星号与函数名括起来，表示当前星号修饰的是函数名，而不是类型 //可以理解为将pam替换成了(*pf)//通常，要声明指向特定类型的指针，可以首先编写这种函数的原型，然后用(*pf)替换函数名即可pf = pam; // 函数名pam本身就是函数地址，所以可直接赋值，无需用取址符，但是要求pam的特征标必须与pf相同pf = &amp;pam; //但实际上，也可以使用取址符，效果与上面 等价（why？）//以下三种方式都是合法且等价的double x = pam(4);double y = (*pf)(5);double z = pf(6); 从上面的语句可以看出，C++同时允许使用带星号和不带星号的函数指针，最神奇的是效果居然是等价的（对于给函数指针赋值时，函数名带取址符和不带取址符的效果也是等价的！）！ 导致以上“神奇事情”发生的原因是，在C++指定函数指针标准时，出现了两种不同的声音：一种学派认为，由于pf是函数指针，而*pf是函数，因此应将(*pf)()用作函数调用。另一种学派认为，由于函数名是指向该函数的指针，指向函数的指针的行为应该与函数名相似，因此应将pf()用作函数调用使用。C++进行了折衷——这两种方式都是正确的，虽然看上去它们在逻辑上是相冲突的。 下面的声明语句指出了一个函数指针数组，其中每个指针都指向这样的函数：将const double*和int作为参数，返回一个const double *1const double* (*pa[3]) (const double *, int ) = &#123; f1,f2,f3&#125;; 这里不能使用auto，因为auto智能用于单值初始化，而不能用于初始化列表，但声明数组pa后，可以使用auto：1auto pb = pa; // pa和pb都是指向函数指针的指针，使用时可以用下标法，也可以当做指针使用：123456const double *px = pa[0](av,3);const double *py = (*pa[0])(av,3); //如果带星号，则不能少括号//要获得值，可使用运算符const double *x = *pa[0](av,3); //少了括号以后，会取得返回值地址指向的值const double *y = *(*pb[1])(av,3); 如果继续声明了指向函数指针整个数组的指针，则使用方法又有些不同，见下面：12345auto pc = &amp;pa; //pc指向pa的地址*pc[3]; // 相当于 pa[3],代表一个包含三个指针的指针数组(*pc)[3]; // 代表pc是一个函数指针，指向含3个元素的指针数组 函数对象函数对象的实质是对运算符()的重载。 ＃ 函数对象与函数指针的比较 函数对象的优势在于，可以把附加对象保存在函数对象中，也可以存储一些其他的额外信息，甚至可以用来封装类成员函数指针。 但是，函数对象本身并不是指针，因此在使用函数指针的场合中，往往无能为力。例如，不能将对象传给qsort函数！因为它只接受函数指针。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11新特性enable_if与SFINAE用法解析]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84enable-if%E7%94%A8%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[SFINAESDINAE的全称是Substitution failure is not an error。意思是“匹配失败并非错误” 通常，在使用模板时，编译器会根据传入的参数来推导最适合的模板函数，在这个推导过程中只要有一个可以正确推导出来（但是不能有多个，否则会引起歧义），那么其他几个模板推导就是会产生编译错误，程序也可以正常通过。如下面的代码所示：123456789101112131415struct Test &#123; typedef int foo;&#125;;template &lt;typename T&gt;void f(typename T::foo) &#123;&#125; // Definition #1template &lt;typename T&gt;void f(T) &#123;&#125; // Definition #2int main() &#123; f&lt;Test&gt;(10); // Call #1. f&lt;int&gt;(10); // Call #2. Without error (even though there is no int::foo) //thanks to SFINAE.&#125; std::enable_if这是一个模板类，该模板可能的实现如下：（也许会有其他版本，但大意都差不多）12345template&lt;bool B, class T = void&gt;struct enable_if &#123;&#125;; //在C++中，struct和class除了默认访问权限不同外，无任何差异template&lt;class T&gt;struct enable_if&lt;true, T&gt; &#123; typedef T type; &#125;; 以上表示，若B为true，则std::enable_if模板类拥有公开成员typedef type ，等于T；否则，无该成员。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第十七章～第十八章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter17-18%2F</url>
    <content type="text"><![CDATA[18.3 新的类功能18.3.1 特殊的成员函数18.3.2 默认的方法和禁用的方法在C++中，如果只在程序中提供了移动构造函数，那么编译器将不会自动创建默认的构造函数、复制构造函数和复制赋值构造函数。在这种情况下，可以使用关键字default显式的声明这些方法的默认版本：（即只给出函数头，后接=default，则这些方法的默认版本就会被创建） 12345678class Someclass&#123; public: Someclass(Someclass &amp;&amp;); Someclass() = default; Someclass(const Someclass &amp;) = default; Someclass&amp; operator=(const Someclass &amp;) = default;&#125; 另一方面，关键字delete可用于禁止编译器使用特定的方法，例如，要禁止复制对象，可禁用复制构造函数和复制赋值运算符（之前是通过将其访问权限设值为private来实现的，现在的实现方法更容易理解，且不易犯错）：12 关键字default智能用于6个特殊的成员函数，但delete可用于任何成员函数。delete的一种可能用法是禁止特定的转换。 18.6 可变参数模板]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的mt19937]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84mt19937%2F</url>
    <content type="text"><![CDATA[std::mersenne_twister_engine该类型是一个基于梅森缠绕器算啊费的随机数生成器，可以产生高质量的无符号随机整数 mt19937 和 mt19937_64这两个类型分别是具有预先定义参数的随机数引擎，有松本与西村设计 成员函数seed、operator、discard、min、max 等等 非成员函数operator==、operator!=、等等 成员对象word_size、state_size等等 具体见：https://zh.cppreference.com/w/cpp/numeric/random/mersenne_twister_engine]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的pragma和#ifndef组合语句的联系与区别]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84pragma%E5%92%8Cifndef%E7%BB%84%E5%90%88%E8%AF%AD%E5%8F%A5%E7%9A%84%E8%81%94%E7%B3%BB%E4%B8%8E%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在C++中，为了避免同一个文件被include多次，常常需要在文中加上一些保证，主要有两种方式，二者用法如下：1#pragma once 12345#ifndef __HEAD__#define __HEAD__#endif 通常，在能够支持这两种方式的编译器上，二者没有太大的区别。下面简介说一下二者格子的优缺点： 对于#ifndef方式来说，不光可以保证同一个文件不会被包含多级，也能保证内容完全相同的两个文件不会被不小心同时包含，但这样就需要编译器每次都扫描头文件内部，因此会使得编译时间相对较长。另外一个缺点就是该方式需要自定义宏名称，当项目很大时，宏名称有“撞车”的风险。 对于#pragma once方式来说，首先是需要code的代码很少，另外不需要自定义宏名称，避免了“撞车”的风险，但是#pragma once提供的保证仅仅是：同一个物理意义上的文件不会被包含多次。如果两个文件内容完全一样，则该方式仍然会重复包含。 结合以上分析，我个人倾向于使用#pragma once，因为需要code的代码更少，无需自想宏名称，另外，出现包含多个内容相同的文件的情况也很很少的（除非有意，否则不太可能出现这种情况）。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《OpenCV3编程入门》]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-OpenCV3%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的有关int的各种形式以及为什么要有size_t类型]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E6%9C%89%E5%85%B3int%E7%9A%84%E5%90%84%E7%A7%8D%E5%BD%A2%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[int,int32_t,int64_t 数据类型特别是int相关的类型在不同位数机器的平台下长度不同。C99标准并不规定具体数据类型的长度大小，只规定级别。作下比较： 16位平台 char 1个字节8位short 2个字节16位int 2个字节16位long 4个字节32位指针 2个字节 32位平台 char 1个字节8位short 2个字节16位int 4个字节32位long 4个字节long long 8个字节指针 4个字节 64位平台 char 1个字节short 2个字节int 4个字节long 8个字节（区别）long long 8个字节指针 8个字节（区别） 为了保证平台的通用性，程序中尽量不要使用long数据库型。同时，通过int_32t等形式来明确int型数据所占有的位数。 另外 还有size_t size_t是一些C/C++标准在stddef.h中定义的。这个类型足以用来表示对象的大小。 size_t的真实类型与操作系统有关，在32位架构中被普遍定义为： typedef unsigned int size_t; 而在64位架构中被定义为： typedef unsigned long size_t;size_t在32位架构上是4字节，在64位架构上是8字节，在不同架构上进行编译时需要注意这个问题。而int在不同架构下都是4字节，与size_t不同；且int为带符号数，size_t为无符号数。 size_t 这个类型的意义：size_t和unsigned int有所不同,size_t的取值range是目标平台下最大可能的数组尺寸,一些平台下size_t的范围小于int的正数范围,又或者大于unsigned int.最典型的,在x64下,int还是4,但size_t是8.这意味着你在x64下最大可能开辟的数组尺寸是2^64.如果你使用int或者unsigned int,那么在x64下如果你的代码中全部使用uint作为数组的尺寸标记,那么你就会失去控制2^32尺寸以上的数组的机会.虽然现在在x64上开辟一个大于2^32大小的连续数组依然是个不大可能的事情,但是……….“640K内存对于任何人来说都足够了”——比尔盖茨 学过计算机组成原理应该不会对此有疑问。int小于等于数据线宽度，size_t大于等于地址线宽度。size_t存在的最大原因可能是因为：地址线宽度历史中经常都是大于数据线宽度的。在数据只有8位的年代，地址率先进入10位，12位，在数据16位的年代，地址也已经进入了20位，24位。目前的int普遍是32位，而size_t在主流平台中都是64位。size_t为什么存在？因为无论int还是unsigned都很可能小于size_t需要的大小，所以必须有个size_t。—补充：据说题主对_t有疑惑。这个问题很简单，仅仅是因为作者选择这样的命名作为编码规范而已。类型名与变量名共享相同的命名空间，所以通常需要在命名方面刻意区分出来。在遥远的 C 时代，发明者很可能是想建议所有的类型名后面加_t，只不过这并没有成为更普遍的编码规范罢了。而现今Java的规范倒比较容易让人接受：大写开头的是类型名，小写开头的是变量名跟函数名，虽然具体细则有不同，但原意都是一样的：变量与类型共享同一个命名空间，因而需要在命名规则上刻意区分开来。 为什么size_t 重要？https://jeremybai.github.io/blog/2014/09/10/size-t 前言：使用size_t可能会提高代码的可移植性、有效性或者可读性，或许同时提高这三者。 在标准C库中的许多函数使用的参数或者返回值都是表示的用字节表示的对象大小，比如说malloc(n) 函数的参数n指明了需要申请的空间大小，还有memcpy(s1, s2, n)的最后一个参数，表明需要复制的内存大小，strlen(s)函数的返回值表明了以’\0’结尾的字符串的长度（不包括’\0’），其返回值并不是该字符串的实际长度，因为要去掉’\0’。 或许你会认为这些参数或者返回值应该被申明为int类型（或者long或者unsigned），但是事实上并不是。C标准中将他们定义为size_t。标准中记载malloc的申明应该出现在，定义为： void *malloc(size_t n); memcpy和strlen的申明应该出现在中： void memcpy(void s1, void const s2, size_t n);size_t strlen(char const s); size_t还经常出现在C++标准库中，此外，C++库中经常会使用一个相似的类型size_type，用的可能比size_t还要多。 据我所知，大部分的C和C++程序员害怕这些库使用size_t，因为他们不知道size_t代表什么或者为什么这些库需要使用它，归根结底，原因在于他们什么时候什么地方需要用到它。 可移植性问题 早期的C语言（由Brian Kernighan 和 Dennis Ritchie 在The C Programming Language书中所写，Prentice-Hall, 1978）并没有提供size_t类型，C标准委员会为了解决移植性问题将size_t引入，举例如下： 让我们来写一个可移植的标准memcpy函数，我们将会看到一些不同的申明和它们在不同平台不同大小的地址空间上编译下的情况。 回忆memcpy(s1, s2, n)函数，它将s2指向地址开始的n个字节拷贝到s2指向的地址，返回s1，这个函数可以拷贝任何数据类型，所以参数和返回值的类型应该为可以指向任何类型的void，同时，源地址不应该被改变，所以第二个参数s2类型应该为const void，这些都不是问题。 真正的问题在于我们如何申明第三个参数，它代表了源对象的大小，我相信大部分程序员都会选择int： void memcpy(void s1, void const *s2, int n); 使用int类型在大部分情况下都是可以的，但是并不是所有情况下都可以。int是有符号的，它可以表示负数，但是，大小不可能是复数。所以我们可以使用unsigned int代替它让第三个参数表示的范围更大。 在大部分机器上，unsigned int的最大值要比int的最大值大两倍，比如说再也给16位的机器上，unsigned int的最大值为65535，int的最大值为32767。 尽管int类型的大小依赖于C编译器的实现，但是在给定的平台上int对象的大小和unsigned int对象的大小是一样的。因此，使用unsigned int修饰第三个参数的代价与int是相同的： void memcpy(void s1, void const *s2, unsigned int n); 这样似乎没有问题了，unsigned int可以表示最大类型的对象大小了，这种情况只有在整形和指针类型具有相同大小的情况下，比如说在IP16中，整形和指针都占2个字节（16位），而在IP32上面，整形和指针都占4个字节（32位）。（参见下面C数据模型表示法） C数据模型表示法 最近，我偶然发现几篇文章，他们使用简明的标记来表述不同目标平台下c语言数据的实现。我还没有找到这个标记的来源，正式的语法，甚至连名字都没有，但他似乎很简单，即使没有正规的定义也可以很容易使用起来。这些标记的一边形式形如： I nI L nL LL nLL P nP。 其中每个大写字母（或成对出现）代表一个C的数据类型，每一个对应的n是这个类型包含的位数。I代表int，L代表long，LL代表long long，以及P代表指针（指向数据，而不是函数）。每个字母和数字都是可选的。 例如，I16P32架构支持16位int和32位指针类型，没有指明是否支持long或者long long。如果两个连续的类型具有相同的大小，通常省略第一个数字。例如，你可以将I16L32P32写为I16LP32，这是一个支持16位int，32位long，和32位指针的架构。 标记通常把字母分类在一起，所以可以按照其对应的数字升序排列。例如，IL32LL64P32表示支持32位int，32位long，64位long long和32位指针的架构；然而，通常写作ILP32LL64。 不幸的是，这种memcpy的申明在I16LP32架构上（整形是16-bit 长整形和指针类型时32-bits）显得不够用了，比如说摩托罗拉第一代处理器68000，在这种情况下，处理器可能拷贝的数据大于65535个字节，但是这个函数第三个参数n不能处理这么大的数据。 什么？你说很容易就可以改正？只需要把memcpy的第三个参数的类型修改一下： void memcpy(void s1, void const *s2, unsigned long n); 你可以在I16LP32目标架构上使用这个函数了，它可以处理更大的数据。而且在IP16和IP32平台上效果也还行，说明它确实给出了memcpy的一种移植性较好的申明。但是，在IP16平台上相比于使用unsigned int，你使用unsigned long可能会使你的代码运行效率大打折扣（代码量变大而且运行变慢）。 在标准C中规定，长整形（无论无符号或者有符号）至少占用32位，因此在IP16平台上支持标准C的话，那么它一定是IP16L32 平台。这些平台通常使用一对16位的字来实现32位的长整形。在这种情况下，移动一个长整形需要两条机器指令，每条移动一个16位的块。事实上，这个平台上的大部分的32位操作都需要至上两条指令。 因此，以可移植性为名将memcpy的第三个参数申明为unsigned long而降低某些平台的性能是我们所不希望看到的。使用size_t可以有效避免这种情况。 size_t类型是一个类型定义，通常将一些无符号的整形定义为size_t，比如说unsigned int或者unsigned long，甚至unsigned long long。每一个标准C实现应该选择足够大的无符号整形来代表该平台上最大可能出现的对象大小。 使用size_t size_t的定义在, , , , 和这些标准C头文件中，也出现在相应的C++头文件, 等等中，你应该在你的头文件中至少包含一个这样的头文件在使用size_t之前。 包含以上任何C头文件（由C或C++编译的程序）表明将size_t作为全局关键字。包含以上任何C++头文件（当你只能在C++中做某种操作时）表明将size_t作为std命名空间的成员。 根据定义，size_t是sizeof关键字（注：sizeof是关键字，并非运算符）运算结果的类型。所以，应当通过适当的方式声明n来完成赋值： n = sizeof(thing); 考虑到可移植性和程序效率，n应该被申明为size_t类型。类似的，下面的foo函数的参数也应当被申明为sizeof： foo(sizeof(thing)); 参数中带有size_t的函数通常会含有局部变量用来对数组的大小或者索引进行计算，在这种情况下，size_t是个不错的选择。 适当地使用size_t还会使你的代码变得如同自带文档。当你看到一个对象声明为size_t类型，你马上就知道它代表字节大小或数组索引，而不是错误代码或者是一个普通的算术值。 在我接下来的一些文章的例子中会使用size_t，敬请期待！]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的aligned_alloc]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84aligned-alloc%2F</url>
    <content type="text"><![CDATA[std::aligned_alloc定义于头文件 void* aligned_alloc( std::size_t alignment, std::size_t size );(C++17 起) 分配 size 字节的未初始化存储，由 alignment 指定其对齐。 size 参数必须是 alignment 的整数倍。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[轻量级深度学习框架ZeroTensor：（二）tensor类的设计与实现]]></title>
    <url>%2Fz_post%2F%E9%A1%B9%E7%9B%AE-ZeroTensor-Tensor%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Tensor类Tensor类是zerotensor框架的所有数据的基类，它的成员变量和成员函数如下所示： 有关类内部的具体实现可以在源码中的/src/data/tensor.h中看到。]]></content>
      <categories>
        <category>项目</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++中的lambda表达式]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[C++中的lambda与函数对象lambda表达式是C++11中引入的一项新技术，利用lambda表达式可以编写内嵌的匿名函数，用以替换独立函数或者函数对象，并且使代码更可读。但是从本质上来讲，lambda表达式只是一种语法糖，因为所有其能完成的工作都可以用其它稍微复杂的代码来实现。但是它简便的语法却给C++带来了深远的影响。 如果从广义上说，lambda表达式产生的是函数对象。函数对象的本质上是一个类而不是一个函数，在类中，对象重载了函数调用运算符()，从而使对象能够项函数一样被调用，我们称这些对象为函数对象（Function Object）或者仿函数（Functor）。相比lambda表达式，函数对象有自己独特的优势。下面我们开始具体讲解这两项黑科技。 lambda表达式先从一个简单的例子开始，我们定义一个输出字符串的lambda表达式，如下所示，表达式一般都是从方括号[]开始，然后结束于花括号{}：12auto basic_lambda = []&#123;cout&lt;&lt;"Hello Lambda"&lt;&lt;endl;&#125; //定义简单的lambda表达式basic_lambda(); //调用 下面分别是包含参数和返回类型的lambda表达式：12auto add = [] (int a, int b)-&gt;int &#123; return a+b;&#125; //返回类型需要用`-&gt;`符号指出auto multiply = [](int a, int b) &#123;return a*b;&#125; //一般可以省略返回类型，通过自动推断就能得到返回类型 lambda表达式最前面的方括号提供了“闭包”功能。每当定义一个lambda表达式以后，编译器会自动生成一个 匿名类 ，并且这个类重载了()运算符，我们将其称之为闭包类型（closure type）。在运行时，这个lambda表达式会返回一个匿名的闭包实例，并且该实例是一个右值。闭包的一个强大之处在于其可以通过传值或引用的方式捕捉其封装作用域内的变量，lambda表达式前面的方括号就是用来定义捕捉模式以及变量的lambda捕捉块，如下所示：123456int main()&#123; int x = 10; // 定义作用域内的x，方面下面的lambda捕捉 auto add_x = [x](int a)&#123; return a+x; &#125; // 传值捕捉x auto multiply_x = [&amp;x](int a) &#123;return a*x;&#125; //引用捕捉x&#125; 当lambda捕捉块为空时，表示没有捕捉任何变量。对于传值方式捕捉的变量x，lambda表达式会在生成的匿名类中添加一个非静态的数据成员，由于闭包类重载()运算符是使用了const属性，所以不能在lambda表达式中修改传值方式捕捉的变量，但是如果把lambda标记为mutable，则可以改变，如下所示：12345int x = 10;auto add_x = [x](int a) mutable&#123; x * = 2; return a+x;&#125;cout&lt;&lt;add_x(10)&lt;&lt;endk; //输出30return 0; 而对于引用方式捕捉的变量，无论是否标记为mutable，都可以对变量进行修改，至于会不会在匿名类中创建数据成员，需要看不同编译器的具体实现。 lambda表达式只能作为右值，也就是说，它是不能被赋值的12345auto a=[]&#123; cout&lt;&lt;"A"&lt;&lt;endl; &#125;auto b=[]&#123; cout&lt;&lt;"B"&lt;&lt;endl; &#125;a = b; // 非法，lambda表达式变量只能做右值auto c = a; // 合法，生成一个副本 造成以上原因是因为禁用了赋值运算符：1ClosureType&amp; operator=(const ClosureType&amp;) = delete; 但是没有禁用复制构造函数，所以仍然可以用是一个lambda表达式去初始化另一个（通过产生副本）。 关于lambda的捕捉块，主要有以下用法： []：默认不捕捉变量 [=]：默认以值捕捉所有变量（最好不要用） [&amp;]：默认以引用捕捉所有变量（最好不要用） [x]：仅以值捕捉变量x，其他变量不捕捉 [&amp;x]：仅以引用捕捉x，其他变量不捕捉 [=, &amp;x]：默认以值捕捉所有变量，但是x是例外，通过引用捕捉 [&amp;, x]：默认以引用捕捉所有变量，但是x是例外，通过值捕捉 [this]：通过引用捕捉当前对象（其实是复制指针） [* this]：通过传值方式捕捉当前对象 通过以上的说明，可以看到lambda表达式可以作为返回值，赋值给对应类型的函数指针，但是使用函数指针貌似并不是那么方便，于是STL在头文件&lt;functional&gt;中定义了一个多态的函数对象封装std::function，其功能类似于函数指针。它可以绑定到任何类函数对象，只要参数与返回类型相同。如下面的返回一个bool且接收两个int的函数包装器：1std::function&lt;bool(int, int)&gt; wrapper = [](int x, int y) &#123; return x&lt;y; &#125; lambda表达式还有一个很重要的应用是其可以作为函数的参数，如下所示：1234int value = 3;vector&lt;int&gt; v&#123;1,2,3,4,5,6,7&#125;;int count == std::count_if(v.begin, v.end(), [value](int x)&#123;return x&gt;value;&#125;); 下面给出lambda表达式的完整语法：1234567// 完整语法[ capture-list ] ( params ) mutable(optional) constexpr(optional)(c++17) exception attribute -&gt; ret &#123; body &#125;// 可选的简化语法[ capture-list ] ( params ) -&gt; ret &#123; body &#125; [ capture-list ] ( params ) &#123; body &#125; [ capture-list ] &#123; body &#125; capture-list：捕捉列表，这个不用多说，前面已经讲过，记住它不能省略； params：参数列表，可以省略（但是后面必须紧跟函数体）； mutable：可选，将lambda表达式标记为mutable后，函数体就可以修改传值方式捕获的变量； constexpr：可选，C++17，可以指定lambda表达式是一个常量函数； exception：可选，指定lambda表达式可以抛出的异常； attribute：可选，指定lambda表达式的特性； ret：可选，返回值类型； body：函数执行体。 lambda新特性（C++14）在C++14中，lambda又得到了增强，一个是泛型lambda表达式，一个是lambda可以捕捉表达式。 lambda捕捉表达式前面讲过，lambda表达式可以按传值或者引用捕捉在其作用域范围内的变量。而有时候，我们希望捕捉不在其作用域范围内的变量，而且最重要的是我们希望捕捉右值。所以C++14中引入了表达式捕捉，其允许用任何类型的表达式初始化捕捉的变量，如下：12345678// 利用表达式捕获，可以更灵活地处理作用域内的变量int x = 4;auto y = [&amp;r = x, x = x + 1] &#123; r += 2; return x * x; &#125;();// 此时 x 更新为6，y 为25// 直接用字面值初始化变量auto z = [str = "string"]&#123; return str; &#125;();// 此时z是const char* 类型，存储字符串 string 可以看到捕捉表达式扩大了lambda表达式的捕捉能力，有时候你可以用std::move初始化变量。这对不能复制只能移动的对象很重要，比如std::unique_ptr，因为其不支持复制操作，你无法以值方式捕捉到它。但是利用lambda捕捉表达式，可以通过移动来捕捉它： 123auto myPi = std::make_unique&lt;double&gt;(3.1415);auto circle_area = [pi = std::move(myPi)](double r) &#123; return *pi * r * r; &#125;;cout &lt;&lt; circle_area(1.0) &lt;&lt; endl; // 3.1415 泛型lambda表达式从C++14开始，lambda表达式支持泛型：其参数可以使用自动推断类型的功能，而不需要显示地声明具体类型。这就如同函数模板一样，参数要使用类型自动推断功能，只需要将其类型指定为auto，类型推断规则与函数模板一样。这里给出一个简单例子： 1234auto add = [](auto x, auto y) &#123; return x + y; &#125;;int x = add(2, 3); // 5double y = add(2.5, 3.5); // 6.0 函数对象函数对象是一个广泛的概念，因为所有具有函数行为的对象都可以称为函数对象。这是一个高级抽象，我们不关心对象到底是什么，只要其具有函数行为即可。函数行为是指可以使用()调用并传递参数，如下所示：1function(arg1, arg2, ...); //函数调用 由此，lambda表达式也是一个函数对象。该函数对象实际上是一个匿名类的实例，且这个类实现了函数调用运算符()。 泛型提供了高级抽象，不论是lambda表达式、函数对象、还是函数指针，都可以传入到STL算法中（如for_each）。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的枚举类]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84%E6%9E%9A%E4%B8%BE%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[p372一、简述 强类型枚举（Strongly-typed enums），号称枚举类型，是C++11中的新语法，用以解决传统C++枚举类型存在的缺陷。传统C++中枚举常量被暴漏在外层作用域中，这样若是同一作用域下有两个不同的枚举类型，但含有相同的枚举常量也是不可的，比如： enum Side{Right,Left};enum Thing{Wrong,Right};这是不能一起用的。 另外一个缺陷是传统枚举值总是被隐式转换为整形，用户无法自定义类型。C++11中的强类型枚举解决了这些问题。————————————————————————— 二、强类型枚举 强类型枚举使用enum class语法来声明，如下： enum class Enumeration{ VAL1, VAL2, VAL3=100, VAL4};这样，枚举类型时安全的，枚举值也不会被隐式转换为整数，无法和整数数值比较，比如（Enumeration：：VAL4==10会触发编译错误）。 另外枚举类型所使用的类型默认为int类型，也可指定其他类型，比如： enum calss Enum:unsigned int{VAL1,VAL2};正如前面所说，强类型枚举能解决传统枚举不同枚举类下同枚举值名的问题，使用枚举类型的枚举名时，必须指明所属范围，比如：Enum::VAL1，而单独的VAL1则不再具有意义。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重写C++中异常类的what方法]]></title>
    <url>%2Fz_post%2FCpp-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95-%E9%87%8D%E5%86%99Cpp%E4%B8%AD%E5%BC%82%E5%B8%B8%E7%B1%BB%E7%9A%84what%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[待补充完善：错误原因以及为什么这么修改坑源在实现项目ZeroTensor的专属异常类时，需要实现exception类的what方法。 出现问题及解决办法首先需要看一下exception基类中关于what方法的原始定义： 12 下面是正确的重写方式 1const char *what() const throw() override &#123; return error_msg_.c_str(); &#125; 如果去掉throw() ， 则会报错： 12looser throw specifier for ‘virtual const char* zerotensor::ZerotensorError::what() const’ const char *what() const override &#123; return error_msg_.c_str(); &#125; 如果将char * 改为 string，则会报错： 12error: ‘const string zerotensor::ZerotensorError::what() const’ cannot be overloaded const string what() const throw() override &#123; return error_msg_; &#125;]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>踩坑记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的mutable关键字]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84mutable%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[在C++中，mutable是为了突破const的限制而设置的。被mutable修饰的变量，将永远处于可变的状态，即使在一个const函数中，甚至结构体变量或者类对象为const，其mutable成员也可以被修改。 mutable在类中只能够修饰非静态数据成员。mutable 数据成员的使用看上去像是骗术，因为它能够使const函数修改对象的数据成员。然而，明智地使用 mutable 关键字可以提高代码质量，因为它能够让你向用户隐藏实现细节，而无须使用不确定的东西。我们知道，如果类的成员函数不会改变对象的状态，那么这个成员函数一般会声明成const的。但是，有些时候，我们需要在const的函数里面修改一些跟类状态无关的数据成员，那么这个数据成员就应该被mutalbe来修饰。 12345678910111213141516171819202122232425262728293031323334struct tagData&#123; int a; mutable int b;&#125;;class clsData&#123;public: int a; mutable int b; void show() const &#123; a = 2;//错误，不能在const成员函数中修改普通变量 b = 5;//正确 printf("a: %d, b: %d\r\n"); &#125;&#125;;int _tmain(int argc, _TCHAR* argv[])&#123; //结构体变量为const，其mutable成员也可以被修改 const tagData dat = &#123;0, 0&#125;; dat.a = 8;//编译错误 dat.b = 9;//编译通过 //类对象为const，其mutable成员也可以被修改 clsData cls; cls.show(); return 0;&#125; const承诺的是一旦某个变量被其修饰，那么只要不使用强制转换(const_cast)，在任何情况下该变量的值都不会被改变，无论有意还是无意，而被const修饰的函数也一样，一旦某个函数被const修饰，那么它便不能直接或间接改变任何函数体以外的变量的值，即使是调用一个可能造成这种改变的函数都不行。这种承诺在语法上也作出严格的保证，任何可能违反这种承诺的行为都会被编译器检查出来。 mutable的承诺是如果某个变量被其修饰，那么这个变量将永远处于可变的状态，即使在一个const函数中。这与const形成了一个对称的定义，一个永远不变，而另外一个是永远可变。 看一个变量或函数是否应该是const，只需看它是否应该是constant或invariant，而看一个变量是否应该是mutable，也只需看它是否是forever mutative。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用CUDA进行一维数组的矢量求和]]></title>
    <url>%2Fz_post%2FCUDA-CUDA%E7%A4%BA%E4%BE%8B%E5%AD%A6%E4%B9%A0-%E4%B8%80%E7%BB%B4%E6%95%B0%E7%BB%84%E7%9A%84%E7%9F%A2%E9%87%8F%E6%B1%82%E5%92%8C%2F</url>
    <content type="text"><![CDATA[基于GPU的一维数组的矢量求和 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;cstdio&gt;#define N 20//如果忘记写global，会报错：error: a host function call cannot be configured__global__ void add(int *a, int *b, int *c)&#123; int tid = blockIdx.x; if(tid&lt; N)&#123; c[tid] = a[tid] + b[tid]; &#125;&#125;int main()&#123; int a[N]; int b[N]; int c[N]; int *dev_a, *dev_b, *dev_c; /*cudaMalloc((void**)&amp;dev_a, N*sizeof(int));*/ /*cudaMalloc((void**)&amp;dev_b, N*sizeof(int));*/ /*cudaMalloc((void**)&amp;dev_c, N*sizeof(int));*/ cudaMalloc(&amp;dev_a, N*sizeof(int)); cudaMalloc(&amp;dev_b, N*sizeof(int)); cudaMalloc(&amp;dev_c, N*sizeof(int)); for(int i =0 ; i&lt;N; i++)&#123; a[i] = i; b[i] = i; &#125; cudaMemcpy(dev_a, a, N*sizeof(int), cudaMemcpyHostToDevice); cudaMemcpy(dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice); add&lt;&lt;&lt;N,1&gt;&gt;&gt;(dev_a,dev_b,dev_c); cudaMemcpy(c, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost); cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); for(int i =0 ;i&lt;N; i++)&#123; //如果错误的访问量dev_c[i]，会报告段错误 std::cout&lt;&lt;c[i]&lt;&lt;std::endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA示例学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第十六章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter16%2F</url>
    <content type="text"><![CDATA[第十六章 string类和标准模板库16.1 string 类头文件： string：支持的是string类 string.h / cstring： 支持的是C-风格字符串的C库字符串函数 16.1.1 构造字符串string类的构造函数如下（用ctor标识，这是传统C++中构造函数的缩写，表中的NBTS（null-terminated string）标识以空字符结束的字符串——传统的C字符串，size_type 是一个依赖于实现的整型，在头文件string中定义）： 构造函数 描述 string(const char *s) 将string对象初始化为s指向的NBTS string(size_type n, char c) 创建一个包含n个元素的string对象，其中每个元素都被初始化为字符c string(const string &amp;str) 将一个string对象初始化为string对象str（复制构造函数） string() 创建一个默认的string对象，长度为0（默认构造函数） string(const char *, size_type n) 将string对象初始化为s指向的NBTS的前n个字符，即使超过了NBTS的结尾 template string(Iter begin, Iter end) 将string对象初始化为区间[begin,end)内的字符，其中begin和end的行为就像指针，用于指定位置，范围包括begin在内，但不包括end string(const string &amp;str, size_type pos = 0, size_type n = npos) 将一个stirng对象初始化为对象str从位置pos开始到结尾的字符，或从pos开始的n个字符 string(string &amp;&amp; str) noexcept C++11新增，它将一个string对象初始化为对象str，并可能修改str（移动构造函数） string(initializer_list il) C++11新增，它将一个string对象初始化为初始化列表il中的字符 在使用构造函数时都进行了简化，即隐藏了这样一个事实：string实际上是模板具体化basic_string&lt;char&gt;的一个typedef，同时省略了与内存管理相关的参数 16.1.2 string类输入对于C-风格字符串，有3种方式：12345char info[100];cin &gt;&gt; info; //read a word 遇到空格换行停止cin.getline(info, 100); // read a line, discard \ncin.get(info, 100); // read a line, leave \n in queue 对于string对象，有两种方式：123string stuff;cin &gt;&gt; stuff; // read a wordgetline(cin, stuff); // read a line, discard \n 两个版本的getline都有一个可选参数，用于指定使用哪个字符来确定输入的边界。在功能上，它们之间主要的区别在于，string版本的getline()将自动调整目标string对象的大小，使之刚好能够存储输入的字符。1234567char fname[10];string lname;cin &gt;&gt; fname; // could be a problem if input size &gt; 9 chcin &gt;&gt; lname; // can read a very,very long wordcin.getline(fname,10); // may truncate inputgetline(cin, fname); // no truncation 在设计方面的一个区别是，读取C-风格字符串的函数是istream类的方法，而string版本是独立的函数。这就是对于C-风格字符串输入，cin是调用对象；而对于string对象输入，cin是一个函数参数的原因。这种规则也适用于&lt;&lt;形式：1123cin.operator&gt;&gt;(fname);operator&gt;&gt;(cin, lname); string类可以自动调整对象的大小，使之与输入匹配，但也存在一些限制： string对象的最大允许长度： 由常量string::npos 指定，通常是最大的unsigned int值 程序可以使用的内存量 string版本的getline()函数从输入中读取字符，并将其存储到目标string中，知道发生下列三种情况之一： 到达文件尾，在这种情况下，输入流的eofbit将被设置，这意味着方法fail()和eof()都将返回true； 遇到分界字符（默认为\n），在这种情况下，将把分解字符从输入流中删除，但不存储它 读取的字符数打到最大允许值（string::npos与可供分配的内存字节数中较小的一个），在这种情况下，将设置输入流的failbit，这意味着方法fail()返回true。 16.1.3 使用字符串C++对每个关系运算符进行了重载，以便能够将string对象与另一个string对象或C-风格字符穿进行比较：123operator&lt;(const string &amp;, const string &amp;);operator==(const string &amp;, const char *);operator!=(const char *, const string &amp;); size()和length()方法都返回字符串中的字符数，二者没有区别，前者是为提供STL兼容性添加的，后者是较早版本的string类使用的方法名。 可以以多种不同的方式在字符串中搜索给定的子字符串或字符。下表描述是了find()方法的4个版本。 方法原型 描述 size_type find(const string &amp;st ,size_type pos=0) const 从字符串的pos位置开始，查找子字符串str。如果找到，则返回该字符串首次出现时其首字符的索引；否则，返回string::npos size_type find(const char* s, size_type pos=0) const 从字符串的pos位置开始，查找子字符串s。如果找到，则返回该字符串首次出现时其首字符的索引；否则，返回string::npos size_type find(const char *s, size_type pos=0,size_type n) 从字符串的pos位置开始，查找s的前n个字符组成的子字符串。如果找到，则返回该子字符串首次出现时其首字符的索引；否则，返回string::npos size_type find(char ch, size_type pos = 0) const 从字符串的pos位置开始，查找字符ch。如果找到，则返回该字符首次出现的位置；否则，返回string::npos 除此之外，string库还提供了其他相关的方法：rfind() ,find_first_of(), find_last_of(), find_first_not_of(), find_last_not_of()等等。 16.1.4 stirng类还提供了哪些功能还有很多功能，具体可看p665或者附录F。 16.1.5 字符串种类表面上看起来string类是基于char类型的，但是，实际上，string库是基于一个模板类的：12345template&lt; class CharT, class Traits = std::char_traits&lt;CharT&gt;, class Allocator = std::allocator&lt;CharT&gt;&gt; class basic_string; 模板basic_string有多个具体化，每个具体化都有一个typedef名称，如string、wstring、u16string等等。 16.2 智能指针模板类智能指针是行为类似于指针的 类对象 。共有三种可以帮助管理动态内存分配的智能指针模板： auto_ptr unique_ptr shared_ptr 模板auto_ptr是C++98提供的解决方案，C++11已将其摒弃，并提供了后两种解决方案。 16.2.1 使用智能指针以上三个智能指针模板（auto_prt,unique_ptr,shared_ptr)都定义了类似指针的对象，可以将new获得的地址赋给这种对象。 当智能指针过期时，其析构函数将使用delete来释放内存，因此，如果将new返回的地址赋给这种对象，将无需记住稍后释放这些内存。（要创建智能指针，需包含头文件memory） 16.2.2 有关智能指针的注意事项当多个指针对象指向同一个对象时，三种智能指针的区别： auto_ptr：调用多次析构函数，执行多次delete，报错 unique_ptr：建立所有权（ownership）概念，对于特定的对象，只能有一个智能指针可拥有它，在删除后，会将所有权转让 shared_ptr：利用引用计数（reference counting），仅当最后一个指针过期时，才调用delete。 16.2.3 unique_ptr为何优于auto_prtunique_ptr更安全： 它使用了C++11新增的移动构造函数和右值引用unique_ptr可以用于数组的变体：auto_ptr智能使用new和delete，而unique_ptr可以还可以使用new []和delete []。 警告： 使用new分配内存时，才能使用auto_ptr和shared_ptr，使用new[]分配内存时，不能使用它们 不使用new分配内存时，不能使用auto_ptr和shared_ptr 不使用new和new[]分配内存时，不能使用unique_ptr 16.2.4 选择智能指针如果程序要使用多个指向同一个对象的指针，应选择shared_ptr。 16.3 标准模板库STL提供了一组表示容器、迭代器、函数对象和算法的模板。 容器：是一个与数组类似的单元，可以存储若干个值。STL容器是同质的，即存储的值的类型相同 迭代器：能够用来遍历容器的对象，与能够遍历数组的指针类似，是广义指针 函数对象：类似于函数的对象，可以是类对象或函数指针 算法：完成特定任务（如find，verse等）。 STL不是面对对象的变成，而是一种不同的编程模式——泛型编程（generic programming） 关于更多个STL方法和函数可以看附录G 16.3.1 模板类vector分配器：与string类相似，各种STL容器模板都接受一个可选的模板参数，该参数指定使用哪个分配器对象来管理内存。如果省略了该模板参数的值，则容器类模板将默认使用allocator&lt;T&gt;类，这个类使用new和delete。 123template &lt;class T, class Allocator = allocator&lt;T&gt; &gt;class vector &#123;...&#125; 16.3.2 可对矢量执行的操作每个容器类都定义了一个合适的迭代器，该迭代器的类型是一个名为iterator的typedef，其作用域为整个类。 1vector&lt;double&gt;::iterator pd; // pd an iterator 超尾(past-the-end) ：一种迭代器，指向容器的最后一个元素的后面，就像是C-风格字符串最后一个字符后面的空字符一样，由end()成员函数标识。 16.3.3 对矢量可执行的其他操作对于搜索、排序、随机排序等等很常见的操作，矢量模板类并没有包含！！ 但是 ，STL从更广泛的角度定义了非成员（non-member）函数来执行这些操作，即不是为每个容器类定义find()函数，而是定义了一个适用于所有容器类的非成员函数find()。这种设计理念省去了大量重复的工作。 另一方面，STL有时也会定义一个功能相同的成员函数。这是因为对有些操作来说，类特定算法的效率比同于算法高，比如，vector的成员函数swap()的效率比非成员函数swap()高，但非成员函数可以交换冷儿类型不同的容器的内容。 16.3.4 基于范围的for循环（C++11）第五章的示例： 123double prices[5] = &#123;4.99, 10.99, 6.87, 7.99, 8.48&#125;;for (double x : prices) cout &lt;&lt; x &lt;&lt; std::endl; 在这种for循环中，先声明一个类型与容器内容类型相同的变量，然后写明容器名称，接下来就可以访问，如下面的代码可以写的更加精简： 1234for_each(books.begin(), books.end(), ShowReview); //for_each形式for( auto x:books) ShowReview(x)); // 注意，for_each不能修改容器的容器，而基于范围的for循环可以通过指定一个引用参数来修改内容。例如，假设有如下函数：1void InflateReview(Review &amp;r) &#123;r.rating++;&#125; 可使用如下循环的books的每个元素执行该函数1for(auto &amp;x:books) InflateReview(x); 16.4 泛型编程面向对象编程关注的是编程的数据方面，而泛型编程关注的是算法。它们之间的共同点是抽象和创建可重用代码，但它们的理念决然不同。泛型编程旨在编写独立于数据类型的代码。 16.4.1 为何使用迭代器模板使得算法独立于存储的数据类型，而迭代器使算法独立于使用的容器类型。 例如，在使用find函数时，可以用迭代器来实现不依赖于具体类型的查找。 实际上，作为一种编程风格，最好避免直接使用迭代器，而应尽可能使用STL函数(如for_each()）来处理细节。也可以使用C++11新增的基于范围的for循环。 16.4.2 迭代器类型不同的算头对迭代器要求不同，有的只要求可读，有的要求可随机访问等等。STL定义了5种不迭代器，并根据所需的迭代器类型对算法进行了描述： 输入迭代器 输出迭代器 正向迭代器 双向迭代器 随机访问迭代器 对于不同的算法，需要的迭代器不同，如下面两种算法分别需要输入迭代器和随机访问迭代器 12345template&lt;typename InputIterator, typename T&gt;InputIterator find(InputIterator first, InputIterator last, const T &amp;value);template&lt;typename RandomAccessIterator&gt;void sort(RandomAccessIterator first, RandomAccessIterator last); 对于这5种迭代器，都可以执行解除引用操作（即*运算符），也可进行比较，看其是相等还是不相等（==，！=运算符）。如果两个迭代器相同，则对它们执行解除引用操作得到的值将相同。 输入迭代器 &emsp;&emsp;术语“输入”是从程序的角度出发的，即来自容器的信息被视为输入（从迭代器传到程序中），因此，输入迭代器可被程序用来读取容器中的信息。需要输入迭代器的算法将不会修改容器中的值 &emsp;&emsp;输入迭代器必须能做访问容器中所有的值，这是通过支持++运算符（前缀格式operator++和后缀格式operator++(int)）来实现的。 &emsp;&emsp; 注意： 并不能保证输入迭代器第二次遍历容器时，顺序不变。另外，输入迭代器被递增后，也不能保证其先前的值仍然可以被解除引用。基于输入迭代器的任何算法都应当是单通行（single-pass）的，不依赖于前一次便利时的迭代器值，也不依赖于本次遍历中前面的迭代器值。—— 输入迭代器是单向迭代器，可以递增，但不能倒退 输出迭代器 &emsp;&emsp;同理，“输出”指用于将信息从程序传输给容器的迭代器（从程序输出到迭代器中），输出迭代器智能解除引用让程序修改容器值，而不能读取容器内的值。输出迭代器也是单通行的，只能写不能读。 正向迭代器 &emsp;&emsp; 只是用++运算符遍历容器，每次沿容器向前移动一个元素。与输入和输出迭代器不同的是，它总是按相同的顺序遍历一系列值，另外，将正向迭代器递增后，仍然可以对前面的迭代器值解除引用，并得到对应的值 。这些特征使得正向迭代器是多通行的（可以多次通行容器）。 双向迭代器 &emsp;&emsp;双向迭代器具有正向迭代器的所有特性，且同时支持两种（前缀和后缀）递减运算符。 随机访问迭代器 &emsp;&emsp;有些算法（如排序和二分检索）要求能够直接跳到容器中的任何一个元素（将想数组下标访问一样），因此就有了随机访问迭代器。该迭代器具有双向迭代器的所有特性，同时添加了支持随机访问的操作和用于对元素进行排序的关系运算符。 16.4.3 迭代器层次结构可以看出，迭代器类型形成了一个层次结构。后面的迭代器不仅拥有之前迭代器的所有功能，同时还有自己的功能。 每个容器类都定义了一个类级typedef名称——iterator。（实际上这使用指针实现的，并不是一种新的类型）。 16.4.4 概念、改进和模型STL有若干个用C++语言无法表达的特性，如迭代器种类。正向迭代器是一系列要求，而不是类型。（迭代器通常可以用常规指针实现，所以迭代器本身并不是一种类型）。 STL使用术语“概念（concept）”来描述一系列的要求。 概念可以具有类似继承的关系，就像双向迭代器继承了正向迭代器的功能。然而，不能将C++继承机制用于迭代器，但从概念上看，它确实能够继承，有些STL文献使用属于改进（refinement）来表示这种概念上的继承，因此，双向迭代器是对正向迭代器概念的一种改进。 概念的具体实现被称为模型（model）。因此，指向int的常规指针是一个随机访问迭代器模型，同时也是一个正向迭代器模型。 将指针用作迭代器 &emsp;&emsp;迭代器是广义上的指针，其本身就是一种指针，因此，STL算法可以使用指针来对基于指针的非STL容器进行操作。例如，可将STL算法用于数组。123const int SIZE = 100;double Receipts[SIZE];sort(Receipts,Receipts+SIZE); //用STL的sort算法对数组进行排序 &emsp;&emsp;STL提供了一些预定义迭代器，如ostream_iterator和istream_iterator模板等，都定义在iterator头文件中。 其他有用的迭代器 &emsp;&emsp;头文件iterator还提供了其他一些专用的预定义迭代器类型，它们是reverse_iterator, back_insert_iterator, front_insert_iterator和insert_iterator等。 16.4.5 容器种类 容器概念：概念是具有名称（如容器、序列容器、关联容器等）的通用类别 容器类型：是可用于创建具体容器对象的模板。以前的11个容器类型为：deque, list, queue, priority_queue, stack, vector, map, multimap, set, multiset和bitset。 C++11新增了：forward_list, unordered_map, unordered_multimap, unordered_set和unordered_multiset，且不再将bitset视为容器，而将其视为一种独立的类别。 容器概念 &emsp;&emsp;没有与基本容器概念对应的类型，但概念描述了所有容器类都通用的元素。存储在容器中的类型必须是可复制构造和可赋值的。STL特定操作的时间复杂度有三种，分别是：编译时间、固定时间和线性时间。 C++11新增的容器要求 &emsp;&emsp;下表列出了C++11新增的通用容器要求，其中，复制赋值和移动赋值之间的差别在于，复制操作保留源对象，而移动操作可修改源对象，还可能转让所有权，而不做任何复制。如果源对象是临时的，移动操作的效率将高于常规复制。 表达式 返回类型 说明 复杂度 X u(rv); 调用移动构造函数后，u和值和rv的原始值相同 线性 X u = rv; 作用同上 a = rv; X&amp; 调用移动赋值运算符后，u的值和rv的原始值相同 线性 a.cbegin() const_iterator 返回指向容器第一个元素的const迭代器 固定 a.cend() const_iterator 返回超尾值的const迭代器 固定 序列 16.4.6 关联容器16.4.7 无序关联容器16.5 函数对象函数对象也叫做仿函数 (funtor). 仿函数是可以以函数方式与()运算符结合使用的任意对象. 重载后的()运算符将使得能够响函数那样使用类对象. 16.5.1 仿函数概念16.5.2 预定义的仿函数16.5.3 自适应仿函数16.7.3 使用initializer_list]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11新特性-初始化列表]]></title>
    <url>%2Fz_post%2FCpp-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[统一的初始化过程在 C++98/03 中, 我们只能对普通数组和POD( plain old data, 简单来说就是可以用memcpy复制的对象) 类型使用初始化列表, 如下:123456int arr[3] = &#123;1,2,3&#125;;struct A&#123; int x; int y;&#125;a = &#123;1,2&#125;; 但是在C++11中, 初始化列表的适用性被放大, 可以作用于任何类型对象的初始化 12345T test&#123;&#125;;//如:int a&#123;2&#125;; // 与 int a=2; int a(2); 等价std:vector&lt;double&gt; pyments &#123;35.99, 129.22&#125;;Foo fool&#123;123,456,789&#125;; //Foo为一个类或结构体 C++的类对象创建过程C++ 在创建类时需要经过两个阶段：分配空间（Allocation）和初始化（Initialization） 分配空间创建C++类对象的第一步就是为其分配内存空间。对于全局对象，静态对象以及分配在栈区域内的对象，对它们的内存分配是在编译阶段就完成了，而对于分配在堆区域内的对象，它们的分配是在运行是动态进行的。内存空间的分配过程涉及到两个关键的问题：需要分配空间的大小以及是否有足够的内存空间来满足分配。 初始化首先需要区分两个概念：初始化（Initialization）和赋值（Assignment）。初始化早于赋值，它是随着对象的诞生一起进行的。而赋值是在对象诞生以后又给予它一个新的值。 在C++中，提供了类成员的初始化列表，并且初始化列表是先于构造函数体内的代码执行的。 关于成员初始化列表对于以下三种情况，必须使用成员初始化列表 需要初始化的数据成员是对象的情况（包含继承情况下，通过显示调用父类的构造函数对父类数据成员进行初始化） 需要初始化const修饰的类成员 需要初始化引用成员数据 子类初始化父类的私有成员，需要在（并且只能在）参数初始化列表中显示调用父类的构造函数。（这种其实可以并到第一种情况，因为初始化私有成员，就以为着初始化对象） 类对象是默认使用初始化列表的，当没有无参构造函数时，就必须显式使用初始化列表（所以不论如何，都会使用到初始化列表）。 当类成员中含有一个const对象时，或者一个引用时，必须经过成员初始化列表进行初始化，因为const对象或者引用在声明的同时必须初始化，而在构造函数中，做的是对它们的赋值，并不是初始化。 构造函数与初始化列表成员初始化列表只能用于构造函数 在类的实现中，构造函数体内“初始化”的实际上是赋值而不是初始化。也就是说，当代码运行到构造函数内部时，初始化列表已经执行完了，因此相当于是先初始化了一遍，然后又赋值了一遍，重复计算，浪费效率，因此应该优先使用初始化列表。同时，当没有默认的无参构造函数时，就一定会使用初始化列表。(即使自己没有在含参构造函数中都是基本数据类型, 不强制显式使用初始化列表, 也会自动调用初始化列表, 而在构造函数内部执行的仅仅是赋值) 创建派生类对象时，程序首先调用基类构造函数，然后再调用派生类构造函数。基类构造函数负责初始化继承的数据成员；派生类构造函数主要用于初始化新增的数据成员。派生类构造函数总是需要调用一个基类构造函数。当基类没有默认的构造函数时，就必须显式指明调用哪一个构造函数。在继承关系中, 必须显式的在初始化列表中对基类初始化, 因为只有基类初始化完成后, 才能进行子类初始化, 而当进入子类构造函数内部时, 子类初始化已经完成. 注意：除了虚基类以外，类只能将值传递会相邻的基类，但后者可以使用相同的机制将信息传递给上层相邻的基类，以此类推。 初始化列表的顺序构造函数需要初始化的数据成员，不论是否显式的出现在构造函数的成员初始化列表中，都会在该处完成初始化，并且初始化的顺序和变量声明时的顺序是一致的，与列表中的先后顺序无关]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中类的对象和对象指针之间的区别]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%B1%BB%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%92%8C%E5%AF%B9%E8%B1%A1%E6%8C%87%E9%92%88%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[很关键的一点：定义对象实例时，分配了内存，指针变量则未分配类对象所需内存。 类的指针:他是一个内存地址值,他指向内存中存放的类对象(包括一些成员变量所赋的值).对象,他是利用类的构造函数在内存中分配一块内存(包括一些成员变量所赋的值). 指针变量是间接访问，但可实现多态（通过父类指针可调用子类对象），并且没有调用构造函数。直接声明可直接访问，但不能实现多态，声明即调用了构造函数（已分配了内存）。 类的对象:用的是内存栈,是个局部的临时变量.类的指针:用的是内存堆,是个永久变量,除非你释放它. 1.在类的声明尚未完成的情况下，可以声明指向该类的指针，但是不可声明该类的对象… 例如：含有纯虚成员函数的抽象类。2.父类的指针可以指向子类的对象.. 在应用时: 1.引用成员: 对象用” . “操作符; 指针用” -&gt; “操作符. 2.生命期: 若是成员变量,则是类的析构函数来释放空间;若是函数中的临时变量,则作用域是该函数体内.而指针,则需利用delete 在相应的地方释放分配的内存块. 注意:用new ,一定要delete.. C++的精髓之一就是多态性，只有指针或者引用可以达到多态。对象不行类指针的优点：第一实现多态。第二，在函数调用，传指针参数。不管你的对象或结构参数多么庞大，你用指针，传过去的就是4个字节。如果用对象，参数传递占用的资源就太大了]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的const关键字]]></title>
    <url>%2Fz_post%2FCpp-const%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[const与#define相比, 有何优点? const常量具有数据类型, 而宏常量没有数据类型. 编译器可以对前者进行类型安全检查, 而对后者只会进行字符替换, 没有类型安全检查, 容易发生意想不到的错误. 有些集成化的调试工具可以对const常量进行调试, 但是不能对宏常量进行调试 const常用形式及代表含义 含义 使用形式 及 作用 const常量 const int Max = 100; int const Max =100 const与int的位置可互换，二者等价 修饰后的变量在程序的任意位置将不能再被修改，就如同常数一样，任何修改该变量的尝试都会导致编译错误（由于常量在定义以后就不能再被修改，所以定义时必须初始化）。 对于类中的const成员变量必须通过初始化列表进行初始化 const引用 const int i = 1024； const int &amp;ref_i = i; int const &amp;ref2_i = i后两句等价int &amp;const ref3_i = i这种方式未定义，会报错：‘const’ qualifiers cannot be applied to ‘int&amp;’ cosnt引用是指向const对象或者普通对象的引用，该引用ref_i可以读取对象i的值，但是，不能修改 i的值，任何对ref_i的赋值都是非法的。 并且 不能用普通引用指向const常量（int &amp;ref4_i = i; // 非法）。（注意，一旦引用被定义，它就不能再指向其它对象，这是引用本身的性质，这也是为什么引用必须进行初始化的原因）。 总结来说就是const引用只是表明：保证不会通过此引用间接的改变被引用的对象！ const指针 1）int age = 39; const int *ptr = &amp;age; 2）const int age = 98; const int *ptr = &amp;age;3）const int age = 32; int *ptr = &amp;age;4）int age = 90; int * const ptr = &amp;age; 对于1），const修饰的是int，表明ptr指向一个const int，但是ptr和age本身都不是const，不能使用ptr来修改age的值，但是age自身可以修改自己的值，同时，ptr也可以转而指向其它变量； 对于2），const修饰的是int，表明ptr指向一个const int，同时age本身就是const，说明既不能通过ptr，也不能通过age来修改变量age的值，但是ptr本身仍然不是const，因此可以转而指向其它变量；对于3），C++禁止将const变量的地址赋给非const指针（除非使用强制类型转换）；对于4），const修饰的是*，表明ptr本身是一个常量指针，这使得ptr自身的值不能改变，也就是只能指向age，不能指向其它变量 const函数参数 void fun(const int * i); void fun(const int &amp; i); 不能在函数体内修改指针或引用指向的值，但是这里指针可以转而指向其他值（其实没多大用，因为指针本身就是值传递，即使改变指向，也不会影响实参的指向，除非用二级指针） const函数返回值 const int fun( int i); 阻止用户修改返回值，返回值必须要相应的赋给一个具有const属性的变量 const成员函数 &lt;类型说明符&gt; &lt;函数名&gt; ( &lt;参数表&gt; ) const; 不会修改类的成员数据，也不能调用其它非const成员函数。 可以利用const限定符实现函数重载。 const对象默认会调用const成员函数 const限定符和static的区别 静态变量的值虽然只能进行一次初始化，但是它的值是可变的。而const的值是不可变的。 const定义的变量在超出其作用域后空间就会被释放，而static定义的静态变量不会释放空间，而是一直存在，知道程序结束 static表示静态，类的静态成员函数、静态成员变量都是和类相关的，而不是和具体的对象相关，即使没有具体对象，也能调用静态成员函数和静态成员变量。而类则是和具体的对象相关的，不同的对象独占一个const变量 static静态成员变量不能在类的内部进行初始化，智能在类的外部进行初始化，并且初始化时不能加static关键字。之后，无需创建对象也能通过类使用该静态变量，同时 由于const变量只是针对于某个类的对象而言的，类可以创建多个不同的对象，每个对象都有自己的const变量，又因为const变量值只能进行一次初始化而不仅再次赋值，因此， const变量也不能在类的内部初始化，而只能在类构造函数的初始化列表中进行 。 如果想要创建整个类的恒定变量，那么应用使用static const来声明（const枚举量也可以） 关于const的其它注意事项将非const指针赋给const指针（两级间接关系）p222 const引用与不可寻址值const引用可以用不同类型的对象初始化（主要能从一种类型转换到另一种类型即可），也可以用字面常量初始化，如下所示： 12345double dval = 3.14159;//下3行仅对const引用才是合法的const int &amp;ir = 1024;const int &amp;ir2 = dval; //隐式类型转换，生成临时变量const double &amp;dr = dval + 1.0; 首先要知道，上面的初始化对于非cosnt引用是不合法的，将导致编译错误！ 原因：引用在内部存放的是一个对象的地址，它是该对象的别名。对于不可寻址的值，如文字常量，以及不同类型的对象，编译器为了实现引用，必须生成一个临时对象，将该对象的值置入临时对象中，引用实际上指向该对象（对该引用的操作就是对该临时对象的操作），但用户不能访问它。因此，C++为了不让用户通过引用修改该临时变量，强制要求必须使用const引用。 const引用与临时变量由于上面提到的原因， C++引用类型在面对会产生临时变量的语句时，必须使用const引用来指向！！切记！！ 1234567891011int i = 100;int *&amp;p_i = &amp;i;//错误int *const &amp;p_i = &amp;i; //正确写法，const修饰int *//&amp;代表p_i是一个引用（别名），*代表这个引用（别名）是一个指向int类型的指针，//而后面的&amp;i代表取i的地址，注意，这里会生成一个存储地址的临时变量，因此，指向该临时变量的引用（别名）必须为const，//也就是说，这个指向int的指针本身必须是const的，因为不能修改临时变量的值const int i = 100;int *&amp;p_i = &amp;i;// 错误，有临时变量生成，引用必须是constint *const &amp;p_i = &amp;i; //错误，由于i本身就是cosnt，因此指针必须是一个指向cosnt int的指针const int *const &amp;p_i = i;//正确 const引用与非const引用在内存中的对比内存地址都是一样的，网上有的写不一样，是错误的！ 1234567891011cosnt int t = 9;const int &amp;k = t;cout&lt;&lt;&amp;k&lt;&lt;endl; // 0012FF74cout&lt;&lt;&amp;t&lt;&lt;endl; // 0012FF74int t = 9;int &amp;k = t;const int &amp;m = t;cout&lt;&lt;&amp;k&lt;&lt;endl; // 0012FF74cout&lt;&lt;&amp;m&lt;&lt;endl; // 0012FF74cout&lt;&lt;&amp;t&lt;&lt;endl; // 0012FF74 不允许非const引用指向需要临时对象的对象或值，即，编译器产生临时变量的时候引用必须为const!!!!切记！！12345678910int iv = 100;int * &amp;pir = &amp;iv;//错误，非const引用对需要临时对象的引用int *const &amp;pir = &amp;iv;//okconst int ival = 1024;int *&amp;pi_ref = &amp;ival; //错误，非const引用是非法的const int *&amp;pi_ref = &amp;ival; //错误，需要临时变量，且引用的是指针，而pi_ref是一个非常量指针const int * const &amp;pi_ref = &amp;ival; //正确//补充const int *p = &amp;ival;const int *&amp;pi_ref = p; //正确 const变量的生存期、作用域和链接性临时变量一般会在语句块的末尾自动释放，但是有两个例外： 将临时对象作为初始化因子，例如string s = string(&quot;hello world&quot;); 将一个常量引用变量绑定到这个临时对象上。 作用域在全局作用域中声明的const变量，只能在当前文件中访问，如果要在其他文件中使用，则必须用extern显式的声明const变量全局变量。（在C中，可以不用extern？） 用const修饰函数参数是否应将void Func(int x) 改写为void Func(const int &amp;x)，以便提高效率？完全没有必要，因为内部数据类型的参数不存在构造、析构的过程，而复制也非常快，“值传递”和“引用传递”的效率几乎相当。问题是如此的缠绵，我只好将“const &amp;”修饰输入参数的用法总结一下。对于非内部数据类型的输入参数，应该将“值传递”的方式改为“const 引用传递”，目的是提高效率。例如将void Func(A a) 改为void Func(const A &amp;a)。 对于内部数据类型的输入参数，不要将“值传递”的方式改为“const 引用传递”。否则既达不到提高效率的目的，又降低了函数的可理解性。例如void Func(int x) 不应该改为void Func(const int &amp;x)。 用const修饰函数的返回值 如果给以“指针传递”方式的函数返回值加const 修饰，那么函数返回值（即指针）的内容不能被修改，该返回值只能被赋给加const 修饰的同类型指针。 如果函数返回值采用“值传递方式”，由于函数会把返回值复制到外部临时的存储单元中，加const 修饰没有任何价值。 const 成员函数(const的作用：说明其不会修改数据成员) 1int GetCount(void) const; // const 成员函数]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的virtual关键字]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84virutal%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[虚函数与运行多态多态：多态按字面的意思就是多种形态。当类之间存在层次结构，并且类之间是通过继承关联时，就会用到多态。C++ 多态意味着调用成员函数时，会根据调用函数的对象的类型来执行不同的函数。 先看最简单的情况，也就是最普通形式的继承，且父类和子类的方法都是一般成员方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Car&#123; public: Car()&#123;cout&lt;&lt;"Car constructor"&lt;&lt;endl;&#125; ~Car()&#123;cout&lt;&lt;"Car destructor"&lt;&lt;endl;&#125; // 若将成员成员函数声明为const，则该函数不允许修改类的数据成员 void start() const&#123;cout&lt;&lt;"car start"&lt;&lt;endl;&#125; void stop() const&#123;cout&lt;&lt;"car stop"&lt;&lt;endl;&#125;&#125;;//Benz类，单一继承自Carclass Benz : public Car&#123; public: Benz()&#123;cout&lt;&lt;"Benz constructor"&lt;&lt;endl;&#125; ~Benz()&#123;cout&lt;&lt;"Benz destructor"&lt;&lt;endl;&#125; void start() const&#123;cout&lt;&lt;"Benz start"&lt;&lt;endl;&#125; void stop() const&#123;cout&lt;&lt;"Benz stop"&lt;&lt;endl;&#125;&#125;;// Baoma类，单一继承自Carclass Baoma:public Car&#123; public: Baoma()&#123;cout&lt;&lt;"Baoma constructor"&lt;&lt;endl;&#125; ~Baoma()&#123;cout&lt;&lt;"Baoma destructor"&lt;&lt;endl; &#125; void start() const&#123;cout&lt;&lt;"Baoma start"&lt;&lt;endl;&#125; void stop() const&#123;cout&lt;&lt;"Baoma stop"&lt;&lt;endl;&#125; private: int speed;&#125;;//以上三个类均具有start和stop的同名成员函数//调用成员函数start和stopvoid carFunction(Car *car)&#123; car-&gt;start(); car-&gt;stop();&#125;int main(int argc,char *argv[])&#123; Car *benz = new Benz(); cout&lt;&lt; size of(Benz)&lt;&lt;endl; carFunction(benz); Car *baoma = new Baoma(); cout&lt;&lt; size of(Baoma)&lt;&lt;endl; carFunction(baoma); delete benz; delete baoma; return 0;&#125; 输出结果如下：123456789101112Car constructorBenz constructor1 //内部没有成员变量,因此只有一个字节的空间car startcar stopCar constructorBaoma constructor4 //函数是不占用内存的,baoma中有一个int类型.所以 size of为4car startcar stopCar destructorCar destructor 首先，为什么Benz类内部明明没有任何变量，还具有一个字节的 size ？这是因为C++编译器不允许对象为零长度（试想一个长度为0的对象在内存中怎么存放？怎么获取它的地址？）。为了避免这种情况，C++强制给这种类插入一个缺省成员，长度为1。如果有自定义的变量，那么变量将取代这个缺省成员。 其次，Benz和Baoma都是继承自Car类，根据 里氏替换原则 ，父类能够出现的地方，那么子类也一定能出现。依赖抽象而不去依赖具体,在上述的函数调用过程中,我们传进去的是benz和baoma指针.但是在调用函数的时候,它并没有去调用子类的方法,这也就是一般成员函数的局限性,就是在编译的时候,一般性的函数已经被静态的编译进去,所以在调用的时候不能去选择动态调用. 另外, 这里的指针都是基类指针, 如果函数不是 virtual 的，则进行的是静态绑定，即在编译期间就决定了其调用的函数. 所以, 在删除时, 只会调用基类的析构函数, 而不会调用子类的析构函数. 如果将指针的类型声明为子类类型, 那么调用顺序是先调用子类的析构函数, 再调用基类的析构函数. 里氏替换原则：派生类（子类）对象可以在程式中代替其基类（超类）对象 加入vitural关键字修饰的函数,将父类函数变为虚函数,看看变化：在某个类中的某个函数之间加了 virtual 关键字以后, 该函数就会变成虚函数, 同时, 该类的所有派生类都会默认将此函数当做是虚函数, 无需显式使用 virtual 关键字注明. 派生类经常(但不总是)覆盖它要继承的虚函数, 如果派生类没有覆盖基类中的某个虚函数, 则派生类会自动继承基类版本的虚函数作为自己的虚函数. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//和上面几乎一样，都是一般的成员方法，只不过加上了virtual关键字#include&lt;iostream&gt;using namespace::std;class Car&#123; public: Car()&#123; cout&lt;&lt;"Car constructor"&lt;&lt;endl; &#125; ~Car()&#123; cout&lt;&lt;"Car destructor"&lt;&lt;endl; &#125; virtual void start() &#123; cout&lt;&lt;"car start"&lt;&lt;endl; &#125; virtual void stop() &#123; cout&lt;&lt;"car stop"&lt;&lt;endl; &#125;&#125;;class Benz : public Car&#123; public: Benz()&#123; cout&lt;&lt;"Benz constructor"&lt;&lt;endl; &#125; ~Benz()&#123; cout&lt;&lt;"Benz destructor"&lt;&lt;endl; &#125; //子类继承父类,如果是虚函数,可以写上vitural也可以不写 virtual void start() &#123; cout&lt;&lt;"Benz start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Benz stop"&lt;&lt;endl; &#125;&#125;;class Baoma:public Car&#123; public: Baoma()&#123; cout&lt;&lt;"Baoma constructor"&lt;&lt;endl; &#125; ~Baoma()&#123; cout&lt;&lt;"Baoma destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Baoma start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Baoma stop"&lt;&lt;endl; &#125; private: int speed;&#125;;void carFunction(Car *car)&#123; car-&gt;start(); car-&gt;stop();&#125;int main(int argc,char *argv[])&#123; Car *benz = new Benz(); cout&lt;&lt; size of(Benz)&lt;&lt;endl; carFunction(benz); Car *baoma = new Baoma(); cout&lt;&lt; size of(Baoma)&lt;&lt;endl; carFunction(baoma); delete benz; delete baoma; return 0;&#125; 输出结果如下： 12345678910111213Car constructorBenz constructor8Benz startBenz stopCar constructorBaoma constructor16Baoma startBaoma stopCar destructorCar destructor 从上面的输出结果中可以看到,加入了虚函数之后,调用不同指针对象指定函数的时候,这个时候都是去自动调用当前对象类中的具体函数形式,而不是像一般函数的调用一样,只是去调用父类的函数.这就是virtural关键字的作用,因为一般函数调用编译的时候是静态编译的时候就已经决定了,加入了virtural的函数,一个类中函数的调用并不是在编译的时候决定下来的,而是在运行时候被确定的,这也就是虚函数. 虚函数就是由于在编写代码的时候并不能确定被调用的是基类的函数还是哪个派生类的函数，所以被 为“虚”函数。 虚函数只能借助于指针或者引用来达到多态的效果， 直接声明的类对象无法达到多态目的。 这里可以看到, 指针 size 不再是1和4, 而是变成了8和16, 这是因为虚函数需要一张虚函数表来维护, 因此会使类的 size 改变, 具体原理可看下一节. 另外, 注意到这里在删除指针时, 由于指针的类型是基类, 因此同样只会调用基类的析构函数. 总结： 虚函数的调用取决于指向或者引用的对象的类型，而不是指针或者引用自身的类型。 注意: C++中的虚函数的作用主要是实现了多态的机制。关于多态，简而言之就是用父类型别的指针指向其子类的实例，然后通过父类的指针调用实际子类的成员函数。 对C++ 了解的人都应该知道虚函数（Virtual Function）是通过一张虚函数表（Virtual Table）来实现的。简称为V-Table。在这个表中，主是要一个类的虚函数的地址表，这张表解决了继承、覆盖的问题，保证其容真实反应实际的函数。这样，在有虚函数的类的实例中这个表被分配在了这个实例的内存中，所以，当我们用父类的指针来操作一个子类的时候，这张虚函数表就显得由为重要了，它就像一个地图一样，指明了实际所应该调用的函数 带有虚函数的对象自身确实插入了一些指针信息，而且这个指针信息并不随着虚函数的增加而增大,这也就是为什么上述增加了虚函数后,出现了 size 变大的现象 虚函数表与虚函数表针织C++中虚函数这种多态的性质是通过虚函数表指针和一张虚函数表来实现的： vtable（虚函数表）：每一个含有虚函数的类都会维护一个虚函数表，里面按照声明顺序记录了虚函数的地址 vptr（虚函数表指针）：一个指向虚函数表的指针，每个对象都会拥有这样的一个指针 上面简单介绍了虚函数表的作用, 下面我们详细讨论一下虚函数表的注意事项.每个包含了虚函数的类都包含一个虚函数表.当一个类(A)继承另一个类(B)时, 类A会继承类B的函数的调用权, 所有如果一个基类包含了虚函数, 那么其继承类也可调用这些虚函数, 换句话说, 如果一个类继承了包含虚函数的基类, 那么这个类也拥有自己的虚表.虚表是一个指针数组, 其元素是指向虚函数的指针.虚表是对应类而言的, 而不是对应某个具体的对象, 一个类只需要一个虚表即可, 同一个类的所有对象都是用同一个小标.为了使用虚表, 类或对象内部都会包含一个虚表指针, 用来指向自己所使用的虚表. 为了让每个包含虚表的类的对象都拥有一个虚表指针, 编译器会在类中添加一个指针*__vptr来指向自己的虚表, 这样, 类的对象在创建时便拥有了这个指针, 且这个指针的值会自动被设置为指向类的虚表.假设类A是虚基类, 类B继承类A, 类C有继承类B, 则它们的虚表关系如下图所示: 可以看到, 类A, B, C 中都会有一个专门的指针来指向虚表(一般都处于类或对象实例的最前面, 主要是为了提高取得函数表的速度), 并且指向的不是同一个虚表, 而是每个类都有自己的虚表, 只不过这些虚表最终指向的虚函数有可能相同(也有可能不同). 接下来看看下面这个简单的例子：12345678910111213class A&#123;public: virtual void fun();&#125;;class B&#123;public: void fun();&#125;; size of(A) &gt; size of(B) // true，因为A比B多了一个虚函数表指针 下面再来看看刚刚那个加薪的例子，其多态调用的形式如下图： 通常情况下，编译器在下面两处地方添加额外的代码来维护和使用虚函数表指针： 在每个构造函数中。此处添加的代码会设置被创建对象的虚函数表指针指向对应类的虚函数表 在每次进行多态函数调用时。 无论何时调用了多态函数，编译器都会首先查找vptr指向的地址（也就是指向对象对应的类的虚函数表），一旦找到后，就会使用该地址内存储的函数（而不是基类的函数）。 单继承时的虚函数表无虚函数覆盖:维护了一个虚函数表, 并且表中函数指针指向基类的各个虚函数. 子类中新添加的虚函数会放到虚函数表的后面 虚函数覆盖:对于同签名的函数, 会用子类的虚函数覆盖掉基类的同签名虚函数, 同样也只是维护一个虚函数表. 多重继承时的虚函数表多重继承会有多个虚函数表, 几重继承, 就会有几个虚函数表. 这些表按照派生的顺序依次排列, 如果子类改写了父类的虚函数, 那么就会用子类自己的虚函数覆盖虚函数表的相应位置, 如果子类有新的虚函数, 那么就会添加到第一个函数表的末尾. 假设类B继承了包含虚函数的类A1和类A2, 则其虚函数表的情况如下所示: 无虚函数覆盖:继承类几个基类, 就会维护几张虚函数表(按照继承顺序在实例最开始排列), 并且表中函数指针会指向其基类的各个虚函数, 子类中新添加的虚函数会放到第一个虚函数表的后面 虚函数覆盖:会用子类中的同签名虚函数同时覆盖两个基类. 其余与无覆盖时的情况相同. 为什么虚函数表指针的类型为void *因为对于虚函数表来说, 一个类中的所有虚函数都会放到这个表中, 但是不同的虚函数对应的函数指针类型各不相同, 所以这个表的类型也就无法确定. 为什么虚函数表前要加const因为虚函数表是在编译时, 由编译器自动生成的, 并且不会发生改变, 当有多个B类的实例时, 每个实例都会维护一个虚函数表指针, 但是这些指针指向的都是同一个虚函数表, 该表是一个常量表. 类的 size 与虚函数从上面一节的代码示例中我们已经发现, 在 C++ 中, 普通函数只是一种表示, 其本身并不会占有任何内存, 而如果类中没有任何变量或者虚函数时, 类的 size 不会为1, 而是会自动插入一个字节, 并且在类的 size 大于1的时候, 该字节会被覆盖掉. 下面我们就从头开始讨论一下在 C++ 中是如何计算类的 size 的. 类的 size 为零123456789101112131415161718192021222324252627#include&lt;iostream&gt;using namespace::std;class Car&#123; public: Car()&#123; cout&lt;&lt;"Car constructor"&lt;&lt;endl; &#125; ~Car()&#123; cout&lt;&lt;"Car destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"car start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"car stop"&lt;&lt;endl; &#125;&#125;;int main(int argc,char *argv[])&#123; Car *p_car = new Car(); Car car; cout&lt;&lt; size of(Car)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(car)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p_car)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 上面的输出为:123456Car constructorCar constructor118Car destructor 我们知道, 在 C++ 中, 普通函数只是在名义上存在于类中, 实际上函数的 size 并不会包括在类中, 因此, 这个类的 size 应该为0, 但是对于 size 为0的类我们无法存储其地址, 因此会额外赋予一个字节的 size , 注意当类的 size 不为0时, 这个字节就会被覆盖. 类中字节非对齐1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace::std;class Car&#123; public: Car()&#123; cout&lt;&lt;"Car constructor"&lt;&lt;endl; &#125; ~Car()&#123; cout&lt;&lt;"Car destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"car start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"car stop"&lt;&lt;endl; &#125; private: char c;&#125;;int main(int argc,char *argv[])&#123; Car *p_car = new Car(); Car car; cout&lt;&lt; size of(Car)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(car)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p_car)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 上面代码的输出为:123456Car constructorCar constructor118Car destructor 我们逐行来分析一下, 首先, 前两行代表调用了类的构造函数, 分别对应的对象指针和对象, 接下来, 我们求得类Car的 size 为1个字节, 这是因为在类中有一个char类型的变量, 对象car的 size 也为一个字节, 与类的 size 保持一致, 而对象指针的 size 为8个字节 , 因为对象指针的 size 仅与当前平台的编译器有关, 与类的 size 无关, 无论类的 size 是多少, 其对象指针的值都为8. 再来看一下继承时的情况:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;iostream&gt;using namespace::std;class Base1&#123; public: Base1()&#123; cout&lt;&lt;"Base1 constructor"&lt;&lt;endl; &#125; ~Base1()&#123; cout&lt;&lt;"Base1 destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Base1 start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Base1 stop"&lt;&lt;endl; &#125; private: char c;&#125;;class Base2&#123; public: Base2()&#123; cout&lt;&lt;"Base2 constructor"&lt;&lt;endl; &#125; ~Base2()&#123; cout&lt;&lt;"Base2 destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Base2 start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Base2 stop"&lt;&lt;endl; &#125; private: char c;&#125;;class Derived:public Base1, public Base2&#123; public: Derived()&#123; cout&lt;&lt;"Derived constructor"&lt;&lt;endl; &#125; ~Derived()&#123; cout&lt;&lt;"Derived destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Derived start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Derived stop"&lt;&lt;endl; &#125; private: char c;&#125;;int main(int argc,char *argv[])&#123; Base1 *p = new Derived(); Derived d; cout&lt;&lt; size of(Derived)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(d)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 输出为:123456789101112Base1 constructorBase2 constructorDerived constructorBase1 constructorBase2 constructorDerived constructor338Derived destructorBase2 destructorBase1 destructor 可以看到总共的 size 为两个基类所占空间和子类所占空间之和(同名不会冲突, 可以通过命名空间区分).这里只调用了一次析构函数, 因为new对应的内存必须要delete才能释放. 上面的两段代码并没有进行字节对齐, 原因是因为之后更大的变量出现, 所以可以用当前的 size 而无需进行对齐 对齐方式: 变量存放的起始地址相对于结构的起始地址的偏移量必须为某个数值的倍数. 同时会根据当前结构中的元素的最大字节数将总的 size 补成最大字节数的倍数. Char偏移量必须为sizeof(char)``即1的倍数int偏移量必须为sizeof(int)即4的倍数float偏移量必须为sizeof(float)即4的倍数double偏移量必须为sizeof(double)即8的倍数Short偏移量必须为sizeof(short)即2的倍数 虚函数表指针 偏移量必须为sizeof(vptr)`, 即8的倍数(64位系统) 123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;using namespace::std;class Car&#123; public: Car()&#123; cout&lt;&lt;"Car constructor"&lt;&lt;endl; &#125; ~Car()&#123; cout&lt;&lt;"Car destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"car start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"car stop"&lt;&lt;endl; &#125; private: char c; int speed1; int speed2; int speed3; int speed4;&#125;;int main(int argc,char *argv[])&#123; Car *p_car = new Car(); Car car; cout&lt;&lt; size of(Car)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(car)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p_car)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 代码与上面的代码基本相同, 只不过多了4个int类型的变量, 输出结果如下:123456Car constructorCar constructor20208Car destructor 按理说, 类中的 size 应为: $4\times 4 + 1 = 17$ 字节, 但是这里却为20字节, 这是因为 在C++中, 会对类进行字节对齐, 这点和 struct 有些相似, 对齐后, 会使类的 size 变成4个整数倍. 注意这里对象指针的 size 依然为8个字节, 与类的 size 无关. 最后, 只调用了一次析构函数, 这是因为用 new 申请的内存不会自动释放, 必须使用 delete 手动释放才可以. 虚函数对类 size 的影响注意, 对于不同的系统和编译器, sizeof的计算结果可能不一样, 简单来说, 虚函数表指针在32位系统中占4个字节, 在64位系统中占8个字节 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include&lt;iostream&gt;using namespace::std;class Base1&#123; public: Base1()&#123; cout&lt;&lt;"Base1 constructor"&lt;&lt;endl; &#125; ~Base1()&#123; cout&lt;&lt;"Base1 destructor"&lt;&lt;endl; &#125; virtual void start() &#123; cout&lt;&lt;"Base1 start"&lt;&lt;endl; &#125; virtual void stop() &#123; cout&lt;&lt;"Base1 stop"&lt;&lt;endl; &#125;&#125;;class Base2&#123; public: Base2()&#123; cout&lt;&lt;"Base2 constructor"&lt;&lt;endl; &#125; ~Base2()&#123; cout&lt;&lt;"Base2 destructor"&lt;&lt;endl; &#125; virtual void start() &#123; cout&lt;&lt;"Base2 start"&lt;&lt;endl; &#125; void stop() &#123; //同一个类中只会有一个虚函数表指针, 所以这里为虚或者不为虚, 最终的类的 size 是相同的 cout&lt;&lt;"Base2 stop"&lt;&lt;endl; &#125;&#125;;class Derived:public Base1, public Base2&#123; public: Derived()&#123; cout&lt;&lt;"Derived constructor"&lt;&lt;endl; &#125; ~Derived()&#123; cout&lt;&lt;"Derived destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Derived start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Derived stop"&lt;&lt;endl; &#125;&#125;;int main(int argc,char *argv[])&#123; Base1 *p = new Derived(); Derived d; cout&lt;&lt; size of(Derived)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(d)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 输出结果如下:123456789101112Base1 constructorBase2 constructorDerived constructorBase1 constructorBase2 constructorDerived constructor16168Derived destructorBase2 destructorBase1 destructor 从上面的代码中可以看出, Base1 类中具有两个虚函数, Base2 类中具有一个虚函数, 因为对于同一个类来说, 只会维护一个虚函数表指针, 所以不论类中的虚函数的个数为多少个, 都只会产生一个虚函数表指针, 同时这里由于子类继承了两个虚基类, 所以会有两个虚函数表, 也就是要维护两个虚函数表指针, 因此其类的 size 为 $8+8=16$. 而对于对象指针p来说, size 与类无关. 最后, 析构函数也只调用了一次, 因为没有用delete手动释放new对应的内存. 如果使用了虚函数, 则类的对齐方式会发生变化, 不再是与4的倍数对齐, 而是与8的倍数对齐, 如下所示, 在每个类中增加了char类型的变量, 按理说增加的总字节数应该为3, 但是由于字节对齐, 类的 size 会变成8个倍数: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;iostream&gt;using namespace::std;class Base1&#123; public: Base1()&#123; cout&lt;&lt;"Base1 constructor"&lt;&lt;endl; &#125; ~Base1()&#123; cout&lt;&lt;"Base1 destructor"&lt;&lt;endl; &#125; virtual void start() &#123; cout&lt;&lt;"Base1 start"&lt;&lt;endl; &#125; virtual void stop() &#123; cout&lt;&lt;"Base1 stop"&lt;&lt;endl; &#125; private: char c;&#125;;class Base2&#123; public: Base2()&#123; cout&lt;&lt;"Base2 constructor"&lt;&lt;endl; &#125; ~Base2()&#123; cout&lt;&lt;"Base2 destructor"&lt;&lt;endl; &#125; virtual void start() &#123; cout&lt;&lt;"Base2 start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Base2 stop"&lt;&lt;endl; &#125; private: char c;&#125;;class Derived:public Base1, public Base2&#123; public: Derived()&#123; cout&lt;&lt;"Derived constructor"&lt;&lt;endl; &#125; ~Derived()&#123; cout&lt;&lt;"Derived destructor"&lt;&lt;endl; &#125; void start() &#123; cout&lt;&lt;"Derived start"&lt;&lt;endl; &#125; void stop() &#123; cout&lt;&lt;"Derived stop"&lt;&lt;endl; &#125; private: char c;&#125;;int main(int argc,char *argv[])&#123; Base1 *p = new Derived(); Derived d; cout&lt;&lt; size of(Derived)&lt;&lt;endl; // 类的 size cout&lt;&lt; size of(d)&lt;&lt;endl; // 对象的 size cout&lt;&lt; size of(p)&lt;&lt;endl; // 对象指针的 size return 0;&#125; 虚函数控制下的运行多态有什么用？假如我们在公司的人事管理系统中定义了一个基类 Employee(员工)，里面包含了升职、加薪等虚函数。 由于Manager(管理人员)和Engineer(工程人员)的加薪和晋升流程是不一样的，因此我们需要实现一些继承类并重写这些函数。 有了上面这些以后，到了一年一度每个人都要加薪的时候，我们只需要一个简单的操作就可以完成，如下所示123456void globalRaiseSalary(Employee *emp[], int n)&#123; for (int i = 0; i &lt; n; i++) emp[i]-&gt;raiseSalary(); // 会根据emp具体指向的对象类型，来选择合适的函数行为 // Polymorphic Call: Calls raiseSalary() // according to the actual object, not according to the type of pointer&#125; 虚函数使得我们可以创建一个统一的基类指针，并且调用不同子类的函数而无需知道子类对象究竟是什么 虚函数中的默认参数先看下面的代码12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;using namespace std;class Base&#123;public: virtual void fun ( int x = 0 ) &#123; cout &lt;&lt; "Base::fun(), x = " &lt;&lt; x &lt;&lt; endl; &#125;&#125;;class Derived : public Base&#123;public: // 这里的virtual关键字可以省略，因为只要基类里面被声明为虚函数，那么在子类中默认都是虚的 virtual void fun ( int x )// 或者定义为 virtual void fun ( int x = 10) &#123; cout &lt;&lt; "Derived::fun(), x = " &lt;&lt; x &lt;&lt; endl; &#125;&#125;;int main()&#123; Derived d1; Base *bp = &amp;d1; bp-&gt;fun(); return 0;&#125; 上面的代码输出始终为：1Derived::fun(), x = 0 解释： 首先，参数的默认值是不算做函数签名的，因此，即使基类有默认值，子类没有，这两个函数的函数签名仍然被认为是相同的，所以在调用bp-&gt;fun();，仍然调用了子类的fun函数，但是因为没有给出x的值，所以采用了基类函数给出的默认值0. 当基类给出默认值0，子类给出默认值10时，返回结果仍然是默认值0，这是因为，参数的默认值是静态绑定的，而虚函数是动态绑定的，因此， 默认参数的使用需要看指针或者引用本身的类型，而不是指向对象的类型。-小结：根据上面的分析，在虚函数中最好不要使用默认参数，否则很容易引起误会！ 静态函数可以被声明为虚函数吗静态函数不可以声明为虚函数，同时也不能被const和volatile关键字修饰。如下面的声明都是错误的：123virtual static void fun()&#123;&#125;static void fun() const &#123;&#125; // 函数不能被const修饰，但是返回值可以 原因主要有两个方面： static成员函数不属于任何类对象或类实例，所以即使给此函数加上virtual也是没有意义的 虚函数依靠vptr和vtable来处理，vptr是一个指针，在类的构造函数中创建生成，并且只能用this指针来访问它，静态成员函数没有this指针，所以无法访问vptr。 构造函数可以为虚函数吗构造函数不可以声明为虚函数。同时除了inline之外，构造函数不允许使用其他任何关键字，原因如下： 尽管虚函数表vtable是在编译阶段就已经建立的，但指向虚函数表的指针vptr是在运行阶段实例化对象时才产生的。 如果类含有虚函数，编译器会在构造函数中添加代码来创建vptr。 问题来了，如果构造函数是虚的，那么它需要vptr来访问vtable，可这个时候vptr还没产生。 因此，构造函数不可以为虚函数。 我们之所以使用虚函数，是因为需要在信息不全的情况下进行多态运行。而构造函数是用来初始化实例的，实例的类型必须是明确的。 因此，构造函数没有必要被声明为虚函数。 析构函数可以为虚函数吗析构函数可以声明为虚函数。如果我们需要删除一个指向派生类的基类指针时，应该把析构函数声明为虚函数。事实上，只要一个类有可能会被其他类所继承，就应该声明虚析构函数（哪怕该析构函数不执行任何操作）。原因是因为基类指针被删除后, 不会调用派生类的析构函数, 只会调用基类的析构函数, 因此, 需要将析构函数声明为虚的, 来使得进行 delete时, 调用子类的虚构函数： 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;class base &#123; public: base() &#123; cout&lt;&lt;"Constructing base \n"; &#125; // virtual ~base() ~base() &#123; cout&lt;&lt;"Destructing base \n"; &#125; &#125;;class derived: public base &#123; public: derived() &#123; cout&lt;&lt;"Constructing derived \n"; &#125; ~derived() &#123; cout&lt;&lt;"Destructing derived \n"; &#125;&#125;;int main(void)&#123; derived *d = new derived(); base *b = d; delete b; return 0;&#125; 以上代码输出：123Constructing baseConstructing derivedDestructing base 可见，继承类的析构函数没有被调用，delete时只根据指针类型调用了基类的析构函数。 正确的操作是，基类和继承类的析构函数都应该被调用，解决方法是将基类的析构函数声明为虚函数。 如下所示:1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;class base &#123; public: base() &#123; cout&lt;&lt;"Constructing base \n"; &#125; // virtual ~base() ~base() &#123; cout&lt;&lt;"Destructing base \n"; &#125; &#125;;class derived: public base &#123; public: derived() &#123; cout&lt;&lt;"Constructing derived \n"; &#125; ~derived() &#123; cout&lt;&lt;"Destructing derived \n"; &#125;&#125;;int main(void)&#123; derived *d = new derived(); base *b = d; delete b; return 0;&#125; 输出结果为:1234Constructing baseConstructing derivedDestructing derivedDestructing base 虚函数可以为私有函数吗虚函数可以被私有化，但有一些细节需要注意12345678910111213141516171819202122#include&lt;iostream&gt;using namespace std;class Derived;class Base &#123;private: virtual void fun() &#123; cout &lt;&lt; "Base Fun"; &#125;friend int main();&#125;;class Derived: public Base &#123;public: void fun() &#123; cout &lt;&lt; "Derived Fun"; &#125;&#125;;int main()&#123; Base *ptr = new Derived; ptr-&gt;fun(); return 0;&#125; 输出结果为：1Derived fun() 基类指针指向继承类对象，则调用继承类对象的函数 int main()必须声明为Base类的友元，否则编译失败。编译器报错：ptr无法访问私有函数。当然，把基类声明为public，继承类为private，该问题就不存在了。 虚函数可以被内联吗通常类成员函数都会被编译器考虑是否进行内联。但通过基类指针或者引用调用的虚函数必定不能被内联。当然，实体对象调用虚函数或者静态调用时可以被内联，虚析构函数的静态调用也一定会被内联展开。 纯虚函数与抽象类纯虚函数：在基类中只声明不定义的虚函数，同时要求任何派生类都要实现该虚函数。在基类中实现纯虚函数的方法是在函数原型后加“=0”。 抽象类：含有纯虚函数的类为抽象类 纯虚函数的特点以及用途总结如下： 如果不在继承类中实现该函数，则继承类仍为抽象类； 派生类仅仅只是继承纯虚函数的接口，因此使用纯虚函数可以规范接口形式 抽象类无法实例化对象 抽象类可以有构造函数 析构函数被声明为纯虚函数是一种特例，允许其有具体实现。（有些时候，想要使一个类称为抽象类，但刚好有没有任何合适的纯虚函数，最简单的方法就是声明一个纯虚的析构函数）]]></content>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的static关键字]]></title>
    <url>%2Fz_post%2FCpp-static%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[static关键字常用方法 用法 示例 说明 在局部变量前使用static int main(){ static int a;} 该变量具有静态持续性, 但是无链接性 静态成员数据 class A{static int a}; int A::a = 1 只能在类外部定义并初始化静态成员变量, 不能在类内部声明时定义或初始化静态变量, 因为声明只是描述了如何分配内存, 但实际上并不真正分配内存 静态成员函数 与成员数据类似 无this指针 const static成员数据 class Myclass{ public: const static int a = 3; 声明并在内部初始化(也可在在外部初始化, 注意不带static关键字) 首先：在类中声明的静态变量，一定要进行初始化，并且，如果不是const static类型的静态变量，则需要在类外初始化，同时初始化的时候一定要带上类名和作用域解析符。 123456789101112131415161718192021class sum&#123; public: static int i ; static int s; sum()&#123;i++; s+=i;&#125;; ~sum()&#123;&#125;; static void set()&#123; //静态函数只能使用静态变量 i = 0; s = 0; &#125;; &#125;;int sum::i =0; // 非const static类型，要在类外初始化int sum::s = 0; // 且必须带上类名sum和作用域解析符class Solution &#123;public: int Sum_Solution(int n) &#123; sum::set(); // 直接用类名调用函数，则该函数必须是静态的 sum a[n]; return sum::s; &#125;&#125;; 存储持续性C++使用三种（在C++11中是四种）不同的方案来存储数据，这些方案的区别就在于 数据保留在内存中的时间 ：(Primer p304) 自动存储持续性: 在程序开始执行其所在的函数或者代码块时被创建, 在执行完函数或代码块时, 其内存会被自动释放. 静态存储持续性: 在整个程序运行过程当中都存在 线程存储持续性（C++11） 动态存储持续性: 用new运算符分配的内存将一直存在(堆中), 直到使用delete运算符将其释放或者 程序结束 为止. 作用域和链接性 和C语言一样，C++也为 静态 存储持续性变量提供了3种链接性(其他类型的持续性变量均是无链接性的)，这三种链接性都在整个程序执行期间存在，与自动变量相比，它们的寿命更长。(Primer p309) 外部链接性（可在其他文件中访问） 内部链接性（只能在当前文件中访问） 无链接性（只能在当前函数或代码块中访问，与自动变量不同的是，就算不在函数中，变量也存在，只是不能访问） 由于静态变量的数目在程序运行期间是不变的，因此程序不需要使用特殊的装置（如栈）来管理它们, 而是将它们放在全局数据区。编译器将分配固定的内存块来存储所有的静态变量，这些变量在整个程序执行期间一直存在。另外，如果没有显式地初始化静态变量，编译器将把它设置为0。在默认情况下，静态 数组和结构将每个元素或成员的所有位都设置为0。p309 创建三种链接性的静态持续变量：p309 外部链接性：必须在代码块的外面声明 内部链接性：必须在代码块的外面声明，并使用static限定符 无链接性：必须在代码块内部声明，并使用static限定符123456789int global = 1000; //静态持续变量，外部链接性，作用域为整个文件static int one_file = 50; //静态持续变量，内部链接性，作用域为整个文件int main()&#123; ...&#125;void funct1(int n)&#123; static int count = 0; //静态持续变量，无链接性，作用域为局部 int llama = 0;&#125; 五种变量存储方式：p310 自动 寄存器 静态，无链接 静态，外部链接 静态，内部链接 关键字重载： 关键字的含义取决于上下文，static用于局部声明，以指出变量是无链接性的静态变量时，表示的是存储持续性。而用于代码块外的声明时，static表示内部链接性，因为位于代码块外的变量已经是静态持续性了。p310 静态变量的初始化： 静态变量有三种初始化方式：零初始化（变量设为零）、常量表达式初始化和动态初始化。 零初始化和常量表达式初始化被统称为静态初始化，这意味着在编译器处理文件时初始化变量，动态初始化意味着变量将在编译后初始化。p310 静态变量的初始化过程： 首先，所有静态变量都被零初始化，而不管程序员是否显式地初始化了它。接下来，如果使用常量表达式初始化了变量，且编译器仅根据文件内容（包括被包含的头文件）就可计算表达式，编译器将执行常量表达式初始化。必要时，编译器将执行简单计算。最后，剩下的变量将被动态初始化。 常量表达式并非只能是使用字面常量的算术表达式。（sizeof运算符也可以）p310 链接性为外部的变量通常简称为外部变量，也称全局变量，它们的存储持续性为静态，作用域为整个文件。p310 静态持续性、外部链接性 单定义规则（One Definition Rule，ODR）： 变量只能定义一次。为满足这种需求，C++提供了两种变量声明：p311 定义声明（简称定义）：为变量分配存储空间。 引用声明（简称声明）：不给变量分配存储空间，引用已有的变量。使用关键字extern12double up; //定义声明exterm int blem; //blem在别处定义 如果要在多个文件中使用外部变量，只需在一个文件中包含该变量的定义（单定义规则），但在使用该变量的其他所有文件中，都必须使用关键字extern声明它。p311 1234567//file01.cppextern int cats = 20; // 由于初始化，所以这里是定义而非声明int dogs = 22; //定义//即使去掉file01.cpp文件中的extern也无妨，效果相同。//file02.cppextern int cats; //使用extern且无初始化，说明使用的是其他文件的catsextern int dogs; //同上 静态持续性、内部链接性 将作用域为整个文件的变量声明为静态外部变量（内部链接性），就不必担心其名称与其他文件中的外部变量发生冲突. 通常, 当在函数体内定义一个变量时, 该变量是一个自动存储变量, 每当运行到该语句时都会给该局部变量分配 栈内存, 而随着程序退出函数体, 系统就会自动收回这一部分内存. 但有时候我们需要在两次调用之间对变量的值进行保存. 通常的想法是定义一个全局变量来实现, 但是这样一来, 变量就不再属于该函数本身了. 因此, 可以使用静态局部变量来解决, 静态局部变量保存在全局数据区, 而不是保存在栈中, 每次的值保持到下一次调用, 知道下次赋新值.123456789101112131415//file1int errors = 20;//file2int errors = 5;int main()&#123; cout&lt;&lt;errors; //报错，errors与file1中的外部变量重定义 ...&#125;//解决方法：file2static int errors = 5;int main()&#123; cout&lt;&lt;errors; // 输出5&#125; 静态存储持续性、无链接性 局部静态变量：虽然该变量只在该代码块中可用，但它在该代码块不处于活动状态时仍然存在。因此在两次函数调用之间，静态局部变量的值将 保持不变 。 另外，如果初始化了静态局部变量，则程序 只在启动时进行一次初始化 。以后再调用函数时，将不会被再次初始化。p315 说明符和限定符 存储说明符（storage class specifier）：p317 auto（在C++11中不再是说明符） register static extern thread_local（C++11新增的） mutable：即使结构（或类）变量为const，其某个成员也可以被修改 cv-限定符（cv-qualifer）：p317 const：内存被初始化后，程序便不能再对它进行修改 volatile：即使程序代码没有对内存单元进行修改，其值也可能发生变化 在默认情况下 全局变量的链接性为外部，但 const全局常量的链接性为内部 (因为是全局的, 所以默认已经具有静态持续性) 。因此，将一组常量放在头文件中，其他引用该头文件的文件都相当于自己定义了私有的常量，这就是能够将常量定义放在头文件中而不会重定义的原因。p318 如果处于某种原因，程序员希望某个常量的链接性为外部的，则可以使用extern关键字来覆盖默认的内部链接性，extern const int states = 50;，在这种情况下，必须在所有使用该常量的文件中使用extern关键字来声明它。同时这种情况下就不能将该常量放在头文件中了, 因为链接性已经变成外部, 这样会引起重复定义. p318 函数的链接性和static关键字当static关键字作用于函数时, 它改变的只是函数的链接性, 函数的持续性永远为静态 C++不允许在一个函数中定义另一个函数，因此 所有函数的存储持续性都自动为静态，即在整个程序执行期间都一直存在 。p318 在默认情况下，函数的链接性为外部。即可以在文件间共享，使用extern来指出函数实在另一个文件中定义的（可选）。p318 可以使用关键字static将函数的链接性设置为内部 ，使之只能在一个文件中使用，必须同时在原型和函数定义中使用该关键字。p318 内联函数不受单定义规则的约束，这允许程序员能够将内联函数的定义放在头文件中。但是C++要求同一个函数的所有内联定义都必须相同。 p319 C++查找函数顺序：静态（在本文件中找）——外部（在所有的程序文件中找）——在库函数中找。因此如果定义了一个与库函数同名的函数，编译器优先使用程序员定义的版本（C++不推荐这样做）。p319 语言链接性 不同的语言采用了不同的链接性，为了解决这种问题，需要特别指定函数采用的链接性（默认为C++链接性）。p319 面向对象的static关键字(类中的static关键字)静态成员数据在类内的数据成员的声明前加上关键字static, 该数据成员就是类内的静态数据成员, 具有以下特点: 静态数据成员被当作是 类的成员 (注意与具体对象无关), 无论这个类的对象被定义了多少个, 静态数据成员在程序中也只有一份拷贝(只分配一次内存), 由该类型的所有对象共同持有. 静态数据成员存储在全局数据区. 由于静态数据成员在定义时必须分配空间, 所以只能在类中对静态数据成员声明, 而不能在类中对静态数据成员进行定义, 需要放到类外定义, 且定义时不要加static关键字, 但是必须指明命名空间. 如下所示: 12345678class Myclass&#123;public: int getprivate: int a = 1; static float Sum; // 不能再类内部声明时定义或初始化静态变量, 因为声明只是描述了如何分配内存, 但实际上并不真正分配内存&#125;float Myclass::Sum = 0; // 只能在类外部定义并初始化静态成员变量 静态数据成员和普通数据成员一样要遵从public, protected, private等访问规则. 因为静态数据成员是独立于具体对象的, 因此, 即使没有产生任何类的实例, 我们仍然可以对其进行访问. 如果静态数据成员的访问权限允许的话(public), 可以通过下面两种方式来直接访问: &lt;类对象名&gt;.&lt;静态数据成员名&gt; &lt;类名&gt;::&lt;静态数据成员名&gt; 同全局变量相比(静态持续性, 外部链接性), 使用静态数据成员有两个优势: 静态数据成员没有进入程序的全局命名空间, 因此不存在与程序中其他全局变量名冲突的可能性 可以实现信息隐藏, 静态数据成员可以是private成员, 而全局变量不能. 静态成员函数与静态数据成员一样, 也可以创建一个静态成员函数, 它为类的全部对象服务, 而不是为某一个具体的对象服务 ,具有以下特点: 在定义(实现)静态成员函数时, 同样不能带有关键字static 静态成员之间可以互相访问, 包括静态成员函数访问静态数据成员和静态成员函数 非静态成员函数可以任意的访问静态成员和非静态成员. 静态成员函数不能访问任何非静态成员 由于没有this指针上的开销, 因此静态成员函数在速度上会小优于类的全局函数 类对象或对象指针可以像调用普通函数一样使用.或-&gt;来调用静态成员函数, 也可以使用&lt;类名&gt;::&lt;静态成员函数名&gt;()的方式来调用 静态成员函数与this指针普通的成员函数一般都隐含了一个this指针, this指针指向类的对象本身, 因为普通成员函数总是属于类的某个具体对象, 通常情况下, this指针是缺省的, 如函数 func() 实际上是this-&gt;func(), 但是与普通函数相比, 静态成员函数由于不与任何对象相联系, 因此它不具有this指针, 从这个意义上讲, 它就 无法访问属于类对象的非静态数据成员, 也无法访问非静态成员函数, 只能访问或调用静态的数据或函数. const与staticconst 只是对于单个类对象来说是常量, 而对于整个类来说实际上是变量, 如果要维护一个对于整个类来说的常量, 应该使用const static或者static cosnt(二者等价)来声明, 与普通static成员数据不太一样的是, const static成员数据需要在类中声明的同时就进行初始化(因为const数据在定义是必须初始化, 而static又是的该变量独立于具体对象, 所以必须在声明时初始化或者在类外部初始化, 也不能在构造函数初始化列表中初始化) :123456789class Myclass&#123;public: const static int a = 3; // 声明并初始化 cosnt int b = 4; //这样是可以的, 但是这样所有变量的b都为4,且不能改变, 还不如声明为const static //所以最好还是在构造函数的初始化列表中对非静态const进行初始化 static const int c;&#125;const int Myclass::c = 5; //在类外部初始化. 不要带static关键字]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双流手语识别系统：（一）框架设计]]></title>
    <url>%2Fz_post%2F%E9%A1%B9%E7%9B%AE-%E5%8F%8C%E6%B5%81%E6%89%8B%E8%AF%AD%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E2%80%94-%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"></content>
      <categories>
        <category>项目</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vector数据的内存分配问题]]></title>
    <url>%2Fz_post%2FCpp-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95-Vector%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[有问题 待修改首先，要知道，程序所拥有的栈资源是及其有限的（Linux下用ulimit -a或者ulimit -s可查当前栈的大小。 因此在写程序时，绝对不能肆意使用占空间，否则就会报出Segmentation fault。 在用vector实现三维数据时，以下的代码就会产生段错误 123456789101112131415161718#include &lt;vector&gt;using namespace std;int main() &#123; int HEIGHT=2,WIDTH=3,DEPTH=5; // construct array3D[HEIGHT][WIDTH][DEPTH] vector&lt;vector&lt;vector&lt;double&gt; &gt; &gt; array3D; array3D.resize(HEIGHT); for (int i = 0; i &lt; HEIGHT; ++i) &#123; array3D[i].resize(WIDTH); for (int j = 0; j &lt; WIDTH; ++j) array3D[i][j].resize(DEPTH); &#125; return 0;&#125; 以上出现段错误的原因在于申请了过多的vecotr，导致占空间不够用，从而出现段错误，现在来看以下vector在内存中具体是如何存储的，首先看一下以下三种方式的声明： 1234std::vector&lt;T&gt; vec;std::vector&lt;T&gt;* Vec = new std::vector&lt;T&gt;();std::vector&lt;T*&gt; vec; 假设T是一个类型或者一个定义好的类，则以上三种情况的内存分配情况如下： 对于std::vector&lt;T&gt; vec；vec在栈上（stack），而其中的元素T保存在堆上（heap）； 对于std::vector&lt;T&gt;* Vec = new std::vector&lt;T&gt;()；vec和其中的元素T都保存在堆上； 对于std::vector&lt;T*&gt; vec；vec在栈上（stack），而其中的元素T保存在堆上（heap）；和第一种情况类似。 存储在栈上的元素，往往无需手动管理内存空间，通常会自动释放，而在堆上的空间，则需要手动管理。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>踩坑记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcc/g++的编译链接原理及注意事项]]></title>
    <url>%2Fz_post%2FCpp-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95-%E5%85%B3%E4%BA%8Egcc%E7%9A%84%E7%BC%96%E8%AF%91%E5%92%8C%E9%93%BE%E6%8E%A5%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[LINUX下默认搜索头文件及库文件的路径编译连接多个文件编译在linux下编译，下面有三个文件，分别是1.cpp 和 2.cpp 和myhead.h 文件。 1.cpp 123456789#include &lt;iostream&gt;#include "myhead.h"using namespace std;int main()&#123; print(); cout&lt;&lt;"yes !"&lt;&lt;endl; return 0;&#125; 2.cpp 123456789#include &lt;iostream&gt;#include "myhead.h"using namespace std;void print()&#123; std::cout&lt;&lt;" print "&lt;&lt;std::endl; cout&lt;&lt;&#125; myhead.h 12345#ifndef __myhead_h#define __myhead_hvoid print();#endif 假如他们都在一个目录下面，那么编译流程： 12g++ -c 2.cpp #将2.cpp 编译成2.o 文件g++ 1.cpp -o a.out 2.o #多个文件一起链接 or123g++ -c 2.cppg++ -c 1.cppg++ 1.o 2.o -o test]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>踩坑记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[声明模板类对象，报错“undefined reference to”]]></title>
    <url>%2Fz_post%2FCpp-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95-%E6%A8%A1%E6%9D%BF%E7%B1%BB%E6%8A%A5%E9%94%99undefined_reference_to%2F</url>
    <content type="text"><![CDATA[坑源实现ZeroTensor项目中的Shape3D类时，为了可以处理多种类型的数据（int，double等），需要使用模板类 出现问题和解决办法在实现的时候将模板类的声明和定义写在的不同位置，编译时报错：1undefined reference to XXXX 这是因为模板类并不是普通的类和成员函数！它们只是说明了如何生成类和成员函数定义。因此，不能将模板成员函数放在独立的实现文件中。 由于模板不是函数，因此它们不能单独编译。模板必须与特定的模板实例化请求一起使用。为此，最简单的方法是将所有模板信息放在一个头文件中，并在要使用这些模板的文件中包含该头文件！ 还有另一种解决办法就是在stack.h文件的末尾加上#include stack.cpp，并在stack.cpp文件中去掉对应的包含语句。]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>踩坑记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《CUDA By Example》]]></title>
    <url>%2Fz_post%2FCUDA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-CUDAByExample%2F</url>
    <content type="text"><![CDATA[WHY CUDA？ WHY NOW？]]></content>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第十五章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter15%2F</url>
    <content type="text"><![CDATA[第十五章 友元、异常和其他15.1 友元类并非只能拥有友元函数，也可以将类作为友元。在这种情况下，友元类的所有方法都可以访问原始类的私有成员和保护成员。哪些函数、成员函数或类为友元是由类定义的，而不能从外部强加友情。因此 ，尽管友元被授予从外部访问类的私有部分的权限，但它们并不与面向对象的编程思想相悖。 15.1.1 友元类关于需要使用友元类的情况，可以想象电视机类和遥控器类之间的关系，首先，它们不是is-a，其次，也不是has-a，但是它们确实存在某个关系，这就是——友元。下面的语句使Remote成为友元类，友元声明可以位于公有、私有或保护部分，其所在的位置无关紧要。由于Remote类中访问了Tv类的成员，因此，编译器必须了解Tv类以后，才能处理Remote类，最简单的方法是首先定义Tv类，另一种方法是使用前向声明（forward delaration），这将稍后介绍。12345class Tv&#123; ... friend class Remote; ...&#125;; 以上代码，在电视机类里面声明遥控器类为友元类以后，遥控器类就可以访问电视机类的私有成员变量，而无需让电视机类修改自身的访问权限，就是说电视机对外界的访问权限是不变的，并没有破坏类的封装行。 15.1.2 友元成员函数可以选择仅让特定的类成员变成另一个类的友元，而不必让整个类成员友元，但是这样做稍微有点麻烦，必须小心排列各种声明和定义的顺序。 123class TV&#123; friend void Remote::set_chanel(TV &amp;t, int c);&#125; 要使编译器能够处理上面的代码，它必须知道Remote的定义，否则，它无法知道Remote是类还是别的什么，也无法确定set_chanel是这个类的方法。而前面又说了，Tv应定义在Remote定义之前，这就导致了循环依赖问题。避开这种问题的方法是使用前向声明（forward declaration）。为此，需要在Remote定义的前面插入下面的语句：1class Tv; 这样，新的排列顺序如下：1234567class Tv;class Remote &#123; ...&#125;;class Tv&#123; ...&#125;; 注意，Tv类与Remote类的定义不可以调换，因为编译器在Tv类的声明者是看到Remote的一个方法被声明为Tv类的友元之前，应该先看到Remote类的声明和set_chanel()方法的声明。 15.1.3 其他友元关系可以通过让类彼此成为对方的友元来实现互相影响对象的功能。即除了Remote是Tv的友元外，Tv还是Remote的友元。需要注意一点的是，对于使用Remote对象的Tv方法，其原型可在Remote类声明之前声明，但必须在Remote类声明之后定义，以便编译器有足够的信息来编译该方法。 15.1.4 共同的友元需要使用友元的另一种情况是，函数需要访问两个类的私有数据。因此，可以将函数左右两个类的友元 15.2 嵌套类（内部类）在C++中，可以将类声明放在另一个类中，在另一个类中声明的类被成为嵌套类（nested class），它通过提供新的类型类作用域来避免名称混乱。包含类的成员函数可以创建和使用被嵌套类的对象，而仅当声明位于公有部分，才能在包含类的外面使用嵌套类，而且必须使用作用域解析符。 对类进行嵌套与包含并不同。包含意味着将类对象作为另一个类的成员，而对类进行嵌套不会创建类成员，而是 定义了一种类型 ，可以在包含类的其他方法内声明该类型的变量，该类型仅在包含嵌套类声明的类中有效。 15.2.1 嵌套类和访问权限 作用域 &emsp;&emsp;如果嵌套类是在另一个类的私有部分声明的，则只有这个类知道它。也就是说，只能通过这个类的成员来使用嵌套类，就像私有变量一样。而这个类的派生类，和外部的程序都无法访问到该嵌套类。 &emsp;&emsp;如果嵌套类是在另一个类的保护部分声明的，则它对于后者及其派生类都是可见的，但是对于外部师姐是不可见的。 &emsp;&emsp;如果嵌套类是在另一个类的公有部分声明的，则允许后者、后者的派生类以及外部世界访问它。并且，由于嵌套类的作用域为包含它的类，因此在外部世界使用它时，必须使用类限定符。 &emsp;&emsp;嵌套结构和枚举的作用域与嵌套类相同。下表总结了嵌套类、结构和枚举的作用域特征 声明位置 包含它的类是否可以使用它 从包含它的类派生的类是否可以使用它 在外部是否可以使用 私有部分 是 否 否 保护部分 是 是 否 公有部分 是 是 是，通过类限定符使用 访问控制 &emsp;&emsp; 类可见后，起决定作用的将是访问控制。对嵌套类访问权的控制规则与对常规类相同。例如，在Queue类声明中声明了Node嵌套类，这并没有赋予Queue类任何对Node类的访问特权，也没有赋予Node类任何对Queue类的访问特权。因此，Queue类对象只能显式的访问Node对象的公有成员。 15.2.2 模板中的嵌套模板类可以正常使用嵌套类，不会带来额外的问题。 15.3 异常程序有时会遇到运行阶段错误，导致程序无法正常的走下去。如，试图打开一个不可用的文件、请求过多的内存、遭遇不能接受的值等等。 C++的异常处理机制是一个相对较新的功能，有些老式编译器可能没有实现，有些编译器可能默认关闭这种特性，需要在选项中开启。比如“零除” 这种异常，很多新编译器通过生成一个表示无穷大的特殊浮点值来处理，cout将其显示为Inf、inf、INF等，有些编译器可能会直接崩溃。 15.3.1 调用abort()abort()函数的原型位于头文件cstdlib（stdlib.h）中，其典型实现是向标准错误流发送消息abnormal program termination（程序异常终止），然后终止程序。它还返回一个随实现而异的值，告诉操作系统（或者父进程），处理失败。abort()是否刷新文件缓冲区（用于存储读写到文件中的数据的内存区域）取决于实现。如果愿意，也可以使用exit()，该函数刷新文件缓冲区，但不显示消息。 15.3.2 返回错误码一种比异常终止更灵活的方法是，使用函数的返回值来指出问题。如果某些函数的任何数值返回都是有效的，那么可以增加一个指针参数或引用参数，来将返回值返回，同时将函数的返回值改成bool类型，来指出是否返回成功。 另一种方法使用一个全局变量。可能问题的函数可以在出现问题时将该全局变量设值为特定的值，而调用程序可以检查该变量。 15.3.3 异常机制C++异常是对程序运行过程中发生的异常情况的一种响应。异常提供了将控制权从程序的一个部分传递到另一个部分的途径。对异常的处理有三个组成部分： 引发异常 使用处理程序捕获异常 使用try块处理异常 throw语句实际上是跳转，即命令程序跳到另一条语句。throw关键字表示引发异常，紧随其后的值（例如字符串或对象）指出了异常的特征。 程序使用异常处理程序（exception handler）来捕获异常，异常处理程序位于要处理问题的程序中，catch关键字表示捕获异常。处理程序以关键字catch开头，随后是位于括号中的类型声明，他指出了异常处理程序要响应的异常类型。 catch关键字和异常类型用作标签，指出当异常被引发时，程序应跳到这个为止执行。异常处理程序也被称为catch块。 try块标识可能引起特定异常的代码块，他后面跟一个或多个catch块。try块是由关键字try只是的，关键字try的后面是一个由花括号括起的代码块，表明需要注意这些代码引发的异常。 在默认情况下，如果函数引发了异常，而没有try块或没有匹配的处理程序时，程序将调用abort()函数。（默认行为可修改）。 15.3.4 将对象用作异常类型通常，引发异常的函数将传递一个对象。这样做的重要优点之一是，可以使用不同的异常类型来区分不同的函数在不同情况下引发的异常。另外，对象可以携带信息，程序员可以根据这些信息来确定引发异常的原因。 根据不同的对象类型，可以跟不同的catch块进行匹配，类型不匹配的catch块将跳过不执行。 15.3.5 异常规范和C++11C++98新增了异常规范（exception specification）的功能，但是在C++11中却被摒弃了。 新增关键字noexcept指出函数不会引发异。 15.3.6 栈解退C++中处理函数的调用和返回时，会让程序将调用函数的指令的地址（返回地址）放到栈中。当被调用的函数执行完毕后，程序将使用该地址来确定从哪里开始继续执行。另外，函数调用将参数放到栈中。在栈中，这些函数参数被视为自动变量。如果被调用的函数创建了新的自动变量，则这些变量也将被添加到栈中。如果被调用的函数调用了另一个函数，则后者的信息将被添加到栈中，以此类推。 现在假设函数由于出现异常（而不是由于返回）而终止，则程序也将释放栈中的内存，但不会在释放栈的第一个返回地址后停止，而是继续释放栈，直到找到一个位于try块中的返回地址。随后，控制权将转到块尾的异常处理程序，而不是函数调用后面的第一条语句。——这个过程被称为栈解退。 栈解退的意义在于：对于普通的函数返回来说，仅仅会调用该函数放在栈中的对象的析构函数，而throw语句则处理try块和throw之间整个函数调用序列放在栈中的对象。所以，如果没有栈解退这种特性，则引发异常后，对于中间函数调用放在栈中的自动类对象，其析构函数将不会被调用。 也就是说：程序进行栈解退以回到能够捕获异常的地方时，将释放栈中的自动存储型变量。如果变量是类对象，将为该对象调用析构函数。 15.3.7 其他异常特性虽然throw-catch机制类似于函数返回机制，但还是有些不同之处： 函数fun()中的返回语句将控制权返回到调用fun()的函数，但throw语句将控制权向上返回到第一个包含能够捕获相应异常的try-catch组合。 引发异常时编译器总是创建一个临时拷贝，即使异常规范和catch块中指定的是引用。 1234567891011121314151617181920class problem&#123;...&#125;;...void super() throw(problem)&#123; ... if( oh_no )&#123; problem oops; throw oops; &#125;&#125;...try&#123; super();&#125;catch(problem &amp;p)&#123; //这里p虽然声明为引用，但是p指向的是oops的副本而不是oops本身，这是件好事，因为函数`super()`执行完毕后，oops将不复存在。//既然如此，为何还要特意声明为引用？因为引用还有另一个重要特征：基类引用可以执行派生类对象。//这有一个很大的用法在于，假设有一个异常类层次结构，并要分别处理不同的异常类型，则使用基类引用将能够捕获任何异常对象；//而使用派生类对象只能捕获它所属类及从这个类派生而来的类的对象。引发的异常对象将被第一个与之匹配的catch块捕获。//这意味着catch块的排列顺序应该与派生顺序相反。也就是要将捕获派生类的catch放前面，捕获基类的catch放后面 //statements&#125; 15.3.8 exception类较新的C++编译器将异常合并到语言中，并在exception头文件（以前为exception.h或except.h定义了exception类，C++可以把它用作其他异常类的基类。 C++库定义了很多基于exceptin的异常类型 stdexcept异常类 &emsp;&emsp;头文件stdexcept定义了其他几个异常类，首先，该文件定义了logic_error和runtime_error类，它们都是以公有方式从exception派生而来的。这两个新类被用作两个派生类系列的基类。 &emsp;&emsp;异常类系列logic_error描述了典型的逻辑错误： domain_error; invalid_argument; length_error; out_of_bounds. &emsp;&emsp;runtime_error异常类系列描述了可能在运行期间发生但难以预计和防范的错误： range_error; overflow_error; underflow_error. &emsp;&emsp;一般，logic_error系列异常表明存在可以通过编程修复的问题，而runtime_error系列异常表明存在无法避免的问题。 &emsp;&emsp;如果上述库类不能满足需求，则应该从logic或runtime异常类中进行派生（而不是从exception），以确保派生出来的异常类可以归入同一个继承层次结构中。 bad_alloc异常和new &emsp;&emsp;对于使用new导致的内存分配问题，C++的最新处理方式是让new引发bad_alloc异常。头文件new包含bad_alloc类的声明，他是从exception类公有派生出来的。但在以前，当无法分配请求的内存量时，new返回一个空指针。 空指针和new &emsp;&emsp;老代码的逻辑是根据new返回的指针是否为空来判断是否失败的，为兼容这种情况，C++标准提供了一种在失败时返回空指针的new，如下所示： 1234567Big* pb;pb = new(std::nothrow) Big[10000];if(pb==0)&#123; cout&lt;&lt;"error"; exit(EXIT_FAILURE);1&#125; 15.3.9 异常、类和继承异常、类和继承以三种方式相互关联： 像C++标准库一样，从一个异常类派生出另一个 在类定义中嵌套异常类声明，从而将异常类组合到类中去 上面的嵌套声明通过继承传给子类 15.3.10 异常何时会迷失方向异常被引发后，在两种情况下会导致问题： 如果异常是在带异常规范的函数中引发的（C++11虽然摈弃了异常规范，但仍有人使用），则必须与规范列表中的某种异常匹配，否则称为意外异常（unexpected exception）。在默认情况下，程序会异常终止。 如果异常不是在函数中引发的（或者函数没有异常规范），则必须捕获该异常，如果没被捕获，则被称为未捕获异常（uncaught exception）。在默认情况下，程序会异常终止。 可以对以上默认情况进行修改： 未捕获异常：未捕获异常不会导致程序理科异常终止。相反，程序将首先调用函数terminate()。在默认情况下，terminate()调用abort()函数。可以使用set_terminate()函数指定terminate()应调用的函数来修改其默认行为：1234567void my_quit()&#123; cout&lt;&lt;"quit"; exit(5); // 退出状态值设为5&#125;...set_terminate(my_quit); 意外异常：通过给函数指定异常规范，可以让函数的用户知道要捕获哪些异常，如下所示：1234567double Argn(double , double ) throw (out_of_bounds);try&#123; x = Argn(a,b);&#125;catch(out_of_bounds &amp; ex)&#123; ...&#125; C++11摒弃它的原因之一是：异常规范机制处理起来比较麻烦。p640 在意外异常发生时，将调用unexpected()函数，这个函数将调用terminate()，后者在默认情况下调用abort()。 C++提供了一个set_unexpected ()函数，但限制更严格。p640 15.3.11 有关异常的注意事项从前面关于如何使用异常的讨论可知，应在设计程序时就加入异常处理功能，而不是以后再添加。 但是这样做会增加代码量，同时异常和动态内存分配并非总能协同工作。 一句话：异常处理很复杂 15.4 RTTIRTTI：运行阶段类型识别（RunTime Type Identification） 这是一项比较新的特性，一些旧的C++编译器不支持，还有一些编译器提供了开关RTTI的设值。 15.4.1 RTTI的用途RTTI旨在为程序在运行阶段确定对象的类型提供一种标准方式。 15.4.2 RTTI的工作原理C++有三个支持RTTI的元素： 如果可能的话，dynamic_cast运算符将使用一个只想基类的指针来生成一个只想派生类的指针；否则，该运算符返回0——空指针 typeid运算符返回一个指出对象的类型的值 type_info结构存储了有关特定类型的信息 RTTI只适用于包含虚函数的类： 只能将RTTI用于包含虚函数的类层次结构，原因在于只有对于这种类层次结构，才应该将派生类对象的地址赋给基类指针。 dynamic_cast运算符 &emsp;&emsp; 该运算符是最常用的RTTI组件， 它不能回答“指针指向的是哪类对象”这样的问题， 但能够回答“是否可以安全地将对象的地址赋给特定类型的指针”这样的问题。 &emsp;&emsp; 与知道“是哪类对象”相比，知道“类型转换是否安全”更通用，也更有用。主要是因为，通常项知道类型的原因在于：知道类型后，就可以知道调用特定的方法是否安全。 而要调用方法，类型并不一定要完全匹配，而可以是定义了方法的虚拟版本的基类类型。 &emsp;&emsp; 用法：ym1Superb* pm = dynamic_cast&lt;Superb *&gt;(pg); &emsp;&emsp; 通常，如果指向的对象(*pt)的类型为Type或从Type直接间接派生而来的类型，则下面的表达式将指针pt转换为Type类型的指针，并作为结果赋给ps，否则ps结果为0，即空指针：1ps = dynamic_cast&lt;Type *&gt;(pt) &emsp;&emsp; 也可以将dynamic_cast用于引用，其用法稍微有点不同：没有与空指针对应的引用值，因此无法使用特殊的引用值来指针失败。当请求不正确时，dynamic_cast将引发类型为bad_cast的异常，这种异常是从exception类派生而来的，在头文件typeinfo中定义，可以像下面这样使用：1234567#include &lt;typeinfo&gt;...try&#123; Superb &amp; rs = dynamic_cast&lt;Superb&amp;&gt;(rg);&#125;catch(bad_cast&amp;)&#123; ...&#125; typeid运算符和type_info类 &emsp;&emsp;typeid运算符使得能够确定两个对象是否为同种类型。它与sizeof有些相像，可以接受两种参数： 类名 结果为对象的表达式 &emsp;&emsp;typeid运算符返回一个对type_info对象的引用，其中，type_info是头文件typeinfo中定义的一个类。type_info类重载了==和!=运算符，可以进行类型间的比较。下面的代码判断pg指向的是否是一个Magnificent对象：1typeid(Magnificent) == typeid(* pg); //只会判断指针的类型，当基类指针指向子类时，得到的也是基类的类型 &emsp;&emsp;如果pg是一个空指针，程序将引发bad_typeid异常。该异常类型是从exception类派生而来的，是在头文件typeinfo中声明的。 误用RTTI的例子 如果在扩展的if else语句系列使用了typeid，则应高了是否应该是否虚函数和dynamic_cast。 15.5 类型转换运算符C语言中的类型转换运算符太过松散，因此，在C++中，提供了更严格的限制允许的类型转换，并添加4个类型转换运算符，使转换过程更规范： dynamic_cast const_cast static_cast reinterpret_cast const_cast 运算符用于执行只有一种用途的类型转换，即改变值为const或volatile，其语法与dynamic_cast运算符相同。1const_cast &lt;type-name&gt; (expression) 如果类型的其他方面也被修改，则上述类型转换将出错。也就是说，除了const或volatile特征可以不同外，type-name和expression的类型必须相同。 提供该运算符的原因是，有时候可能需要这样一个值，它在大多数时候是常量，而有时有事可以修改的。此时就可以声明为const，并在需要的时候使用const_cast。 static_cast运算符的语法与其他类型转换运算符相同：1static_cast &lt;type-name&gt; (expression) 仅当type-name可被隐式转换成expression所属的类型或expression可被隐式转换为type-name所属的类型时，上述转换才是合法的，否则将出错。 reinterpret_cast运算符用于天生危险的类型转换。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[轻量级深度学习框架ZeroTensor：（一）框架设计]]></title>
    <url>%2Fz_post%2F%E9%A1%B9%E7%9B%AE-ZeroTensor-%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"></content>
      <categories>
        <category>项目</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[FocalLoss-ICCV2017]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ICCV2017-FocalLoss%2F</url>
    <content type="text"><![CDATA[文章: Focal Loss for Dense Object Detection作者: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár 核心亮点(1) 分析并指出了One Stage方法精度不高的原因: 极度不平衡的正负样本比例: anchor是一种类似sliding windows的选框方式, 这会使得正负样本的比例接近1000:1, 而且绝大部分负样本都是easy example. 梯度优化过程被easy example过度影响: 这些easy example的loss虽然不高, 但由于数量众多, 最终合起来会对loss有很大的贡献, 从而导致优化的时候过度关注这些easy example, 这样会收敛到一个不够好的结果. (2) 提出了解决正负样本比例和easy example 问题的Focal loss: FL(p_t) = -(1-p_t)^{\gamma} log(p_t)核心思想很简单, 就是在优化过程中逐渐减低那些easy example的权重, 这样会使得训练优化过程对更有意义的样本有更高的偏置. PS:注一: 为什么Focal Loss没有用在Two Stage方法上面? 这是因为以RCNN为代表的一系列Two Stage会在区域候选推荐阶段采用两个问题来降低正负样本比例和easy example问题带来的影响: 采用NMS算法将物体位置候选框降低到一到两千个，更重要的是，这一到两千个可能位置并不是随机选取的，它们移除了大量的易分类负样本（背景框） 采用了biased-minibatch的采样策略, 比如，保证正样本和负样本的比例为1：3进行训练（这其实相当于起到了 $\alpha$ 因子的作用 Focal Loss 的两个重要性质: 当一个样本被分错的时候, $p_t$ 是很小的, 比如当 $y=1$ 时, $p_t$ 要小于0.5才算是错分类, 此时 $p_t$ 就比较小, 反之亦然, 因此调制系数就会趋近于1, 也就是说相比原来的loss没有太大的改变, 而当 $pt$ 趋近于1的时候, 说明此时分类正确而且是易分类样本, 调制系数就会趋近于0, 也就是该样本对总的loss的贡献度很小. 当 $gamma=0$ 的时候, focal loss就是传统的交叉熵损失, 随着 $gamma$ 的增加, 调制系数的影响力也会增加 论文细节背景介绍首先，Lin等人回顾了一下目前目标检测的发展概况，先进的目标检测算法目前大概分成两种，一种是one-stage的，一种是two-stage的。one-stage的代表作YOLO以一种简单统一的网络成功实现了实时检测的目的，但是它的准确率并不是best。相反的，two-stage的代表作RCNN系列则是在准确率方面完胜其他模型，但是它的检测速度真的是慢的有的可怜（相比于YOLO）。 于是，Lin他们就开始研究造成这种现象的原因，接着他们就发现原来是在训练的时候，后景数据相比于前景数据较少的缘故。为了解决这个问题，Lin等人对标准的交叉熵损失函数进行修改，降低那些已经分类很好的样例对loss的影响比重，称之为Focal Loss，也就是会特别关注某一些loss的意思。 最后，Lin等人实现了一个使用Focal Loss的简单的检测系统，最终在速度上几乎赶上了YOLO，并且在准确性了超过了现有的所有检测算法。源码可以在facebook的Detectron上取得：https://github.com/facebookresearch/Detectron . 作者介绍了一下two-stage方法的简单流程以及相关的论文，然后又介绍了以下YOLO和SSD等one-stage方法的概况，然后，作者指出这one-stage方法精确度低的主要原因是源于训练集数据分布的不平衡导致的。 由此，作者就引出了自己的Focal Loss损失函数，指出Focal Loss函数在面对具有较高confidence的样例时，其影响因子会接近于0,直观上来说，就说Focal Loss更关注那些hard examples。另外，作者还提出，Focal Loss的形式不是唯一的，很多其他的实现方法也能达到相似的结果。（其实我觉得这说明损失函数这一块还有很多工作可以往下研究） 为了证明Focal Loss的有效性，作者实现了一个one-stage检测模型，命名为RetinaNet（Retina是视网膜的意思），基于ResNet-101-FPN实现，还用到了feature pramid和anchor boxes等思想。 传统检测系统主要是基于sliding-window paradigm的一类方法：HOG， DPMs等等。虽然滑动窗口类的方法在目标检测领域处于一线地位，但是随着deep learning的出现和研究，滑动窗口方法渐渐失去光芒。 Two-stage Detectorstwo-stage方法的先驱是Selective Search work，它会首先提取出一个稀疏的候选框集合（稀疏是指只有很少一部分包含物体），然后对这些候选框进行分类，看是否包含物体，或包含哪种物体。 之后，RCNN的诞生标志着深度学习技术成功引入目标检测领域，利用cnn网络对特征的高度抽象和提取，rcnn在物体检测的准确率上大幅度提高，后期的RCNN系列又不断的提出新的方法来提升准确率和速度，到Faster RCNN时，提出了RPN网络，将候选框选取阶段和分类阶段都放在了统一个网络，使之可以进行端到端训练。后续还有更多的关于这一系列的工作继续被人们研究着。 One-stage DetectorsOverFeat算是首个现代的基于深度学习的one-stage检测方法，而最近的SSD和YOLO更是激起了人名对one-stage方法的研究热情，但是one-stage方法最令人诟病的地方就在于它们较低的准确率。 为此，本文的工作就是想要知道是否one-stage检测算法可以在精确度上匹敌two-stage检测算法，同时还要保持一定的检测速度。 于是，作者提出了Focal Loss，一种新的损失函数，利用这个损失函数，可以在保持现在模型大框架不变的基础上，达到最好的检测水平！ 样本类别不均衡 Class Imbalance不管是传统的one-stage检测方法如boosted detectors， DMPs，还是最近的方法SSD，都会在训练阶段面临 $10^4\sim 10^5$ 个候选区域，这其中会包含大量的背景区域，也就是负样本，这种不平衡会造成两个问题： 在训练时，在大多数位置都是负样本，这样只会贡献更多无用的信号 大量的负样本会导致模型在一定程度上的退化 对于此问题，常用的解决方案是在训练阶段设计更复杂的样本抽取策略，但是这样速度就会受影响。而本文提出的Focal Loss，不仅解决了样本不均的问题，而且不需要增加额外的抽取策略，可以更快训练 Robust Estimation有很多工作乐于设计健壮的损失函数，具体可以看看原文的参考文献，这里就不说了（主要因为没有什么好讨论的地方，作者也只是提了一下） Focal Loss为了便于理解，从交叉熵（CE）的二分类问题出发： CE(p,y) = \begin{cases} -log(p)& \text {if y=1} \\ -log(1-p) & \text{otherwise}\end{cases}当二分类问题中的样本分布不均时，数量多的样本的损失值对最终函数的影响会淹没数量少的样本产生的影响。多分类问题也是如此。 为了便于表示，对上面的公式进行改写： $CE(p,y) = CE(p_t) = -log(p_t)$，于是有： p_t = \begin{cases} p & \text{if y = 1} \\ 1-p & \text{otherwise} \end{cases}Balanced Cross Entropy一个常用的解决办法就是引入一个权重因子 $\alpha \in [0,1]$，然后分别令 $\alpha$ 和 $1 - \alpha$作为两个类别的权重，$\alpha$ 的取值可以是根据类别出现的频率决定，也可以作为超参数，利用交叉验证来选取较好的值。我们使用这种方法作为baseline来与Focal loss进行比较： CE(p_t) = -\alpha log(p_t)Focal Loss Definition本文的实验结果表明，类别分布不均会对交叉熵损失函数带来很大的影响。那些很容易被分类的负样本（背景等）贡献了损失函数及其梯度中的大部分影响力。尽管baseline方法的 $\alpha$ 因子可以平衡正负样本之间的比例，但它仍然不能把握好简单样本和困难样本的比例（应该困难样本多一些，简单样本少一些，这样有利于模型的健壮性）。于是，作者就提出了Focal Loss，主要引入了一个“调制因子” $(1-p_t)^\gamma$ ，其中 $\gamma \ge 0$ ，具体的形式化表示如下： FL(p_t) = -(1-p_t)^\gamma log(p_t)直观上来讲，这个“调制因子”可以降低easy example对loss的contribution。同时，还应注意到，Focal Loss的形式不是唯一固定的，作者还在使用了其他不同形式的因子，比如结合了 $\alpha$ 因子： FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t)后文的大部分实验都使用的是上面这个形式的Focal Loss。 Class Imbalance and Model Initialization二值分类模型在初始的时候，对两个类别的预测概率是均等的，在这种初始化条件下，如果某一个类别出现的次数过多，就会对损失函数产生较大的影响。为了解决这个问题，作者特意提出了“先入为主”的概念，也就是使得模型在开始的时候，对稀有类别（如前景类别）的预测概率的初始值设置的低一些，如0.01 。 经过实验表明，这样的方法可以提升模型训练的稳定性。 Class Imbalance and Two-stage DetectorsTwo-stage Detector 并没有使用类似 $\alpha$ 因此的方法来解决样本不均的问题。相反的，它们通过两个机制来降低这个问题带来的影响：（1）two-stage模式和（2）biased minibatch取样。首先，two-stage模式会在第一阶段就将近乎无限物体位置可能性降低到一到两千个，更重要的是，这一到两千个可能位置并不是随机选取的，它们移除了大量的易分类负样本（背景框）。第二，这些方法还设计了biased minibatch的取样策略，比如，保证正样本和负样本的比例为1：3进行训练（这其实相当于起到了 $\alpha$ 因子的作用。 RetinaNet DetectorRetinaNet是一个单一的、统一的网络，它由一个backbone网络和两个task-specific子网络组成。backbone网络是现成的，主要负责计算卷积特征图谱。第一个子网络负责物体分类任务，第二个子网络负责bounding box回归任务，它们都是在backbone网络输出的卷积图谱上进行计算的。 Feature Pyramid Network Backbone： 采用了FPN作为backbone网络。 Anchors: 和FPN一样，对P3到P7使用了不同大小的anchors Classification Subnet： 该子网络是一个较小的FCN，连接在FPN的每一层。 值得注意的一点是，该子网络并不与Box Regression Subnet共享参数，二者是互相独立的。 Box Regresion Subnet： 与分类子网络并行，该子网络也是一个FCN网络，连接在FPN的每一层上。目标是让anchor通过位移回归到gt box附近。 Inference and TrainingInference： RetinaNet是有基于FPN和backbone和两个基于FCN的子网络组成的一个统一的单一网络，因此，在inference阶段，只需要简单的通过前向传播经过整个网络即可。为了提高速度，本文在每个FPN层级上，只会处理最多1000个box prediction。 Focal Loss： 使用了上文提到的Focal Loss。取 $\gamma=2$ 。在训练阶段，本文强调将focal loss应用到所有100k个anchors上，主要目的是为了与RPN和SSD等模型作对比。 从实验结果上看，当 $\gamma$ 的值取得较大时，$\alpha$ 的值就应该取消一些（for$\gamma=2$ , $\alpha = 0.25$ works best)。 Initialization： 本文分别实现了ResNet-50-FPN和ResNet-101-FPN。 对其中初始值可参见原文。 Optimization： 使用了SGD优化方法，在8个GPU上训练，每个minibatch有16张图片（一个GPU包含2张图片）。 损失函数为focal loss和标准smooth L1损失函数之和。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOv3]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-YOLOv3-Arxiv2018%2F</url>
    <content type="text"><![CDATA[摘要作者对YOLOv2进行了一些改进，使之在保持实时检测的同时，准确率又有所提升了。 介绍作者说他这一年（18年）基本没干啥，就是打打电话，玩玩推特，偶尔还帮别人干点活。。 然后因为只对YOLO做了一些改进，但是并没什么特别的地方，因此就写了这一篇技术报告,而没有选择发表成论文形式。 The Deal作者说了，他们大部分的工作都是从别人那里吸取好的点子，同时训练了一个新的分类器网络（比别人的好，恩。。） Bounding Box Prediction和YOLO9000一样，在预测bounding box时使用了dimension clusters和anchor boxes。 YOLOv3在预测每个bouding box的objectness score时，使用的是logistic regression。 与faster rcnn不同的是，我们的系统只会给每个gt object指派一个bounding box。如果没有指派的话，就说明没有对象的box坐标，只有objectness。 Class Prediction每个box使用了多标签分类，我们不选择softmax是因为发现它很难取得好的效果，因此，改用一个单独的logistic classifiers。在训练阶段，使用binary cross-entropy loss来进行类别预测。 Predictions Across ScalesYOLOv3在三种不同的scales下进行预测。 Feature Extractor作者使用了一个新的网络模型来提取特征，主要是在Darknet-19中引入了residual network stuff，最终模型的卷积层数达到53层，也就是Darknet-53。 Training仍然使用不带hard negative mining的图片训练。同时使用了multi-scale training，data augmentation，batch normalization，以及其他的一些标准程序。 How We Do根据不同的评价标准，YOLO的性能差异较大，总的来说主要是因为YOLO虽然能标出物体的大致位置，但是画出的框并不是“完美”，使得在IOU要求高的评价标准上，YOLO的得分很低。 另外， 之前的YOLO在检测小物体上往往有很多瓶颈，而目前的YOLO已经在慢慢克服这方面的缺陷 Things We Tried That Didn’t WorkAnchor box $x,y$ offset predictions Linear $x,y$ predictions instread of logistic Focal loss Dual IOU thresholds and truth assignment What This All means最后，作者说了为什么要选择其他的评价标准。 对于人类来说，很难直接区分出IOU0.3和IOU0.5之间的差别，那么我们要求计算机这样做是否合理呢（我认为是合理的。。。） 最后作者说出了对计算机视觉未来发展的一些“愿景”。（作者反对隐私泄漏和军事用途）]]></content>
  </entry>
  <entry>
    <title><![CDATA[YOLO9000]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-YOLOv2-CVPR2017%2F</url>
    <content type="text"><![CDATA[核心亮点https://zhuanlan.zhihu.com/p/35325884 关键技术论文细节背景介绍https://zhuanlan.zhihu.com/p/40659490 摘要本文提出了一个新的，实时目标检测模型，YOLO9000。首先，作者使用了不同的提升技巧来优化YOLO模型，同时，利用多尺度的训练方法，YOLO可以方便的在speed和accuracy之间进行tradeoff。在67FPS时，YOLOv2可以在VOC2007上活得76.8的mAP，在40FPS时，YOLOv2可以或者78.6mAP，超过了Faster RCNN和SSD的性能表现。尽管只有200个类里面的44个类的数据，YOLO9000仍然可以在ImageNet上获得19.7的mAP，对于不在COCO里面的156个类，YOLO可以获得16.0的mAP。9000的含义是说YOLO-v2可以运行在超过9000个不同的物体类别，同时保持实时检测。 介绍目前关于分类任务的数据集数量远远超过检测任务的数据集大小，而短期内，检测任务的数据集数量无法快速增长。对此，本文提出了一个新的方法来利用现有的分类任务的数据集，进而扩充当前目标检测系统的检测范围。 同时，本文还提出了一个联合训练方法，使得我们可以同时在检测数据集和分类数据集上训练检测器。（检测数据集提升定位能力，分类数据集提升类别容量和系统健壮性） 本文分两步：首先将YOLO升级到YOLOv2,然后利用本文提出的数据集联合方法来进行联合训练。 BetterYOLO的主要缺点在于定位错误和较低的召回率。 本文在优化这些缺点时，并不是选择扩大网络的规模，而是将整个网络简化，使表征信息更容易学习。根据之前的工作，我们采用了很多方法来提升YOLO的性能。 Batch Normalization：在所有的卷积层之上加上BN，可以提升2%的mAP，并且可以去掉dropout层而不产生过拟合。 高分辨率分类器 High Resolution Classifier：之前的YOLO是利用ImageNet的224大小的图像预训练的，然后在检测时，会将224的图像放大到448尺寸。在YOLOv2,首先在448的ImageNet图像上进行finetune 10 epochs。这给了一定时间让网络适应更大的尺寸大小，然后再在该网络进行物体检测的finetune。 这可以提升4%mAP。 Convolutional With Anchor Boxes：YOLO使用叠在卷积层之上的全连接层的特征提取器来直接预测bounding box的坐标。 相比于YOLO，Faster RCNN使用了精心挑选的方式来获得预测的boundign box，它在anchor box的基础上进行预测，并且其预测层是卷积层。为此，本文移除了YOLO的全连接层，改用anchor box来预测bouding box。 首先，移除了一个pool层，从而使网络卷积层的输出有更高的分辨率。另外，还将网络的输入图像的分辨率降到416,这么做的原因是作者希望在特征图谱上得到奇数个locations，这样一来，就由一个center cell。YOLO的结构可以使416的图像降到13×13的大小。 在使用anchor box时，我们将类别预测问题从位置标定问题中分离出来，然后为每个anchor box预测类别和是否有物体。和YOLO一样，预测是否有物体时会预测gt和proposed box的IOU，类别预测时会计算给定有物体的条件下给出属于每个class的条件概率。 原来的YOLO会对每张图片产生98个box，而使用anchor box后，每张图片会产生上千个box。 不用anchor box时，本文的模型可以达到69.5的mAP和81%的recall。而是用了anchor box后，可以到大69.2的mAP和88%的recall。虽然mAP变低了，但是recall的提升说明本模型还有很大的提升空间。 Dimension Cluster：在使用anchor box时，主要遇到了两个问题。 第一：anchor box的维度是手动标定的。 anchor值的选择会对最终结果有一定影响。为了解决这个问题，我们不采用手动标定的方法，而是对训练集的boudning boxes用k-means clustering来自动找到较好的anchor值。如果使用基于欧式距离的标准k-means，那么更大的box就会产生更多的error。为了不让box的大小对最终的anchor值有影响，我们使用下面的式子作为距离度量： d(\text{box},\text{centroid}) = 1 - IOU(\text{box}, \text{centroid})最终在模型复杂度和高召回率的权衡下，本文选择 $k=5$ 。 直接位置预测 Direct location prediction使用anchor box的第二问题就是：模型不稳定，尤其是在早起迭代阶段。稳定性差的主要来源是box的坐标 $(x,y)$ ，在RPN网络中，网络会预测 $t_x$ 和 $t_y$ ，于是 $(x,y)$ 的值可以通过下面的公式计算得到： x = (t_x*w_a) - x_ay = (t_y*h_a) - y_a本文不使用上面的方法，而是使用YOLO中的方法，预测相对于grid cell位置的相对坐标，这将gt限制在了0到1之间。这样的参数设置使得参数更容易学习，网络更加稳定。 Fine-Grained Features精细化的13×13的特征图谱对于标定大物体来说已经足够了，同时，由于特征更加细粒度，使得它在标定更小的物体时有一定提升。 Faster RCNN和SSD都在不同的特征图谱上执行，因此，它们得到的是一个区间的图像分辨率大小。 本文采用一个更简单的测率，直接添加一个passthrough层，使得从更早的26×26的层中得到特征。 这个passthrough层将高分辨率的特征和低分辨率的特征连接起来，通过将相邻特征堆叠到不同的channes？ 这将26×26×512的特征图谱变成了一个13×13×2048的特征图谱。 Multi-Scale Training为了使模型更加健壮，使用了不同尺度的图片来训练模型。在训练时，每经过一段迭代次数后，都会改变接受输入图片的size。由于本文的模型输出的尺寸会变成原来1/32,因此选择以下尺寸：{320,352,…,608}。 这样一来，一个网络可以在不同的分辨率下进行目标检测，可以取得更好的效果。 Faster大多数目标检测网络使用了VGG16作为基础网络，但是VGG16需要30.69billion浮点运算，十分复杂。 而本文使用基于GoogleNet的自定义网络，只需要8.52billion浮点运算。（但是精确性低于VGGnet） Darknet最终网络起名为Darknet-19。 具有19个卷积层和5个最大池化层。 Training for classification将网络在标准Imagenet 1000上进行训练。 SDG的初始学习率为0.1, 递减指数为0.4,权重递减为0.0005,momentum为0.9。 在训练时，使用了标准的数据增广方法：random crops，ratations，hue，saturation，exposure shifts等。 Training for detection将上面训练好的网络的最后一层卷积层移除，然后加上三个具有1024个filter的3×3卷积层，并在其中跟一个1×1的卷积层，使输出是我们需要的结果。 比如，对于VOC，需要有5个box，每个box有5个coordinates和20个class，所以需要125个filetes。 同时使用了passthrough层，以便模型可以使用fine grain features。 Stronger本文提出了一种可以联合训练分类数据和检测数据的机制。本文的方法使用检测数据的图像标签来学习物体位置信息，使用分类数据的标签来扩充可以检测的物体的类别。 在训练阶段，我们了检测数据和分类数据混合。当网络模型看到一个带有检测标签的图片时，就会对YOLOv2的整个损失函数进行BP求导，当看到分类图片时，则只会对分类部分的损失函数进行BP求导。 上面的方法具有一些难点：]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO v1]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-YOLOv1-CVPR2016%2F</url>
    <content type="text"><![CDATA[文章: You Only Look Once: Unified, Real-Time Object Detection 作者: oseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi 核心亮点(1) 将检测问题看做是回归问题对于给定的输入图像, YOLO会使用一个单一的网络 同时 给出bounding box的预测结果和对应的类别概率. (2) 没有Region Proposal的过程YOLO采用 $S\times S$ 的网格划分来确定候选框, 如果某个物体的中心落在了某个cell里, 那么这个cell就负责该物体的检测. PS:注一: YOLO中采用 $S\times S$ 的网格划分来确定候选框, 这实际上是一种很粗糙的选框方式, 同时也导致了YOLO在面对小目标物以及群落目标物时, 性能较差.(因为YOLOv1的同一个cell无法预测多个目标) 论文细节背景介绍YOLO将目标检测问题看作是一个回归问题，进而从整张图像中直接得到bounding boxes和对应的class probabilities。 之前的工作都是将检测任务看成是一个分类问题，如RCNN，通过区域提取，分类，区域修正，去重等等一系列工作得到检测结果，这样的模型十分复杂而且很难优化，因为区域提取和分类任务必须单独训练，麻烦且难以调试。 本文将目标检测问题看成是一个回归问题，直接从图片像素中得到bounding box坐标和class probabilities。 YOLO具有三大优点： Fast。 由于不用按照复杂的pipeline进行运作，YOLO只需要一个卷积网络就可以同时预测出多个物体，因此十分快 YOLO在进行推理时，可以看到整幅图片，因此，可以隐式地对物体的周围像素进行分析。这使得YOLO不容易在背景中错误识别。反观Fast RCNN，经常会将背景中的非物体检测出来。 YOLO的泛化性更好，可以学到更一般的特征。在自然图像上训练后，YOLO在艺术图像上可以取得相比于RCNN更好的检测效果。 关键技术YOLO没有提取候选区域的过程, 与之相对的, YOLO采用网格划分的方式来确定物体的候选区域框, 具体来说, YOLO会将图像按照 $S\times S$ 的大小划分成多个cell, 之后, 如果哪个物体的中心落在了某个cell里面, 那么这个cell就负责检测这个物体, 如下图中, 狗的中心落在了红色cell内, 则这个cell负责预测狗. “物体落在哪个cell, 哪个cell就负责预测这个物体” 分为训练和测试两个阶段: 训练阶段. 在训练阶段, 如果物体中心落在这个cell, 那么就给这个cell打上这个物体的label, 让这个cell和该物体关联起来 测试阶段. cell会根据已经训练好的参数来决定自己负责预测哪个物体. 网络的整体架构如下图所示: 从图中可以看出, YOLO网络的输出网格是 7×7 大小的, 另外, 输出的channel数目30, 在每一个cell内, 前20个元素是每个类别的概率值, 然后2个元素对应2个边界框的置信度, 最后8个元素时2个边界框的 $(x,y,w,h)$.(每个cell会预测两个框, 最后选择IOU较大的来复杂物体的预测) 根据网络的输出, 我们可以知道, YOLO的预测目标主要有三个: 类别预测, Confidence预测, Bounding box预测. 在训练阶段，该模型要优化下面的联合目标损失函数(第一行是bounding box预测, 接下来是confidence预测, 最后是类别预测) \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^B I_{ij}^{obj}[(x_i-\hat x_x)^2 + (y_i - \hat y_i)^2] \ + \lambda_{coord}\sum_{i=0}^{S^2} \sum_{j=0}^B I_{ij}^{obj} [(\sqrt w_i - \sqrt{\hat w_i})^2 +(\sqrt h_i - \sqrt{\hat h_i})^2]+ \sum_{i=0}^{S^2}\sum_{j=0}^B I_{ij}^{obj} (C_i - \hat C_i)^2 + \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B} I_{ij}^{noobj}(C_i-\hat C_i)^2+ \sum_{i=0}^{S^2} I_i^{obj} \sum_{c\in \text{classes}} (p_i(c) - \hat p_i(c))^2需要注意的是, 网络并不会总是计算所有的loss项, 具体地说: 对于有物体中心落入的cell, 需要计算分类loss, 两个confidenceloss, 但只计算IOU较大的bounding box loss 对于没有物体中心落入的cell, 只需要计算confidence loss. 另外, 我们发现每一项的计算(即使是分类)都是 L2 loss, 从另一角度体现出YOLO把分类问题转化为了回归问题. 2. 一体化检测YOLO使用整幅图像的特征图谱进行预测，同时预测所有物体的所有bounding box。这样的设计思想，可以使得YOLO进行端到端的训练，并且能够进行实时检测。 系统将整张图片划分成 $S \times S$ 大小的网格。 如果某个物体落入了网格中的某一格，那么这个格子就负责检测该物体。 每个格子会预测B个bounding boxes和B个confidence scores。这些confidence scores反映了模型对这个box里面是否有物体，并且有多大的把握确定。 将confidence定义为 $Pr(Object)\times IOU_{pred}^{truth}$ 。 $IOU_{pred}^{truth}$ 代表真实框和预测框之间的IOU值。 每一个bounding box包含5个预测值：x，y，w，h，和confidence。 每一个grid cell预测C个conditional class probabilities，记为 $Pr(Class_i|Object)$ 。 C与B的个数之间没有直接关系。 在测试阶段，我们将conditional class probabilities和individual box confidence predictions相乘： Pr(Class_i|Object)\times Pr(Object)\times IOU_{pred}^{truth} = Pr(Class_i)\times IOU_{pred}^{truth}由此可以得到针对每个box的特定class的confidence scores。这些scores代表着特定calss出现在box里面的概率，以及预测出来的box在多大程度上适应这个object。 最终预测的tensor维度： $S\times S\times (B\times 5+ C)$ 。 网络设计YOLO：收到GoogleNet的启发，公有24层卷积层和2层全连接层 但是没有使用Inception模块，而是使用了 $3\times 3$ 的卷积层和一个 $1 \times 1$ 的reduction layers（减少depth） fast YOLO：9个卷积层和2个全连接层。 训练首先在ImageNet上进行了预训练。 预训练时，使用前20个卷积层，加上一个平均池化层，和一个全连接层。 使用了Darknet framework。 Ren et al证明在预训练的网络上添加卷积层和全连接层可以提升性能。因此，本文添加了4个卷积层和2个全连接层，都赋予随机初始值。 模型的输入图像像素为448 。 最后一层同时预测class probabities和bounding box coordinates。 我们将box的宽和高都归一化到占图片宽高值的比例，因此coordinates的值在0到1之间。coordiantes的x和y归一化到对特定cell的相对位移，所以它们的值也在0到1之间。 本文最后一层使用线性激活函数，其他层均使用leaky rectified linear 激活函数，如下所示： \phi(x) = \begin{cases} x & \text{if } x>0 \\ 0.1x& \text{otherwise} \end{cases}本文的优化函数为平方和误差。 由于它对localization error的权重和对classification的权重是一样的，因此该函数并不能够很好的匹配我们的目标。为了解决问题，提升了bounding box coordinate predictions的loss，同时降低了confidence predictions的loss。 作者使用了 $\lambda_{coord} = 5$和 $\lambda_{noobj} =5$ 来实现这一目标。 同时为了更好的针对小目标，本文对bounding box的宽和高都使用了平方跟。 YOLO对每个grid cell都会预测出多个bounding boxes，而在训练阶段，我们只需要一个bouding box 来对每个物体负责。选取的原则是与GT有最高的IOU值。 在训练阶段，本文优化下面的联合目标损失函数： \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^B I_{ij}^{obj}[(x_i-\hat x_x)^2 + (y_i - \hat y_i)^2] \ + \lambda_{coord}\sum_{i=0}^{S^2} \sum_{j=0}^B I_{ij}^{obj} [(\sqrt w_i - \sqrt{\hat w_i})^2 +(\sqrt h_i - \sqrt{\hat h_i})^2]+ \sum_{i=0}^{S^2}\sum_{j=0}^B I_{ij}^{obj} (C_i - \hat C_i)^2 + \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B} I_{ij}^{noobj}(C_i-\hat C_i)^2+ \sum_{i=0}^{S^2} I_i^{obj} \sum_{c\in \text{classes}} (p_i(c) - \hat p_i(c))^2batch size为64，a momentum of 0.9 and a decay of 0.0005。 推理阶段平均每张图片会得到98个bounding boxes。 虽然采用了非极大值抑制，但是提升的效果并不高，不如RCNN和DPM那么明显。 YOLO的局限性难以检测小物体和堆积在一起的物体，比如鸟群。 另外，YOLO对于不同大小的物体，其采取的损失函数是一样的，因此，在面对大物体时，细微的差别可能不会引起IOU的大幅变化，但是在面对小物体时，就会产生较大波动。YOLO的错误来源主要是由于定位错误。 和其他检测系统的比较Deformable parts models： DPM使用了滑动窗口的方法来做目标检测。它的检测是由分离的好几段过程完成的。 相比来说，作者的模型统一了所有这些过程，并且取得了更快更好的效果（基本来说就是把DPM吊打了。。。，不过毕竟DPM是2010年的产品，不吊打说不过去了。。） RCNN： RCNN没有使用滑动窗口的方法来获取bounding box，而是使用了Selective Search（之后也不用SS方法了，提出了RPN，继承到模型内部了）。同理，RCNN也是一种多阶段的方法，先画框，再检测，分两步走。YOLO在一定程度了也借鉴了RCNN及其变体的思想，但是YOLO是基于grid cell进行proposes bounding box的，所以最后只生成了98个框，而RCNN的框多大2000个，所以YOLO在速度上肯定是远超RCNN了，另外精度上也比RCNN高（不过RCNN只是region based检测方法的雏形，所以并不说明YOLO比RCNN整个系列都好）。 Other Fast Detectors： RCNN其他系列来了，作为后出生的Fast RCNN和Faster RCNN，当然视为自家的兄弟出了口气，在精度上爆了YOLO，但是速度还是不及YOLO（YOLO是真的快，真正意义上的实时监测系统） Deep MultiBox 14年出来的，SPPNet使用了它进行选框 OverFeat 13年的一篇文章 MultiGrasp 这是Joseph自己的工作，在YOLO之间发的，解决的任务是检测一张的图片中某个包含物体的区域，比YOLO要解决的任务简单的多，没什么好说的 实验 Experiments首先是在VOC2007上做了实验，然后专门针对YOLO和Fast RCNN进行比较，虽然整体mAP没有Fast高，但是在背景上的假正例比Fast少。接着，还给出2012VOC的实验结果。最后，还做了一个从自然图像训练，然后检测艺术作品的实验，提出YOLO可以学到更一般化的特征。 4.1 Comparison to Other Real-Time Systems]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序算法 时间复杂度 最坏/好时间复杂度 空间复杂度 是否稳定 冒泡排序 $O(n^2)$ $O(n^2)$ / $O(n)$ $O(1)$ 稳定 选择排序 $O(n^2)$ $O(n^2)$ / $O(n^2)$ $O(1)$ 不稳定 插入排序 $O(n^2)$ $O(n^2)$ / $O(n)$ $O(1)$ 稳定 快速排序 $O(nlogn)$ $O(n^2)$ / $O(nlogn)$ $O(logn)$ 不稳定 归并排序 $O(nlogn)$ $O(nlogn)$ / $O(nlogn)$ $O(n)$ 或 $O(1)$ 稳定 堆排序 $O(nlogn)$ $O(nlogn)$ / $O(nlogn)$ $O(1)$ 不稳定 希尔排序 $O(nlogn)$ 与步长 $d$ 有关 / $O(nlogn)$ $O(1)$ 基数排序 1. 冒泡从后往前, 相邻的数据两两比较, 一趟完成后, 第一个元素为最大/小值 时间复杂度: $O(n^2)$ 空间复杂度: $O(1)$ 稳定性: 是稳定的, 主要就是遇到相等的元素时不要进行交换操作即可 1234567891011void bubble_sort(vector&lt;int&gt; &amp;input)&#123; for(int i=0; i&lt;input.size(); i++)&#123; for(int bubble = input.size()-1; bubble&gt;i; bubble--)&#123; if(input[bubble-1] &lt; input[bubble] )&#123; //为了保证排序的稳定性, 这里不要用 &lt;= int temp = input[bubble]; input[bubble] = input[bubble-1]; input[bubble-1] = temp; &#125; &#125; &#125;&#125; 2. 选择(交换)排序时间复杂度: $O(n^2)$ 空间复杂度: $O(1)$ 稳定性: 因为选择排序在交换两个元素时, 是不考虑其他元素的相对位置的, 所以, 不管怎么样, 只要发生交换, 就一定会造成不稳定. 但是!!! 如果使用链表或者新开辟一个数组的话, 选择排序也是稳定的, 但实际上这种方法就没有交换的过程, 对于链表来说, 是把节点从链表中拿出来, 组成新的链表, 而不会对原来链表中的元素进行交换. 对于新开辟数组来说, 相当于是用空间换取稳定性, 同样也没有交换过程. 3. 插入排序将数据分为两个部分, 有序部分和无序部分, 一开始有序部分包含只包含第一个元素, 依次将无序部分的元素插入到有序部分 ( 插入的时间复杂度为 $O(n)$ ), 直到所有元素插入完毕. 插入分为数组插入和链表插入(其实堆排序也算是一种插入排序) 下面的时间和空间复杂度均指 数组直接插入, 对于链表, 时间和空间都是 $O(n)$ 时间复杂度: $O(n^2)$ 空间复杂度: $O(1)$ 稳定性: 只要在插入遇到相等元素时, 将新插入的放在最后, 那么就是稳定的. 4. 快排时间复杂度: $nlogn$ 快排的期望时间复杂度为 $O(nlogn)$ , 最坏时间复杂度为 $O(n^2)$, 为了避免出现最坏的情况, 可进行如下改进: 哨兵元素的选择: 为了使每次划分的数不至于使两边相差过大, 我们可以选择三者取中法选择哨兵, 一般根据首尾元素和中间元素进行选择. 小数据量的改进: 递归的快排大概在n&lt;13的时候比插入要慢, 所以我们在n&lt;13的时候可以采用插入排序 相同数字的改进, 在存在大量相同数字的时候, 可以用两个指针保存相同数字的信息, 在划分时不用把它们算进去(这啥意思?//TODO) 递归的优化. 快排有两次递归调用, 我们可以用循环代替后面的一次递归. 空间复杂度: $logn$ 对于就地快排来说, 它本身使用的空间是 $O(1)$ 的, 但是快排在 递归调用过程中, 就需要消耗一定的空间来保存哨兵及两端元素的下标, 而对于快排的非递归实现中, 需要借助两个栈来模拟系统对于数组low和high的存储情况(递归中的哨兵下标实际上会作为下一次递归的low或者high), : 最优的情况下空间复杂度为: $O(logn)$, 每一次都平分数组 最差的情况下空间复杂度为: $O(n)$, 每一次两边都极度失衡 (主要与进行迭代的次数有关, 迭代次数多了, 占用的内存就多了) 稳定性: 因为快排在划分两边的元素时, 会直接交换某两个元素的位置, 并且这种交换与其他元素的值没有关系, 因此, 如果刚好有相同元素, 很容易就会破坏稳定性. 注意: 最好写出对输入的low和high进行越界检查的部分!! 这个有时候需要特别跟面试官提一声 递归实现:要知道Partition内部使用&lt;=的原因所在, 写成&lt;, 会造成死循环(当遇到相等元素时, 会无限交换) 另外, 如果令 P=input[low] , 那么一定要 high--在前, 否则会造成元素覆盖12345678910111213141516171819202122void quickSort(vector&lt;int&gt;&amp; input, int low, int high)&#123; if(input.size()==0 || low&lt;0 || low&gt;=input.size() || high&lt;0 || high&gt;=input.size())&#123; cout&lt;&lt;&quot;error&quot;; exit(0); &#125; int mid = Partition(input, low, high); if(mid&lt;high) quickSort(input, mid+1, high); if(mid&gt;low) quickSort(input, low, mid-1);&#125;int Partition(vector&lt;int&gt;&amp; input, int low, int high)&#123; int p = input[low]; while(low&lt;high)&#123; // 一定要high在前, 否则会造成数组元素覆盖, 回忆头条面试惨痛经历! while(low&lt;high &amp;&amp; p&lt;=input[high]) high--; //这里注意, 如果忘了写=号, 就会陷入死循环 input[low] = input[high]; while(low&lt;high &amp;&amp; p&gt;=input[low]) low++; input[high] = input[low]; &#125; input[low] = p; return low;&#125; 这里Partition用的是&lt;=，那么在high位元素和p相等时，并不会执行交换，而是会high—, 如果忘了写等号, 就会陷入死循环。 非递归实现:12345678910111213141516171819202122232425262728293031323334int partition(vector&lt;int&gt; &amp;input, int low, int high)&#123; int P = input[low]; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; P &lt;= input[high]) high--; input[low] = input[high]; while(low&lt;high &amp;&amp; P &gt;= input[low]) low++; input[high] = input[low]; &#125; input[low] = P; return low;&#125;void quick_sort(vector&lt;int&gt; &amp;input, int low, int high)&#123; if(input.size()==0 || low&lt;0 || low&gt;=input.size() || high&lt;0 || high&gt;=input.size())&#123; cout&lt;&lt;"error"; exit(0); &#125; stack&lt;int&gt; qs_stack; qs_stack.push(high); //入栈顺序一定要注意, 要与后面的操作对应好 qs_stack.push(low); while(!qs_stack.empty())&#123; low = qs_stack.top(); qs_stack.pop(); // low后入栈, 所以就应该先出栈 high = qs_stack.top(); qs_stack.pop(); if(low &gt;= high) continue; int mid = partition(input, low, high); if(low&lt;mid)&#123; qs_stack.push(mid-1);//入栈顺序一定要注意, qs_stack.push(low); &#125; if(mid&lt;high)&#123; qs_stack.push(high);//入栈顺序一定要注意, qs_stack.push(mid+1); &#125; &#125;&#125; 5. 归并排序核心思想是将两个有序数列进行合并，使其成为一个新的有序数列（时间复杂度为 $O(n)$ ）: 将序列没相邻的两个数组进行归并(merge)操作, 形成floor(n/2)个序列, 排序后每个序列包含两个元素 将上述序列在此归并, 形成floor(n/2)个序列, 每个序列包含四个元素 重复上述归并操作, 直到所有元素归并完毕 时间复杂度: $O(nlogn)$ 归并排序即使在最坏情况下, 时间复杂度也是 $O(nlogn)$, 这是它的一大优点. 空间复杂度: $O(n)$ 或 $O(1)$ 常见的归并排序实现算法都是 $O(n)$ 的空间复杂度, 因为它会额外申请一个与待排序数组相同大小的空间用来进行合并操作. 但是, 合并操作可以被优化成原地合并 (耗时会增加, 但是时间复杂度不变), 此时的空间复杂度就变成了 $O(1)$. 稳定性: 在归并排序过程中, 相同元素有可能出现在同一组内或者不同组内, 如果在同一组内, 则默认就是稳定的, 如果在不同组内, 则根据组的前后位置来判断相同元素的顺序. 因此它是稳定的. 关于归并排序的复杂度: 归并排序的时间复杂度不论是最优还是平均还是最差, 都是 $O(nlogn)$在合并时，由两种选择，一种是不使用额外空间的插入合并，这样会增加时间开销。另一种是使用额外空间的合并，这样不增加时间开销，但是需要额外空间。（当然，如果使用的是链表，则没有这种情况，可以既不增加时间，也不增加空间开销） 常规归并排序: 1234567891011121314151617181920212223242526272829303132333435void mergesort(vector&lt;int&gt;&amp; a,int first, int last)&#123; if(first&lt;last)&#123; mid = (last+first)/2; mergesort(a, first, mid); mergesort(a, mid+1, last); mergeArray(a, first,mid,mid+1,last); &#125;&#125;void mergeArray(vector&lt;int&gt;&amp; a, int first1,int last1,int first2,int last2)&#123; vector&lt;int&gt; temp; int i = first1; int j = first2; while(i&lt;=last1 &amp;&amp; j&lt;=last2)&#123; if(a.at(i) &lt; a.at(j))&#123; temp.push_back(a.at(i)); i++; &#125; else&#123; temp.push_back(a.at(j)); j++; &#125; &#125; while(i&lt;=last1)&#123; temp.push_back(a.at(i)); i++; &#125; while(j&lt;=last2)&#123; temp.push_back(a.at(j)); j++; &#125; for(int i = 0; i&lt;temp.size(); i++) a.at(first1+i) = temp.at(i);&#125; 原地归并排序, 空间复杂度$O(1)$: 12345678910111213141516171819202122232425262728void swap_memory(vector&lt;int&gt; &amp;data, int start1, int end1, int start2, int end2)&#123; std::reverse(data.begin()+start1, data.begin()+end1); std::reverse(data.begin()+start2, data.begin()+end2); std::reverse(data.begin()+start1, data.begin()+end2);&#125;void merge_sort(vector&lt;int&gt; &amp;data, int start, int end)&#123; int mid = (start+end)/2; if(start &lt; end)&#123; merge_sort(data, start, mid); merge_sort(data, mid+1, end); merge_inplace(data, start, mid, mid+1, end); &#125;&#125;void merge_inplace(vector&lt;int&gt; &amp;data, int start1, int end1, int start2, int end2)&#123; int i = start1; int j = start2; while(i&lt;j &amp;&amp; j&lt;=end2)&#123; while(i&lt;j &amp;&amp; data[i] &lt;= data[j]) i++; int index = j; while(i&lt;j &amp;&amp; data[j] &lt;= data[i]) j++; swap_memory(data, i, index-1, index, j-1); &#125;&#125; 6. 堆排序堆简介堆排序与快速排序，归并排序一样都是时间复杂度为 $O(NlogN)$ 的几种常见排序方法 堆（二叉堆）可以视为一棵完全的二叉树，完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。 如下图,是一个堆和数组的相互关系: 因此,对于给定的某个节点的下标i, 可以很容易计算出这个节点的父节点, 子节点的下标 Parent(i) = floor((i-1)/2)，i 的父节点下标 Left(i) = 2i + 1，i 的左子节点下标 Right(i) = 2(i + 1)，i 的右子节点下标 堆一般分为两种,大顶堆和小顶堆, 前者每个节点的值都大于它的子节点,后者反之 大顶堆: 小顶堆: 堆排序原理堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。在堆中定义以下几种操作： 最大堆调整（Max-Heapify）：将堆作调整，使得子节点永远小于父节点 创建最大堆（Build-Max-Heap）：将堆所有数据重新排序，使其成为最大堆 堆排序（Heap-Sort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算 基于上面的三种操作,可以进行堆的插入和删除: 插入: 将新元素放置在数组末尾,然后进行堆调整 删除: 移除堆顶,然后将数组末尾元素置于堆顶,然后进行堆调整(删除主要用于排序) 最大堆调整（MAX‐HEAPIFY）的作用是保持最大堆的性质，是创建最大堆的核心子程序，作用过程如图所示： 代码实现: 12345678910111213141516171819202122232425262728293031323334353637383940414243// 递归实现void max_heapify(vector&lt;int&gt; &amp;vec, int index, int heap_size)&#123; int imax = index //mark max index int ileft = index*2+1; // left child int iright = index*2+2; // right child if (ileft &lt; heap_size &amp;&amp; vec[imax] &lt; vec[ileft])&#123; imax = ileft; &#125; if(iright &lt; heap_size &amp;&amp; vec[imax] &lt; vec[iright])&#123; imax = iright; &#125; if( imax != index)&#123; std::swap(vec[imax], vec[index]); max_heapify(vec, imax, heap_size); //由于变换了当前节点,因此子树的堆结构可能被破坏, //递归调整, 这里imax的坐标是左右子节点的下标之一(因为进行了交换) &#125; // 堆是自下而上进行调整的,所以在调整当前节点符合堆要求之前,子树已经符合堆要求, //除非进行了节点交换,否则子树的堆结构不会被破坏, 无需进行额外处理&#125;//非递归实现void max_heapify(vector&lt;int&gt; &amp;vec, int index, int heap_size)&#123; while(true)&#123; int imax = index; int ileft = 2*index+1; int iright = 2*index+2; if(ileft &lt; heap_size &amp;&amp; vec[imax] &lt; vec[ileft])&#123; imax = ileft; &#125; if(iright &lt; heap_size &amp;&amp; vec[imax] &lt; vec[iright])&#123; imax = iright; &#125; if( imax != index )&#123; std::(vec[imax], vec[index]); index = imax; //产生了交换, 可能破坏了左右子树的堆结构, 令index为左右子树之一的下标, 继续调整 &#125;else&#123; break; //如果没有交换，说明当前结构的堆结构已经完成，直接跳出 &#125; &#125;&#125; 创建最大堆（Build-Max-Heap）的作用是将一个数组改造成一个最大堆，接受数组和堆大小两个参数，Build-Max-Heap 将自下而上的调用 Max-Heapify 来改造数组，建立最大堆。因为 Max-Heapify 能够保证下标 i 的结点之后结点都满足最大堆的性质，所以自下而上的调用 Max-Heapify 能够在改造过程中保持这一性质。如果最大堆的数量元素是 n，那么 Build-Max-Heap 从 Parent(n) 开始，往上依次调用 Max-Heapify。 代码实现: 123456789//创建堆void build_maxheap(vector&lt;int&gt; &amp;vec)&#123; int lasti_parent = std::floor((vec.size()-1)/2); for( int i = lasti_parent ; i&gt;=0 ; i--)&#123; max_heapify(vec, i , vec.size()) //从下到上对每个节点进行堆调整，无需从叶子节点开始 //堆的size需要传整个size过去,因为下标从针对整个堆而言的 &#125;&#125; 创建好堆以后,就可以通过移除堆顶来进行排序,每次将堆顶元素和数组末尾元素进行交换(这样可以不借助额外空间完成排序)，然后对数组的前n-1个元素重新进行堆调整构成新的大顶堆, 重复此过程知道堆中只剩下一个元素, 如下图所示: 12345678//代码实现void heap_sort(vector&lt;int&gt; &amp;vec)&#123; build_maxheap(vec); for(int i = vec.size()-1 ; i &gt; 0; i--)&#123; //重复n-1次 std::swap(vec[0] , vec[i]) // Heapify(vec, 0, i); //堆的大小变为i, 所以必须要设置一个变量来标识堆的size,而不是用vec.size() &#125;&#125;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grasp_detection]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Grasp_detection%2F</url>
    <content type="text"></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 基础]]></title>
    <url>%2Fz_post%2FCpp-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[闲杂123Array &lt; Stack&lt;int&gt; &gt; array_stack;//在C++98中，要求至少用一个空白符将两个&gt;符号分开，以免与运算符&gt;&gt;混淆，C++11不要求这样做 花括号与分号在结构体与类定义大括号后面需要分号；其余可要可不要if else、try catch等组合语句如果在中间加了分号会将一个语句块分成两个 cmath123456789#include &lt;cmath&gt;or#include &lt;math.h&gt;ceil(),floor() 向上、向下取整，不在命名std里面。ceil(5/2); // 2ceil(5.0/2); // 3floor(5.0/2); // 2: 数组对于内置数据类型元素的数组，必须使用()来显示指定程序执行初始化操作，否则程序不执行初始化操作： 123int *pia = new int[10]; // 每个元素都没有初始化int *pia2 = new int[10] (); // 每个元素初始化为0 类类型元素的数组，则无论是否使用（），都会自动调用其默认构造函数来初始化： 123string *psa = new string[10]; // 每个元素调用默认构造函数初始化string *psa = new string[10](); // 每个元素调用默认构造函数初始化 std::find()返回范围 [first, last) 中满足特定判别标准的首个元素迭代器，查找失败则返回end迭代器 std::sort()1234567891011// 自定义函数必须写在最外吗，否则无法通过bool mysort(int a, int b)&#123; return a&gt;b; &#125; //降序int main()&#123; std::vector&lt;int&gt; v = &#123;2,3,5,787,8,5&#125;; std::sort(v.begin(), v.end(), mysort); for(auto iter = v.begin(); iter!= v.end(); iter++)&#123; std::cout&lt;&lt;* iter&lt;&lt;std::endl; &#125;&#125; lamda 表达式:123std::sort(s.begin(), s.end(), [](int a, int b) &#123; return b &lt; a; &#125;); 重载operator()运算符1234567struct &#123; bool operator()(int a, int b) const &#123; return a &lt; b; &#125;&#125; customLess;std::sort(s.begin(), s.end(), customLess); //升序 std::reverse()反转[first, last)范围中的元素顺序. (借助 std::swap()实现)123std::vector&lt;int&gt; v&#123;1,2,3&#125;;std::reverse(std::begin(v), std::end(v));std::reverse(v.begin(), v.end()); vector常用操作截取vector中的一部分作为一个新的vector12 清空：clear() 在最后添加元素：push_back() 初始化123vector&lt;string&gt; v3(5, "hello"); // 创建有5个值为“hello”的string类对象的容器std::vector&lt;int&gt; v = &#123;7, 5, 16, 8&#125;; 判断某元素是否存在123vector&lt;string&gt; vStr;int nRet = std::count(vStr.begin(), vStr.end(), "abc");//返回向量中，“abc”元素的个数 at：访问指定字符，有边界检查 str.at(1) front：访问首字符 (C++11) str.front() back：访问最后的字符（C++11）Cpp string类常用操作截取子串s.substr(pos, n) 截取s中从pos开始（包括0）的n个字符的子串，并返回 s.substr(pos) 截取s中从从pos开始（包括0）到末尾的所有字符的子串，并返回 替换子串s.replace(pos, n, s1) 用s1替换s中从pos开始（包括0）的n个字符的子串 查找子串s.find(s1) 查找s中第一次出现s1的位置，并返回（包括0） s.rfind(s1) 查找s中最后次出现s1的位置，并返回（包括0） s.find_first_of(s1) 查找在s1中任意一个字符在s中第一次出现的位置，并返回（包括0） s.find_last_of(s1) 查找在s1中任意一个字符在s中最后一次出现的位置，并返回（包括0） s.fin_first_not_of(s1) 查找s中第一个不属于s1中的字符的位置，并返回（包括0） s.fin_last_not_of(s1) 查找s中最后一个不属于s1中的字符的位置，并返回（包括0） 判断字符串string里面是否含有某个字符串利用string::size_type string::find(string &amp;);函数 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main()&#123; string a="abcdefghigklmn"; string b="def"; string c="123"; string::size_type idx; idx=a.find(b);//在a中查找b. if(idx == string::npos )//不存在。 cout &lt;&lt; "not found\n"; else//存在。 cout &lt;&lt;"found\n"; idx=a.find(c);//在a中查找c。 if(idx == string::npos )//不存在。 cout &lt;&lt; "not found\n"; else//存在。 cout &lt;&lt;"found\n"; return 0;&#125; c++字符串比较大小的两种方法 compare函数的使用： 123456789#include &lt;iostream&gt;using namespace std;int main()&#123; string str1="hello"; cout&lt;&lt;str1.compare("helloo")&lt;&lt;endl;//返回-1； cout&lt;&lt;str1.compare("hello")&lt;&lt;endl;//返回0 ； cout&lt;&lt;str1.compare("hell")&lt;&lt;endl;//返回1；&#125; 使用strcmp(aa1.c_str(),bb2.c_str()) 1234567891011121314#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;int main()&#123; char* str1="hello"; char* str2="hell"; char* str3="helloo"; char* str4="hello"; //原型extern int strcmp(const char* s1,const char* s2); cout&lt;&lt;strcmp(str1,str2)&lt;&lt;endl;//返回1； cout&lt;&lt;strcmp(str1,str3)&lt;&lt;endl;//返回-1； cout&lt;&lt;strcmp(str1,str4)&lt;&lt;endl;//返回0.&#125; 统计字符串中某个字符出现了多少次使用算法库里面的count函数，使用方法是count（begin，end，‘a’），其中begin指的是起始地址，end指的是结束地址，第三个参数指的是需要查找的字符 123456789101112#include &lt;iostream&gt;#include &lt;algotirhm&gt;#include &lt;string&gt;using namespace std;int main()&#123; string temp = "aaabcdaaa!!!"; int num = count(temp.begin(),temp.end(),'a'); cout &lt;&lt;"在字符串" &lt;&lt; temp &lt;&lt; "中，" &lt;&lt;"字母a出现的次数是" &lt;&lt; num &lt;&lt; endl; return 0 ；&#125; 元素访问at：访问指定字符，有边界检查 str.at(1) front：访问首字符 (C++11) str.front() back：访问最后的字符（C++11）Cpp c_str：返回不可修改的C字符数组版本（带’\0’） str.c_str() 交换string的值成员函数： string::swap(string&amp; str) 主要用于交换两个string的值，用法如下： 1234567891011121314151617181920212223242526// swap strings#include &lt;iostream&gt;#include &lt;string&gt;main ()&#123; std::string buyer ("money"); std::string seller ("goods"); std::cout &lt;&lt; "Before the swap, buyer has " &lt;&lt; buyer; std::cout &lt;&lt; " and seller has " &lt;&lt; seller &lt;&lt; '\n'; seller.swap (buyer); std::cout &lt;&lt; " After the swap, buyer has " &lt;&lt; buyer; std::cout &lt;&lt; " and seller has " &lt;&lt; seller &lt;&lt; '\n'; return 0;&#125;/*output:Before the swap, buyer has money and seller has goods After the swap, buyer has goods and seller has money*/ 非成员函数std::swap()可以将string内部的两个元素进行交互。同时，也可以对两个string进行交换 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;string&gt;int main()&#123; std::string str1 = "abc"; std::swap(str1.at(0), str1.at(1)); std::cout&lt;&lt;str1&lt;&lt;std::endl; std::string str2 = "def"; std::swap(str1, str2); std::cout&lt;&lt;str1&lt;&lt;std::endl; return 0;&#125;/*output:bacdef*/ vector连续存储结果, 每个元素在内存上是连续的, 支持 高效的随机访问和尾端插入/删除操作, 但其他位置的插入/删除操作效率低下, 可以看做是一个数组, 但是与数组的区别为: 内存空间的扩展. vector 支持动态大小的数据存储, 而数组则必须指定大小, 但需要扩展空间时, 需要手动实现. vector的内存分配原理及实现:在STL内部实现时, 首先分配一个较大的内存空间预备存储, 即capacity()函数返回的大小, 当超过此分配的空间时, 会再重新分配一块更大的内存空间(VS6.0是两倍, VS2005是1.5倍). 通常默认的内存分配能完成大部分情况下的存储操作. 扩展空间的步骤: 配置一块新空间 将就元素一一移动到新地址中 把原来的空间释放掉 vector的数据安排以及操作方式, 与数组模板Array十分相似, 两者唯一的差别在于空间利用的灵活性, Array的空间扩展需要手动实现 deque双端队列: double-end queue连续存储结果, 即每个元素在内存上是连续的, 类似于vector, 不同之处在于, deque提供了两级数组结构, 第一级完全类似于vector, 代表实际容器, 另一级维护容器的首位地址, 这样, deque除了具有vector的所有功能之外, 还支持高校的首/尾端的插入/删除操作. 优点: 随机访问方便, 支持[]操作符和.at()访问函数 可在两端进行push, pop操作 缺点:占用内存多 list非连续存储结构, 具有两链表结构, 每个元素维护一对前向和后向指针, 因此支持前向/后向遍历. 支持高效的随机插入/删除操作, 但是随机访问效率低下, 且由于需要额外维护指针, 开销也比较大. 每一个节点都包括一个信息块info, 一个前驱指针Pre, 一个后驱指针Post. 优先: 不使用连续内存完成插入和删除操作 在内部方便的进行插入和删除操作 可以在两端push, pop 缺点: 不能进行随机访问, 即不支持[]操作符和.at()访问函数 相对于vector占用内存多 list与vector的区别 vector为存储的对象分配一块连续的地址空间, 随机访问效率很高. 但是插入和删除需要移动大量的数据, 效率较低. 尤其当vector内部元素较复杂, 需要调用复制构造函数时, 效率更低. list中的对象是离散的, 随机访问需要遍历整个链表, 访问效率比vector低, 但是在list中插入元素, 尤其在首尾插入时, 效率很高. vector 是 单向的 的, 而 list 是双向的 (vector为什么单向???) vector 中的 iterator 在使用后就释放了, 但是 list 不同, 它的迭代器在使用后还可以继续使用, 是链表所特有的. queuedequemap1、map简介Map是STL的一个关联容器，它提供一对一（其中第一个可以称为关键字，每个关键字只能在map中出现一次，第二个可能称为该关键字的值）的数据 处理能力，由于这个特性，它完成有可能在我们处理一对一数据的时候，在编程上提供快速通道。这里说下map内部数据的组织，map内部自建一颗红黑树(一 种非严格意义上的平衡二叉树)，这颗树具有对数据自动排序的功能，所以在map内部所有的数据都是有序的，后边我们会见识到有序的好处。map是一类关联式容器。它的特点是增加和删除节点对迭代器的影响很小，除了那个操作节点，对其他的节点都没有什么影响。对于迭代器来说，可以修改实值，而不能修改key。 2、map的功能 自动建立Key － value的对应。key 和 value可以是任意你需要的类型。 根据key值快速查找记录，查找的复杂度基本是Log(N)，如果有1000个记录，最多查找10次，1,000,000个记录，最多查找20次。 快速插入Key -Value 记录。 快速删除记录 根据Key 修改value记录。 遍历所有记录。-3. 使用map 1#include &lt;map&gt; //注意，STL头文件没有扩展名.h 4. map的构造函数map共提供了6个构造函数，这块涉及到内存分配器这些东西，略过不表，在下面我们将接触到一些map的构造方法，这里要说下的就是，我们通常用如下方法构造一个map：1map&lt;int, string&gt; mapStudent; 5. 数据的插入unordered_map与map的区别在STL中, map对应的数据结构是红黑树, 红黑树是一种近似于平衡的二叉查找树, 里面的数据是有序的, 在红黑树上做查找的时间为 $O(lonN)$. 而unordered_map对应哈希表, 哈希表的特点就是查找效率高, 时间复杂度基本为 $O(1)$, 而额外空间复杂度较高. 基本使用1234567891011#include &lt;iostream&gt;#include &lt;unordered_map&gt;#include &lt;string&gt;int main()&#123; std::unordered_map&lt;int, std::string&gt; hmap; hmap.insert(std::make_pair(1, "Scala")); hmap.insert(&#123;3, "three"&#125;); hmap.insert(&#123; &#123;4, "Four"&#125;, &#123;5, "Five"&#125;&#125;); cout&lt;&lt;hmap[1]&#125; set基于红黑树实现 unordered_set基于哈希表实现, 会将传入的值处理成相应的键值, 该键值对应着一个特定的位置, 因此, unordered_set 的各项操作的复杂度平均为常数级(最差为线性), 同时, unordered_set 也是一种 set, 因此, 其关键字不能有重复(重复的会自动合并). 12345678910111213unordered_set&lt;string&gt; stringSet;stringSet.insert("code");stringSet.insert("fast");string key1 = "fast";stringSet.find(key1); // return iter, 指向 faststring key2 = "slow"stringSet.find(key2); // return stringSet.end()vector&lt;int&gt; nums &#123;1,2,3,4,5,6,7,8,9&#125;;unordered_set&lt;int&gt; sets(nums.begin(), nums.end())int key;sets.count(key); // 返回拥有关键key的元素个数, 即只会返回1或0.]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《CUDA C Programming Guide》]]></title>
    <url>%2Fz_post%2FCUDA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-CUDACProgrammingGuide-md%2F</url>
    <content type="text"><![CDATA[第一章 介绍GPU在进行浮点数运算时超高的精确度和速度，其原因在于GPU很擅长解决计算密集型和高并行计算。 多核CPU和GPU的一个挑战在于怎么编写并行的程序来利用这些多核。 CUDA并行变成模式就是希望用一个较低的学习曲线来解决这个问题。 三个关键的抽象：线程组等级、共享内存、障碍同步 第二章 编程模型2.1 Kernelskernel程序用__global__定义。并且用一种新的执行配置语法&lt;&lt;&lt;...&gt;&gt;&gt;来决定CUDA的线程数量。每一个线程都会在给定的“线程ID”下执行kernel，线程ID可以通过kernel内部的threadIdx变量来获取。 下面的代码显示了向量加法。 总共启动了N个线程，每个都执行一个对应位相加运算。 123456789101112//Kernel definition__global__ void VecAdd(float* A, float* B, float* C)&#123; int i = threadIdx.x; C[i] = A[i] + B[i];&#125;int main()&#123; ... // Kernel invocation with N threads VecAdd&lt;&lt;&lt;1,N&gt;&gt;&gt;(A,B,C);&#125; 2.2 线程结构（Thread Hierarchy）为了方便，threadIdx包含三个组成向量，因此，线程可以使用一维，二维或者三维的thread index，由此，可以表示一维，二维或者三维的线程块（“thread block”）。 线程的索引和它的线程ID是直接关联的。对于一维线程块来说，它们是一样的。对于二维线程块来说，索引为 $(x，y)$ 的线程，其线程ID为：$(x+yD_x)$ 。对于三维线程块来说，索引为 $(x,y,z)$ 的线程，其线程ID为： $(x+ yD_x +zD_xD_y)$ 。 下面的代码显示了矩阵加法。 123456789101112131415//Kernel definition__global__ void MatAdd(float A[N][N], float B[N][N], float C[C][C])&#123; int i = threadIdx.x; int j =threadIndx.y; C[i][j] = A[i][j] + B[i][j];&#125;int main()&#123; ... //Kernel invocation with one block of N*N*1 threads int numBlocks = 1; dim3 threadsPerBlock(N,N); //&lt;&lt;&lt;&gt;&gt;&gt;语法可以接受int类型或者dim3类型的数据 MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A,B,C);&#125; thread block的索引可以通过blockIdx变量获得，thread block的维度的size可以通过blockDim变量获得。 扩展上面的MatAdd()代码，使其可以处理多blocks，代码如下： 123456789101112131415161718//Kernel definition__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])&#123; int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadInx.y; if(i&lt;N &amp;&amp; j&lt;N)&#123; C[i][j] = A[i][j] + B[i][j]; &#125;&#125;int main()&#123; ... //Kernel invocation dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y); MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C); ...&#125; 线程块必须独立执行，因此它们必须能够在任意的顺序下并行执行。 CUDA使用__syncthreads()来协调共享内存和各个线程之间的执行。 2.3 内存结构（Memory Hierarchy）由两种额外的只读内存空间可以被所有线程访问到：常量内存空间（constant memory spaces）和纹理内存空间（texture memory spaces）。 2.4 异构编程CUDA编程模型假设所有线程都是在物理上单个的设备上运行的，每个设备都当做是主机的从处理程序。比如，kernel在GPU上执行，而剩下的C程序在CPU上执行。 CUDA编程模型还假设主机和设备都DRAM中各自维护自己的内存，对应这“主机内存（host memory）”和“设备内存（divice memory）”。因此，程序通过调用CUDA运行时来管理主机和设备的内存。 2.5 计算能力（Compute Capability）计算能力用设备的版本号标识，有时也称为“SM version”。有major revision number“X”和minor revision number“Y”组成，记为X.Y。 第三章 编程接口（Programming Interface）CUDA提供了诸多扩展语法来并行变成（如kernel），任何包含这些扩展语法的源文件都需要需要nvcc编译器来编译。 3.1 Compilation with NVCCKernels can be written using the CUDA instruction set architecture, called PTX, whichis described in the PTX reference manual. 3.1.1 编译工作流3.1.1.1 离线编译 Offline Compilationnvcc编译器可以编译包含host code和device code的混合代码。nvcc首先会将device code从host code中分离出来，然后，会进行以下两步： 编译device code到assembly form（PTX code）或者binary form（cubin objec） 修改host code，将&lt;&lt;&lt;...&gt;&gt;&gt;语法替换成必要的CUDA C运行时函数。 被修改的host code要么输出成C code，要么把直接输出成object code。 3.1.1.2 即时编译 Just-in-Time Compilation二进制兼容性 Binary CompatibilityBinary code is architecture-specific。通过-code=sm_35的形式来指定特定的architecture。注意，编译好的二进制代码是向上兼容的，也就是如果二进制代码设定的计算能力版本号为 $X.y$ ，那么该代码就只能运行在 $X.z$上，其中 $z\ge y$ 。 3.1.3 PTX 兼容性一些PTX指令仅仅支持在高计算机能力版本的GPU中使用。 指针某些计算能力版本的PTX code总是可以转换成binary code，进而都更高计算能力的版本中使用。 其他有些版本，则不能直接使用。 3.1.4 应用兼容性为了满足bianry兼容性和PTX兼容性，推荐使用即时编译。 通过-gencode、-arch和-code编译选项来指定计算能力版本号及其兼容性。 3.1.5 C/C++ 兼容性CUDA源文件的前端遵循C++语法规则。host code可以完美支持C++语言发。但是，device code支持C++语法中的一个子集，详细可以参见C/C++ Language Support。 3.1.6 64-Bit Compatibility64位的nvcc会将device code编译成64位模式（即，指针占64位）。此时host code必须在32位模式下执行。 32位device、host同理 -m64 选项可以切换nvcc的位数。 3.2 CUDA C Runtime运行时使用cudart库实现。 或者使用cudart.lib、libcudart.a进行静态链接，或者使用cudart.dll、cudart.so进行动态链接。 正如前面异构编程提到的。CUDA编程模型将系统看走是由device和host组成的一个整体，二者管理各自的内存。 3.2.1 初始化没有专门的初始化函数在初始化阶段，runtime为系统中的每一个device创建上下文。 3.2.2 设备内存 Device MemoryKernels操控device memory之外的代码。所以runtime会提供allocate，deallocate和copy等函数来复制device memory，同时在host和device内存之间传输数据。 Divice memory可以申请linear memory或者CUDA arrays。 CUDA arrays是不透明的内存形式，专门用于优化texture fetching。 Linear memory存在于40位的地址空间中，可以使用cudaMalloc()函数申请，用cudaFree()函数释放，同时，可以用cudaMemcpy()函数来将数据在host和device之间交换。 cudaMallocPitch()和cudaMalloc3D()推荐用来申请2D和3D数组。cudaMemcpy2D。 cudaGetSymbolAddress()用来检索指向内存的地址。 cudaGetSymbolSize()可以得到申请内存的大小。 3.2.3 共享内存 Shared MemoryShared Memory在Thread Hierarchy上要比global memory更快。 矩阵乘法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Matrices are stored in row-major order:// M(row, col) = *(M.elements + row * M.width + col)typedef struct &#123;int width;int height;float* elements;&#125; Matrix;// Thread block size#define BLOCK_SIZE 16// Forward declaration of the matrix multiplication kernel__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);// Matrix multiplication - Host code// Matrix dimensions are assumed to be multiples of BLOCK_SIZEvoid MatMul(const Matrix A, const Matrix B, Matrix C)&#123;// Load A and B to device memoryMatrix d_A;d_A.width = A.width; d_A.height = A.height;size_t size = A.width * A.height * sizeof(float);cudaMalloc(&amp;d_A.elements, size);cudaMemcpy(d_A.elements, A.elements, size,cudaMemcpyHostToDevice);Matrix d_B;d_B.width = B.width; d_B.height = B.height;size = B.width * B.height * sizeof(float);cudaMalloc(&amp;d_B.elements, size);cudaMemcpy(d_B.elements, B.elements, size,cudaMemcpyHostToDevice);// Allocate C in device memoryMatrix d_C;d_C.width = C.width; d_C.height = C.height;size = C.width * C.height * sizeof(float);cudaMalloc(&amp;d_C.elements, size);// Invoke kerneldim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);// Read C from device memorycudaMemcpy(C.elements, Cd.elements, size,cudaMemcpyDeviceToHost);&#125;// Free device memorycudaFree(d_A.elements);cudaFree(d_B.elements);cudaFree(d_C.elements);// Matrix multiplication kernel called by MatMul()__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)&#123;// Each thread computes one element of C// by accumulating results into Cvaluefloat Cvalue = 0;int row = blockIdx.y * blockDim.y + threadIdx.y;int col = blockIdx.x * blockDim.x + threadIdx.x;for (int e = 0; e &lt; A.width; ++e)Cvalue += A.elements[row * A.width + e]* B.elements[e * B.width + col];C.elements[row * C.width + col] = Cvalue;&#125; 3.2.4 Page-Locked Host Memoryplhm有诸多好处，如： 可以使copy更快 可以直接映射到地址空间，消除copy到device memory的需求 在fron-side bus上， plhm的bandwidth更高。 但是，plhm是稀缺资源，因此，在申请时一定要注意不能过度使用。 3.2.4.1 移动内存 Portable Memory一块page-locked内存可以备用在与任意一个设备的连接上。但是默认情况下，使用page-locked内存的好处仅仅会在当前block所申请的设备（或者那些共享了内存的设备）上显现。 为了使这些好处对于所有设备来说都是可行的，block需要将flag cudaHostAllocPortable传递到cudaHostAlloc中去，或者将flag cudaHostRegisterPortable传递到cudaHostRegister中去。 3.2.4.2 Write-Combining Memory该内存通过释放L1和L2的缓存资源，使得更多缓存资源可以给后面的应用使用。 从host中读取write-combining 内存比较慢，所以常常只用于写。 3.2.4.3 Mapped Memory使用cduaHostAllocMapped或者cudaHostRegisterMapped可以使一块page-locked host内存映射到设备的地址空间中。 因此，这样的内存块往往有两个地址，分别是host地址和device地址。 直接kernel中访问host内存有以下几点优势： 无需在device中申请内存块，也无需在device和host中来还传输数据 在执行kernel时，无需使用“流”来overlap数据的传输 3.2.5 异步并发执行 Asynchronous Concurrent ExecutionCUDA将下列操作当作一个独立的任务，它们可以互相并发执行： 在host上计算 在device上计算 将host上的内存传输到device上 将device上的内存传输到host上 3.2.5.1 在Host和Device之间并发执行通过异步调用，许多device操作都可以排成队列，进而利用CUDA driver来执行。这减轻了host线程管理device的负担，使得它可以执行其他任务。下列device操作相对于host来说是异步的。 启动Kernel 将内存拷贝到一个单一的deivce内存中 将内存从host拷贝到device中小于64将内存从host拷贝到device中小于64KB的内存块中 用带有Async后缀的函数来进行内存拷贝 内存设定函数的调用（Memory set function calls） 3.2.5.2 Concurrent Kernel Execution计算能力告诉2.x的版本可以并发启动多个kernels。 可以通过concurrentKernel设备属性来查看是否支持并发启动。 3.2.5.3 Overlap of Data Tranfer and Kernel Execution有一些设备可以从GPU利用kernel执行来进行异步的内存拷贝，应用需要想设备询问是否具有这种功能，可以通过检查asyncEngineCoune设备属性来查看，如果大于0就是支持的。 3.2.5.4 并发数据传输 Concurrent Data Transfers一些计算能力在2.x以上的设备可以在进行数据传输是进行overlap，应用可以通过asyncEngineCount来检查设备是否支持该功能 3.2.5.5 流 Streams应用通过“流”来管理并发的操作。一个“流”代表这一串由“命令”组成的序列 3.2.5.5.1 创建和销毁：Creation and Destruction下面的代码创建了两个流，同时在page-locked内存中申请了flaot类型的hosrtPtr 123456cudaStream_t stream[2];for (int i = 0; i &lt; 2; ++i)cudaStreamCreate(&amp;stream[i]);float* hostPtr;cudaMallocHost(&amp;hostPtr, 2 * size); 这两条流都经过下面的代码定义，执行一个从host到device的内存拷贝操作，一个kernel启动操作，以及一个从device到host的的内存拷贝操作。 123456789for (int i = 0; i &lt; 2; ++i) &#123;cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,size, cudaMemcpyHostToDevice, stream[i]);MyKernel &lt;&lt;&lt;100, 512, 0, stream[i]&gt;&gt;&gt;(outputDevPtr + i * size, inputDevPtr + i * size, size);cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,size, cudaMemcpyDeviceToHost, stream[i]);&#125; 通过cudaStreamDestroy()可以释放流： 1234for (int i =0;i&lt;2;i++)&#123; cudaStreamDestroy(stream[i]);&#125; 3.2.5.5.2 默认流 Default Stream对于使用--default-stream per-thread编译选项的代码来说，默认流是一条常规的流，并且每个主线程都有它自己的默认流 对于使用--default-stream legacy编译选项的代码来说，默认流是一条特殊的流，被称为NULL stream ，并且每一个设备都具有一条单一的NULL stream供所有的host线程使用。它的特殊性源自于它会隐式的进行同步操作。 3.2.5.5.3 显式同步以下几种方式可以对流进行显式同步操作： cudaDeviceSynchronize() cudaStreamSynchronize() cudaStreamWaitEvent() cudaStreamQuery() 为了避免不必要的速率降低，以上所有的同步函数通常用于timing purpoese或者用于孤立一个拷贝或启动的失败。 3.2.5.5.4 隐式同步从两个不同流传过来的指令中的其中一条出现了问题，那么就无法并发运行。 同步操作越晚执行越好。 3.2.5.5.5 Overlapping 行为对于两个流里面的操作，如一个是从host拷贝内存到device，另一个是从device拷贝内存到host，则这两条指令直接存在Overlap。 3.2.5.5.6 回调函数Callbacks运行时提供了可以在流中的任何位置插入回调函数的指令：cudaStreamAddCallback()。 3.2.5.5.7 流优先级 Stream Priorities相对优先级使用cudaStreamCreateWithPriority() 获取优先级范围[highest priority,lowest priority]使用cudaDeviceGetStramPriorityRange。 下面的代码包含了获取当前设备的优先级范围： 1234567// get the range of stream priorities for this deviceint priority_high, priority_low;cudaDeviceGetStreamPriorityRange(&amp;priority_low, &amp;priority_high);// create streams with highest and lowest available prioritiescudaStream_t st_high, st_low;cudaStreamCreateWithPriority(&amp;st_high, cudaStreamNonBlocking, priority_high);cudaStreamCreateWithPriority(&amp;st_low, cudaStreamNonBlocking, priority_low); 3.2.5.6 事件 Eventsruntime 同时还提供了密切监视device进程的方法，主要是通过appliocation来异步记录事件完成时的事件点。 3.2.5.6.1 创建和销毁下面的代码创建了2个事件：123cudaEvent_t event1, event2;cudaEventCreate(&amp;event1);cudaEventCreate(&amp;event2); 下面的代码销毁了2个事件：12cudaEventDestroy(event1);cudaEventDestroy(event2); 3.2.5.6.2 运行时间下面的代码可以用来监视事件的运行时间（从start到end）1234567891011121314cudaEventRecord(start, 0);for (int i = 0; i &lt; 2; ++i) &#123;cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,size, cudaMemcpyHostToDevice, stream[i]);MyKernel&lt;&lt;&lt;100, 512, 0, stream[i]&gt;&gt;&gt;(outputDev + i * size, inputDev + i * size, size);cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,size, cudaMemcpyDeviceToHost, stream[i]);&#125;cudaEventRecord(stop, 0);cudaEventSynchronize(stop);float elapsedTime;cudaEventElapsedTime(&amp;elapsedTime, start, stop); 3.2.5.7 同步调用可以使用cudaSetDeviceFlags()配合一些特定的flags？？ 3.2.6 Multi-Device System3.2.6.1 Device Enumeration一个host系统可以有多个devices，下面的代码展示了如何枚举这些设备，并查询它们的属性，决定可启用CUDA的设备数量。 123456789int deviceCount;cudaGetDeviceCount(&amp; deviceCount);int device;for( device = 0; device&lt;deviceCount; device++)&#123; cudaDeviceProp deviceProp; cudaGetDeviceProperties(&amp;deviceProp, device); printf("Device %d has compute capability %d.%d. \n"), device, deviceProp.major, deviceProp.minor);&#125; 3.2.6.2 Device Selectionhost线程可以在任何时候使用cudaSetDevice()来设置device。device的内存申请和kernel启动都会在当前设置的device上进行，如果没有调用该函数，则当前的设备默认为0. 下面的代码展示了如何设置当前的device，以及申请内存和执行kernel 123456789size_t size = 1024 * sizeof(float);cudaSetDevice(0);float* p0;cudaMalloc(&amp;p0, size);// Allocate memory on device 0MyKernel&lt;&lt;&lt;1000, 128&gt;&gt;&gt;(p0);// Launch kernel on device 0cudaSetDevice(1);// Set device 1 as currentfloat* p1;cudaMalloc(&amp;p1,size);// Allocate memory on device 1MyKernel&lt;&lt;&lt;1000,128&gt;&gt;&gt;(p1);// Launch kernel on device 1 3.2.6.3 Stream and Event Behavior如果stream没有绑定到当前的device，那么kernel launch就会失败 即使stream没有绑定到当前的device，memory copy也会成功 如果input event和input stream 绑定到了不同的diveces上面，那么cudaEventRecord() will fail 如果两个input events绑定到了不同的devices上面，那么cudaEventElapsedTime() will fail 即使input event绑定到了不同于当前device的device上面，cudaEventSynchronize()和cudaEventQuery()仍然will succeed]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++中关于*、&、*&以及&*的解析.md]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E5%85%B3%E4%BA%8E%E6%8C%87%E9%92%88%EF%BC%8C%E6%8C%87%E9%92%88%E5%BC%95%E7%94%A8%E7%9A%84%E8%A7%A3%E6%9E%90-md%2F</url>
    <content type="text"><![CDATA[由一道牛客网《剑指offer》的编程题引发的思考，题目如下： 二叉搜索树与双向链表：输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的节点，只能调整树中结点指针的指向。 按照递归的解题思路，有如下解答： 12345678910111213141516171819202122232425262728293031323334/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: TreeNode* Convert(TreeNode* pRootOfTree) &#123; if(pRootOfTree==nullptr) return nullptr; TreeNode* prenode = nullptr; recurve(pRootOfTree,prenode); while(pRootOfTree-&gt;left!=nullptr) pRootOfTree = pRootOfTree-&gt;left; return pRootOfTree; &#125; void recurve(TreeNode* root, TreeNode*&amp; prenode)&#123; if(root-&gt;left!=nullptr) recurve(root-&gt;left,prenode); root-&gt;left = prenode; if(prenode!=nullptr) prenode-&gt;right = root; prenode = root; if(root-&gt;right!=nullptr) recurve(root-&gt;right,prenode); &#125;&#125;; 代码中23行使用了TreeNode*&amp; prenode，这里，如果缺少了&amp;，则结果会出错！ 以下，对C++中*、&amp;、*&amp;以及&amp;* 四种形式展开讨论。 * 代表指针&amp; 代表引用、别名指针和引用的区别之一： 参数传递时，不管是传值还是传指针，函数都会产生一个临时副本变量，但在传引用时，不会生成临时变量。 *&amp; 首先是一个指针，然后前面的&amp;代表是这个指针的引用。 指针的引用其实就是指针的一个别名，和指针具有相同的地址。&amp;* 首先是一个变量的引用，然后是指向这个引用的指针，但是，因为引用不是对象，没有实际的地址，因此 不能定义指向引用的指针 。问题：向函数中传递指针和传递指针的引用的区别 如果传递的是指针，那么会先复制该指针，在函数内部使用的是复制后的指针，这个指针与原来的指针虽然指向相同的地址，但是如果在函数内部将复制后的指针指向了另外的地址，那么不会影响原来的指针。 但是对于传递指针的引用，如果将传递进来的指针指向了新的地址，那么原始的指针也会指向新的地址，这也是为什么在该题中，必须使用指针的引用，而不能使用指针的原因。就是因为在这段代码中，要对指针指向的值进行更改，而在递归的函数中，又需要保证prenode指向的值保持统一，因此，必须使用指针的引用来使在不同层的递归函数中，prenode指向的值都是一样的。 在传递指针的引用时，还有另外一个问题，那就是如果由于原始的指针不再指向原始对象了，所以如果没有其他指针指向该原始对象的话，就会造成内存泄漏。同理，如果在函数内释放了指针的引用，那么在函数外部就不能在使用原来的指针了，因为原来的内存已经被释放了。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中NULL和nullptr之间的区别.md]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%ADNULL%E3%80%81null%E5%92%8Cnullptr%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB-md%2F</url>
    <content type="text"><![CDATA[首先，C++中没有null，只有NULL和nullptr。 NULL引渡自C语言，一般由宏定义实现，而nullptr则是C++11的新增关键字。在C语言中，NULL被定义为(void*)0,而在C++语言中，NULL则被定义为整数0，编译器一般对其实际定义如下：12345#ifdef __cplusplus#define NULL 0#else#define NULL ((void *)0)#endif 出现C++和C定义不一致的原因是，在C++中不允许(void*)类型进行隐式转换，例如：12345char* a =&quot; Hello&quot;;void foo(void* p)&#123;&#125;foo(a); 以上这种调用方式在C++中是不允许的，在C++中指针必须有明确的类型定义。如上需使用foo((char*)a)才可以;但是将NULL定义为0带来的另一个问题是无法与整数的零区分。因为C++中允许有函数重载，所以可以试想如下函数定义情况： 123void func(int data);void func(char* data); 那么在传入NULL参数时，编译器将无法确定到底使用哪个函数定义，造成编译时错误。nullptr在C++11被引入用于解决这一问题，nullptr可以明确区分整型和指针类型，能够根据环境自动转换成相应的指针类型，但不会被转换为任何整型，所以不会造成参数传递错误。nullptr的一种实现方式如下： 1234567const class nullptr_t&#123;public: template&lt;class T&gt; inline operator T*() const&#123; return 0; &#125; template&lt;class C, class T&gt; inline operator T C::*() const &#123; return 0; &#125;private: void operator&amp;() const;&#125; nullptr = &#123;&#125;; 以上通过模板类和运算符重载的方式来对不同类型的指针进行实例化从而解决了(void*)指针带来参数类型不明的问题，另外由于nullptr是明确的指针类型，所以不会与整形变量相混淆。但nullptr仍然存在一定问题，例如： 123void fun(char* p);void fun(int* p); 在这种情况下存在对不同指针类型的函数重载，此时如果传入nullptr指针则仍然存在无法区分应实际调用哪个函数，这种情况下必须显示的指明参数类型。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Video-based Sign Language Recognition without Temporal Segmentation]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Video_based_Sign_Language_Recognition%2F</url>
    <content type="text"><![CDATA[摘要世界上具有上百万的聋哑患者使用手语进行交流，因此，实现从手语到自然语言的翻译是一件非常有意义的事情。目前，手语识别（SLR）问题主要由两量子问题组成：逐个单词进行识别的独立SLR、对整个句子进行翻译的连续SLR。 目前现有的连续SLR方法基本上都是用独立SLR作为基本模块, 同时结合额外的预处理和后处理操作来实现, 这里的预处理一般是指时域分割,后处理指句子合成. 但是, 时域分割问题本身的误差不可避免就会不断的传递给后续的步骤, 更糟糕的是, 这种独立SLR模块需要经过十分费劲的标注过程来使得一段视频中每一个动作都有与之对应的单词标签. 光这一点就对数据集的获取造成了不小的阻碍. 为了解决这些困难和挑战, 作者就提出了一种新的连续手语识别模型框架, 将其命名为Hierarchical Attention Network with Latent Space(LS-HAN)(基于隐式空间的多级注意力网络), 这个网络框架可以免去时域分割的预处理过程. LS-HAN有三部分组成: 一个双流卷积神经网络: 用于生成视频的特征表示. 隐式空间: 用于建立视频与自然语言之间的语义联系(semantic gap) 多级的注意力网络: 基于隐式空间的识别网络 介绍——Introduction在SLR问题中的一个关键挑战在于表示肢体运动、手势和面部表情的关系描述器的设计。目前主要有两类：手工设计特征 和 基于CNN和特征提取方法。 本文欲计划设计一个双流的3D-CNN模型，用于对视频特征进行提取。 时域分割问题在连续SLR中是一个十分困难的问题。 普遍的解决思路是将连续的SLR解析成独立的单词进行识别，这样就需要解决时域分割问题。 由于需要翻译的动作十分多样，因此很难进行检测，同时，时域分割作为一个预处理步骤，会将分割误差传递给后续步骤。并且，对每个动作打标签也是一项很耗时的任务。 受到基于LSTM的视频描述工作的启发，我们利用一个等级式的注意力网络（Hierarchical Attention Network HAN）规避了时域分割问题。HAN是在考虑结构信息和注意力机制之后，对LSTM的一个扩展。大体思路是将整个视频流送入的HAN中，然后一个单词一个单词的将句子输出。 但是，HAN仅仅是通过前一个单词，来预测后一个单词的最大可能性，忽略了video和句子之间的关系。最终结果是，它可能会出现鲁棒性问题。为了改善这个问题，本文引入隐式共建模型来挖掘视频和句子之间的关系。 一句话总结，本文主要的贡献点有以下三个： 一个新的双流3D CNN，用于生成全局和局部的视频特征表征 一个新的LS-HAN框架，可以规避连续SLR中的时域分割问题 对提出的LS-HAN框架中的相关性损失函数和分辨损失函数进行联合优化 编辑了目前最大的针对连续SLR问题的现代汉语手语识别数据集 相关工作连续SLR大多数现有的SLR研究都是针对独立SLR来做, 有点类似于动作识别。更具挑战性的问题是连续SLR的研究。大多数现有的连续SLR方法都将句子级别的识别分为以下三个阶段：视频时域分割，独立单词识别，基于语言模型的句子融合。例如，DTW-HMM提出了一个基于粗粒度时域分割的阈值矩阵。2017年提出了一个新的基于HMM的语言模型。最近，transitional movements吸引了很多的关注，因为它们可以当作时域分割的基础。 尽管采用时域分割很普遍，但是时域分割问题本质上是很难解决的：因为手语之间的过度动作transitional movements在识别时，依然显得很脆弱和混乱。更糟糕的是, 将连续的SLR任务转化为独立的SLR任务, 需要对大量的数据进行标注, 这是很耗时耗力的. 视频描述生成图像描述生成是一个相关的研究领域，它通过描述视频序列中的场景/物体/动作来生成一段简单的话。一个流行的方法是序列到序列，视频到文本的方法，它将两个LSTM置于CNN之上。 还有的文章使用注意力机制来自动选择可能性最大的帧. 还有其他更多关于LSTM的扩展。 抛开视频描述生成和手语翻译在目标和技术手段上的一些相似之处，二者依然是两个完全不同的任务。前者仅仅是对输入视频的一个简单的总结, 但是后者必须要在语义层面上建立视频与自然语言之间的联系. ( 如对于同一段视频, 前者输出一个人在比划手势是正确的输出, 而后者必须输出这个人比划的收拾的具体含义) 基于潜在空间的学习潜在空间模型是一个用来在不同模态的语义鸿沟之间建立联系的流行的工具。 例如, 有文章使用隐式空间和LSTM结合进行联合学习, 将其用于生成视频描述. 手语视频特征表示手语视频主要由身体的上半部分决定，尤其是手势动作。识别手势动作的主要挑战是手的形态和方向具有很强的不确定性, 由此会组合出大量不同的手势动作。 受到最近深度学习技术在目标检测任务上的进展，我们提出了一个双流的3D CNN模型，用于生成视频特征的表征。这个3DCNN模型会同时接受整个视频帧和剪裁后的手势图像，并且独立的送到两个流中去，最终会通过一个融合机制，将它们融合在一起。因此，这个模型可以提取出整体信息和局部信息的特征编码。 手势检测和跟踪我们首先用fasterrcnn预训练了VOC2007数据，然后，从CSL中选取了400帧进行finetune。之后，所有的视频都逐帧处理。 当手的性状变化很大, 或者被衣服遮挡时, faster rcnn的检测可能会失败, 因此,使用了compressive tracking 双流3D CNN本文基于 C3D（Tran et al 2015） 设计了一个3D CNN双流模型, 该模型将一段手语动作等分成16帧, 并从中提取相应的时序特征. 模型的输入是一串视频截图（16帧）。模型中的上行流被设计用来提取全局手部位置和移动(global hand locations/motions), 缩放到227×227. 下行流关注局部手势信息, 输入是剪裁后的图片(227×227). 这里的这个局部信息就是跟踪两只手, 然后在两只手上画bounding box, 最后将两只手的特征信息按照多通道的方式添加在一起, 送入网络当中. 不管是上行流还是下行流, 他们的网络结构都是一样的, 包含8个卷积层和5个池化层. 最后是两个全连接层, 上行流和下行流的特征图谱会在这里结合. 双流CNN模型首先会对独立的SLR数据集进行预训练。训练完以后会把网络的softmax和最后一层fully connected layer都拿掉. (也就是说我们要的只是前面的卷积网络特征提取器, 深度学习, 说白了, 就是特征学习, 留下最后一层全连接层, 主要是为了将多维度的卷积特征图谱转换成一个一维向量, 方便后面的流程). 这个全连接层上面有4096个神经元, 因此, 最后输出的就是一个4096长度的一维向量. 上面的整个过程就可以看做是用一个滑动窗口在时间维度上对视频流不断的截图, 然后将截图源源不断的送入到神经网络当中, 目的是为了获取每一种视频截图的特征表征, 由于采用了这种双流结构, 因此这种特征表征结合了全局和局部信息(全局就是手移动的轨迹和位置, 局部就是手具体做了什么动作), 某种程度上可以认为是将当前的视频截图编码成了一个4096维的特征向量, 这个特征向量可以高度概括这一帧当中的信息, 最终, 一整段视频就会被编码一连串的4096为的向量, 最终就成了一个n×4096为的特征矩阵, n就是总共截取的帧数. LS-HAN ModelHAN(Hierarchical Attention Network) 是LSTM的一种扩展, 主要针对输入数据使用了注意力机制. 模型的损失函数同时考虑了视频与句子之间的相关性误差 $E_r$, 以及HAN的识别误差 $E_c$. \min_{\theta_r, \theta_c} \frac{1}{N}上式中, N 代表了训练集中的实例数量, $V^{(i)},S^{(i)}$ 代表第 $i$ 个视频实例(注意这里不是截图)和句子, $\theta_r, \theta_c$ 分别代表隐式空间和HAN的参数. R是正则项. $\lambda_1, \lambda_2$ 用于调节这三项的权重占比. 下面对这个损失函数的每一项具体介绍 视频和句子之间的隐式空间如图2所示，我们模型框架的输入是视频和对应标注好的句子。视频用提取到的global-local特征表示，句子中的每一个单词都用one hot向量表示。我们令视频为 $V=(v_1,v_2,…,v_n)$, 令句子为 $S=(s_1,s_2,…,s_m)$, 这里每一个 $v_i$ 都代表的是视频中的一个截图的特征向量, n表示总共的截图数量, $s_i$ 表示句子中的每一个单词的one-hot向量,m为单词数目. 隐式空间的目标就是构建一个”空间”, 在这个空间里面, 可以建立起视频截图特征向量和句子单词onehot向量之间的联系, 也就是说, 我们要将 $V$ 和 $S$ 映射到同一个隐式空间中去, 于是, 映射后的视频截图和句子可以表示成: $f_v(V) = (v_1’, v_2’, …,v_n’)$ 和 $f_s(S) = (s_1’, s_2’,…,s_m’)$ 这里, $f_v$ 和 $f_s$ 分别对应这一个映射函数, 可以用矩阵表示: f_v(x) = T_vx, f_s(x) = T_s x其中, 权重矩阵 $T_v \in R^{D_s\times D_c}$, $T_s \in R^{D_s\times D_w}$, $D_s$ 就是隐式空间的维度. 接下来, 我们就需要衡量 $f_v(V)$ 和 $f_s(S)$ 之间的相关程度. 用DTW(Dynamic Time Warping)算法来找到最小的累加和距离,同时还会考虑时域的路径. (因为视频片段和word基本是一一对应的) D[i,j] = min(D[i-1, j], D[i-1, j-1]) + d(i,j)d(i,j) = \|T_vv_i - T_s s_j\|_ 2上式中, $D[i,j]$ 代表了 $(v_1’,…v_i’)$ 和 $(s_1’,…s_j’)$ 的距离, 而$d(i,j)$ 则代表了 $v_i’$ 和 $s_j’$ 之间的距离(二范式). 于是, 我们将视频实例和一个句子之间的损失定义为: E_r(V,S;\theta_r) = D(n,m)上面的DTW算法有一个假设前提, 那就是如果句子中的单词出现在前面, 那么与这个单词对应的视频片段也应该出现在前面. 正常来说这样的假设不是特别合理, 比如说对于视频描述这个任务来说, 这个假设就很不合适, 但是手语识别他有自身的特殊性, 在进行短句子的手语动作时, 这种假设是可行的, 而这篇文章使用的数据集中的句子都不超过10个词, 所以这里我们就暂时认为这个假设是对的吧. 上面还有个warping path的概念, 实际上就调整视频和句子之间的对齐程度. 上面公式的具体实现不是贪心, 而是基于回溯的, 所以最后找到的就是最小的. 隐式空间的目标是建立连接语义鸿沟的空间。 将视频截图和句子映射到同一个隐式空间中。 Dynamic Time Warping（DTW）算法：衡量 $f_v 和 f_s$之间的相关性。 Recognition with HAN受到最近sequence to sequence模型的启发，识别问题可以看作是在给定video的情况下，求句子的log条件分布率的估计。（LSTM）。 首先，输入帧序列通过隐式空间进行编码，然后，根据每一个帧序列的特征向量来预测对应的单词。 扩展HAN中的编码器使其能够反应多级结构(从clips到word, 或者从word到clips)，同时引入注意力机制。 如图4所示, 蓝色的输入分别代表隐式空间中的clips序列特征和words序列特征. 可以看出, 该模型包含两个编码器和一个解码器。每一个编码器都是一个带有注意力机制的 bidirectional LSTM，而解码器一个单独的LSTM。 clip编码器将视频截图编码，使其对齐到单词向量上。如图5所示, 本文在经验上选取了3种对齐机制：（实验证明3比较好，因为本文选择3） 将clips分成两个子序列 (总共就两个word) 每两个clips分出一个子序列 (总共有L/2个word) 均分出7个子序列（因为在训练集中的句子平均含有7个单词）。 经过编码以后, 会生成对应的特征表示向量, 然后就需要进行解码, 在解码的时候我们是知道标注的真实句子信息的, 首先 #START 表示开始符号, 根据隐式向量的特征表示, LSTM会先预测第一个单词 $y_1$, 然后, 继续用LSTM在当前处的输出 $h_t$ 和第一个真实单词’the’的onehot向量来预测第二个单词, 就是这样一直下去, 直到遇到#END为止. 用Softmax函数在输入为 $h_t$ 下条件下输出 $y_t$ 的概率 (概率最大的那个就是当前cell的输出): p(y_t | h_t)这一部分的损失函数为(这里这个是log损失函数, 如果预测结果是正确项的条件概率越接近于1, 则log对应项就越小): E_c(V,S;\theta_r, \theta_c)Learning and Recognition of LS-HAN Model最终, 我们的损失函数可以写成下面这样: \min_{T_v,T_s,\theta}在进行训练优化时, 分别对隐式空间的参数 $T_v, T_s$ 和HAN的参数 $\theta$ 求偏导 实验数据总共有两个开源数据，一个是CSL，另一个是德国手语数据集RWTH-PHONEIX-Weather。 CSL包含25k个标注的视频实例，总共超过100小时，共50位手语者。17K用于训练，2K用于验证，6K用于测试。每一个视频实例都会与一个完整的句子相关联. RWTH-PHONEIX-Weather包含7K天气预测的句子，共9位手语者。 所有的视频均为25帧每秒，分辨率为210×260 。 5672用于训练, 540用于验证, 629用于测试. 实验设置每一段视频都会被分成16帧, 要么缩放, 要么裁剪, 输入大小为 227×227. 输出是4096长度的一维向量, 正如前面介绍的那样. Evaluation Metircs 评价标准Accuracy = 1 - \frac{S + I + D}{N} \times 100%上式中, SID分别代表将预测句子转换成真实句子所需要的最少的替换(substitution), 插入(insertion)和删除(deletion)操作, N代表句子的真实长度. 注意, 由于这三种操作都会降低准确率, 因此准确率是有可能为负值的 (顺便我还感觉这个评价标准也太粗糙了, 感觉不是精心设计过的标准, 但是貌似还有人用, 而且一般好的实验都是会用很多不同标准横向纵向去比较的, 所以我感觉作者只用这一个评价标准, 要么就是手语识别没有特别好, 特征规范的通用标准, 要么就是作者在别的标准上表现不好, 这个表现我也没太查到, 所以也只是猜测.). 实验结果和分析先来看表2, 因为本文的方法主要是基于LSTM来做的, 因此, 第一部分是各种LSTM的变种方法, 这些变种方法基本都不是用来解决手语识别问题的, 所以精度差是自然的(我也不知道作者为啥要这么比, 感觉有点不公平). 再看第二部分, 第二部分确确实实都是针对手语识别问题, 而且都是针对连续手语识别问题, 但是这些方法居然比第一部分的还低, 这我就有点搞不懂了, 然后我看到这个年份, 我就感觉这个数据怎么这么奇怪啊, 07年到14年这都7年的时间就提升了0.01, 我就感觉匪夷所思. 我感觉这篇文章整个的模型结构和设计思路还是挺不错的, 但是这个实验部分就真的支撑数据让人有点不信服. 表3: 这个表3看起来勉强还行, 是和最近发表的paper方法比较的, 这个精度提升确实不高. HAN和LS的关系]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第十四章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter14%2F</url>
    <content type="text"><![CDATA[第十四章 C++中的代码重用除了公有继承之外，还有其他促进代码重用的方法： 包含/组合/层次话：在类中使用另一个类的对象做成员 私有或保护继承：用于实现has-a关系，即新的类将包含另一个类的对象 14.1 包含对象成员的类string类 valarray类：这是一个模板类 14.2 私有继承另一种实现has-a关系的途径——私有继承。 使用私有继承，基类的公有成员和保护成员都将成为派生类的私有成员。这意味着基类方法将不会成为派生对象公有接口的一部分，但可以在派生类的成员函数中使用它们。 “包含”是将对象作为一个命名的成员添加到类中，而“私有继承”将对象作为一个未被命名的继承对象添加到类中。 初始化基类组件，对于继承类，需要使用成员初始化列表语法，使用类名而不是成员名称来标识构造函数。 访问基类的方法：可以通过将私有的成员函数包含在一个公有函数中来访问该私有方法。 访问基类对象：通过this指针和强制类型转换来创建对应的对象或引用 访问基类的友元函数：用类名显式的限定函数名不适合友元函数，这是因为友元不属于类。然而，可以通过显式的转换为基类来调用正确的函数。 14.2.2 使用包含还是私有继承。通常，应使用包含来建立has-a关系。如果新类需要访问原有类的保护成员，或需要重新定义虚函数，则应使用私有继承。 14.2.3 保护继承保护继承是私有继承的变体。使用保护继承时，基类的公有成员和保护成员都将成为派生类的保护成员。 当从派生类派生出另一个类时，私有继承和保护继承之间的区别在于： 使用私有继承时，第三代类将不能使用基类的接口，这是因为继承的公有方法在派生类中将变成私有方法。使用保护继承时，基类的公有方法在第二代中将变成受保护的，因此第三代派生类可以使用它们。 各种继承方式: 特征 公有继承 保护继承 私有继承 公有成员变成 派生类的公有成员 派生类的保护成员 派生类的私有成员 保护成员变成 派生类的保护成员 派生类的保护成员 派生类的私有成员 私有成员变成 只能通过基类接口访问 只能通过基类接口访问 只能通过基类接口访问 能否隐式向上转换 能 能（但只能在派生类中） 否 隐式向上转换（implicit upcasting）：意味着无需进行显式类型转换，就可以将基类指针或引用指向派生类对象。 14.2.4 使用using重新定义访问权限使用保护派生或私有派生时，基类的公有成员将成为保护成员或私有成员。假设要设即为的方法在派生类外面可用，方法之一是定义一个使用该基类方法的派生类方法。 另一种方法是，将函数调用包装在另一个函数调用中，即使用一个using声明来指出派生类可以使用特定的基类成员，即使采用的是私有派生。例如，假设希望通过Student类能够使用valarray的方法min()和max()，可以如下书写：12345class Student: private std::string, private std::valarray&lt;double&gt;&#123; public: using std::valarray&lt;double&gt;::min; using std::valarray&lt;double&gt;::max;&#125; 注意：using声明只使用成员名——没有圆括号、函数特征表和返回类型。 有一种老式的不带using声明的方法，它看起来就像是不包含关键字using的using声明，但是这种方法已被摒弃，即将体制使用。 14.3 多重继承MI描述的是有多个直接基类的类，与单继承一样，公有MI表示的也是is-a关系。例如，可以从Waiter类和Singer类派生出SingingWaiter类。 私有MI和保护MI可以表示has-a关系。 MI可能会带来很多新问题，其中最重要的问题是： 从两个不同的基类继承同名方法; 从两个或更多相关基类那里继承同一个类的多个实例。 14.3.1 有多少个Work如果多个类来自于同一个基类，而当前类又继承这多个类，那么，就会有多个最原始的基类副本，者造成了二义性。 为了解决以上问题，C++引入了一种新技术——虚基类（virtual base calss），使MI成为可能。 虚基类：虚基类使得从多个类（它们的基类相同）派生出的对象之只继承一个基类对象。 新的构造函数规则：使用虚基类时，需要对类构造函数采用一种新的方法。对于非虚基类，唯一可以出现在初始化列表中的构造函数是即时基类构造函数。但这些构造函数可能需要将信息传递给其基类。（详细请看p558） 14.3.2 哪个方法因为多重继承，有时会继承多个同名方法，因此，需要指出使用哪一个方法。 可以使用作用域解析运算符来指定使用的方法：12SingingWaiter newhire("Elise", 2005, 6, soprano);newhire.Singer::Show(); 然而，更好的方法是在SingingWaiter中重新定义Show()，并指出要使用哪个Show()。如下所示： 123void SingingWaiter::Show()&#123; Singer::Show;&#125; 对于多继承，使用模块化的方式而不是递增方式来在派生类的同名函数中使用基类函数，即提供一个只显示Work组件的方法和一个只显示Waiter组件 或 Singer组件的方法。然后，在SingingWaiter::Show()方法中将组件组合起来。详细见p559。 总结： 在祖先相同时，使用MI必须引入虚基类，并修改构造函数初始化列表的规则。 下面介绍一些有关MI的问题。 混合使用虚基类和非虚基类 当类通过多条虚途径和非虚途径继承某个特定的基类时，该类将包含一个表示所有的虚途径的基类子对象和分别表示各条非虚途径的多个基类子对象。 虚基类和支配 派生类中的名称优先于直接或间接祖先类中的相同名称。 如果无法用优先规则判断出使用哪个名称，则会导致二义性。 14.4 类模板C++的类模板为生成通用的类声明提供了一种更好的方法（C++最初不支持模板，单模板被引入后，就一直在演化，因此有的编译器可能不支持这里的所有特性）。模板提供参数化（parameterized）类型，即能够将类型名作为参数传递给接收方来建立类或函数。 C++库提供了多个模板类，如vector、array、valarray等等。 14.4.1 定义类模板模板类以下面这样的代码开头：1234567template &lt;class T&gt;class Stack&#123;private: ...public: ...&#125;; 这里使用class并不意味着Type必须是一个类，而只是表明Type是一个通用的类型说明符，在使用模板时，将使用时间的类型替换它。较新的C++实现推荐使用关键字typename来代替class。 当模板被调用时，Type将被具体的类型值（如int或string）取代。 同样，可以使用模板成员函数替换原有类的类方法，每个函数头都将以相同的模板声明打头（如果在类声明中定义了方法，即内联定义，则可以省略模板前缀和类限定符：1234template &lt;typename T&gt;bool Stack&lt;T&gt;::push(const T&amp; item)&#123; ...&#125; 注意： 模板声明本身并不是类和成员函数，它们属于C++编译器指令，说明了如何生成对应的类和成员函数定义。而模板的具体实现则被称为实例化（instantiation）或具体化（specialization）。 不能将模板成员函数放在独立的实现文件中（以前，C++标准确实提供了关键字export，让您能够将模板成员函数放在独立的实现文件中，但支持该关键字的编译器不多，C++11不再这样使用export，而是将其保留用于其他用途）。 由于模板不是函数，它们不能单独编译，模板必须与特定的模板实例化请求一起使用。为此，最简单的方法是就所有模板信息放在一个头文件中，并在要使用这些模板的文件中包含该头文件。 14.4.2 使用模板类可以用所需的具体类型替换泛型名，就可以声明一个类型为模板类的对象： 12Stack&lt;int&gt; kernels;Stack&lt;string&gt; colonels; 注意，必须显式的提供所需的类型，这与常规的函数模板是不同的，因为编译器可以根据函数的参数类型来确定要生成哪种函数。 14.4.3 深入探讨模板类关于使用指针在作为Stack的类型，比如用字符指针替换string来作为T类型。这样会带来一些问题。 char* s：单纯的char* s并没有给s分配合适的空间，这会使s的值存在某些不合适的内存单元中 char s[40]：这虽然分配了空间，但是s的大小固定，且s本身是数组名，虽然代表地址，但是无法进行运算，有些操作会引起冲突。 char* po = new char[40]：这次分配了空间，po也成为了变量，但仍有问题，具体看p573。 但是并不是说不能使用指针作为T，只是在使用时，需要多家注意，考虑谨慎。 14.4.4 数组模板示例和非类型参数使用非类型参数来说模板达到某些目的 12345678910template &lt;typename T, int n&gt;class ArrayTP&#123;private: ...public: ...&#125;;ArrayTP&lt;double,12&gt; one;ArrayTP&lt;double,13&gt; two; 表达式参数方法的主要缺点是，每组数组的大小都将生成自己的模板，而利用构造函数的方法只会生成一个类声明，并将数组大小信息传递给类的构造函数，详细见p578。 14.4.5 模板多功能性可以将常规类的技术用于模板类，模板类可以用作基类，也可用作组件类，还可用作其他模板的类型参数。 递归使用模板 使用多个类型参数 12345678910template &lt;typename T1, typename T2&gt;class Pair&#123;private: T1 a; T2 b;public: T1&amp; first(); T2&amp; second(); ...&#125;; 默认模板参数 可以为类型参数提供默认值：1template &lt;class T1, class T2 = int&gt; class Topo&#123;...&#125;; 14.4.6 模板的具体化模板以泛型的方式描述类，而具体化是使用具体的类型生成类声明。 隐式实例化（implicit instantiation） &emsp;&emsp;声明一个或多个对象，指出所需的类型，编译器使用通用模板提供的处方生成具体的类定义： 1ArrayTP&lt;int, 100&gt; stuff; &emsp;&emsp;编译器在需要对象之前，不会生成类的隐式实例化，如下面的代码，第二条语句才会使编译器生成类定义，并根据定义创建一个对象12ArrayTP&lt;double, 30&gt; *pt;pt = new ArrayTP&lt;double, 30&gt;; 显式实例化（explicit instantiation） &emsp;&emsp;当使用关键字template并指出所需类型来声明类时，编译器将生成类声明的显式实例化。在这种情况下，孙然没有创建或提及类对象，编译器也将生成类声明（包括方法定义）。和隐式实例化一样，也将根据通用模板来生成具体化。（这里没搞懂）1template class ArrayTP&lt;string, 100&gt;; 显式具体化（explicit specialization） &emsp;&emsp;显式具体化是特定类型（用于替换模板中的泛型）的定义。有时候，可能需要在为特殊类型实例化时，对模板进行修改，使其行为不同。在这种情况下，可以创建显式具体化。（这块也没看懂） &emsp;&emsp;另外，假设模板是用&gt;运算符来对值进行比较，对于数字，管用。如果T表示一个type，则只要定义了T::operator&gt;()方法，这也管用。但如果T是由const char*表示的字符串，这将不管用。实际上，模板倒是可以正常工作，但字符串将按地址（按照字母顺序）排序。这要求类定义使用strcmp()，而不是&gt;来对值进行比较。 部分具体化（partial specialization） &emsp;&emsp;C++允许部分具体化，即部分限制模板的通用性。例如，可以给类型参数之一指定具体类型，下面的代码将T2具体化为int，但T1保持不变：1234//general templatetemplate &lt;class T1, class T2&gt; class Pair &#123;...&#125;;//specialization with T2 set to inttemplate &lt;class T1&gt; class Pair&lt;T1, int&gt; &#123;...&#125;; 14.4.7 成员模板模板可用作结构、类或模板类的成员。要完全实现STL的设计，必须使用这项特性。 14.4.8 将模板作为参数模板除了可以包含类型参数（typename T）和非类型参数（int n）之外，还可以包含本身就是模板的参数，如下所示：123template &lt;template &lt;typename T&gt; class Thing&gt; class Crab//其中，template&lt;typename T&gt;class是类型，Thin是参数 14.4.9 模板类和友元模板类声明也可以有友元。模板的友元分三类： 非模板友元 1234567template &lt;class T&gt;class HasFriend&#123;public: friend void counts(); //(1) friend void report(HasFriend &amp;); //(2) 错误 friend void report(HasFriend&lt;T&gt; &amp;); //(3) 正确&#125; &emsp;&emsp;上述代码中的（1）式在模板中将一个常规函数声明为友元，该声明使counts()函数成为模板所有实例化的友元，counts()函数不是通过对象调用的（它是友元，不是成员函数），也没有对象参数。它通过以下几种方式访问HasFriend对象：访问全局对象;使用全局指针访问非全局对象;创建自己的对象;访问独立于对象的模板类的静态数据成员。如果要为友元函数提供模板类参数，则不能通过（2）式来达到目的，原因是不存在HasFriend这样的对象，而只有特定的具体化，如HasFriend&lt;short&gt;，这里short可以用T表示，因为参数传递时就会指明T的类型，因此，要提供模板类参数，必须指明具体化。 约束（bound）模板友元，即友元的类型取决于类被实例化时的类型1234567891011//（1）template &lt;typename T&gt; void counts();template &lt;typename T&gt; void reprot(T&amp;);//（2）template &lt;typename T&gt;class HasFriendT&#123; ... friend void counts&lt;TT&gt;(); friend void report&lt;&gt;(HasFriendT&lt;TT&gt; &amp;);&#125;; &emsp;&emsp;使友元本身成为模板，使累得每一个具体化都获得与友元匹配的具体化，包含三步：首先，在类定义之前声明每个模板函数，如（1）所示，然后，在函数中再次将模板声明为友元，声明中的&lt;&gt;指出这是模板具体化，对于report()，&lt;&gt;可以为空，因为可以从函数参数推断出如下模板类型参数：HasFriendT&lt;TT&gt;，也可以写完整：report&lt;HasFriendT&lt;TT&gt; &gt; (HasFriendT&lt;TT&gt; &amp;)。但counts函数没有参数，因此必须使用模板参数语法&lt;TT&gt;来指明具体化。最后一步是友元提供模板定义。 非约束（unbound）模板友元，即友元的所有具体化都是类的每一个具体化的方式 12345template &lt;typename T&gt;class ManyFriend&#123; ... template &lt;typename C, typename D&gt; friend void show2(C&amp;, D&amp;);&#125;; &emsp;&emsp;对于非约束友元，友元模板类型参数与模板类类型参数是不同的，如上代码所示。 14.4.10 模板别名（C++11）可以使用typedef为模板具体化指定别名：1234567//define three typedef aliasestypedef std::array&lt;double,12&gt; arrd;typedef std::array&lt;int, 12&gt; arri;typedef std:array&lt;std::string,12&gt; arrst;arrd gollons;arri days;arrst months; C++11提供了一种新的可以简化上述任务的方法——使用模板提供一系列别名，如下所示：123456template&lt;typename T&gt; using arrtype = std::array&lt;T,12&gt;;//这将arrtype定义为一个模板别名，可以使用它来指定类型，如下所示arrtype&lt;double&gt; gallons;arrtype&lt;int&gt; days;arrtype&lt;std::string&gt; months;//总之， arrtype&lt;T&gt; 就表示类型 std::array&lt;T,12&gt;]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入应用C++11——代码优化与工程级应用》]]></title>
    <url>%2Fz_post%2FCpp-Book-%E6%B7%B1%E5%85%A5%E5%BA%94%E7%94%A8Cpp11%2F</url>
    <content type="text"><![CDATA[第一章 使用C++11让程序更简洁、更现代1.1 类型推导C++11引入了auto和decltype关键字实现类型推导。 1.1.1 auto类型推导1、auto关键词的新意义auto 可用于隐式类型定义。 不同于Python等动态类型语言（运行时才确定数据类型），隐式类型定义的类型推导发生在编译器。（C++是静态类型语言） 使用auto声明的变量必须立刻初始化，以让编译器推断出它的实际类型，并在 编译 时将auto占位符替换为真正的类型。 2、auto的推导规则 当不声明为指针或引用时，auto的推导结果会将初始化表达式的引用和cv限定符抛弃 当声明为指针或引用时，auto的推导结果将保持初始化表达式的cv属性。 3、auto的限制 auto不能用于函数参数 auto不能用于非静态成员变量 auto无法定义数组 auto无法推导出模板参数 4、什么时候用auto 当类型的名称很长时，可以用auto简化代码 当不确定变量应用被定义成什么类型时，如泛型函数的参数类型。 注意： auto虽然好用，但是不应该过度使用，否则，会严重降低代码的可读性和可维护性。 1.1.2 decltype1、获知表达式的类型 decltype关键字用于在编译时推导出一个表达式的类型，其语法格式为decltype(exp)，该关键字并不会真正计算表达式的值。 2、decltype的推导规则 当exp是标识符、类访问表达式时，decltype(exp)和exp的类型一致 当exp是函数调用时，decltype(exp)和返回值的类型一致 其他情况，若exp是一个左值，则decltype(exp)是exp类型的左值 引用 ，否则和exp类型一致。 3、decltype的实际应用 decltype的应用多出现在泛型编程中。 decltype也经常用在通过变量表达式抽取变量类型上。 1.1.3 返回类型后置语法——auto和decltype的结合使用]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的左值、右值、右值引用解析]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84%E5%B7%A6%E5%80%BC_%E5%8F%B3%E5%80%BC_%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[左值、右值C++11对C++98中的右值进行了扩充。在C++11中右值又分为纯右值（prvalue，Pure Rvalue）和将亡值（xvalue，eXpiring Value）。 在C++11中可以取地址的的就是左值，反之，不能取地址的、没有名字的就是右值（将亡值或纯右值）。举个例子，int a = b+c, a 就是左值，其有变量名为a，通过&amp;a可以获取该变量的地址；表达式b+c、函数int func()的返回值是右值，在其被赋值给某一变量前，我们不能通过变量名找到它，＆(b+c)这样的操作则不会通过编译。 纯右值的概念等同于我们在C++98标准中右值的概念，指的是临时变量和不跟对象关联的字面量值； 左值与右值的根本区别在于是否允许取地址&amp;运算符获得对应的内存地址 将亡值则是C++11新增的跟右值引用相关的表达式，这样表达式通常是将要被移动的对象（移为他用），比如返回右值引用T&amp;&amp;的函数返回值、std::move的返回值，或者转换为T&amp;&amp;的类型转换函数的返回值。右值引用本身就是一个xvalue。 不能根据在等号左边还是右边来判断左值和右值左值出现在等号右边的情况：12int a = 2;int c = a; 右值出现在等号左边的情况（不能作为赋值的对象，赋值没有意义）：1((i&gt;0) ? i : j) = 1; 右值、将亡值在理解C++11的右值前，先看看C++98中右值的概念：C++98中右值是纯右值，纯右值指的是临时变量值、不跟对象关联的字面量值。临时变量指的是非引用返回的函数返回值、表达式等，例如函数int func()的返回值，表达式a+b；不跟对象关联的字面量值，例如true，2，”C”等。 将亡值可以理解为通过“盗取”其他变量内存空间的方式获取到的值。在确保其他变量不再被使用、或即将被销毁时，通过“盗取”的方式可以避免内存空间的释放和分配，能够延长变量值的生命期。 左值引用、右值引用左值引用就是对一个左值进行引用的类型。右值引用（C++11新特性）就是对一个右值进行引用的类型，事实上，由于右值通常不具有名字，我们也只能通过引用的方式找到它的存在。 右值引用和左值引用都是属于引用类型。无论是声明一个左值引用还是右值引用，都必须立即进行初始化。而其原因可以理解为是引用类型本身自己并不拥有所绑定对象的内存，只是该对象的一个别名。左值引用是具名变量值的别名，而右值引用则是不具名（匿名）变量的别名。 左值引用通常也不能绑定到右值，但 常量左值引用 是个“万能”的引用类型。它可以接受非常量左值、常量左值、右值对其进行初始化。不过常量左值所引用的右值在它的“余生”中只能是只读的。相对地，非常量左值只能接受非常量左值对其进行初始化。 1234567int &amp;a = 2; # 左值引用绑定到右值，编译失败int b = 2; # 非常量左值const int &amp;c = b; # 常量左值引用绑定到非常量左值，编译通过const int d = 2; # 常量左值const int &amp;e = d; # 常量左值引用绑定到常量左值，编译通过const int &amp;&amp;b =2; # 常量左值引用绑定到右值，编程通过 右值引用本质上也是一种引用，只是它必须且只能绑定在右值上。由于右值引用只能绑定在右值上，而右值要么是字面常量，要么是临时对象，所以： 右值引用的对象，是临时的，即将被销毁 右值引用的对象，不会在其他地方使用 这两个特性意味着：接受和使用右值引用的代码，可以自由的接管所引用对象的资源，而无需担心对其他代码逻辑造成数据破坏 右值值引用通常不能绑定到任何的左值，要想绑定一个左值到右值引用，通常需要std::move()将左值强制转换为右值，例如： 123456int a=2;int b=1;int &amp;&amp;r1 = 3; //编译通过，右值引用可以绑定到字面常量上int &amp;&amp;r2 = a+b; //编译通过，右值引用可以绑定到临时变量上int &amp;&amp;r3 = a; # 编译失败int &amp;&amp;r4 = std::move(a); # 编译通过 右值引用本身是左值右值引用本身是左值，通过下面的代码，我们发现，可以通过右值引用的名字得到他的地址，因此，右值引用本身就是左值：12int&amp;&amp; r1 = 2;std::cout&lt;&lt;&amp;r1; //编译通过，输出r1的地址 下表列出了在C++11中各种引用类型可以引用的值的类型。值得注意的是，只要能够绑定右值的引用类型，都能够延长右值的生命期。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译型语言、解释型语言、静态类型语言、动态类型语言的概念与区别]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E7%BC%96%E8%AF%91%E5%9E%8B%E8%AF%AD%E8%A8%80%E3%80%81%E8%A7%A3%E9%87%8A%E5%9E%8B%E8%AF%AD%E8%A8%80%E3%80%81%E9%9D%99%E6%80%81%E7%B1%BB%E5%9E%8B%E8%AF%AD%E8%A8%80%E3%80%81%E5%8A%A8%E6%80%81%E7%B1%BB%E5%9E%8B%E8%AF%AD%E8%A8%80%E7%9A%84%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[编译型语言和解释型语言1、编译型语言需通过编译器（compiler）将源代码编译成机器码，之后才能执行的语言。一般需经过编译（compile）、链接（linker）这两个步骤。编译是把源代码编译成机器码，链接是把各个模块的机器码和依赖库串连起来生成可执行文件。 优点：编译器一般会有预编译的过程对代码进行优化。因为编译只做一次，运行时不需要编译，所以编译型语言的程序执行效率高。可以脱离语言环境独立运行。 缺点：编译之后如果需要修改就需要整个模块重新编译。编译的时候根据对应的运行环境生成机器码，不同的操作系统之间移植就会有问题，需要根据运行的操作系统环境编译不同的可执行文件。 代表语言：C、C++、Pascal、Object-C以及最近很火的苹果新语言swift 2、解释型语言解释性语言的程序不需要编译，相比编译型语言省了道工序，解释性语言在运行程序的时候才逐行翻译。 优点：有良好的平台兼容性，在任何环境中都可以运行，前提是安装了解释器（虚拟机）。灵活，修改代码的时候直接修改就可以，可以快速部署，不用停机维护。 缺点：每次运行的时候都要解释一遍，性能上不如编译型语言。 代表语言：JavaScript、Python、Erlang、PHP、Perl、Ruby 3、混合型语言既然编译型和解释型各有缺点就会有人想到把两种类型整合起来，取其精华去其糟粕。就出现了半编译型语言。比如C#,C#在编译的时候不是直接编译成机器码而是中间码，.NET平台提供了中间语言运行库运行中间码，中间语言运行库类似于Java虚拟机。.net在编译成IL代码后，保存在dll中，首次运行时由JIT在编译成机器码缓存在内存中，下次直接执行（博友回复指出）。 Java先生成字节码再在Java虚拟机中解释执行。 严格来说混合型语言属于解释型语言。C#更接近编译型语言。 动态语言和静态语言1、动态语言是一类在运行时可以改变其结构的语言：例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。通俗点说就是在运行时代码可以根据某些条件改变自身结构。 主要动态语言：Object-C、C#、JavaScript、PHP、Python、Erlang。 2、静态语言与动态语言相对应的，运行时结构不可变的语言就是静态语言。如Java、C、C++。 3、注意：很多人认为解释型语言都是动态语言，这个观点是错的！Java是解释型语言但是不是动态语言，Java不能在运行的时候改变自己结构。反之成立吗？动态语言都是解释型语言。也是错的！Object-C是编译型语言，但是他是动态语言。得益于特有的run time机制（准确说run time不是语法特性是运行时环境，这里不展开）OC代码是可以在运行的时候插入、替换方法的。 C#也是动态语言，通过C#的反射机制可以动态的插入一段代码执行。 动态类型语言和静态类型语言1、动态类型语言很多网上资料把动态类型语言和动态语言混为一谈，简直是误人子弟。动态类型语言和动态语言是完全不同的两个概念。动态类型语言是指在运行期间才去做数据类型检查的语言，说的是数据类型，动态语言说的是运行时改变结构，说的是代码结构。 动态类型语言的数据类型不是在编译阶段决定的，而是把类型绑定延后到了运行阶段。（Python只有到了运行时才能确定某一个变量的具体类型） 主要语言：Python、Ruby、Erlang、JavaScript、swift、PHP、Perl。 2、静态类型语言 静态语言的数据类型是在编译其间确定的或者说运行之前确定的，是在编译阶段就完成的，编写代码的时候要明确确定变量的数据类型。 主要语言：C、C++、C#、Java、Object-C。 3、注意：相当一部分程序员，认为解释型语言都是动态类型语言，编译型语言都是静态类型语言。这个也是错的。swift是编译型语言但是它也是动态类型语言。C#和Java是解释型语言也是静态类型语言。 强类型语言和弱类型语言1、强类型语言：强类型语言，一旦一个变量被指定了某个数据类型，如果不经过强制类型转换，那么它就永远是这个数据类型。你不能把一个整形变量当成一个字符串来处理。 主要语言：Java、C#、Python、Object-C、Ruby 2、弱类型语言：数据类型可以被忽略，一个变量可以赋不同数据类型的值。一旦给一个整型变量a赋一个字符串值，那么a就变成字符类型。 主要语言：JavaScript、PHP、C、C++（C和C++有争议，但是确实可以给一个字符变量赋整形值，可能初衷是强类型，形态上接近弱类型） 3、注意：一个语言是不是强类型语言和是不是动态类型语言也没有必然联系。Python是动态类型语言，是强类型语言。JavaScript是动态类型语言，是弱类型语言。Java是静态类型语言，是强类型语言。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《剑指offer》]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98-%E5%89%91%E6%8C%87offer%2F</url>
    <content type="text"><![CDATA[1. 二维数组中的查找题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数 解法一: 每一行用一次二分时间复杂度: $O(nlogn)$ (该复杂度无法通过牛客OJ) 123456789101112131415class Solution &#123;public: bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) &#123; for( int i = 0; i&lt;array.size(); i++)&#123; int low = 0, high = array[i].size()-1; while(low &lt; high)&#123; int mid = (low+high) / 2; if(target &gt; array[i][mid]) low = mid; else if(target &lt; array[i][mid]) high = mid; else return true; &#125; &#125; return false; &#125;&#125;; 每一行二分需要 logn 的时间, 总共有n行. 解法二: 从左下角开始时间复杂度: $O(n)$ 从左下角开始, 向右为大数方向, 向上为小数方向, 每次至少移动一位, 总共需要移动n次 12345678910111213class Solution &#123;public: bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) &#123; int i = array.size()-1; int j = 0, len = array[0].size(); while(i&gt;=0 &amp;&amp; j &lt; len)&#123; if(target &gt; array[i][j]) j++; // target在大数方向 else if(target &lt; array[i][j]) i--; // target在小数方向 else return true; &#125; return false; &#125;&#125;; 2. 替换空格题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 解法: 从前向后记录空格，从后向前替换空格时间复杂度: $O(n)$ 需要注意, 如果替换的空间超过了length的申请空间, 则需要重新申请空间 123456789101112131415161718192021222324252627282930class Solution &#123;public: void replaceSpace(char *str,int length) &#123; int white_count = 0; int char_count = 0; char *s = str; while(*s!='\0')&#123; if(*s == ' ') white_count++; else char_count++; s++; &#125; char* res_str = str; if(char_count + white_count*3 &gt;length) res_str = new char[char_count+white_count*3 + 1]; int old_length = char_count+white_count+1; int new_length = char_count+white_count*3 + 1; while(old_length &gt;=0 &amp;&amp; new_length &gt;= 0)&#123; if(str[old_length] != ' ')&#123; res_str[new_length--]=str[old_length--]; &#125;else&#123; old_length--; res_str[new_length--]='0'; res_str[new_length--]='2'; res_str[new_length--]='%'; &#125; &#125; str = res_str; &#125;&#125;; 3.从尾到头打印链表题目描述输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。123456789/*** struct ListNode &#123;* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) &#123;* &#125;* &#125;;*/ 解法一: reverse时间复杂度: $O(n)$空间复杂度: $O(n)$ 顺序访问, 然后将vector里面的元素逆置. 这两部操作时间复杂度皆为 $O(n)$ 123456789101112class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; res; while(head!=nullptr)&#123; res.push_back(head-&gt;val); head = head-&gt;next; &#125; std::reverse(res.begin(), res.end()); return res; &#125;&#125;; 解法二: swap (实际上依然是reverse)时间复杂度: $O(n)$空间复杂度: $O(n)$ 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; res; if(head == nullptr) return res; while(head!=nullptr)&#123; res.push_back(head-&gt;val); head = head-&gt;next; &#125; int mid = (res.size()-1)/2; int len = res.size()-1; for(int i =0 ;i&lt;=mid;i++) std::swap(res[i], res[len-i]); return res; &#125;&#125;; 解法三: 栈时间复杂度: $O(n)$空间复杂度: $O(2n)$ 1234567891011121314151617class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; stack&lt;int&gt; s; vector&lt;int&gt; res; if(head == nullptr) return res; while(head!=nullptr)&#123; s.push(head-&gt;val); head = head-&gt;next; &#125; while(!s.empty())&#123; res.push_back(s.top()); s.pop(); &#125; return res; &#125;&#125;; 4.重建二叉树题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 123456789/** * Definition for binary tree * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */ 首先需要注意一点的是: 这里前序和中序不能包含重复的元素, 否则无法确定前序和中序中节点的对应关系 解法一: 递归时间复杂度: $O(空间复杂度: 根据前序和中序的对应关系, 利用递归完成构建, 构建时, 先构建当前节点, 然后是左右子树. 1234567891011121314151617class Solution &#123;public: TreeNode* reConstructBinaryTree(vector&lt;int&gt; pre,vector&lt;int&gt; vin) &#123; return helper(pre, 0, pre.size()-1, vin, 0, vin.size()-1); &#125; TreeNode* helper(vector&lt;int&gt; &amp;pre, int pre_i, int pre_j, vector&lt;int&gt; &amp;vin, int vin_i, int vin_j)&#123; if(pre_i &gt; pre_j || vin_i &gt; vin_j) return nullptr; TreeNode* node = new TreeNode( pre[pre_i] ); // 当前节点 int v = vin_i; while(pre[pre_i] != vin[v]) v++; //找到前序在中序中的对应节点, 这里可以用哈希表来改进查找的复杂度 node-&gt;left = helper(pre, pre_i+1, pre_i+v-vin_i, vin, vin_i, v-1); node-&gt;right = helper(pre, pre_i+v-vin_i+1, pre_j, vin, v+1, vin_j); return node; &#125;&#125;; 解法二: 迭代123456789101112131415161718192021222324class Solution &#123;public: TreeNode* reConstructBinaryTree(vector&lt;int&gt; pre,vector&lt;int&gt; vin) &#123; stack&lt;TreeNode*&gt; tree_stack; if(pre.size() == 0) return nullptr; TreeNode * root = new TreeNode(pre[0]); tree_stack.push(root); int index = 0; for(int i = 1; i&lt;pre.size() ; i++)&#123; TreeNode * cur_node = tree_stack.top(); if(cur_node-&gt;val != vin[index])&#123; cur_node-&gt;left = new TreeNode(pre[i]); tree_stack.push(cur_node-&gt;left); &#125;else&#123; while( !tree_stack.empty() &amp;&amp; tree_stack.top()-&gt;val == vin[index])&#123; // 注意, 这里不能用cur_node, 而必须用tree_stack.top() cur_node = tree_stack.top(); tree_stack.pop(); index++; &#125; cur_node-&gt;right = new TreeNode(pre[i]); tree_stack.push(cur_node-&gt;right); &#125; &#125; return root; &#125;&#125;; 5.用两个栈实现队列6.旋转数组的最小数字7.斐波那契数列f(n) = \begin{cases} 0, & n=0 \\ 1, & n=1(或2) \\ f(n-1)+f(n-2), & n>2 \end{cases}解法一: 递归(超时,超内存)12345678class Solution &#123;public: int jumpFloor(int number) &#123; if(number==0) return 0; if(number==1 || number==2) return 1; return jumpFloor(number-1)+jumpFloor(number-2); &#125;&#125;; 解法二: 迭代1234567891011121314class Solution &#123;public: int Fibonacci(int n) &#123; if(n ==0) return 0; if(n==1 || n==2) return 1; int n1=1, n2=1; for(int i=3; i&lt;=n; i++)&#123; int temp = n2; n2 = n2+n1; n1 = temp; &#125; return n2; &#125;&#125;; 解法二: 迭代8.跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 解法一: 递归(超时, 超内存)设n个台阶的跳法有 $f(n)$ 种, 当青蛙跳上一个台阶后, 剩下的走法就是只有n-1个台阶的走法, 因此就是 $f(n-1)$ , 同理, 如果当前跳了2个台阶, 那么剩下的就是f(n-2), 因此有以下公式: f(n) = \begin{cases} 1, & n=1 \\ 2, & n=2 \\ f(n-1) + f(n-2) & n>1 \end{cases}从上可以看出, 这道题就是斐波那契数列的变种, 不同之处在于初始值不同(因为台阶为2时, 有两种跳法), 因此解法同上. 解法二: 迭代123456789101112131415class Solution &#123;public: int jumpFloor(int number) &#123; if(number==0) return 0; if(number==1) return 1; int n1 = 1; int n2 = 2; for(int i=3; i&lt;=number; i++)&#123; int temp=n2; n2 = n2 + n1; n1 = temp; &#125; return n2; &#125;&#125;; 9.变态跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解法一: 递归对于n个台阶, 可以跳1次, 2次, …., n次, 那么对应的剩下的台阶的跳法就有 $f(n-1), f(n-2), …, f(n-n)$ 种, 所以有下式: f(n) = f(0) + f(1) + f(2) + f(3) + ... + f(n-2) + f(n-1) = f(n-1) + f(n-1)f(n) = \begin{cases} 1, & n=0 \\ 1, & n=1 \\ 2*f(n-1), & n>=2 \end{cases}12345678class Solution &#123;public: int jumpFloorII(int number) &#123; if(number==0) return 1; if(number==1) return 1; return jumpFloorII(number-1) * 2; &#125;&#125;; 10.矩形覆盖对于 $n \times 2$ 大小的矩形, 可以竖着排列一个 $2\times 1$ 矩形, 或者横着排列上下两个 $2\times 1$ 的矩形, 那么对应的剩下的矩形面积就分别为 $(n-1) \times 2$ 和 $(n-2) \times 2$, 所以有下式: f(n) = \begin{cases} 1, & n=1 \\ 2, & n=2 \\ f(n-1) + f(n-2) & n>1 \end{cases}解法一: 递归(超时)解法二: 迭代1234567891011121314class Solution &#123;public: int rectCover(int number) &#123; if(number==0) return 0; if(number==1) return 1; int n1=1, n2=2; for(int i=3; i&lt;=number; i++)&#123; int temp = n2; n2 = n2+n1; n1 = temp; &#125; return n2; &#125;&#125;; 11.二进制中1的个数题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 解法一: 按位与&amp;时间复杂度: $O(1)$, 因为最多比较32次(long为64次)空间复杂度: $O(1)$ 注意: (n&amp;i) 一定要带括号, 因为它的优先级比==, != 等符号低.123456789101112class Solution &#123;public: int NumberOf1(int n) &#123; int i = 1; int count = 0; while( i!=0 )&#123; if( (n&amp;i) != 0) count++; // 注意, 这里的判断条件是 !=0, 并且 n&amp;i 一定要带括号 i = i &lt;&lt; 1; &#125; return count; &#125;&#125;; 解法二: n&amp;(n-1)一个整数 $n$, 将其与 $n-1$ 按位逻辑与, 得到的数刚好是将 $n$ 最右边的1置为0(其他位不变), 那么一个数有多少个1, 就可以进行多少次这样的操作.1234567891011class Solution &#123;public: int NumberOf1(int n) &#123; int count = 0; while(n!=0)&#123; n = n&amp;(n-1); count++; &#125; return count; &#125;&#125;; 12.数值的整数次方题目描述给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 解法一: 递归当n为偶数时: $x^n = x^{n/2} \times x^{n/2}$当n为奇数时: $x^n = x\times x^{n/2} \times x^{n/2}$ 12345678910class Solution &#123;public: double myPow(double x, int n) &#123; if(n==0) return 1.0; unsigned int un = abs(n); //注意这里必须是 unsigned类型, 就算是long , 也必须带unsigned, 主要是因为abs(INT_MIN)仍为负数INT_MIN! if(n &lt; 0) x = 1/x; return (un%2==0) ? myPow(x*x, un/2) : x*myPow(x*x, un/2); &#125;&#125;; 解法二: 非递归n要么为偶数, 要么为奇数, 就算为奇数, 也可以拆分成 $x\times x^{n-1}$ 的形式, 对于偶数n, 可以写成 $x^{n/2} \times x{n/2}$ 的形式, 对于 $x^{n/2}$, 可以继续按奇数偶数进行拆分. 举例来说, 对于x=2, n=10 , 可以写成 $2^{10} = 2^{5} \times 2^{5}$ 对于 $2^5$ , 可以写成, $2 \times 2^2 \times 2^2$, 可以看出, x每次与自身相乘后, n的次数就会变成原来二分之一, 这样, 可以用循环实现幂乘的操作, 如下所示. 123456789101112131415161718class Solution &#123;public: double myPow(double x, int n) &#123; if(n==0) return 1.0; unsigned int un = abs(n); //注意这里必须是 unsigned类型, 就算是long , 也必须带unsigned, 主要是因为abs(INT_MIN)仍为负数INT_MIN! if(n &lt; 0) x = 1/x; double res =1.0; while(un&gt;0)&#123; if(un%2 == 1)&#123; res * = x; &#125; x * =x; un /= 2; &#125; return res; &#125;&#125;; 13.调整数组顺序使奇数位于偶数前面14.链表中倒数第k个节点题目描述输入一个链表，输出该链表中倒数第k个结点。 解法一: 两个指针时间复杂度: $O(n)$ 遍历一次空间复杂度: $O(1)$ 1234567891011121314class Solution &#123;public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) &#123; ListNode* p1 = pListHead, * p2 = pListHead; for(unsigned int i=0 ; i&lt;k; i++)&#123; if(p2==nullptr) return nullptr; p2 = p2-&gt;next; &#125; while(p2 != nullptr)&#123; p1 = p1-&gt;next; p2=p2-&gt;next; &#125; return p1; &#125;&#125;; 15.反转链表题目描述输入一个链表，反转链表后，输出新链表的表头。 解法一: 两个指针pre和cur时间复杂度: $O(n)$ 一次遍历空间复杂度: $O(1)$ 利用两个指针pre和cur维持当前节点和前一个节点, 然后执行反转操作 1234567891011121314class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; ListNode* pre = nullptr; ListNode* cur = pHead; while(pHead != nullptr)&#123; cur = pHead; pHead = pHead-&gt;next; cur-&gt;next = pre; pre = cur; &#125; return cur; &#125;&#125;; 16.合并两个排序的链表题目描述输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 解法一: 原地合并用辅助指针head申请一个指向头结点的指针, 并用cur维护当前节点, 通过比较大小进行插入合并1234567891011121314151617181920212223242526272829303132/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) &#123; if(pHead1 == nullptr) return pHead2; if(pHead2 == nullptr) return pHead1; ListNode * head = new ListNode(0); ListNode * cur = head; while(pHead1!=nullptr &amp;&amp; pHead2!=nullptr)&#123; if(pHead1-&gt;val &lt; pHead2-&gt;val)&#123; cur-&gt;next = pHead1; cur = cur-&gt;next; pHead1 = pHead1-&gt;next; &#125;else&#123; cur-&gt;next = pHead2; cur = cur-&gt;next; pHead2 = pHead2-&gt;next; &#125; &#125; if(pHead1!=nullptr) cur-&gt;next = pHead1; if(pHead2!=nullptr) cur-&gt;next = pHead2; return head-&gt;next; &#125;&#125;; 解法二: 递归123456789101112131415class Solution &#123;public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) &#123; if(pHead1==nullptr) return pHead2; if(pHead2==nullptr) return pHead1; if(pHead1-&gt;val &lt; pHead2-&gt;val)&#123; pHead1-&gt;next = Merge(pHead1-&gt;next, pHead2); return pHead1; &#125;else&#123; pHead2-&gt;next = Merge(pHead1, pHead2-&gt;next); return pHead2; &#125; &#125;&#125;; 17.树的子结构题目描述输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 解法一: 非递归每找到一个相等的节点, 就判断就是为子树 采用的是先根遍历的非递归写法, 在入栈之前就进行判断. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2) &#123; if(pRoot2 == nullptr) return false; std::stack&lt;TreeNode * &gt; pre_root; while(!pre_root.empty() || pRoot1!=nullptr)&#123; while(pRoot1!=nullptr)&#123; if(pRoot1-&gt;val == pRoot2-&gt;val &amp;&amp; is_subtree(pRoot1, pRoot2)) return true; pre_root.push(pRoot1); pRoot1 = pRoot1-&gt;left; &#125; if(!pre_root.empty())&#123; pRoot1 = pre_root.top(); pre_root.pop(); pRoot1 = pRoot1-&gt;right; &#125; &#125; return false; &#125; bool is_subtree(TreeNode * pRoot1, TreeNode * pRoot2)&#123; std::stack&lt;TreeNode * &gt; pre_root; while(!pre_root.empty() || pRoot2!=nullptr)&#123; while(pRoot2!=nullptr)&#123; if(pRoot1==nullptr || pRoot2-&gt;val != pRoot1-&gt;val) return false; pre_root.push(pRoot1); pre_root.push(pRoot2); pRoot1 = pRoot1-&gt;left; pRoot2 = pRoot2-&gt;left; &#125; if(!pre_root.empty())&#123; pRoot2 = pre_root.top(); pre_root.pop(); // 注意入栈与出栈的顺序要刚好相反 pRoot1 = pre_root.top(); pre_root.pop(); pRoot1 = pRoot1-&gt;right; pRoot2 = pRoot2-&gt;right; &#125; &#125; return true; &#125;&#125;; 解法二: 递归123456789101112131415161718class Solution &#123;public: bool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2) &#123; bool result = false; //用result变量来记录是否已经是子树, 如果result一旦为true, 就直接返回, 不用再继续递归 if(pRoot1 != nullptr &amp;&amp; pRoot2 != nullptr)&#123; result = is_subtree(pRoot1,pRoot2); if(!result) result = HasSubtree(pRoot1-&gt;left, pRoot2); if(!result) result = HasSubtree(pRoot1-&gt;right, pRoot2); &#125; return result; &#125; bool is_subtree(TreeNode* pRoot1, TreeNode* pRoot2)&#123; if(pRoot2 == nullptr ) return true; if(pRoot1==nullptr || pRoot2-&gt;val != pRoot1-&gt;val) return false; return is_subtree(pRoot1-&gt;left, pRoot2-&gt;left) &amp;&amp; is_subtree(pRoot1-&gt;right, pRoot2-&gt;right); &#125;&#125;; 18.二叉树的镜像题目描述操作给定的二叉树，将其变换为源二叉树的镜像。输入描述:二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 解法一: 递归先根遍历 1234567891011121314151617181920/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: void Mirror(TreeNode * pRoot) &#123; if(pRoot == nullptr) return; TreeNode * temp = pRoot-&gt;left; pRoot-&gt;left = pRoot-&gt;right; pRoot-&gt;right = temp; Mirror(pRoot-&gt;left); Mirror(pRoot-&gt;right); &#125;&#125;; 解法二: 非递归先根遍历 1234567891011121314151617181920class Solution &#123;public: void Mirror(TreeNode * pRoot) &#123; std::stack&lt;TreeNode * &gt; pre_root; TreeNode * cur = pRoot; while(!pre_root.empty() || cur!=nullptr)&#123; while(cur!=nullptr)&#123; TreeNode * temp = cur-&gt;left; cur-&gt;left = cur-&gt;right; cur-&gt;right = temp; pre_root.push(cur); cur = cur-&gt;left; &#125; if(!pre_root.empty())&#123; cur = pre_root.top(); pre_root.pop(); cur = cur-&gt;right; &#125; &#125; &#125;&#125;; 19.顺时针打印矩阵题目描述输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 解法一: 按层打印时间复杂度: $O(n)$空间复杂度: $O(n)$ 按照层从外而内进行打印, 需要注意层的边界条件, 以及上下层和左右层之间不能重复. 1234567891011121314151617181920212223class Solution &#123;public: vector&lt;int&gt; printMatrix(vector&lt;vector&lt;int&gt; &gt; matrix) &#123; vector&lt;int&gt; res; if(matrix.size()==0 ||matrix[0].size()==0) return vector&lt;int&gt;&#123;&#125;; int row = matrix.size(), col = matrix[0].size(); int layers = (std::min(row,col) + 1)/2; for(int layer=0; layer&lt;layers; layer++)&#123; for(int i=layer, j=layer; j&lt; col-layer; j++ ) res.push_back(matrix[i][j]); for(int i=layer+1, j=col-layer-1; i&lt;row-layer-1; i++) res.push_back(matrix[i][j]); // 这里的 i &gt; (row-1)/2 也可以写作 layer != row-1-layer, 避免上下重复 for(int i=row-layer-1, j=col-layer-1; i &gt; (row-1)/2 &amp;&amp; j &gt;=layer; j--) res.push_back(matrix[i][j]); // 这里的 j &lt; col/2 也可以写作 layer != col-1-layer, 避免左右重复 for(int i=row-layer-2, j=layer; j &lt; col/2 &amp;&amp; i&gt;layer; i--) res.push_back(matrix[i][j]); &#125; return res; &#125;&#125;; 20.包含min函数的栈题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 解法: 利用辅助栈实现应用一个辅助栈，压的时候，如果A栈的压入比B栈压入大，B栈不压，，，，小于等于，AB栈同时压入，出栈，如果，AB栈顶元素不等，A出，B不出。 12345678910111213141516171819class Solution &#123;public: stack&lt;int&gt; s1,s2; void push(int value) &#123; s1.push(value); if(s2.empty()) s2.push(value); else if(s1.top() &lt;= s2.top()) s2.push(value); &#125; void pop() &#123; if(s1.top()==s2.top()) s2.pop(); s1.pop(); &#125; int top() &#123; return s1.top(); &#125; int min() &#123; return s2.top(); &#125;&#125;; 21.栈的压入、弹出序列题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 解法: 模拟栈的压入, 弹出123456789101112131415161718192021class Solution &#123;public: bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123; stack&lt;int&gt; s; if(pushV.size()==0) return false; s.push(pushV[0]); int i = 0, j=1; while(!s.empty())&#123; if(s.top() != popV[i])&#123; if(j&lt;pushV.size()) s.push(pushV[j++]); else return false; &#125;else&#123; s.pop(); i++; &#125; &#125; return true; &#125;&#125;; 更简洁的写法: 123456789101112class Solution &#123;public: bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123; stack&lt;int&gt; s; if(pushV.size()==0) return false; for(int i=0, j=0; i&lt;pushV.size() ;)&#123; s.push(pushV[i++]); while(j&lt;popV.size() &amp;&amp; s.top() == popV[j])&#123;s.pop(); j++;&#125; &#125; return s.empty(); &#125;&#125;; 22.从上往下打印二叉树题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 解法: 层次遍历用一个变量cur_len来维护当前层的节点数, 这样就无序额外存储层深等其他信息.12345678910111213141516171819202122232425262728/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: vector&lt;int&gt; PrintFromTopToBottom(TreeNode* root) &#123; vector&lt;int&gt; res; if(root==nullptr) return res; std::queue&lt;TreeNode*&gt; q_tree; q_tree.push(root); while(!q_tree.empty())&#123; int cur_len = q_tree.size(); // 获取当前层节点数目 for(int i=0; i&lt;cur_len; i++)&#123; //直到遍历完当前层节点 TreeNode* cur_node = q_tree.front(); q_tree.pop(); res.push_back(cur_node-&gt;val); if(cur_node-&gt;left != nullptr) q_tree.push(cur_node-&gt;left); if(cur_node-&gt;right != nullptr) q_tree.push(cur_node-&gt;right); &#125; &#125; return res; &#125;&#125;; 23.二叉搜索树的后序遍历序列题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 解法: 根据后序序列的特性设计递归判断规则123456789101112131415161718192021class Solution &#123;public: bool VerifySquenceOfBST(vector&lt;int&gt; sequence) &#123; if(sequence.size()==0) return false; return helper(sequence, 0, sequence.size()-1); &#125; bool helper(vector&lt;int&gt; &amp;sequence, int start, int end)&#123; if(start&gt;=end) return true; int cur_i = start; while(cur_i &lt; end &amp;&amp; sequence[cur_i] &lt; sequence[end]) cur_i++; int mid = cur_i; while(cur_i &lt; end)&#123; if(sequence[cur_i] &lt;sequence[end]) return false; cur_i++; &#125; bool b1 = helper(sequence, start, mid-1); bool b2 = helper(sequence, mid, end-1); return b1&amp;&amp;b2; &#125;&#125;; 24.二叉树中和为某一值的路径题目描述输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 解法一: 递归解法先根遍历 123456789101112131415161718192021222324252627/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: vector&lt;int&gt; v_list; vector&lt;vector&lt;int&gt; &gt; res; int cur_number = 0; vector&lt;vector&lt;int&gt; &gt; FindPath(TreeNode* root,int expectNumber) &#123; if(root==nullptr) return res; cur_number += root-&gt;val; v_list.push_back(root-&gt;val); if(cur_number == expectNumber &amp;&amp; root-&gt;left==nullptr &amp;&amp; root-&gt;right==nullptr) res.push_back(v_list); if(root-&gt;left != nullptr ) FindPath(root-&gt;left, expectNumber); if(root-&gt;right != nullptr) FindPath(root-&gt;right, expectNumber); cur_number -= root-&gt;val; v_list.pop_back(); return res; &#125;&#125;; 另一种写法: 通过减法控制当前的和123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; v_list; vector&lt;vector&lt;int&gt; &gt; res; //int cur_number = 0; vector&lt;vector&lt;int&gt; &gt; FindPath(TreeNode* root,int expectNumber) &#123; if(root==nullptr) return res; expectNumber -= root-&gt;val; // 注意这里是减法 v_list.push_back(root-&gt;val); if(0 == expectNumber &amp;&amp; root-&gt;left==nullptr &amp;&amp; root-&gt;right==nullptr) //条件语句变为 0 == expectNumber res.push_back(v_list); if(root-&gt;left != nullptr ) FindPath(root-&gt;left, expectNumber); if(root-&gt;right != nullptr) FindPath(root-&gt;right, expectNumber); //cur_number -= root-&gt;val; //注意, 可以不加这条语句 v_list.pop_back(); return res; &#125;&#125;; 解法二: 非递归1234567891011121314151617181920212223242526272829303132/*非递归法：后序遍历1.进栈时候，把值同时压入路径的向量数组，修正路径的和2.出栈时候，先判断和是否相等，且该节点是否是叶节点，判断完成后保持和栈一致，抛出路径，修改路径的和3.向量数组和栈的操作要保持一致*/class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; FindPath(TreeNode* root, int expectNumber) &#123; stack&lt;TreeNode*&gt; s; vector&lt;int&gt; v; vector&lt;vector&lt;int&gt; &gt; res; while (root || !s.empty())&#123; while (root)&#123; s.push(root); v.push_back(root-&gt;val); expectNumber -= root-&gt;val; //能左就左，否则向右 root = root-&gt;left ? root-&gt;left : root-&gt;right; &#125; root = s.top(); if (expectNumber == 0 &amp;&amp; root-&gt;left == NULL &amp;&amp; root-&gt;right == NULL) res.push_back(v); s.pop(); v.pop_back(); expectNumber += root-&gt;val; //右子数没遍历就遍历，如果遍历就强迫出栈 if (!s.empty() &amp;&amp; s.top()-&gt;left == root) root = s.top()-&gt;right; else root = NULL;//强迫出栈 &#125; return res; &#125;&#125;; 25.复杂链表的复制题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 注意链表的复制不同于其他复制，在进行链表复制时，必须创建新的节点，同时，不能通过newnode-&gt;next = oldnode-next对新节点进行赋值，这是因为这样赋值会使新链表指向旧链表的节点，造成混乱。 正确解题思路： 先对原链表中的每一个节点进行复制，将复制的节点插入到原节点之后，比如原链表是A-&gt;B-&gt;C，则复制后应该变成A-&gt;A1-&gt;B-&gt;B1-&gt;C-&gt;C1。 再按照原始链表中随机指针的指向，对新节点的随机指针进行赋值。 将链表拆分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*struct RandomListNode &#123; int label; struct RandomListNode *next, *random; RandomListNode(int x) : label(x), next(NULL), random(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: RandomListNode* Clone(RandomListNode* pHead) &#123; if(pHead==NULL) return NULL; //少考虑这种情况会发生段错误 RandomListNode* curnode = pHead; //C++允许在声明结构变量时省略关键字struct，但是C不允许 while(curnode!=NULL)&#123; RandomListNode* clonenode = new RandomListNode(curnode-&gt;label); clonenode-&gt;next = curnode-&gt;next; curnode-&gt;next = clonenode; curnode = clonenode-&gt;next; &#125; curnode = pHead; while(curnode!=NULL)&#123; // 因为random有可能指向前面的节点, 所以必须在拆分链表之前进行random指针的赋值, 而不能在拆分链表的同时进行赋值 if(curnode-&gt;random!=NULL)&#123; //少考虑这种情况会不满足个别用例 curnode-&gt;next-&gt;random = curnode-&gt;random-&gt;next; curnode = curnode-&gt;next-&gt;next; &#125; else&#123; curnode-&gt;next-&gt;random = NULL; curnode = curnode-&gt;next-&gt;next; &#125; &#125; curnode = pHead; RandomListNode* newhead = pHead-&gt;next; RandomListNode* newcur = pHead-&gt;next; while(curnode!=NULL)&#123; if(newcur-&gt;next == NULL)&#123; curnode-&gt;next = NULL; break; &#125; curnode-&gt;next = newcur-&gt;next; newcur-&gt;next = newcur-&gt;next-&gt;next; curnode = curnode-&gt;next; newcur = newcur-&gt;next; &#125; return newhead; &#125;&#125;; 26.二叉搜索树与双向链表题目描述输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的节点，只能调整树中结点指针的指向。 解法一：自己的思路后序遍历，递归实现，首先将左子树全部变成有序的，然后将右子树全部变成有序的。由于在返回时，返回的是左右子树的根节点，因此，在将当前根节点与左右子树拼接时，需要移动到左子树的最后一个元素上（最大），与当前根节点的left拼接。对于右子树，要移动到右子树的第一个元素上（最小），与当前根节点的right拼接。 这里有一个需要注意的地方，以下两种声明方式，指针一定要初始化之后才能使用，会使代码结果表现不同，前者超时，后者通过 12TreeNode* pre,* next;TreeNode* pre=nullptr,* next=nullptr; 123456789101112131415161718192021222324252627282930313233343536373839404142/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: TreeNode* Convert(TreeNode* pRootOfTree) &#123; if(pRootOfTree==nullptr) return nullptr; TreeNode* node = recurve(pRootOfTree); while(node-&gt;left!=nullptr) node = node-&gt;left; return node; &#125; TreeNode* recurve(TreeNode* pRootOfTree)&#123; TreeNode* pre=nullptr,* next=nullptr; // 这里，如果没有指定nullptr，则程序会超时！！！ if(nullptr!=pRootOfTree-&gt;left) pre = recurve(pRootOfTree-&gt;left); if(nullptr!=pRootOfTree-&gt;right) next = recurve(pRootOfTree-&gt;right); if(pre!=nullptr)&#123; while(pre-&gt;right!=nullptr) pre=pre-&gt;right; pRootOfTree-&gt;left = pre; pre-&gt;right = pRootOfTree; &#125; if(next!=nullptr)&#123; while(next-&gt;left!=nullptr) next = next-&gt;left; pRootOfTree-&gt;right = next; next-&gt;left = pRootOfTree; &#125; return pRootOfTree; &#125;&#125;; 解法二：中序遍历，递归实现由于对搜索二叉树来说，中序遍历的结果就是有序的，因此，只需要通过维护一个prenode指针来标记当前节点的上一个节点即可完成双向有序链表。 注意，这里有一个非常关键的点，那就是TreeNode*&amp; prenode，如果少了&amp;引用标识，则结果错误！具体原因看文章关于*&amp;和*的联系和区别。12345678910111213141516171819202122232425class Solution &#123;public: TreeNode* Convert(TreeNode* pRootOfTree) &#123; if(pRootOfTree==nullptr) return nullptr; TreeNode* prenode = nullptr; recurve(pRootOfTree,prenode); while(pRootOfTree-&gt;left!=nullptr) pRootOfTree = pRootOfTree-&gt;left; return pRootOfTree; &#125; void recurve(TreeNode* root, TreeNode*&amp; prenode)&#123; if(root-&gt;left!=nullptr) recurve(root-&gt;left,prenode); root-&gt;left = prenode; if(prenode!=nullptr) prenode-&gt;right = root; prenode = root; if(root-&gt;right!=nullptr) recurve(root-&gt;right,prenode); &#125;&#125;; 解法三：中序遍历，非递归实现基于中序遍历的非递归方法，思路与解法二一致。 但是这里有个疑问，为什么使用下面的代码会发生段错误。1234567while(pRootOfTree-&gt;left!=nullptr) pRootOfTree = pRootOfTree-&gt;left;return pRootOfTree;发生段错误的原因主要是因为没有对pRootOfTree进行空指针检查,就直接使用了该指针的成员变量, 访问了本不存在的内存, 从而造成了段错误, 修改方法是在程序前加上空指针检查 以下代码额外设置了一个指针指向第一个节点，以避免使用上面代码带来的段错误。 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: stack&lt;TreeNode*&gt; S_node; TreeNode* Convert(TreeNode* pRootOfTree) &#123; if(pRootOfTree==nullptr) return pRootOfTree; TreeNode* P = pRootOfTree; // TreeNode* node = pRootOfTree; 进行了空指针检查, 所以不用再使用这个指针了, 下面也是同理 TreeNode* pre = nullptr; while(P!=nullptr||!S_node.empty())&#123; while(P!=nullptr)&#123; S_node.push(P); P = P-&gt;left; &#125; if(!S_node.empty())&#123; P = S_node.top(); P-&gt;left = pre; if(pre!=nullptr)&#123; pre-&gt;right = P; &#125;//else //node = P; pre = P; S_node.pop(); P = P-&gt;right; &#125; &#125; while(pRootOfTree-&gt;left!=nullptr) pRootOfTree = pRootOfTree-&gt;left; //return node; return pRootOfTree; &#125;&#125;; 另一种看起来逻辑性更好的写法:123456789101112131415161718192021222324252627282930class Solution &#123;public: TreeNode* Convert(TreeNode* pRootOfTree) &#123; if(pRootOfTree == nullptr) return pRootOfTree; std::stack&lt;TreeNode*&gt; s_tree; TreeNode* cur_node = pRootOfTree; TreeNode* head = nullptr; //双向链表的头指针 TreeNode* pre_node = nullptr; //双向链表的pre指针 while(!s_tree.empty() || cur_node!=nullptr)&#123; while(cur_node!=nullptr)&#123; s_tree.push(cur_node); cur_node = cur_node-&gt;left; &#125; if(!s_tree.empty())&#123; cur_node = s_tree.top(); s_tree.pop(); if(head==nullptr)&#123; head = cur_node; pre_node = head; &#125;else&#123; pre_node-&gt;right = cur_node; cur_node-&gt;left = pre_node; pre_node = cur_node; &#125; cur_node = cur_node-&gt;right; &#125; &#125; return head; &#125;&#125;; 27.字符串的排列题目描述输入一个字符串，按字典序打印出该字符串中字符的所有排列。例如输入字符串abc，则打印出由字符a，b，c所能排列出来的所有字符串abc，acb，bac，cab和cab。 解法一: 递归思路（没想到）：将一个字符串看成两个部分，前一部分为首位字母，剩下的是后一部分。通过将首位字母与后一部分的所有字符交换（包括跟自己交换），可以得到第一个位置的所有可能情况。然后，再将剩下的部分看作是一个新的字符串，同样将剩余部分分成两部分，其中，第一部分是剩余部分的首位。如此，可以按照递归进行处理。 12345678910111213141516171819202122232425262728class Solution &#123;public: //bool my_sort(string s1, string s2)&#123; return s1&lt;s2;&#125;; vector&lt;string&gt; Permutation(string str) &#123; vector&lt;string&gt; v_string; if(str=="") return v_string; PermutationHelp(v_string, 0, str); std::sort(v_string.begin(), v_string.end()); return v_string; &#125; void PermutationHelp(vector&lt;string&gt;&amp; v_string, int pos, string str)&#123; /* if(pos == 0 )&#123; v_string.push_back(str); PermutationHelp(v_string, pos+1, str); &#125; \*/ //这里i=pos而不是pos+1的原因是：如果用pos+1,会导致丟解，即自己与自己交换的那种情况没有继续向下递归 for(int i=pos; i&lt;str.length(); i++)&#123; std::swap(str.at(pos), str.at(i)); if(std::count(v_string.begin(), v_string.end(), str) == 0) // 重复检查, 这里需要遍历, 会大大提高程序复杂度 v_string.push_back(str); PermutationHelp(v_string, pos+1, str); //std::swap(str.at(pos), str.at(i)); //能够注释本行的原因是因为上面已经利用count进行了重复检查. 但是实际上, 这是一种不太好的做法, 更好的写法在下面, 无需进行count重复检查 &#125; &#125;&#125;; 更简洁的写法:123456789101112131415161718192021222324class Solution &#123;public: vector&lt;string&gt; Permutation(string str) &#123; vector&lt;string&gt; res; if(str == "") return res; permute_helper(res, 0, str); std::sort(res.begin(), res.end()); return res; &#125; void permute_helper(vector&lt;string&gt; &amp;res, int pos , string &amp;str)&#123; if(pos == str.size()) res.push_back(str); // 注意, 这里是在pos==str.size()才将str放入res中, 这与上面的逻辑看起来好像有些矛盾 // 实际上, 当pos==str.size()时, 包含了所有可能情况, 即一次交换也没有发生(只与自身交换), 或者交换了某些位置等情况 else&#123; for(int i = pos; i&lt;str.size(); i++)&#123; if(str[pos] == str[i] &amp;&amp; pos!=i) continue; //防止重复出现, 如"aa", 则只输出一个 [a,a] std::swap(str[pos], str[i]); permute_helper(res, pos+1, str); std::swap(str[pos], str[i]); &#125; &#125; &#125;&#125;; 解法二: 迭代对于已经排列好的n-1个字符, 如果来了第n个字符, 则这个字符可以插入到n-1个字符的n个位置上, 注意控制字符是否重复, 即对于”aaaaa”来说, 如果新来的字符为’a’, 则这个’a’只有一种插法, 因此, 我们做判断: a与位置i(0~3)上的字符如果相等, 则不插入, 故而a只会插入到位置4上.(虽然位置4不存在, 但是插入时是能以超尾位置插入的) 12345678910111213141516171819202122class Solution &#123;public: vector&lt;string&gt; Permutation(string str) &#123; vector&lt;string&gt; res(1,""); if(str=="") &#123;res.pop_back(); return res;&#125; for(int i=0; i&lt;str.size(); i++)&#123; vector&lt;string&gt; res_tmp(std::move(res)); for(int j=0; j&lt;res_tmp.size(); j++)&#123; for(int k=0; k&lt;=res_tmp[0].size(); k++)&#123; if(k&lt;res_tmp[0].size() &amp;&amp; str[i] == res_tmp[j][k]) continue; //跳过重复排列, 例如将a插入a的两端,只选择插一端即可(a插入1位置), 另一端跳过(a插入0位置) string str_tmp = res_tmp[j]; str_tmp.insert(k, 1, str[i]); res.push_back(str_tmp); &#125; &#125; &#125; std::sort(res.begin(), res.end()); return res; &#125;&#125;; 28.数组中出现次数超过一半的数字题目描述：数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 思路一：如果数组是有序的，那么，出现次数超过数组长度一半的数字一定位于数组的中间位置，如果中间位置的数字出现次数小于数组长度的一半，那么就不存在。该方法需要进行排序，所以算法时间复杂度为 $nlog(n)$ 。1234567891011121314class Solution &#123;public: int MoreThanHalfNum_Solution(vector&lt;int&gt; numbers) &#123; //bool mysort(int a, int b) &#123;return a&lt;b;&#125; vector&lt;int&gt; counts; std::sort(numbers.begin(), numbers.end()); int n = std::count(numbers.begin(), numbers.end(), numbers.at((int)numbers.size()/2)); if (n &gt; numbers.size()/2) return numbers.at((int)numbers.size()/2); else return 0; &#125;&#125;; 思路二：Patition根据快排的思想，由于该数字一定在数组的中间位置，那么可以借助Partition来实现，随机选一个数字进行Partition，如果返回的mid索引最终停在N/2处，那么该索引对应的数字就有可能是答案，此时，只需统计该数字的出现次数即可。 该方法的时间复杂度是 $O(n)$ ，因为只会执行一边的Partition，并不会执行另一边. Partition的时间复杂度为 $O(n)$, 找到mid == numbers.size()/2的复杂度为 $O(logn)$, 因此总的时间复杂度为 $O(nlogn)$. 需要注意，具体在代码中看 12345678910111213141516171819202122232425262728293031323334353637383940414243bool mysort(int a, int b) &#123;return a&lt;b;&#125;class Solution &#123;public: int MoreThanHalfNum_Solution(vector&lt;int&gt; numbers) &#123; int low = 0 ; int high = numbers.size()-1; int mid = Partition(numbers, low, high); while(mid != numbers.size()/2)&#123; if(mid &lt; numbers.size()/2)&#123; low = mid + 1; mid = Partition(numbers, low, high); &#125;else&#123; high = mid - 1; mid = Partition(numbers,low, high); &#125; &#125; if(std::count(numbers.begin(), numbers.end(), numbers.at(mid)) &gt; numbers.size()/2) return numbers.at(mid); else return 0; &#125; int Partition(vector&lt;int&gt;&amp; numbers, int low, int high)&#123; int p = numbers.at(low); int mid = low; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; p &lt; numbers.at(high)) high--; //这里如果p用的是&lt;,则需要下面的low++逻辑，否则，会陷入死循环，如果用的是&lt;=，则在返回时，会返回首个元素的坐标 numbers.at(low) = numbers.at(high); if(low!=high) low++; while(low&lt;high &amp;&amp; p &gt; numbers.at(low)) low++; numbers.at(high) = numbers.at(low); if(low!=high) high--; &#125; numbers.at(low) = p; if(low == high) return mid; else return low; &#125;&#125;; 思路三：同增异减如果数组中存在这样一个数，那么这个数的出现次数一定大于其他所有数的出现次数总和，因此，设置两个变量，一个number用来存储数组中的第一个数，另一个num置为1,如果下一个数与number数相同，则num加一，否则减1,如果num被减为0,那么number转而存储下一个数，同时将num置为1。 这样，如果存在这个数，最终这个数一定为number，且num大于1。 123456789101112131415161718// 这个解法是错误的, 不论怎么处理, 最后都要做count&gt;half的检查, 这个方法能通过牛客的原因是因为牛客官方测例不够, 对于&#123;2,2,3,3,5,5&#125;的情况, 很明显应该输出0, 但是这个方法输出的是5class Solution &#123;public: int MoreThanHalfNum_Solution(vector&lt;int&gt; numbers) &#123; int number = numbers.at(0); int num = 1; for(vector&lt;int&gt;::iterator iter = numbers.begin()+1; iter != numbers.end(); iter++)&#123; if(num == 0)&#123; //这里与下面的区别之一是，一定要放在for训练内部的前面 number = * (iter-1); //区别之二这里如果使用iter-1,则无须在最后做count检查 num = 1; &#125; if(number == * iter) num++; else num--; &#125; if(num &gt;= 1) return number; //这里，num只需要&gt;=1 即可，仔细想一想这是为什么，为啥用了iter-1,就不用检查count。 else return 0; &#125;&#125;; 12345678910111213141516171819class Solution &#123;public: int MoreThanHalfNum_Solution(vector&lt;int&gt; numbers) &#123; int number = numbers.at(0); int num = 1; for(int i = 1; i&lt;numbers.size(); i++)&#123; if(num==0)&#123; number = numbers[i-1]; num=1; &#125; if(number == numbers[i]) num++; else num--; &#125; if(count(numbers.begin(), numbers.end(), number) &gt; numbers.size()/2) return number; //由于上面用的是iter，所以最终的num为1的数，只是有可能是我们要得数字，因此，需要进行检查。 else return 0; &#125;&#125;; 29.最小的K个数题目描述：输入n个整数，找出最小的K个数，例如输入4,5,1,6,2,7,3,8，则输出1,2,3,4。 一定要考虑边界情况： 数组为空 k大于数组size k小于0 思路一：最直接的想法，就是先对数组排序，然后输出前k个数。复杂度为 $nlog(n) + n$ 。 快排12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; vector&lt;int&gt; k_input; if(k &gt; input.size() || input.size()&lt;=0) return k_input; int low = 0; int high = input.size()-1; quickSort(input, low, high); //vector&lt;int&gt; k_input(&amp;input.at(0), &amp;input.at(k-1)); for(int i=0; i&lt;k; i++) k_input.push_back(input.at(i)); return k_input; &#125; void quickSort(vector&lt;int&gt;&amp; input, int low, int high)&#123; int mid = Partition(input, low, high); if(mid&lt;high) quickSort(input, mid+1, high); if(mid&gt;low) quickSort(input, low, mid-1); &#125; int Partition(vector&lt;int&gt;&amp; input, int low, int high)&#123; int p = input[low]; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; p&lt;=input[high]) high--; input[low] = input[high]; while(low&lt;high &amp;&amp; p&gt;=input[low]) low++; input[high] = input[low]; &#125; input[low] = p; return low; &#125;&#125;; 思路二：遍历整个数组，将当前元素与k_input数组进行比较，按照顺序插入，并且超出k的部分删除，最终直接返回k_input。时间复杂度 $O(nk)$ 。（该思想与冒泡排序思想类似）。12345678910111213141516171819202122class Solution &#123;public: vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; vector&lt;int&gt; k_input; if(k&gt;input.size() || input.size()&lt;=0) return k_input; k_input.push_back(input.at(0)); for(auto iter = input.begin()+1; iter!=input.end(); iter++)&#123; for(int i =0 ;i&lt;k; i++)&#123; if(i == k_input.size())&#123; k_input.push_back(*iter); break; &#125;else if(*iter &lt; k_input.at(i))&#123; k_input.insert(k_input.begin()+i, *iter); break; &#125; &#125; if(k_input.size() &gt; k) k_input.pop_back(); &#125; return k_input; &#125;&#125;; 解法三: 大顶堆遍历数组, 维护一个大顶堆, 每遇到一个比堆顶小的数, 就将其插入大顶堆 (如果是找最大的k个数, 就用小顶堆) 时间复杂度: $O(nlogk)$空间复杂度: $O(k)$ 借助priority_queue数据结构12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; priority_queue&lt;int&gt; q; vector&lt;int&gt; res; if(k&lt;=0 || k&gt;input.size()) return res; // 边界条件检查, 是否会出现段错误或输出结果错误 for(int i=0; i&lt;input.size(); i++)&#123; if(i&lt;k) q.push(input[i]); else if(input[i] &lt; q.top())&#123; q.pop(); q.push(input[i]); &#125; &#125; while(!q.empty())&#123; res.push_back(q.top()); q.pop(); &#125; return res; &#125;&#125;; 利用数组实现堆 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123;public: void heapify(vector&lt;int&gt; &amp;vec_heap, int index, int heap_size)&#123; int max=index; int left=index*2+1; int right = index*2+2; if(left&lt;heap_size &amp;&amp; vec_heap[max] &lt; vec_heap[left])&#123; //int temp = max; // 无需交换max和left, 只需记录max的值即可, 下面的right同理 max = left; //left = temp; &#125; if(right&lt;heap_size &amp;&amp; vec_heap[max] &lt; vec_heap[right])&#123; //int temp = max; max = right; //right = temp; &#125; if(index != max)&#123; std::swap(vec_heap[index], vec_heap[max]); heapify(vec_heap, max, heap_size); &#125; &#125; vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; vector&lt;int&gt; res; if(k&lt;=0 || k &gt; input.size()) return res; for(int i = 0; i&lt;k;i++)&#123; res.push_back(input[i]); &#125; for(int i=k-1; i&gt;=0;i--)&#123; //初始化堆 heapify(res, i, k); &#125; for(int i = k; i &lt; input.size(); i++)&#123; //i要从k开始, 因为k之前的已经是堆了 if(input[i] &lt; res[0])&#123; res[0] = input[i]; heapify(res, 0, k); &#125; &#125; for(int i = res.size()-1; i&gt;0; i--)&#123; std::swap(res[0], res[i]); heapify(res, 0, i); &#125; return res; &#125;&#125;; 另一种写法:1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: void heapify(vector&lt;int&gt; &amp;vec_heap, int index, int heap_size)&#123; int max = index; int left = index*2 + 1; int right = index*2 + 2; if(left &lt; heap_size &amp;&amp; vec_heap[left] &gt; vec_heap[max]) max=left; if(right &lt; heap_size &amp;&amp; vec_heap[right] &gt; vec_heap[max]) max=right; if(max!=index)&#123; std::swap(vec_heap[max], vec_heap[index]); heapify(vec_heap, max, heap_size); &#125; &#125; vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; vector&lt;int&gt; res; if(k&lt;=0 || k&gt;input.size()) return res; for(int i=k-1; i&gt;=0; i--) heapify(input, i,k); for(int i=k; i&lt;input.size(); i++)&#123; if(input[i] &lt; input[0])&#123; //std::swap(input[i], input[0]); input[0] = input[i]; heapify(input, 0, k); &#125; &#125; for(int i=k-1; i&gt;=0; i--)&#123; res.push_back(input[0]); std::swap(input[0], input[i]); heapify(input, 0, i); // i变小, 所以从k-1开始 &#125; return res; &#125;&#125;; 解法四: 快速选择算法时间复杂度: 平均为 $O(n)$ 复杂度分析: 每次都会扔掉一半, 所以每次进行检查的元素个数为之前的一半, 所有时间复杂度大致为: T(n) = n + n/2 + n/8 + ... + (n/2)^k = n*(1-2^{-k})/(1-2^{-1}) = 2n也就是说, 只要枢纽元素的选择使得两边的元素数量尽可能均衡, 就可以得到 $O(n)$ 的时间复杂度 123456789101112131415161718192021222324252627class Solution &#123;public: vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; vector&lt;int&gt; res; if(k&lt;=0 || k&gt;input.size()) return res; quick_select(input, 0, input.size()-1, k); //运行完 quick_select 以后, k之前的元素都比k位置上的元素小 for(int i = 0; i&lt;k; i++) res.push_back(input[i]); return res; &#125; void quick_select(vector&lt;int&gt; &amp;vec, int low, int high, int k)&#123; int P = vec[low] while(low &lt; high)&#123; while(low&lt;high &amp;&amp; P&lt;=vec[high]) high--; vec[low] = vec[high]; while(low&lt;high &amp;&amp; P&gt;=vec[low]) low++; vec[high] = vec[low]; &#125; vec[low] = P; //此时, low所处位置为枢纽元P, low之前的都小于P, low之后的都大于P if(low == k-1) return; //如果low所处位刚好为k-1, 则从这之前的k个元素一定是最小的(包括vec[low]自身) else if( k &lt; low) quick_select(vec, 0, low, k); else quick_select(vec, low+1, high, k); &#125;&#125;; 问题扩展 1输入是两个整数数组, 他们任意两个数的和有可以组成一个数组, 求这个和中的前k个数 分析: 假设两个整数数组为A和B, 各有N个元素, 任意两个数的和组成的数组C就有 $N^2$ 个, 那么可以把这些和看成N个有序数列, 由此, 问题就转变成了在这 $N^2$ 个有序数列里, 找到前k个最小的元素. A[1]+B[1] &lt;= A[1]+B[2] &lt;= A[1]+B[3] &lt;= … A[2]+B[1] &lt;= A[2]+B[2] &lt;= A[2]+B[3] &lt;= … … A[N]+B[1] &lt;= A[N]+B[2] &lt;= A[N]+B[3] &lt;= … 问题扩展 2有两个序列A和B都按照升序排列, 对于 1&lt;=i,j&lt;=k, 求k个最小的(ai+bj), 要求算法尽量高效. 30.连续子数组的最大和题目描述HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 解法一：穷举穷举遍历，时间复杂度 $O(n^2)$ 。 1234567891011121314151617class Solution &#123;public: int FindGreatestSumOfSubArray(vector&lt;int&gt; array) &#123; int max=array.at(0); for(auto iter=array.begin(); iter!=array.end(); iter++)&#123; //if(*iter &gt; 0)&#123; int temp = 0; for(auto it = iter; it!=array.end(); it++)&#123; temp += *it; if(temp &gt; max) max = temp; //&#125; &#125; &#125; return max; &#125;&#125;; 另一种写法(感觉更好理解些)12345678910111213141516class Solution &#123;public: int FindGreatestSumOfSubArray(vector&lt;int&gt; array) &#123; if(array.size() == 0) return INT_MIN; int max_sum = array[0]; int cur_sum = 0; for(auto x : array)&#123; cur_sum += x; if(cur_sum &gt; max_sum) // 更新max_sum max_sum = cur_sum; if(cur_sum &lt; 0) // 如果当前和为负, 则重置cur_sum cur_sum = 0; &#125; return max_sum; &#125;&#125;; 解法二：最优-两个变量记录sum$O(n)$ 的方法，根据数组性质，设置两个变量，一个记录当前的最大值，一个记录当前的子序列之和。首先，如果当前子序列之和为负，那么就是说，从当前位置开始的子序列，比从之前位置开始的子序列大，那么就可以不考虑从之前位置开始的子序列，之前累计的和也被抛弃。 1234567891011121314class Solution &#123;public: int FindGreatestSumOfSubArray(vector&lt;int&gt; array) &#123; if(array.size() == 0) return 0; int max_sum = array[0]; int cur_sum=0; for(int i=0; i&lt;array.size(); i++)&#123; cur_sum += array[i]; if(cur_sum &gt; max_sum) max_sum = cur_sum; if(cur_sum &lt; 0) cur_sum = 0; &#125; return max_sum; &#125;&#125;; 解法三：dp动态规划。与解法二的思路异曲同工，核心思想可有下述公式表示。 $f(i)代表以第i个数字结尾的子数组的连续最大和$ f(x)= \begin{cases} pData[i]& {i=0 或者f(i-1)\le 0} \\ f(i-1)+pData[i]& {i\ne 0 并且 f(i-1) > 0} \end{cases}上面的形式是递归的，通常情况下都用递归的方式来分析动态规划问题，但最终都会基于循环去编码。 上述公式对应的非递归形式就是思路二的代码。 递归写法：1234567891011121314151617181920class Solution &#123;public: int FindGreatestSumOfSubArray(vector&lt;int&gt; array) &#123; int max = array.at(0); f(array, array.size()-1, max); return max; &#125; int f(vector&lt;int&gt;&amp; array, int i, int&amp; max)&#123; if(i==0) return array.at(0); int f1 = f(array, i-1, max); if(f1&lt;0) f1 = array.at(i); else&#123; f1 = f1+ array.at(i); &#125; if(f1&gt; max) max =f1; return f1; &#125;&#125;; 31.整数中1出现的次数（从1到整数n中1出现的次数）题目描述求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 解法一：时间复杂度: $O(nm)$, m为数字的长度 直接借助C++函数，先将int转换成string，然后count计算string里面‘1’的个数。（这种方法可能面试不会满意，可以提一下，不过肯定有其他方法） 12345678910111213class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int count_1 = 0 ; for(int i = 1; i&lt;=n; i++)&#123; string str = std::to_string(i); count_1 += std::count(str.begin(), str.end(), '1'); &#125; return count_1; &#125;&#125;; 解法二：对每个数字进行除和求余的运算，得到每个数字中1的个数，然后将个数相加。 该方法的复杂度为 $O(nlogn)$ ，该种思想过于直接，时间复杂度较高，属于次等方案。（注意：这里的log底数按理说是10 ，但说大O记法是不考虑常数的，所以直接表示成log就可以） 123456789101112131415161718192021222324class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int count =0 ; for(int i =1 ;i&lt;=n;i++)&#123; int i1 = has1(i); if(i1) count+=i1; &#125; return count; &#125; int has1(int num)&#123; int count=0; while(num)&#123; if(num%10 == 1) count++; num /= 10; &#125; return count; &#125;&#125;; 解法三：时间复杂度: $O(logn)$ 设定整数点（如1、10、100等等）作为位置点i（对应n的各位、十位、百位等等），分别对每个数位上为1的情况有多少种进行分析 根据设定的整数位置，对n进行分割，分为两部分，高位n/i，低位n%i 当i表示百位，且百位对应的数&gt;=2时,如n=31456,i=100，则a=314,b=56，此时百位为1的情况有a/10+1=32（最高两位0~31，百位为1,共32种），每一种都包含100个连续的点，即共有(a%10+1) * 100种情况百位为1 当i表示百位，且百位对应的数为1时，如n=31156， i=100，则a=311,b=56，此时百位对应的就是1，则共有a%10(最高两位0-30)种情况是包含100个连续点，当最高两位为31（即a=311），本次只对应部分情况00~56，共b+1种，所有点加起来共有（a%10*100）+(b+1)种情况可以是百位为1 当i表示百位，且百位对应的数为0,如n=31056,i=100，则a=310,b=56，此时百位为1的情况有a/10=31种（最高两位0~30） 综合以上三种情况，当百位对应0或2时，有(a+8)/10次包含所有100个点，当百位为1时，即(a%10==1)为真时，另外需要增加部分情况b+1种 之所以补8，是因为当百位为0 或者 1 时，则a/10==(a+8)/10，当百位&gt;=2，补8会产生进位位，效果等同于(a/10+1) 1234567891011121314class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int count = 0; for(int i =1; i&lt;=n; i*=10)&#123; int a = n/i; int b = n%i; count+=(a+8)/10*i+(int)(a%10==1)*(b+1); &#125; return count; &#125;&#125;; 这种方法是一种通用解法, 可以用来求解整数0~n中x的出现次数, 其中, x 代表1~9中(0的情况貌似也差不多?)的任意一个数, 通用写法如下12345678910111213class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int count = 0; for(int i = 1; i&lt;=n ;i = i*10)&#123; int a = n/i; int b = n%i; count += (a+(10-x-1))/10 * i + (int)(a%10==x) * (b+1); //在这里根据x的值, 可以求得不同情况下的解, 例如, 若要求8的出现次数, 则为:count += (a+1)/10 * i + (int)(a%10==8) * (b+1); &#125; return count; &#125;&#125;; 解法四：剑指offer的递归方法，没看懂，感觉好像有错误？ 32.把数组排成最小的数题目描述：输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 难点： 找出一个新的排序规则，同时要证明这个排序规则是有效的 看到将两个int整数拼接在一起，就应该想到大数问题 解法一：主要考虑如何制定一个合理的判断规则： 比较两个字符串s1, s2大小的时候，先将它们拼接起来，比较s1+s2,和s2+s1那个大，如果s1+s2大，那说明s2应该放前面，所以按这个规则，s2就应该排在s1前面。 基于上面的规则，首先将vector&lt;int&gt;转换成对应的vector&lt;string&gt;，然后直接利用快排进行排序，最后将排好序的字符串向量拼接输出。 时间复杂度为主要在排序，因此为 $O(nlogn)$ 。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123;public: string PrintMinNumber(vector&lt;int&gt; numbers) &#123; if(numbers.size() == 0) return ""; vector&lt;string&gt; str_numbers; for(auto it=numbers.begin(); it!=numbers.end(); it++)&#123; str_numbers.push_back(std::to_string(*it)); &#125; int low =0; int high = numbers.size()-1; quickSort(str_numbers, low, high); string s=""; for(auto it = str_numbers.begin(); it != str_numbers.end(); it++)&#123; s+=*it; &#125; return s; &#125; void quickSort(vector&lt;string&gt;&amp; str_numbers, int low , int high)&#123; int mid = Partition(str_numbers, low, high); if(mid&lt;high) quickSort(str_numbers,mid+1, high); if(mid&gt;low) quickSort(str_numbers, low, mid-1); &#125; int Partition(vector&lt;string&gt;&amp; str_numbers, int low, int high)&#123; string p = str_numbers.at(low); while(low&lt;high)&#123; string s1= p + str_numbers.at(high); string s2= str_numbers.at(high) + p; while(low&lt;high &amp;&amp; s1.compare(s2) &lt;=0)&#123; high--; s1 = p + str_numbers.at(high); s2 = str_numbers.at(high) + p; &#125; str_numbers.at(low) = str_numbers.at(high); s1 = p + str_numbers.at(low); s2 = str_numbers.at(low) + p; while(low&lt;high &amp;&amp; s1.compare(s2) &gt;=0)&#123; low++; s1 = p + str_numbers.at(low); s2 = str_numbers.at(low) + p; &#125; str_numbers.at(high) = str_numbers.at(low); &#125; str_numbers.at(low) = p; return low; &#125;&#125;; 更整洁的写法:1234567891011121314151617181920212223242526272829303132class Solution &#123;public: bool cmp(int a, int b)&#123; string sa = std::to_string(a); string sb = std::to_string(b); return sa+sb &lt;= sb+sa; &#125; string PrintMinNumber(vector&lt;int&gt; numbers) &#123; if(numbers.size()==0) return ""; //!!!!少了这句话会产生段错误 quick_sort(numbers, 0, numbers.size()-1); string res; for(auto num : numbers) res += std::to_string(num); return res; &#125; int partition(vector&lt;int&gt; &amp;numbers, int low, int high)&#123; int P = numbers[low]; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; cmp(P, numbers[high])) high--; numbers[low] = numbers[high]; while(low&lt;high &amp;&amp; cmp(numbers[low], P)) low++; numbers[high] = numbers[low]; &#125; numbers[low] = P; return low; &#125; void quick_sort(vector&lt;int&gt; &amp;numbers, int low , int high)&#123; int mid = partition(numbers, low, high); if(low&lt;mid) quick_sort(numbers, low, mid-1); if(mid&lt;high) quick_sort(numbers, mid+1, high); &#125;&#125;; 用C++的内置排序函数:1234567891011121314151617class Solution &#123;public: struct&#123; bool operator()(int a, int b) const&#123; string sa = std::to_string(a); string sb = std::to_string(b); return sa+sb &lt; sb+sa; &#125; &#125;cmp; string PrintMinNumber(vector&lt;int&gt; numbers) &#123; std::sort(numbers.begin(), numbers.end(), cmp); string res; for(auto num : numbers) res += std::to_string(num); return res; &#125;&#125;; 33.丑数题目描述：把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数 解法一：穷举判断最简单的方法，就是对所有整数进行判断，该方法很容易超时 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: int GetUglyNumber_Solution(int index) &#123; if(index&lt;=0) return 0; int i = 0; int num = 1; int ugly=0; while(i&lt;index)&#123; if(IsUgly(num))&#123; i++; ugly = num; &#125; num++; &#125; num--; return num; &#125; bool IsUgly(int num)&#123; while(num%2==0) num /=2 ; while(num%3==0) num /= 3; while(num%5==0) num /= 5; if(num == 1) return true; else return false; &#125;&#125;; 解法二：根据丑数性质构造丑数时间复杂度: $O(n^2)$空间复杂度: $O(n)$ 用空间换时间，用一个数组将之前的丑数都存起来，然后，在判断下一个丑数时，不用对逐个整数判断，而只是与丑数和2,3,5的乘积进行判断 1234567891011121314151617181920212223242526272829303132333435// 使用指针时，一定要千万注意，指针会改变指向地址的值，使得其他指向该地址的指针，其指向的值也跟着变！class Solution &#123;public: int GetUglyNumber_Solution(int index) &#123; if(index&lt;=0) return 0; int* UglyArray = new int[index]; UglyArray[0] = 1; for(int i = 1 ;i &lt; index; i++)&#123; int *Ugly2 = UglyArray; int *Ugly3 = UglyArray; int *Ugly5 = UglyArray; while(*Ugly2 * 2 &lt;= UglyArray[i-1]) Ugly2++; while(*Ugly3 * 3 &lt;= UglyArray[i-1]) Ugly3++; while(*Ugly5 * 5&lt;= UglyArray[i-1]) Ugly5++; UglyArray[i] = Min(*Ugly2 *2, *Ugly3 *3, *Ugly5 *5); &#125; int num = UglyArray[index-1]; delete[] UglyArray; return num; &#125; int Min(const int&amp; a, const int&amp; b,const int&amp; c)&#123; int x = a&lt;b? a:b; return x&lt;c? x:c; &#125;&#125;;//* 解法三: 最优解法二的思路是正确的, 但是代码的实习上有重复计算, 例如, 最开始丑数集合为 {1}, 经过三次循环后, 丑数集合为 {1, 2, 3, 5}, 此时, 当进行第四次循环时, 会重复计算 1×2, 1×3, 1×5. 根据丑数的定义, 从 1 开始, 可以得到 2,3,5 这三个丑数, 然后从 2,3,5 开始(此时不用管1了), 可以得到 4,6,10,6,9,15,10,15,25 九个丑数, 根据这个思路, 我们可以指定三个变量来指示当前应该与2,3,5相乘的丑数, 而不是从第一个丑数开始遍历. 12345678910111213141516171819class Solution &#123;public: int GetUglyNumber_Solution(int index) &#123; if(index &lt;=1) return 0; vector&lt;int&gt; ugly_numbers(index); ugly_numbers[0] = 1; int t2=0, t3=0, t5=0; for(int i=1; i&lt;index; i++)&#123; int ugly_num = std::min( ugly_numbers[t2]*2, std::min(ugly_numbers[t3]*3, ugly_numbers[t5]*5)); if(ugly_num == ugly_numbers[t2]*2) t2++; // 因为新增的最小丑数为 t2指示的丑数与2相乘, 所以t2之前的数都不可能再与2相乘组成新的丑数, 所以无需检查t2之前的数 if(ugly_num == ugly_numbers[t3]*3) t3++; // 注意, 这里没有用else if 的原因是, 可能会出现重复的情况, 对于这种情况, 重复的指示器都要++, 避免将重复的丑数添加到数组中 if(ugly_num == ugly_numbers[t5]*5) t5++; ugly_numbers[i] = ugly_num; &#125; return ugly_numbers[index-1]; &#125;&#125;; 34.第一个只出现一次的字符题目描述在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写） 解法一（自想）每遇到一个字符，判断其是否是第一次出现，如果是则将它存在一个vector once里面，如果不是，则判断该字符是否在另一个vector more里面，如果没在，则该once中的该字符转移到mul里面，接着判断下一个字符。最终，输出once里面的首个元素。 该方法时间复杂度为 $O(n^2)$，并不令人满意。 123456789101112131415161718192021222324252627282930class Solution &#123;public: int FirstNotRepeatingChar(string str) &#123; vector&lt;int&gt; once_char; vector&lt;char&gt; mul_char; for(int i = 0; i&lt; str.length();i++)&#123; bool isfirst = true; for(auto it = once_char.begin(); it!=once_char.end(); it++)&#123; if(str[*it] == str[i])&#123; isfirst = false; once_char.erase(it); mul_char.push_back(str[i]); break; &#125; &#125; if(isfirst)&#123; auto mul_it = find(mul_char.begin(), mul_char.end(), str[i]); if(mul_it != mul_char.end()) isfirst = false; &#125; if(isfirst)&#123; once_char.push_back(i); &#125; &#125; if(once_char.empty()) return -1; return once_char.front(); &#125;&#125;; 解法二：牛客借助哈希表，时间复杂度为 $O(n)$。哈希表的构造可以用256大小的数组实现，字符对应的int值可作为哈希表的索引，表内的内容存储了该字符出现的次数。总共需要遍历两次字符串，第一次更新数组内字符出现的次数，第二次找到首个出现次数为1的字符。空间复杂度为 $O(1)$ （256是常数） 123456789101112131415161718class Solution &#123;public: int FirstNotRepeatingChar(string str) &#123; int hash_map[256]; for(int i =0;i&lt;256;i++) hash_map[i] = 0; //若少了初始化数组，则通不过，经过验证，数组默认内部不是0,而是随机数？ //有一种更标准初始化为0的方法，无需显式while循环：int hash_map[256]= &#123;0&#125;; for(int i = 0; i&lt;str.length(); i++)&#123; hash_map[int(str[i])]++; &#125; for(int i = 0; i&lt;str.length(); i++)&#123; if(hash_map[int(str[i])] == 1) return i; &#125; return -1; &#125;&#125;; 35.数组中的逆序对题目描述在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007输入描述: 题目保证输入的数组中没有的相同的数字 数据范围： 对于%50的数据,size&lt;=10^4 对于%75的数据,size&lt;=10^5 对于%100的数据,size&lt;=2*10^5 注意：仔细思考，这道题的P的数量会非常大，对于长度为n的数组，其P值最大可为 $\frac{n(n-1)}{2}$ 个。根据体重给出的数据，n最大可为 $2\times 10^5$ ，因此，P最大为 $\frac{2\times 10^5\times(2\times10^5 -1)}{2} \approx 2\times 10^{10}$,因此，使用int类型的数据时，有可能超过限制。所以，要使用long！( int类型数据范围为-21 4748 3648 到 21 4748 3647, 数量级在 $10^9$ 左右) 解法一（自） 暴力求解，时间复杂度 $O(n^2)$ ，这样做肯定不行 解法二（剑指）: 归并排序, 递归实现将数组中的元素进行归并排序, 排序的时候, 如果前面子数组的元素大于后面的元素, 那么可以组成的逆序对的数量就是后面元素剩余的元素数量(两个子数组各自都已经排好序). 这里需要注意的几点： 初始化是，将data数据复制到temp中，然后在递归时，将data和temp数组交换传递，可以不用在数组融合时，将temp中的数据复制到data中， 减少计算次数 数组融合时使用的while循环，条件均为 $&lt;=$ 或 $&gt;=$。 每次得到P的一部分时，都进行取余数，可保证P的值不会过大。（但还是要用long型整数） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class Solution &#123;public: int InversePairs(vector&lt;int&gt; data) &#123; if (data.size() == 0) return 0; vector&lt;int&gt; temp= data; //int P=0; long P = mergeSort(data,temp, 0, data.size()-1); return P%1000000007; &#125; long mergeSort(vector&lt;int&gt;&amp; data,vector&lt;int&gt;&amp; temp, int first, int last)&#123; int mid = (last + first)/2; long inv1=0,inv2=0,inv=0; if(first&lt; last)&#123; //这里，首先temp和data相同，因此对于mergeSort来说，可以顺序颠倒 //此时相当于把temp当前真实数组，而data当作了缓存空间 //经过mergeSort后，data里面数据就是分别排好序的 //所以传向mergeArray时，要把data放前面，把temp放后面 inv1 = mergeSort(temp, data, first, mid); //必须temp在前, 因为temp是已经将子数组排序过的 inv2 = mergeSort(temp, data, mid+1, last); inv = mergeArray(data, temp, first, mid, mid+1, last); //此处data在前的原因是经过mergeSort以后, data变成了排序号的. //上面这种写法的可读性不好, 如果在mergArray函数里面, 使data = temp , 就能写出下面这种可读性较好的形式(但是由于多了赋值操作, 在牛客上会超时). 但是使用for循环只复制需要复制的部分, 就不会超时 /* inv1 = mergeSort(data, temp, first, mid); inv2 = mergeSort(data, temp, mid+1, last); inv = mergeArray(data, temp, first, mid, mid+1, last); */ &#125; return (inv1+inv2+inv)%1000000007; &#125; long mergeArray(vector&lt;int&gt;&amp; data, vector&lt;int&gt;&amp; temp, int first1,int last1,int first2,int last2)&#123; long int inv = 0; int t = last2; int i = last1; int j = last2; while(i&gt;=first1 &amp;&amp; j&gt;=first2)&#123; if(data.at(i) &gt; data.at(j))&#123; temp.at(t) = data.at(i); inv += j-first2+1;; i--; t--; &#125;else&#123; temp.at(t) = data.at(j); j--; t--; &#125; &#125; while(i&gt;=first1)&#123; temp.at(t) = data.at(i); t--; i--; &#125; while(j&gt;=first2)&#123; temp.at(t) = data.at(j); j--; t--; &#125; // data=temp; //这个赋值操作会使得代码整体的可读性较好, 但是可能会超时 /*for(int i=first1; i&lt;=last2; i++) data[i] = temp[i]; //这种复制每次只会复制一部分, 故而没有超时 */ return inv%1000000007; &#125;&#125;; 解法三（剑指）: 归并排序, 迭代实现36.两个链表的第一个公共节点题目描述输入两个链表，找出它们的第一个公共结点。 解法一：栈时间复杂度: $O(m+n)$, 遍历两个链表空间复杂度: $O(m+n)$, 两个栈 分析公共子节点的特点，首先，是单向链表，因此，从第一个公共子节点开始，后面的都是一样的，所以最好是能从链表的最后一项还是比较。但由于是单向链表，因此只能从头访问，从能访问最后的节点。 就像是先进先出一样 因此，考虑用两个辅助栈来帮助实现～ 1234567891011121314151617181920212223242526272829303132/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123; stack&lt;ListNode*&gt; s1; stack&lt;ListNode*&gt; s2; for(ListNode* cur = pHead1; cur!=nullptr; cur = cur-&gt;next)&#123; s1.push(cur); &#125; for(ListNode* cur = pHead2; cur!=nullptr; cur = cur-&gt;next)&#123; s2.push(cur); &#125; ListNode* firstCN = nullptr; while(!s1.empty() &amp;&amp; !s2.empty())&#123; if(s1.top() == s2.top())&#123; firstCN = s1.top(); s1.pop(); s2.pop(); &#125;else break; &#125; return firstCN; &#125;&#125;; 解法二: 常数空间复杂度时间复杂度: $O(m+n)$, 遍历两次空间复杂度: $O(1)$, 不使用额外空间 首先遍历得到两个链表的长度, 然后先让长链表前进长度差个节点, 接着两个链表共同向前遍历, 当相遇时即为第一个公共节点. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123; if(headA==nullptr || headB==nullptr) return nullptr; int lenA = 0; ListNode *curA = headA; while(curA !=nullptr)&#123; lenA++; curA = curA-&gt;next; &#125; int lenB = 0; ListNode *curB = headB; while(curB != nullptr)&#123; lenB++; curB = curB-&gt;next; &#125; if(lenA &gt; lenB)&#123; int len = lenA-lenB; curA = headA; while(len--)&#123; curA = curA-&gt;next; &#125; curB = headB; while(curA!=nullptr &amp;&amp; curB!=nullptr)&#123; if(curA == curB) return curA; curA = curA-&gt;next; curB = curB-&gt;next; &#125; return nullptr; &#125;else&#123; int len = lenB-lenA; curB = headB; while(len--)&#123; curB = curB-&gt;next; &#125; curA = headA; while(curA!=nullptr &amp;&amp; curB!=nullptr)&#123; if(curA == curB) return curA; curA = curA-&gt;next; curB = curB-&gt;next; &#125; return nullptr; &#125; &#125;&#125;; 37.数字在排序数组中出现的次数题目描述统计一个数字在排序数组中出现的次数。 解法一（自想）先利用二分查找找到该数字的下标，然后统计该数字左右两边的相等数的个数，虽然二分查找的时间复杂度为$O(logn)$，但是在对该数左右两边查看相等数个数时，时间复杂度为 $O(n)$，因此，最终的时间复杂度应为 $O(n)$ 。 （这样的复杂度不会让面试官满意） 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: int GetNumberOfK(vector&lt;int&gt; data ,int k) &#123; int count = 0; if(data.size() == 0) return count; int index = binarySearch(data, k, 0, data.size()-1); if(index == -1) return count; else&#123; int i = index-1; while(index&lt;data.size() &amp;&amp; data.at(index) == k)&#123; index++; count++; &#125; while(i&gt;=0 &amp;&amp; data.at(i) == k)&#123; i--; count++; &#125; &#125; return count; &#125; int binarySearch(vector&lt;int&gt;&amp; data, int num, int first, int last)&#123; if(first == last)&#123; if(data.at(first) == num) return first; else return -1; &#125; int mid = (first+last)/2; if(data.at(mid) == num) return mid; else if(data.at(mid) &gt; num) return binarySearch(data, num ,first, mid); else return binarySearch(data, num, mid+1, last); &#125;&#125;; 解法二：牛客分析上面的方法，时间复杂度高的主要原因来自于最后的顺序检索。设想一下，如果知道目标数字出现的第一个位置和最后一个位置，是否就不用再进行顺序检索了？ 于是，可以将二分查找算法改成分别查找目标数字的首次出现位置和末次出现位置。也就是说，如果mid上的数字等于num，同时mid-1（mid&gt;0）上的数字不等于num，则mid为首次出现位置，否则，首次出现位置就应该还在前半段，同理，末次出现位置也是相似的道理。 结合以上讨论，将二分查找分成两个函数，分别找首次和末次位置，这样时间复杂度就是 $O(logn)$，无需进行顺序查找。 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: int GetNumberOfK(vector&lt;int&gt; data ,int k) &#123; int count = 0; if(data.size() == 0) return count; int index1 = binarySearchFirst(data, k, 0, data.size()-1); int index2 = binarySearchLast(data, k, 0, data.size()-1); if(index1 == -1 || index2 == -1) return 0; else return index2-index1+1; &#125; int binarySearchFirst(vector&lt;int&gt;&amp; data, int num, int first, int last)&#123; if(first &gt; last) return -1; int mid = (first+last)/2; if(data.at(mid) == num)&#123; if(mid == 0 || data.at(mid-1) != num) return mid; else return binarySearchFirst(data, num ,first, mid-1); &#125; else if(data.at(mid) &gt; num) return binarySearchFirst(data, num ,first, mid-1); else return binarySearchFirst(data, num, mid+1, last); &#125; int binarySearchLast(vector&lt;int&gt;&amp; data, int num, int first, int last)&#123; if(first &gt; last) return -1; int mid = (first+last)/2; if(data.at(mid) == num)&#123; if(mid==last || data.at(mid+1)!=num) return mid; else return binarySearchLast(data, num, mid+1, last); &#125; else if(data.at(mid) &gt; num) return binarySearchLast(data, num ,first, mid-1); else return binarySearchLast(data, num, mid+1, last); &#125;&#125;; 解法三: 解法二的非递归实现(更简洁易懂)1234567891011121314151617181920212223class Solution &#123;public: int GetNumberOfK(vector&lt;int&gt; data ,int k) &#123; int low = 0, high = data.size()-1; int first_k = -1, last_k = -1; if(data.size() == 0) return 0; while(low &lt; high)&#123; int mid = (low+high)/2; if(data[mid] &lt; k) low = mid+1; else high = mid; //在data[mid] == k时并不退出, 而是继续判断, 知道low==high时, 才会退出, 此时 low 和 high 都应指向最左侧的k值 &#125; if(data[low] != k) return 0; else first_k = low; high = data.size()-1; while(low &lt; high)&#123; int mid = (low+high+1)/2; // 因为只有 high 会移动到mid的下一位, 而low是等于mid的, 所以必须让mid更偏向右侧, 上面的逻辑也是同理, 希望让mid更偏向左侧 if(k &lt; data[mid]) high = mid-1; else low = mid; &#125; last_k = low; return last_k - first_k + 1; &#125;&#125;; 解法四: 寻找插入位置因为data中都是整数，所以可以稍微变一下，不是搜索k的两个位置，而是搜索 k-0.5 和 k+0.5 这两个数应该插入的位置，然后相减即可。因为数组中不存在 k-0.5 和 k+0.5 这两个数, 因此, 我们只需不断二分查找, 直到 low &gt; high 为止即可. 123456789101112131415class Solution &#123;public: int GetNumberOfK(vector&lt;int&gt; data ,int k) &#123; return binary(data, k+0.5) - binary(data, k-0.5); &#125; int binary(vector&lt;int&gt; &amp;data, double k)&#123; int low =0 , high = data.size()-1; while(low &lt;= high)&#123; // 这里的等于号是必不可少的 int mid = (low+high)/2; if(data[mid] &lt; k) low = mid+1; else high = mid - 1; &#125; return low; &#125;&#125;; 38.二叉树的深度题目描述输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 解法一: 非递归利用BFS广度优先遍历（错了，树没有广度遍历，这个应该叫层次遍历），结合一个专门存储当前节点所处深度的队列实现 利用一个layer_count变量来记录当前层总共的节点数, 每次当pop了当前节点数个节点后, depth都会增1，最终的树深度，就应该等于广度优先遍层次遍历历最后一个访问节点所处的深度。（因为这肯定是最后一层，也就是最深的一层） 123456789101112131415161718192021222324252627282930/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: int TreeDepth(TreeNode* pRoot) &#123; if(pRoot == nullptr) return 0; queue&lt;TreeNode*&gt; tree_q; tree_q.push(pRoot); int depth = 0; while(!tree_q.empty())&#123; int layer_count = tree_q.size(); //记录当前层共有多少节点 for(int i =0 ; i&lt;layer_count; i++)&#123; // 根据当前层节点进行pop TreeNode* cur_node = tree_q.front(); tree_q.pop(); if(cur_node-&gt;left != nullptr) tree_q.push(cur_node-&gt;left); if(cur_node-&gt;right != nullptr) tree_q.push(cur_node-&gt;right); &#125; depth++; &#125; return depth; &#125;&#125;; 解法二：牛客二叉树中的某个节点的深度，就是其左子树深度和右子树深度较大者+1 ， 二叉树的深度就是根节点的深度，所以，利用递归的思想实现。（代码简洁，但是复杂复杂度好像和广度优先一样，都是n？ 是这样吗？） 123456789101112class Solution &#123;public: int TreeDepth(TreeNode* pRoot) &#123; if(pRoot==nullptr) return 0 ; int depth1 = 1, depth2 = 1; if(pRoot-&gt;left!=nullptr) depth1 = TreeDepth(pRoot-&gt;left)+1; if(pRoot-&gt;right!=nullptr) depth2 = TreeDepth(pRoot-&gt;right)+1; return depth1&gt;depth2 ? depth1 : depth2; &#125;&#125;; 更简洁的写法:12345678class Solution &#123;public: int TreeDepth(TreeNode* pRoot) &#123; if(pRoot == nullptr) return 0; return max( TreeDepth(pRoot-&gt;left)+1, TreeDepth(pRoot-&gt;right)+1); &#125;&#125;; 39.平衡二叉树题目描述输入一棵二叉树，判断该二叉树是否是平衡二叉树。 解法一（自想）时间复杂度: $O(n)$, 每个节点至多访问一次空间复杂度: $O(n)$, 有可能需要进行 n 次递归 将题目看作是求左右子树的深度，如果深度差超过1,那么就不是二叉树，返回一个特殊的标识（-1），这种方法属于一边遍历，一边判断，只需要遍历每个节点一次，通过递归实现。时间复杂度为 $O(logn)$ 有一种“不太好”的方法是每遇到一个节点，就单独求一次这个节点对应的树的深度，这种做法要遍历一个节点很多次，是一种典型的不令人满意的做法, 下面的做法采用了剪枝, 使得对每个节点至多访问一次, 是较好的做法 12345678910111213141516171819class Solution &#123;public: bool IsBalanced_Solution(TreeNode* pRoot) &#123; int tdepth = treeDepth(pRoot); if(tdepth!=-1) return true; else return false; &#125; int treeDepth(TreeNode* root)&#123; if(root == nullptr) return 0; int leftdepth = treeDepth(root-&gt;left); if(leftdepth == -1) return -1; int rightdepth = treeDepth(root-&gt;right); if(rightdepth == -1) return -1; if(abs(leftdepth-rightdepth) &gt; 1) return -1; return max(leftdepth,rightdepth) + 1; &#125;&#125;; 更凝练的写法:1234567891011121314class Solution &#123;public: bool IsBalanced_Solution(TreeNode* pRoot) &#123; return tree_depth(pRoot)==-1 ? false : true; &#125; int tree_depth(TreeNode* pRoot)&#123; if(pRoot == nullptr) return 0; int left_depth = tree_depth(pRoot-&gt;left); //注意, 这里没有 +1, if(left_depth==-1) return -1; // 在这里直接断left_depth判断, 如果发现=-1,就一路返回, 无需再求右子树深度 int right_depth = tree_depth(pRoot-&gt;right); //注意, 这里没有 +1 if(right_depth==-1 || abs(left_depth-right_depth) &gt; 1) return -1; return max(left_depth, right_depth) + 1; // 注意, 这有一定要有+1, 因为树深度就等于左右子树最大深度+1 &#125;&#125;; 40.数组中只出现一次的数字题目描述一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。 （暴力解法就不提了，肯定不是最优。） 解法一：异或注：异或运算符还可以实现无中间变量的两个数字互换：123456789int a=2;int b=4;a = a^b; // a = 2^4 = 6b = a^b; // b = 6^4 = 2a = a^b; // a = 6^2 = 4//同理有a = a + b; // a = 2+4 = 6a = a - b; // b = 6-4 = 2a = a - b; // a = 6-2 = 4 异或运算的性质：任何一个数字异或它自己都等于0 。与0异或则保留原值也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 （这里不限定是一次，只要是奇数次都可以） 本题数列中，有两个出现一次的数字，第一次先全部异或，得到的结果是两个一次数字的异或值，该异或值至少有一位的值为1 (即在这一位上, 两个数字一个为0, 一个为1), 因此，找到这一位，然后根据这一位这数组分成两拨，如此一来，每一拨都变成了上面的简单情况。 （同理，如果有N个一次数字，可以通过不断分拨的方法解决, 例如, 如果有3个一次数字, 则找了为1的那一位, 可以将其分成具有2个一次数字和具有一个一次数字的两拨数组） 12345678910111213141516171819class Solution &#123;public: void FindNumsAppearOnce(vector&lt;int&gt; data,int * num1,int * num2) &#123; if(data.size() &lt; 2 ) return; int xor_res = 0; for(auto x : data) xor_res ^= x; int i = 1; while( (xor_res &amp; i) == 0) // 按位异或的优先级小于 '==' 的优先级, 因此一定要用括号括起来 i = i&lt;&lt;1; *num1=0, *num2=0; //return; for(auto x : data)&#123; if( (x &amp; i) != 0) // 按位与的优先级小于 '!=' , 所以必须用括号 *num1 = *num1 ^ x; else *num2 = *num2 ^ x; &#125; &#125;&#125;; 扩展: 数组中只有一个数出现一次，其他数都出现了2次，找出这个数字1234567int find1From2(vector&lt;int&gt; a)&#123; int len = a.size(), res = 0; for(int i = 0; i &lt; len; i++)&#123; res = res ^ a[i]; &#125; return res; &#125; 扩展: 数组中只有一个数出现一次，其他数字都出现了3次(奇数次)，找出这个数字例如数组a[]={2,4,4,4,6,6,6};结果则返回2；思路则是利用位运算，因为其他数字都出现了三次，那么他们的二进制相同位上1的个数则是3的倍数，这样的话，最后统计完3的倍数的位清0，剩下的1则都是那个只出现一次的数的位。(也可以先统计每一位上面1的个数, 最后模3取余)1234567891011121314151617int find1From3(vector&lt;int&gt; a)&#123; int *bits = new int[32]; //因为整数一般为4字节, 32位 int len = a.size(); for(int i = 0; i &lt; len; i++)&#123; for(int j = 0; j &lt; 32; j++)&#123; bits[j] = bits[j] + ( (a[i]&gt;&gt;j) &amp; 1); # 注意这里的括号是必不可少的, 因为 &amp; 的优先级比 + 低得多 &#125; &#125; int res = 0; for(int i = 0; i &lt; 32; i++)&#123; if(bits[i] % 3 !=0)&#123; res = res | (1 &lt;&lt; i); &#125; &#125; delete[] bits; return res;&#125; 可以看出, 这是一种比较通用的解法, 可以求解某个数字出现一次, 而其他数字出现n次的情况(如果n为偶数, 建议用异或实现). 41.和为S的连续正数序列题目描述小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!输出描述:输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序 解法一（自想）设置两个变量记录当前序列的start位置和end位置，判断当前序列的和: 如果=sum，则存储当前序列，并将start+1,序列前进; 如果&gt;sum,将应减去序列中的最小值，也就是start指向位置的值，然后start+1; 如果&lt;sum，则应该再加上下一个值，也就是end指向的值。 然后再进行上面的循环，直到start指向的位置值为(sum+1)/2,此时就已经不可能出现和为sum的连续序列了。该方法时间复杂度为$O(n)$ 12345678910111213141516171819202122232425262728class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; FindContinuousSequence(int sum) &#123; vector&lt;vector&lt;int&gt;&gt; results; int tmp = 0; int start = 1; for(int end =start; start &lt;= sum/2 ;)&#123; if(tmp == sum)&#123; vector&lt;int&gt; numseq; for(int i = start ; i&lt;end; i++)&#123; numseq.push_back(i); &#125; results.push_back(numseq); tmp -=start; // 需要注意这里的顺序, 一定要先减去了 start 以后, 才能执行 start++ start++; &#125;else if( tmp &gt; sum)&#123; tmp -= start; start++; &#125;else&#123; tmp += end; end++; &#125; &#125; return results; &#125;&#125;; 解法二: 等差序列求和公式42.和为S的两个数字题目描述输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。输出描述:对应每个测试案例，输出两个数，小的先输出。 解法一（自想）设置两个变量，分别指向数组的第一个位置和最后一个位置，然后将这两个变量所指位置的值相加，分以下三种情况： =sum; 判断二者乘积是否比当前最小值小，如果是，则改变最小值的持有值。 不管是否小，都将num1++ 实际上, 根本无需判断是否比当前最小值小, 因为对于和相同的两组数, 数字差值较大的那一组的成绩一定小于数字差值较小的, 因此, 只要找到符合和为sum条件的两个数字, 即可直接返回, 无需进行任何额外判断. &gt;sum; num2--; &lt;sum; num1++;循环以上三步直到 num1&gt;=num2。最后判断minnum1和minnum2的值，如果二者相等，说明数组里面不存在这样的数对儿，返回空vector，若不相等，则输出这两个值。 结论证明：假设：找到两组满足条件的数组对 $（x，y）$、$（x+a,y-a）$，其中（ $x+y=S, 0&lt;a&lt;y-x$ ） x*y-[(x+a)(y-a)]=x*y-x*y-(y-x)a+a2=a[a-(y-x)]因为 $0&lt;a&lt;y-x$ ,所以 $a-(y-x)&lt;0$ ,所以 $a[a-(y-x)]&lt;0$因此 $(x,y)$ 乘积一定比 $(x+a,y-a)$ 小 当第一次找到符合条件的两个数字时, 它们的乘积就一定是最小的, 所以可以直接退出.12345678910111213141516171819202122class Solution &#123;public: vector&lt;int&gt; FindNumbersWithSum(vector&lt;int&gt; array,int sum) &#123; vector&lt;int&gt; res; if(array.size() &lt; 2) return res; int low = 0, high = array.size()-1; //在使用容器的back()方法访问时，必须要确保容器不是空的，否则会出现段错误（访问越界） while(low &lt; high)&#123; if(array[low] + array[high] == sum)&#123; //首次找到就可返回 res.push_back(array[low]); res.push_back(array[high]); return res; //首次找到就可返回 low++; &#125;else if(array[low] + array[high] &lt; sum) low++; else high--; &#125; return res; &#125;&#125;; 43.左旋转字符串题目描述汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 解法一（自想）：利用str.substr(pos,n)注意： 这道题看似简单，实则很容易考虑不全，主要需注意以下几点： n大于str.length()的情况 str.length()=0的情况 n为负数的情况（虽然这里牛客没考虑，我觉得题里没说正数，所以是有负数的可能的） 越是简单的题，越要注意各种情况的考虑，因为这种题的考察点就是考虑是否全面，而不是题怎么解 12345678910111213class Solution &#123;public: string LeftRotateString(string str, int n) &#123; string res = ""; if(str.length() == 0) return str; if (n&gt;=str.length()) n = n % str.length(); if(n&lt;0) n = str.size() + n; 左移n位(负), 等于右移-n位, 等于左移size+n位 res=str.substr(n); res += str.substr(0,n); return res; &#125;&#125;; 解法二（牛客）：反转利用多次反转的方法，首先将字符串按照n的位置分成两部分，然后进行以下三步（abcdefg，2）： 反转前一部分：ba 反转后一部分：gfed 反转整个字符串：bagfed -&gt; defgab 时间复杂度也为$O(n)$ 12345678910111213141516171819class Solution &#123;public: void my_inverse(string &amp;str, int start, int high)&#123; int mid = (start+high)/2; for(int i = start ; i&lt;=mid; i++)&#123; std::swap(str[i], str[high-i+start]); // 这里注意交换时的下标, 因为i是从start开始的, 所以高位的下标应该为high-(i-start) &#125; &#125; string LeftRotateString(string str, int n) &#123; if(n==0) return str; if(n&lt;0) n = str.size() + n; //左移n位(负), 等于右移-n位, 等于左移size+n位 if (n&gt;=str.length()) n = n % str.length(); my_inverse(str, 0, n-1); my_inverse(str, n, str.size()-1); //return str; my_inverse(str, 0, str.size()-1); return str; &#125;&#125;; 利用 std::reverse() 实现:1234567891011121314class Solution &#123;public: string LeftRotateString(string str, int n) &#123; string res = ""; if(str.length() == 0) return str; if (n&gt;=str.length()) n = n % str.length(); if(n&lt;0) n = str.size() + n; //左移-n位, 等于右移n位, 等于左移size-n位 std::reverse(str.begin(), str.begin()+n); std::reverse(str.begin()+n, str.end()); std::reverse(str.begin(), str.end()); return str; &#125;&#125;; 44.翻转单词顺序列牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 解法一: 土办法设值两个标记i，j，都从字符串的最后一位开始，如果当前字符不是空格，那么i指向下一个，直到遇到空格为止，此时，将i到j范围内字符提取出来，然后把令j=i。重复以上过程，直到i=0为止。 该解法时间复杂度为 $O(n)$ 而且只需遍历一边字符串。 但是空间复杂度也为 $O(n)$12345678910111213141516171819202122class Solution &#123;public: string ReverseSentence(string str) &#123; if(str.length() == 0) return str; string res = ""; for(int i = str.length()-1, j=str.length()-1; i&gt;=0; )&#123; if(str[i] == ' ')&#123;//这里注意不能用双引号,双引号代表字符串,在C++内部,""与''表示的是不同的东西 res += str.substr(i+1,j-i); res += " "; i--; j = i; &#125;else if(i == 0)&#123; res += str.substr(i,j-i+1); i--; &#125;else&#123; i--; &#125; &#125; return res; &#125;&#125;; 解法二（牛客）：利用reverse执行两次反转首先反转整个字符串，然后以空格为间隔，反转每个单词。时间复杂度也是$O(n)$ (遍历两次).空间复杂度为 $O(1)$1234567891011121314class Solution &#123;public: string ReverseSentence(string str) &#123; std::reverse(str.begin(), str.end()); int i = 0; for(int j = 0; j&lt;=str.size(); j++)&#123; if(str[j] == ' ' || j == str.size())&#123; std::reverse(str.begin()+i, str.begin()+j); i=j+1; &#125; &#125; return str; &#125;&#125;; 45.扑克牌顺子题目描述LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张)他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 注意:该题目需要注意：1123 这样的顺序返回的是false 解法一(自想):分析能组成顺子的数字的特征，首先，最大的数字和最小的数字他们的差一定要比numbers的size小，否则，肯定连不了顺子。比如12345和2300等。其次，如果数组中出现非0的重复数字，那么也一定不是顺子。因此，代码可以这样写： 找出非0的最大值和最小值 在找最值的时候顺便利用最简单的hash表来存储每个数字出现的次数，hash表长度为14，key值为数字，value值为key值出现的次数，如果value出现&gt;1的情况，则直接返回false 做判断，如果max-min&lt; numbers.size()，则返回true，否则返回false。 以上程序时间复杂度为$O(n)$ ，并且只需要遍历一次numbers。 12345678910111213141516171819202122232425class Solution &#123;public: bool IsContinuous( vector&lt;int&gt; numbers ) &#123; if(numbers.size() == 0) return false; int i = 0; while(numbers[i] == 0) i++; int min=numbers.at(i); int max = numbers.at(i); int zeronum = 0; int count[14] = &#123;0&#125;; for(auto it = numbers.begin(); it!=numbers.end(); it++)&#123; if(*it &lt; min &amp;&amp; *it!=0) min = *it; if(*it &gt; max) max = *it; if(*it == 0) zeronum++; count[*it]++; if(*it != 0 &amp;&amp; count[*it] &gt; 1) return false; &#125; if(max-min &lt;= numbers.size() - 1) return true; else&#123; return false; &#125; &#125;&#125;; 上面对max和min赋值的时候, 有可能会出现需要遍历n次的情况, 用下面的方法稍微改进一下(复杂度不变), 要注意不论是max还是min, 都不能为0.(全0的情况时, 会返回true)12345678910111213141516class Solution &#123;public: bool IsContinuous( vector&lt;int&gt; numbers ) &#123; if(numbers.size() == 0) return false; int poke_hash[14]=&#123;0&#125;; int max_poke = INT_MIN, min_poke = INT_MAX; for(auto item : numbers)&#123; poke_hash[item]++; if(item!=0 &amp;&amp; poke_hash[item]&gt;1) return false; if(item!=0 &amp;&amp; item &lt; min_poke) min_poke = item; else if(item!=0 &amp;&amp; item &gt; max_poke) max_poke = item; if(max_poke!=INT_MIN &amp;&amp; min_poke!=INT_MAX &amp;&amp; max_poke - min_poke &gt;= numbers.size()) return false; &#125; return true; &#125;&#125;; 另一种写法, 可以让判断条件不用写的那么复杂, 推荐使用这种写法:1234567891011121314151617181920class Solution &#123;public: bool IsContinuous( vector&lt;int&gt; numbers ) &#123; int len = numbers.size(); if(len==0) return false; int hash_map[13]&#123;0&#125;; int min=INT_MAX, max=INT_MIN, zero=0; for(auto n : numbers)&#123; if(n==0) continue; else if(hash_map[n-1]&gt;0) return false; else if(n&lt;min) min = n; else if(n&gt;max) max = n; if(min!=INT_MAX &amp;&amp; max!=INT_MIN &amp;&amp; max-min&gt;=len) return false; //如果发现已经不可能出现顺子, 则提前退出 hash_map[n-1]++; &#125; if(min==INT_MAX || max==INT_MIN) return true; if(max - min &lt;len) return true; return false; &#125;&#125;; 解法二（牛客）：排序先排序，在统计0的个数，再用0填补空缺，时间复杂度为 $O(nlogn)$ 不如上面的方法好。 46.圆圈中最后剩下的数：约瑟夫（Josephuse）环问题题目描述0,1,…,n-1这n个数字排成一个圆圈，从数字0开始每次删除m-1处的数字，然后从这个数字的下一位继续从0开始，删除m-1处的数字，求出圆圈里剩下的最后一个数字 解法一（自想）：利用vector维护动态数组模拟约瑟夫环利用一个vector维护一个动态数组，数组内的内容是每个孩子的编号，每次要删除的节点位置，都在index+m-1处，如果index+m-1超过了数组的大小，则对数组的size求余即可。该算法是最简单的一种思路，vector或list在删除时，由于要将后面的元素向前挪，所以erase的时间复杂度为 $O(n)$ ，因此，总的时间复杂度为$O(n^2)$。 空间复杂度为 $O(n)$ 1234567891011121314151617class Solution &#123;public: int LastRemaining_Solution(int n, int m) &#123; if(n&lt;=0 || m&lt;=0) return -1; if(n == 1) return 0; vector&lt;int&gt; joseph_ring; for(int i = 0; i&lt;n; i++) joseph_ring.push_back(i); int index = 0; while(joseph_ring.size() &gt; 1)&#123; index = (index+m-1) % joseph_ring.size(); joseph_ring.erase(joseph_ring.begin() + index); &#125; return joseph_ring[0]; &#125;&#125;; 解法二（牛客）：经典解法，用环形链表模拟圆圈可以用std::list或者std::vector来模拟一个环形链表，由于它们本身不是循环的，因此需要记得手动实现循环逻辑（其实就是解法一） 如果要求不可以使用标准模板库里面的数据容器来模拟环形链表，那么可以自己设计结构体类型，实现一个循环链表。 这里由于链表随机在删除节点时的时间复杂度为 $O(1)$ , 但是无法进行随机访问，只能顺序访问，因此删除时需要先顺序移动到该节点上才行, 所以要时间复杂度仍然为$O(n^2)$ 空间复杂度为$O(n)$。 解法三（牛客）：分析每次删除时的数字规律，总结出以下公式，按照公式编写递归或非递归程序，时间复杂度为 $O(n)$，空间复杂度为 $O(1)$ 。 f(n) = \begin{cases} 0 & n=1 \\ [f(n-1,m)+m]\%n & n>1 \end{cases}思考过程：当把第m个数(下标为m-1)去掉以后, 就只剩下了n-1个数, 此时, 再从下标m开始, 继续进行大小为n-1的约瑟夫环问题. 这里假设我们已经知道了大小为n-1的约瑟夫环问题的解为下标 $x’$, 则 $x’$ 在大小为n的约瑟夫环问题里面的下标应该为: $x = (x’ + m) % n$ . 由此式即可得到上面的递归公式 1234567891011121314//非递归写法class Solution &#123;public: int LastRemaining_Solution(int n, int m) &#123; if(n&lt;=0 || m&lt;=0) return -1; int cur_n=1, res=0; while(cur_n &lt;= n)&#123;// 注意是&lt;=, 因为此处的i代表的是size, 必须要一直计算到i==n为止 res = (res+m) % cur_n; //注意是要除以cur_n, 而不是n cur_n++; &#125; return res; &#125;&#125;; 12345678910递归写法class Solution &#123;public: int LastRemaining_Solution(int n, int m) &#123; if(n&lt;=0 || m&lt;=0) return -1; if(n==1) return 0; return (LastRemaining_Solution(n-1,m)+m)%n; &#125;&#125;; 47.非常规法求前n项和题目描述求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 这道题本身没有实际意义，侧重考察发散性思维和对C++相关机制的理解程度。 解法一：构造函数每声明一个对象，则构造函数都被调用一次，因此，可以借助静态变量来在构造函数内部实现累加操作。 123456789101112131415161718192021class sum&#123; public: static int i ; static int s; sum()&#123;i++; s+=i;&#125;; ~sum()&#123;&#125;; static void set()&#123; i = 0; s = 0; &#125;; &#125;;int sum::i =0;int sum::s = 0;class Solution &#123;public: int Sum_Solution(int n) &#123; sum::set(); sum a[n]; return sum::s; &#125;&#125;; 解法二：虚函数利用虚函数来模拟递归函数，可以在两个类中分别定义函数，其中一个函数充当递归函数的角色，另一个函数处理终止递归的情况，然后在两个函数里二选一。 这里用到了一个小trick，那就是对于整型变量n，执行!!n以后，可以将其转换成布尔值（0和1）。 1234567891011121314151617181920class A&#123; public: virtual int sum(int n)&#123;return 0;&#125;;&#125;;A* Array[2]; //这里必须为指针，否则不会进入B的sum 函数class B : public A&#123; public: virtual int sum(int n)&#123;return Array[!!n]-&gt;sum(n-1) + n;&#125;;&#125;;class Solution &#123;public: int Sum_Solution(int n) &#123; class A a; class B b; Array[0] = &amp;a; Array[1] = &amp;b; return Array[1]-&gt;sum(n); &#125;&#125;; 上面用了虚函数，那么使用普通的函数可以吗？答案是否定的，因为使用普通函数时，无法同时调用两个类的函数，最终只会调用A类的sum函数。 解法三：函数指针同样是上面的思想，不过改为使用函数指针来实现两个函数模拟递归 1234567891011121314151617int A(int n)&#123; return 0;&#125;int B(int n)&#123; static int (*fun[2])(int n) = &#123;A,B&#125;; return fun[!!n](n-1) + n;&#125;class Solution &#123;public: int Sum_Solution(int n) &#123; return B(n); &#125;&#125;; 解法四：模板类使用模板类完成递归，这种方法有一个很大的缺点就是整个过程是在编译阶段完成的，因此无法使用动态的n，而必须是在编译期间就能确定的常量，另外，编译器对递归编译代码的递归深度也是有限制的，所以n不能太大。 48.不用加减乘除做加法49.把字符串转换成整数题目描述将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。输入描述: 输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 解法一（自想）：从头开始逐个字符遍历，每次遇到一个“数字”，就将之间的res×10，然后再加上这个数字。需要特别注意“-123”，“+123”等情况。 时间复杂度为 $O(n)$ 。 12345678910111213141516171819202122class Solution &#123;public: int StrToInt(string str) &#123; int res = 0; bool negative = false; int i = 0; if(str[i] == '-')&#123; negative=true; i++; &#125;else if(str[i] == '+') i++; for(; i &lt; str.length() ;i++)&#123; if( str[i] &gt;= '0' &amp;&amp; str[i] &lt;= '9')&#123; res = res*10 + (int)(str[i] - '0'); &#125;else return 0; &#125; if(negative) res = 0 - res; return res; &#125;&#125;; 注意上面的代码虽然已经解决了牛客的题，但是有几点是需要特别注意的！ 首先，题目很简单，所以这道题的考察点只在于是否将所有情况都考虑到了，以下是一些可能的情况，日后再遇到一定要想起来： 首先考虑如何返回错误，首先不能使用可以转换成数值类型（int，bool，char）的数据直接指明错误（比如返回0，无法得知到底是错误当时真的是0），由此，可以创建一个全局的错误变量，如果要返回错误，则返回0并且将该变量状态改变。 非数字类符号不全是错误输出，如：+123、-123 只输入+和-时，要返回错误 string str==&quot;&quot;时，也要返回错误 如果为char str*，则要判断指针是否为空 一定要考虑数值溢出情况（当转换的数字大于最大正数，小于最小负数时，会溢出） 50.数组中重复的数字题目描述在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 解法一：暴力对于每个数组中的数字，都到前面的数字中去寻找是否有重复的。 时间复杂度： $O(n^2)$ 空间复杂度： $O(1)$ 解法二：哈希建立长度为n的哈希表，每次遇到一个数字x，就在hash[x]增1，如果此时hash[x]变为2，那么就说明有重复。 时间复杂度： $O(n)$ 空间复杂度： $O(n)$ 1234567891011121314151617181920212223class Solution &#123;public: // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false bool duplicate(int numbers[], int length, int* duplication) &#123; vector&lt;int&gt; hash(length); for (int i = 0; i&lt; length; i++)&#123; hash[numbers[i]]++; if(hash[numbers[i]] &gt; 1)&#123; *duplication = numbers[i]; return true; &#125; &#125; return false; &#125;&#125;; 解法三51.构建乘积数组题目描述给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 解法一（自想）：将乘积看成两段，前i-1项的乘积，和后n-i项的乘积，分开计算，最终合并。时间复杂度： $O(n)$ 空间复杂度： $O(n)$ 123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; multiply(const vector&lt;int&gt;&amp; A) &#123; vector&lt;int&gt; B; int tmp = 1; for(auto it = A.begin(); it!=A.end(); it++)&#123; if(it!=A.begin()) tmp *= *(it-1); B.push_back(tmp); &#125; tmp = 1; for(int i = A.size()-1; i &gt;= 0; i--)&#123; if(i!=A.size()-1) tmp *= A.at(i+1); B.at(i) *= tmp; &#125; return B; &#125;&#125;; 52.正则表达式匹配题目描述请实现一个函数用来匹配包括.和*的正则表达式。模式中的字符.表示任意一个字符，而*表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串aaa与模式a.a和ab*ac*a匹配，但是与aa.a和ab*a均不匹配 解法一：（牛客）主要分两种情况： 当前字符的下一个字符不是* 当前字符的字一个字符是* 对于第一种情况：直接判断是否相等（包含‘.’的情况） 对于第二种情况，需要分情况讨论： 当前字符与pattern当前字符不相等，则patter当前只能出现零次，调用match(str, pattern+2) 当前字符与pattern字符相等（包含‘.’的情况），则pattern的选择有两种，出现零次，或者出现一次以上，这两种情况都必须考虑，否则会丢解，如（aab和a.*ab），因此，需要调用match(str, pattern+2) || match(str+1, pattern) 12345678910111213141516171819202122232425class Solution &#123;public: bool match(char* str, char* pattern) &#123; if( *str == '\0' &amp;&amp; *pattern == '\0') return true; if( *str != '\0' &amp;&amp; *pattern == '\0') return false; if( *(pattern+1) != '*')&#123; if(*str!='\0' &amp;&amp; (*str == *pattern || *pattern=='.')) // *str的条件不能丢 return match(str+1, pattern+1); else return false; &#125;else&#123; if(*str!='\0' &amp;&amp; (*str == *pattern || *pattern=='.')) //这里的if else组合语句是必须的，否则会在不能出现多次时，函数仍然考虑出现多次的情况，造成误解 return match(str, pattern+2) || match(str+1, pattern); else return match(str, pattern+2); &#125; &#125;&#125;; 53.表示数值的字符串题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 解法一（自想）：没有难点，考察点主要在于各种情况的考虑（以下均为false）： + - +12.2.2 12e 12e- 12E+4.3 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: bool isNumeric(char* string) &#123; if(*string == &apos;\0&apos;) return false; if(*string == &apos;+&apos; || *string == &apos;-&apos;) string++; if(*string == &apos;\0&apos;) return false; int point_count = 0; while( (*string &gt;= &apos;0&apos; &amp;&amp; *string &lt;= &apos;9&apos;) || *string == &apos;.&apos;)&#123; if (*string == &apos;.&apos;) point_count++; if (point_count &gt; 1) return false; string++; &#125; if(*string == &apos;\0&apos;) return true; if(*string == &apos;e&apos; || *string == &apos;E&apos;) string++; else return false; if(*string == &apos;\0&apos;) return false; if(*string == &apos;+&apos; || *string == &apos;-&apos;) string++; if(*string == &apos;\0&apos;) return false; while( *string &gt;= &apos;0&apos; &amp;&amp; *string &lt;= &apos;9&apos; ) string++; if(*string != &apos;\0&apos;) return false; return true; &#125;&#125;; 54.字符流中第一个不重复的字符解法一（牛客）：哈希表建立一个哈希表和一个char数组（均为256大小），哈希表存储每个字符出现的次数，key为char，value为次数，数组存储所有 曾经 出现过一次的字符。 时间复杂度 $O(n)$ 空间复杂度 $O(1)$ 12345678910111213141516171819202122232425262728class Solution&#123;public: //Insert one char from stringstream char hash_c[256] = &#123;0&#125;; char first_c[256] = &#123;0&#125;; int index =0; void Insert(char ch) &#123; hash_c[ch]++; if(hash_c[ch] == 1)&#123; *(first_c+index) = ch; index++; &#125; &#125; //return the first appearence once char in current stringstream char FirstAppearingOnce() &#123; for(int i =0 ;i&lt;index; i++) if (hash_c[*(first_c+i)] == 1) return *(first_c+i); return '#'; &#125;&#125;; 55.链表中环的入口节点解法一（牛客）假设有环，并且环中的节点数为n，那么只要设值两个指针，一个slow指针指向头结点，另一个fast指针指向第n+1个节点，然后每次slow指针和fast指针都增1，那么肯定会在环的头部相遇（因为fast刚好比slow领先了一个环的长度） 因此，首先需要判断是否有环，思路是：从头结点开始，slow每次走一步，fast每次走两步，那么只要有环，slow和fast就一定会在环中的某个节点处相遇，如果无环，则fast一定先到达空指针 判断有环后，令fast从当前节点开始，继续往下走（每次走一步），并记录步数，最终遇到slow时的步数就是环的长度。 该方法时间复杂度为 $O(n)$ 空间复杂度为 $O(1)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode* slow = pHead; ListNode* fast = pHead; while(slow!=nullptr &amp;&amp; fast != nullptr)&#123; if(slow-&gt;next == nullptr) return nullptr; else slow = slow-&gt;next; if(fast-&gt;next == nullptr || fast-&gt;next-&gt;next == nullptr) return nullptr; else fast = fast-&gt;next-&gt;next; if(slow == fast) break; &#125; fast = fast-&gt;next; int step = 1; while(slow != fast)&#123; fast = fast-&gt;next; step++; &#125; fast = pHead; while(step&gt;0)&#123; fast = fast-&gt;next; step--; &#125; slow = pHead; while(slow!=fast)&#123; slow = slow-&gt;next; fast = fast-&gt;next; &#125; return slow; &#125;&#125;; 解法二（牛客）： 断链法同理，先判断有环无环 然后记录两个指针，一个当前节点指针cur，一个相邻祖先指针pre，每经过一个节点时，都将pre指针的next置为nullptr，则当cur的next为空时，既为环的首个节点。 该方法的时间复杂度为O(n)，且只需遍历两次，且第二次遍历的时候正好遍历n个节点，但是缺点是会破坏链结构，补救办法是使用额外的标记来替代断链，但是这样会增加额外空间开销 1234567891011121314151617181920212223242526272829303132333435363738394041/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode* slow = pHead; ListNode* fast = pHead; if(slow == nullptr) return nullptr; while(slow!=nullptr &amp;&amp; fast != nullptr)&#123; if(slow-&gt;next == nullptr) return nullptr; else slow = slow-&gt;next; if(fast-&gt;next == nullptr || fast-&gt;next-&gt;next == nullptr) return nullptr; else fast = fast-&gt;next-&gt;next; if(slow == fast) break; &#125; if(pHead-&gt;next == pHead) return pHead; //需要特别考虑只有一个节点并且自己组成环的情况 slow = pHead; fast = pHead-&gt;next; while(fast-&gt;next!=nullptr)&#123; slow-&gt;next = nullptr; slow = fast; fast = fast-&gt;next; &#125; return fast; &#125;&#125;; 解法三：没太看懂https://blog.csdn.net/dawn_after_dark/article/details/82564271对应文中解法一 56.删除链表中重复的结点题目描述在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 解法一（自想）：这道题本身比较简单，只需要维护一个pre指针和cur指针，分别指向前一个结点和当前结点，如果当前结点和下一个结点的值相等，那么就删除当前结点，最后我pre指针的next值设置为指向未重复的结点 但是！本题恶心了我很久，一直报段错误，主要原因是有的结点没有做空判断，就访问了结点的val或者next成员，此时如果结点是空的，那么就会报段错误，主要有以下这么几个情况： 头结点本身就是重复的，这个需要删除头结点，另外判断是否重复时，还要检查头结点的下一个结点是否为空，如果为空，则不能访问其val值，否则，报段错误 在进行重复判断时，访问cur-&gt;next-&gt;val时，需要先判断cur-&gt;next是否为空，如果为空，则不能访问其val值 123456789101112131415161718192021222324252627282930313233343536373839/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if(pHead == nullptr || pHead-&gt;next == nullptr) return pHead; ListNode* newHead = new ListNode(0); // 建立一个新的结点，其next用于标识头结点，以便在头结点重复时，指向新的头结点 newHead-&gt;next = pHead; ListNode* cur = pHead; ListNode* pre = newHead; while(cur != nullptr &amp;&amp; cur-&gt;next !=nullptr)&#123; // 注意 这里一定必须是 &amp;&amp; ，如果是|| ，则下面有可能会访问到空结点的val，造成段错误 if(cur-&gt;val == cur-&gt;next-&gt;val)&#123; ListNode* dup = cur-&gt;next; while(cur-&gt;val == dup-&gt;val &amp;&amp; dup!=nullptr)&#123; // 同理，让验证所有欲访问的结点不为空 dup = dup-&gt;next; &#125; cur = dup; pre-&gt;next = cur; &#125;else&#123; cur = cur-&gt;next; pre = pre-&gt;next; &#125; &#125; return newHead-&gt;next; &#125;&#125;; 57.二叉树的下一个节点58.对称的二叉树 题目描述请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 解法一（牛客）：递归要判断一个树是否对称，需要判断其树的左右子节点是否相等，同时还要判断左子树的右子树和右子树的左子树是否相等，以及左子树的左子树和右子树的右子树是否相等，然后如此递归解之： 12345678910111213141516171819202122232425262728293031/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool isSymmetrical(TreeNode* pRoot) &#123; if(pRoot == nullptr ) return true; return isSymHelper(pRoot-&gt;left, pRoot-&gt;right); &#125; bool isSymHelper(TreeNode* subRoot1, TreeNode* subRoot2)&#123; if(subRoot1 == nullptr) return subRoot2==nullptr; if(subRoot2 == nullptr) return false; if(subRoot1-&gt;val != subRoot2-&gt;val) return false; bool b1 = isSymHelper(subRoot1-&gt;right, subRoot2-&gt;left); bool b2 = isSymHelper(subRoot1-&gt;left, subRoot2-&gt;right); return b1&amp;&amp;b2; &#125;&#125;; 解法二（牛客）：非递归关键还是知道怎么样才能判断一个二叉树是否对称，只要采用前序、中序、后序、层次遍历等任何一种遍历方法，分为先左后右和先右后左两种方法，只要两次结果相等就说明这棵树是一颗对称二叉树。 1234567891011121314151617181920212223242526272829303132333435363738394041//以下为层次遍历//与普通遍历不同的是，对于这道题，必须要把左右子树都存入到queue中，不论是否为空，因为只有这样才能将整个二叉树的结构存储起来，以便判断/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool isSymmetrical(TreeNode* pRoot) &#123; queue&lt;TreeNode*&gt; q1; queue&lt;TreeNode*&gt; q2; if( nullptr==pRoot) return true; q1.push(pRoot); q2.push(pRoot); TreeNode* cur1; TreeNode* cur2; while(!q1.empty() &amp;&amp; !q2.empty())&#123; cur1 = q1.front(); q1.pop(); cur2 = q2.front(); q2.pop(); if(cur1 == cur2 &amp;&amp; nullptr == cur1) continue; if(nullptr == cur1 || nullptr == cur2) return false; if(cur1-&gt;val != cur2-&gt;val) return false; q1.push(cur1-&gt;left); q1.push(cur1-&gt;right); q2.push(cur2-&gt;right); q2.push(cur2-&gt;left); &#125; return true; &#125;&#125;; 解法三（牛客）：非递归=非递归算法，利用DFS和BFS=========================== BFS使用Queue来保存成对的节点 出队的时候也是成对成对的 1.若都为空，继续； 2.一个为空，返回false; 3.不为空，比较当前值，值不等，返回false； 确定入队顺序，每次入队都是成对成对的，如left.left， right.right ;left.rigth,right.left 123456789101112131415161718192021222324252627282930313233/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool isSymmetrical(TreeNode* pRoot) &#123; if(pRoot == nullptr) return true; queue&lt;TreeNode*&gt; q; q.push(pRoot-&gt;left); q.push(pRoot-&gt;right); TreeNode* lnode; TreeNode* rnode; while(!q.empty())&#123; lnode = q.front(); q.pop(); rnode = q.front(); q.pop(); if(nullptr == lnode &amp;&amp; nullptr == rnode) continue; if(nullptr == lnode || nullptr == rnode) return false; if(lnode-&gt;val != rnode-&gt;val) return false; q.push(lnode-&gt;left); q.push(rnode-&gt;right); q.push(lnode-&gt;right); q.push(rnode-&gt;left); &#125; return true; &#125;&#125;; DFS使用stack来保存成对的节点 出栈的时候也是成对成对的 ， 1.若都为空，继续； 2.一个为空，返回false; 3.不为空，比较当前值，值不等，返回false； 确定入栈顺序，每次入栈都是成对成对的，如left.left， right.right ;left.rigth,right.left 12345678910111213141516171819202122232425262728293031323334353637/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool isSymmetrical(TreeNode* pRoot) &#123; stack&lt;TreeNode*&gt; s; if(pRoot == nullptr) return true; s.push(pRoot-&gt;left); s.push(pRoot-&gt;right); TreeNode* lnode; TreeNode* rnode; while(!s.empty())&#123; rnode = s.top(); s.pop(); lnode = s.top(); s.pop(); if( nullptr==lnode &amp;&amp; nullptr == rnode) continue; if( nullptr == lnode || nullptr == rnode) return false; if(lnode-&gt;val != rnode-&gt;val) return false; s.push(lnode-&gt;left); s.push(rnode-&gt;right); s.push(lnode-&gt;right); s.push(rnode-&gt;left); &#125; return true; &#125;&#125;; 59.按之字形顺序打印二叉树题目描述请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 解法一（自想, 差评）：利用两个queue，一个用于层次遍历树节点，另一个用于存储对应节点的depth，然后每次访问节点时，都判断当前节点的层数，如果为奇数层，则将该层直接push back到结果向量中，如果为偶数，则将该层数据进行reverse后再push back到结果向量中。 时间复杂度为 $O(n^2)$ 空间复杂度为 $O(n)$ 需要注意的是最后一层的边界条件与其它层不同一样，需要专门判断以下，具体可以看下面的点注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt; &gt; res; if(pRoot==nullptr) return res; queue&lt;TreeNode*&gt; q_node; queue&lt;int&gt; q_depth; q_node.push(pRoot); q_depth.push(1); TreeNode* cur; int depth; int global_depth = 1; vector&lt;int&gt; cur_layer; while(!q_node.empty())&#123; cur = q_node.front(); q_node.pop(); depth = q_depth.front(); q_depth.pop(); if(cur-&gt;left != nullptr)&#123; q_node.push(cur-&gt;left); q_depth.push(depth+1); &#125; if(cur-&gt;right != nullptr)&#123; q_node.push(cur-&gt;right); q_depth.push(depth+1); &#125; if(depth == global_depth)&#123; cur_layer.push_back(cur-&gt;val); if(q_node.empty())&#123; // 对应最后一层的情况，当到了最后一层时，depth不会再继续增1了， //所以不能通过global depth或depth的大小来判断是否进行pushback， //需要通过看是否达到了最后一个节点来判断 if(global_depth % 2 == 1)&#123; res.push_back(cur_layer); &#125;else&#123; reverse(cur_layer.begin(), cur_layer.end()); res.push_back(cur_layer); &#125; &#125; &#125;else&#123; if(global_depth % 2 == 1)&#123; res.push_back(cur_layer); &#125;else&#123; reverse(cur_layer.begin(), cur_layer.end()); res.push_back(cur_layer); &#125; cur_layer.clear(); cur_layer.push_back(cur-&gt;val); global_depth=depth; if(q_node.empty()) res.push_back(cur_layer); //这句话用于处理最后一层只有一个节点的情况，如果只有一个节点的话， //那么当前queue就为空，不会进入下一次循环，从而导致最后一层没有pushback进去 &#125; &#125; return res; &#125;&#125;; 解法二：利用reverse同样的思路，另一种写法，更加简洁，通过while里面内置for循环，来保证每次for循环都会将一整层的节点放进队列中，无需额外的数组来存储depth信息123456789101112131415161718192021222324252627282930313233链接：https://www.nowcoder.com/questionTerminal/91b69814117f4e8097390d107d2efbe0来源：牛客网class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(pRoot == NULL) return res; queue&lt;TreeNode*&gt; que; que.push(pRoot); bool even = false; while(!que.empty())&#123; vector&lt;int&gt; vec; //将vec声明在内部，省去每次的clear操作，clear操作需要对vector进行遍历，并将每个元素置为null？ const int size = que.size(); //当前存的节点数目就是这一层所有的节点，之前层的到已经被取出, 并且这一层的子节点还没有开始入队列 for(int i=0; i&lt;size; ++i)&#123; //将该层所有节点的子节点入队列，同时当到达该层最后一个节点时终止 TreeNode* tmp = que.front(); que.pop(); vec.push_back(tmp-&gt;val); if(tmp-&gt;left != NULL) que.push(tmp-&gt;left); if(tmp-&gt;right != NULL) que.push(tmp-&gt;right); &#125; if(even) //根据奇偶标识判断是否需要reverse std::reverse(vec.begin(), vec.end()); res.push_back(vec); even = !even; &#125; return res; &#125;&#125;; 解法三: 最优(不用reverse)时间复杂度: $O(n)$空间复杂度: $O(n)$ 在解法二中, 复杂度高的原因是因每次遇到偶数层的时候都要进行 reverse, 实际上, 当我们知道了该层的节点个数以后, 我们可以直接开辟一个指定大小的 vector, 然后根据下标随机访问来填入该层的节点值, 这样一来就不用进行 reverse, 并且空间复杂度与解法二相同 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(root == nullptr) return res; std::queue&lt;TreeNode*&gt; q; q.push(root); bool is_odd = true; TreeNode* cur_node; while(!q.empty())&#123; int layer_len = q.size(); vector&lt;int&gt; cur_layer(layer_len); for(int i=0; i&lt;layer_len; i++)&#123; cur_node = q.front(); q.pop(); if(is_odd==true) cur_layer[i] = cur_node-&gt;val; else cur_layer[layer_len-1-i ] = cur_node-&gt;val; if(cur_node-&gt;left!=nullptr) q.push(cur_node-&gt;left); if(cur_node-&gt;right!=nullptr) q.push(cur_node-&gt;right); &#125; res.push_back(cur_layer); is_odd = !is_odd; &#125; return res; &#125;&#125;; 60.把二叉树打印成多行题目描述从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 解法一（半自想）：while循环加for循环，无需额外记录层数，具体看59题解法二分析 时间和空间复杂度为 $O(n)$ 123456789101112131415161718192021222324252627282930313233/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt; &gt; res; if(pRoot== nullptr) return res; queue&lt;TreeNode*&gt; q_node; q_node.push(pRoot); while(!q_node.empty())&#123; vector&lt;int&gt; cur_layer; const int cur_size = q_node.size(); for(int i = 0;i&lt;cur_size; i++)&#123; TreeNode* cur_node = q_node.front(); q_node.pop(); cur_layer.push_back(cur_node-&gt;val); if(cur_node-&gt;left != nullptr) q_node.push(cur_node-&gt;left); if(cur_node-&gt;right != nullptr) q_node.push(cur_node-&gt;right); &#125; res.push_back(cur_layer); &#125; return res; &#125;&#125;; 61.序列化二叉树62.二叉搜索树的第k个节点题目描述给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 解法一（自想）：中根遍历，遍历到第k个节点时将其输出，如果k大于节点数量，输出nullptr, 时间复杂度 $O(n)$ 12345678910111213141516171819202122232425262728293031323334353637/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: TreeNode* KthNode(TreeNode* pRoot, int k) &#123; if(pRoot == nullptr) return nullptr; stack&lt;TreeNode*&gt; s_node; TreeNode* P = pRoot; //ctor&lt;TreeNode*&gt; vec_node; int cur_k = 0; while(P!=nullptr || !s_node.empty())&#123; while(P!=nullptr)&#123; s_node.push(P); P = P-&gt;left; &#125; if(!s_node.empty())&#123; P = s_node.top(); s_node.pop(); cur_k++; if(cur_k == k) break; P = P-&gt;right; &#125; &#125; if(cur_k == k) return P; return nullptr; &#125;&#125;; 63.数据流中的中位数解法一(自想):插入时用vector的insert方法,按顺序插入,空间为 $O(n)$ ,时间复杂度为$O(n)$ 返回中位数时直接利用下标,时间复杂度和空间复杂度都为 $O(1)$. 这里关于vector的insert方法,有两个需要注意的点: it = vec.insert(it,num); 如果后序还要继续插入的话, 就必须将insert的结果重新赋值给it, 否则如果没有重新赋值而直接继续使用it的话,会导致段错误, 这里因为已经不需要继续插入了,所以可以用break直接跳出,无需赋值 插入时,如果num比vec里面所有的数都大, 那么会导致插入失败, 此时 ,应使用push_back将num插入到最后 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: vector&lt;int&gt; vec; void Insert(int num) &#123; if(vec.size() == 0)&#123; vec.insert(vec.begin(), num); return; &#125; bool is_insert=false; for(auto it=vec.begin(); it!=vec.end(); it++)&#123; if(*it &gt; num)&#123; vec.insert(it,num); is_insert=true; break; &#125; &#125; if(!is_insert) vec.push_back(num); &#125; double GetMedian() &#123; if(vec.size() == 0) return 0; int x1 = vec.size()/2; int x2 = (vec.size()-1)/2; return (vec[x1]+vec[x2])/2.0; &#125;&#125;; 解法二:插入的时候不考虑排序,在查找中位数时可以使用基于Partition的方法,时间复杂度为 $O(n)$. 解法三:AVL树插入时间复杂度为 $O(logn)$ 找中位数时间复杂度为 $O(1)$ 解法四(牛客):用大顶堆和小顶堆思路: 如果能够保证数据容器左边的数据都小于右边的数据，这样即使左、右两边内部的数据没有排序，也可以根据左边最大的数及右边最小的数得到中位数。如何快速从一个容器中找出最大数？用最大堆实现这个数据容器，因为位于堆顶的就是最大的数据。同样，也可以快速从最小堆中找出最小数。 因此可以用如下思路来解决这个问题：用一个最大堆实现左边的数据容器，用最小堆实现右边的数据容器。往堆中插入一个数据的时间效率是 O(logn)。由于只需 O(1)时间就可以得到位于堆顶的数据，因此得到中位数的时间效率是 O(1)。 首先要保证数据平均分配到两个堆中，因此两个堆中数据的数目之差不能超过 1 还要保证最大堆中里的所有数据都要小于最小堆中的数据 当数据的总数目是偶数时，按照前面分配的规则会把新的数据插入到最小堆中。如果此时新的数据比最大堆中的一些数据要小，怎么办呢？ 可以先把新的数据插入到最大堆中，接着把最大堆中的最大的数字拿出来插入到最小堆中。由于最终插入到最小堆的数字是原最大堆中最大的数字，这样就保证了最小堆中的所有数字都大于最大堆中的数字。 当需要把一个数据插入到最大堆中，但这个数据小于最小堆里的一些数据时，这个情形和前面类似。 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: priority_queue&lt;int, vector&lt;int&gt;, std::less&lt;int&gt; &gt; q_max; priority_queue&lt;int, vector&lt;int&gt;, std::greater&lt;int&gt; &gt; q_min; void Insert(int num) &#123; if( q_max.size()&gt; q_min.size() )&#123; q_max.push(num); int tmp = q_max.top(); q_max.pop(); q_min.push(tmp); &#125;else&#123; q_min.push(num); int tmp = q_min.top(); q_min.pop(); q_max.push(tmp); &#125; &#125; double GetMedian() &#123; double res; if(q_max.size() == q_min.size())&#123; int x1 = q_max.top(); int x2 = q_min.top(); res = (x1+x2)/2.0; &#125;else&#123; res = q_max.top(); &#125; return res; &#125;&#125;; 插入时间复杂度为 $O(logn)$ 找中位数时间复杂度为 $O(1)$ 64.滑动窗口的最大值题目描述给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 解法一(自想):用最直接的办法, 每次求出滑动窗口内的最大值, 然后存到max_res向量里面, 该方法时间复杂度为 $O(nm)$ . 空间为 $O(n)$ 123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; maxInWindows(const vector&lt;int&gt;&amp; num, unsigned int size) &#123; vector&lt;int&gt; max_res; if(size == 0) return max_res; //无符号整数, 要首先考虑size为0的情况, 否则会导致下面的程序数组越界 for(int i = 0 ; i&lt; num.size()-size+1 ; i++)&#123; int tmp_max; //if(i&lt;num.size()) tmp_max = num[i]; //这里的if语句看起来是多余的, 实际上可以帮助进行数组越界检查, 有助于快速确定bug位置 for(int j = i+1; j&lt; i+size ; j++)&#123; if(num[j] &gt; tmp_max) tmp_max = num[j]; // 这里同样可以进行越界检查, 有助于bug定位, bug修复后可去掉 &#125; max_res.push_back(tmp_max); &#125; return max_res; &#125;&#125;; 解法二(讨论区):使用双端队列deque, 从下标0开始, 一直到n-1, 每次进行如下步骤: 当前元素是否比队列中最后一个元素大, 如果大, 说明队列元素以后也不可能再成为较大值, 直接pop, 如此循环, 直到队列为空或者遇到比当前值大的元素 判断队列中队首的元素是否过期(若队空则直接下一步, 无需判断), 若过期, 则pop, 否则, 不管( 只看队首, 队内的元素是否过期不影响算法, 因为就算过期后面也会将其淘汰) 将当前元素的下标存到队尾 将新的队首元素存到结果向量max_res中 注意: 队列里面存的是下标, 而不是元素本身的值, 后面在提到队列的元素值时, 均是指队列中存储的下标对应的元素值. 时间复杂度分析: 不是 $O(n*szie)$ 而是 $O(n)$ ? 原因: 假设队列里面的正好包含size个元素(最多就为size个), 那么这三个元素对应的值一定是递减的, 因为如果不是递减中, 在进行第一个判断时, 就会将其移除, 此时, 如果新来了一个元素, 如果该元素值小于队列中所有的值, 那么就只可能进行一次判断, 而不是循环size次, 而如果均大于队列中的值, 那么队列中的元素个数就会变成1个, 这样, 在下次进行判断时, 只会与一个元素做判断, 如果是元素值位于中间, 那么下一次做判断的元素个数也会减少一部分, 综上, 内部while循环时, 相对于普通的循环嵌套, 该种循环可以认为是常数级(虽然还是与size的大小有关, 但是总体来说, 要做的判断次数比通常的循环小).12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; maxInWindows(const vector&lt;int&gt;&amp; num, unsigned int size) &#123; vector&lt;int&gt; max_res; deque&lt;int&gt; dq_index; for(int i =0; i&lt; num.size(); i++)&#123; while(!dq_index.empty() &amp;&amp; num[i] &gt; num[dq_index.back()] )&#123; dq_index.pop_back(); &#125; if(!dq_index.empty() &amp;&amp; i-dq_index.front()&gt;= size) dq_index.pop_front(); dq_index.push_back(i); if(i&gt;=size-1) max_res.push_back(num[dq_index.front()]); &#125; return max_res; &#125;&#125;; 65.矩阵中的路径题目描述请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 解法一:这是一个可以用回朔法解决的典型题。首先，在矩阵中任选一个格子作为路径的起点。如果路径上的第i个字符不是ch，那么这个格子不可能处在路径上的第i个位置。如果路径上的第i个字符正好是ch，那么往相邻的格子寻找路径上的第i+1个字符。除在矩阵边界上的格子之外，其他格子都有4个相邻的格子。重复这个过程直到路径上的所有字符都在矩阵中找到相应的位置。 由于回朔法的递归特性，路径可以被开成一个栈。当在矩阵中定位了路径中前n个字符的位置之后，在与第n个字符对应的格子的周围都没有找到第n+1个字符，这个时候只要在路径上回到第n-1个字符，重新定位第n个字符。 由于路径不能重复进入矩阵的格子，还需要定义和字符矩阵大小一样的布尔值矩阵，用来标识路径是否已经进入每个格子。 当矩阵中坐标为（row,col）的格子和路径字符串中相应的字符一样时，从4个相邻的格子(row,col-1),(row-1,col),(row,col+1)以及(row+1,col)中去定位路径字符串中下一个字符如果4个相邻的格子都没有匹配字符串中下一个的字符，表明当前路径字符串中字符在矩阵中的定位不正确，我们需要回到前一个，然后重新定位。 一直重复这个过程，直到路径字符串上所有字符都在矩阵中找到合适的位置 本题一定要注意边界条件即特殊情况的判断: 当矩阵所有元素一样时(这种情况一定要注意先) 当矩阵只有一个元素时(这两种情况要注意, 先进入递归程序, 然后再对flag矩阵进行判断, 否则, 当子串和矩阵大小一样时, 就无法判断到下一个字符是否==’\0’了)- 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123;public: bool hasPath(char* matrix, int rows, int cols, char* str) &#123; if(str[0] == &apos;\0&apos;) return true; int* flag_matrix = new int[rows*cols]; for(int i = 0; i&lt;rows; i++)&#123; for(int j =0 ;j&lt;cols; j++)&#123; flag_matrix[i*cols+j] = 1; &#125; &#125; for(int i = 0; i&lt; rows; i++)&#123; for(int j = 0;j&lt; cols; j++)&#123; if(matrix[i*cols+j] == str[0])&#123; bool is_path = hasPath_helper(matrix,flag_matrix,i,j, rows, cols, str, 0); if(is_path) return true; &#125; &#125; &#125; delete []flag_matrix; return false; &#125; bool hasPath_helper(char* matrix,int* flag_matrix, int i, int j, int rows, int cols, char* str,int x)&#123; if(str[x] == &apos;\0&apos;) return true; if(i&lt;0 || i&gt;=rows || j&lt;0 || j&gt;=cols) return false; if(flag_matrix[i*cols+j] == 0 || matrix[i*cols+j] != str[x]) return false; flag_matrix[i*cols+j] = 0; bool is_path = hasPath_helper(matrix, flag_matrix, i, j-1, rows, cols, str, x+1) || hasPath_helper(matrix, flag_matrix, i-1 , j, rows, cols, str, x+1) || hasPath_helper(matrix, flag_matrix, i, j+1, rows, cols, str, x+1) || hasPath_helper(matrix, flag_matrix, i+1, j, rows, cols, str, x+1); flag_matrix[i*cols+j] = 1; return is_path; &#125;&#125;; 66.机器人的运动范围题目描述地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 解法一:回溯法, 如果当前节点的位数值满足要求, 那么从当前节点开始, 满足要求的格子数字应该等于” 1+左+右+上+下”, 其中方向代表这个方向上的满足要求的格子数. 注意每走过一次格子, 需要将flag矩阵中当前格子的标识设为”已走过(1)”, 并且, 由于此任务是统计符合条件的格子总数, 所以和一般的回溯法不同, 不能在递归结束后将该格子的标识重新复位(否则不同路径上回到同一个格子重复计数). 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: int mc_helper(int threshold,int cur_i, int cur_j, vector&lt; vector&lt;int&gt; &gt;&amp; flag_matrix)&#123; int rows = flag_matrix.size(); int cols = flag_matrix[0].size(); int cur_val = cur_i/10 + cur_i%10 + cur_j/10 + cur_j%10; if(cur_val &gt; threshold || cur_i&lt;0 || cur_i &gt;=rows || cur_j&lt;0 || cur_j&gt;=cols || flag_matrix[cur_i][cur_j]) return 0; flag_matrix[cur_i][cur_j] = 1; return 1 + mc_helper(threshold, cur_i, cur_j-1, flag_matrix)+ mc_helper(threshold, cur_i, cur_j+1, flag_matrix)+ mc_helper(threshold, cur_i-1, cur_j, flag_matrix)+ mc_helper(threshold, cur_i+1, cur_j, flag_matrix); //flag_matrix[cur_i][cur_j] = 0; //return cur_count; &#125; int movingCount(int threshold, int rows, int cols) &#123; if(rows&lt;=0 || cols&lt;=0) return 0; vector&lt; vector&lt;int&gt; &gt; flag_matrix(rows, vector&lt;int&gt;(cols)); int count = mc_helper(threshold,0, 0, flag_matrix); return count; &#125;&#125;;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>算法刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN-CVPR2015]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CVPR2015-FCN%2F</url>
    <content type="text"><![CDATA[文章: Fully Convolutional Networks for Semantic Segmentation作者: Jonathan Long, Evan Shelhamer, Trevor Darrell 核心亮点全卷积网络: 只有最后一层是全连接层, 并且在针对 object detection 任务进行 fine-tuning 时, 会将该全连接层移除. (但是分类任务仍然需要这一层来输出最后的分类结果) (1) 利用FCN网络进行语义级别的图像分割与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类的方法不同, FCN将全连接层转化为卷积层, 使其可以接受任意尺寸的图像输入, 然后采用反卷积层对卷积层的 feature map 进行上采样, 使它恢复到输入图像相同的尺寸,, 从而可以对每个图像像素都产生一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素的分类. 论文细节摘要本文的主要是建立一个全卷积网络，该网络可以接受任意尺寸的图片（因为没有全连接层，所以无需限定图片尺寸）,同时可以十分高效的输出相应尺寸的结果。本文定义和详细描述了全卷积网络的空间信息，解释了它们可以应用与空间密集型预测任务，并且画出了与之前模型的联系。本文将当前流行的AlexNet、VGGNet和GoogLeNet应用到全卷积网络中去，并对其进行迁移学习。然后，本文定义了一个新的网络，它将一个深层的粗糙layer的语义信息和一个浅层的精化layer的语义信息结合起来，最终生成了一个精确的详细的分割结果。本文的模型在众多数据集上都取得了sota表现（2015） 介绍简要介绍了一下从图像分类到语义分割任务中，卷积网络起到的推进作用。并且指出本文的模型是目前（2015）为止第一个可以训练FANs end to end预测像素，并且支持supervised pre-training的网络模型。 本文模型无需任何前处理或后处理操作，也将不会产生随之而来的负面影响。本文还定义了一种新式的“skip”结构来结合深层粗糙的语义信息和浅层，精化的语义信息。后面会详细介绍 相关工作简单提了一下从分类到实例分割的相关论文。然后从以下几个方面进行了介绍。 Fully convolutional networks： 介绍了全卷积网络的发展的现状，从90年代开始，就已经有人开始使用全卷积网络了，但是全卷积网络相对研究成果还是较少。 Dense prediction with convnets： 目前已经有一些工作将convnets应用到了密集型预测任务。这些方法都包含有以下几点共有特征： 模型较小：模型和容量和感受野都有一定限制。 patchwise training：在预测指定像素时，只将此像素和其周围像素作为输入送入模型里训练，即每一个像素都会作为中心像素被训练来预测这个像素所属的分类。patch-wise的问题无疑非常明显，第一：训练中用的patch会大量重叠，非常低效。第二：由于patch-wise大量的计算量，预测的时候很慢。 后处理：超像素映射，随机field正则化，局部分类 input shifting and output interlacing for dense output as introduced by OverFeat 多尺寸金字塔处理 tanh非线性包含 融合（ensembles） FCN则没有以上机制。 与现有模型不同，本文使用image classification作为有监督的预训练，同时fine-tune全卷积，以期望从整张输入图片中快速且高效的学到相应的特征。目前大多数的模型和方法都不是端到端的。 Fully convolutional networks开始的时候介绍了一下卷积网络是怎么回事，在此不坐赘述。 全卷积网络由于没有了全连接层的限制, 因此可以接受任意尺寸的输入, 并且生成相应的维度形状.dFCN的一个real-valued损失函数定义了一个任务：如果损失函数是最后一层中spatial dimensions的总和 $\ell(x;\theta) = \sum_{ij}\ell’(x_{ij};\theta)$ ，那么它的梯度就会是它所有spatial components的梯度的总和。因此对于在整张图片上 $\ell$ 的sgd就是等于将最后一层所有感受野作为一个minibatch的$\ell’$的sgd。 当这些感受野重合度非常大时，layer-by-layer方式的前向计算和反向计算相比于path-by-patch的方式，就变得十分高效。 Adapting classifiers for dense prediction大多数经典网络结构中都具有全连接层, 该层接受固定尺寸的输入, 同时会输出nonspatial outputs. 这使得全连接层的维度固定, 同时放弃了空间坐标(因为全连接都是一维的). 但是, 这些全连接层同样可以被看做是通过卷积核在整个输入区域上进行卷积操作的网络层. 可以将它们全部强制转换成全连接网络, 以便可以接受任意尺寸的输入, 同时输出对应的分类图谱(classifization maps). 该转换过程如图2所示 最主要的是, 尽管最终生成的图谱和原始网络中的差不多, 但是由于大量的重叠区域, 会使得计算成本分摊, 降低计算量. 比如,]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>网络模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《GPU高性能编程CUDA实战 CUDA By Example》]]></title>
    <url>%2Fz_post%2FCUDA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-GPU%E9%AB%98%E6%80%A7%E8%83%BD%E7%BC%96%E7%A8%8BCUDA%E5%AE%9E%E6%88%98CUDAByExample%2F</url>
    <content type="text"><![CDATA[第一章 为什么需要CUDA1.1 本章目标了解历史和发展历程 1.2 并行处理的历史中央处理器性能的提升逐渐变得困难 1.3 GPU计算的崛起1.4 CUDA1.4.1 CUDA架构是什么cuda架构包含了一个统一的着色器流水线，使得执行通用计算的程序能够对芯片上的每个数学逻辑单元（Arithmetic Logic Unit，ALU）进行排列。另外，NVIDIA在实现ALU时都确保它们满足IEEE单精度浮点数学运算的需求，并且可以使用一个裁剪后的指令集来执行通用计算，而不是仅限于执行图形计算。此外，GPU上的执行单元不仅能任意地读写内存，同时还能访问由软件管理的缓存，也称为共享内存。 CUDA架构的所有这些功能都是为了使GPU不仅能执行传统的图形计算，还能高效的执行通用计算。 1.4.2 CUDA架构的使用NVIDIA专门开发了一款编译器来编译CUDA C语言。现在，用户不再需要了解OpenGL或者DirectX图形编程结构，也不需要将通用计算问题伪装为图形计算问题。 1.5 CUDA的应用医学影像、流体力学等等 第二张 入门2.1 本章目标配置环境 2.2 开发环境 支持CUDA的图形处理器 NVIDIA设备驱动程序 CUDA开发工具箱 标准C编译器 由于CUDA C应用程序将在两个不同的处理器上执行计算，因此需要两个编译器。其中一个编译器为GPU编译代码，另一个为CPU编译代码。 第三章 CUDA简介3.1 本章目标第一段cuda c代码、了解host和device之间的区别、了解其他信息 3.2 第一个程序3.2.1 Hello，World！将CPU以及系统的内存成为主机（host）。而将GPU及其内存成为设备（device）。 在GPU设备上执行的函数通常称为核函数（kernel）。 没有核函数，只考虑在主机运行的CUDA代码和标准的C在很大程度上是没有区别的。 12345int main(void)&#123; printf("Hello, World!\n"); return 0;&#125; 3.2.2 核函数调用在上面的示例中添加核函数 1234567891011#include &lt;iostream&gt;__global__ void kernel(void)&#123;&#125;int main(void)&#123; kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(); printf("Hello,World!\n"); return 0;&#125; 上面的代码有两处新增： 一个空的核函数kernel()，并且带有修饰符__global__ 对这个空的核函数的调用语句，并且带有修饰字符&lt;&lt;&gt;&gt; cuda的host代码默认是由系统的标准C编译器来编译的（如Linux的GNU gcc和Windodws的VS），NVIDIA工具只是将代码交给host编译器，它表现出的行为就好像CUDA不存在一样。 而当遇到具有__global__修饰符的函数时，编译器就会将该函数编译为在device上运行。在此例子中，函数kernel()将被交给编译device代码的编译器，而main()函数将被交给host编译器。 而对kernel()函数的调用语句则使用了一种尖括号和两个数值的方式。这将在后面相似介绍。 3.2.3 传递参数以下代码展示了如何像核函数传递参数并取得返回结果 123456789101112131415161718192021#include "stdio.h"__global__ void add(int a, int b, int *c)&#123; *c = a+b;&#125;int main()&#123; int c; int *dev_c; HANDLE_ERROR(cudaMalloc((void**)&amp;dev_c, size(int))); add&lt;&lt;&lt;1,1&gt;&gt;&gt;(2,7,dev_c); HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); printf("2+7 = %d\n",c); cudaFree(dev_c); return 0;&#125; 以上多行代码包含两个概念： 可以像调用C函数那样将参数传递给核函数 当设备执行任何有用的操作时，都需要分配内存，例如将计算值返回给主机 cudaMalloc()函数：（注意，分配内存的指针不是该函数的返回值，这点与malloc()不同） 参数一： 一个指针，指向用于保存新分配内存地址的变量。注意，由于C语言中，指针传递是本身也是值传递的，所以为了使指针本身的值（不是指针地址指向的值）可以改变，因此在传递时要使用双重指针void**，这样做的主要原因还是因为分配内存的指针最终不是通过函数返回，而是直接改变参数值导致的（如果传的是一重指针，则改变的是pd指向的内存空间的数据，而不是pd本身，所以pd也就不能指向GPU的内存了）。 参数二：分配内存的大小 CUDA C的简单行及其强大功能在很大成都上都是来源于它淡化了主机代码和设备代码之间的差异。然而，程序员一定不能在主机代码中对cudaMalloc()返回的指针进行解引用（Dereference）。主机代码可以将这个指针作为参数传递，对其执行算术运算，甚至可以将其转换为另一种不同的类型。但是，绝对不饿昆虫使用这个指针来读取或写入内存。 CUDA中对设备指针的使用限制总结如下： 可以将cudaMalloc()分配的指针传递给在设备上执行的函数 可以在设备代码中使用cudaMalloc()分配的指针进行内存读写操作 可以将cudaMalloc()分配的指针传递给在主机上执行的函数 不能在主机代码中使用cudaMalloc()分配的指针进行内存读写操作 在主机代码中，可以通过调用cudaMemcpy()来访问设备上的内存。这个函数调用的行为类型与标准C中的memcpy()，只不过多了一个参数来指定设备内存指针究竟是源指针还是目标指针。如，当最后一个参数为cudaMemcpyDeviceToHost时，代表运行时源指针是一个设备指针，而目标指针是以个主机指针。此外还有参数cudaMemcpyHostToDevice和cudaMemcpyDeviceToDevice等，如果源指针和目标指针都是位于主机上，那么可以直接调用标准C的memcpy()函数。 3.3查询设备对于拥有多个支持CUDA的设备，需要通过某种方式来确定使用的是哪一个设备。 首先，我们希望知道在系统中有多少个设备是支持CUDA架构的，并且这些设备能够运行基于CUDA C编写的核函数。要获得CUDA设备的数量，可以调用cudaGetDeviceCount()。12int count;HANDLE_ERROR(cudaGetDeviceCount(&amp;count)); 在调用cudaGetDeviceCount()后，可以对每个设备进行迭代，并查询各个设备的相关信息。CUDA runtime将返回一个cudaDeviceProp类型的结构，其中包含了设备的相关属性。相关属性的含义可见书p20。可以利用cudaGetDeviceProperties()来获得i号设备的属性: 12345678910111213#include &lt;iostream&gt;int main()&#123; cudaDeviceProp prop; int count; cudaGetDeviceCount(&amp;count); for(int i =0 ; i&lt; count; i++)&#123; cudaGetDeviceProperties(&amp;prop, i); //对设备的属性执行某些操作 &#125; std::cout&lt;&lt;count&lt;&lt;std::endl;&#125; 在知道了每个可用的属性以后，接下来就可以进行一些具体的操作，如：1std::cout&lt;&lt;prop.major&lt;&lt;std::endl; 3.4 设备属性的使用根据在cudaGetDeviceCount()和cudaGetDeviceProperties()中返回的结果，我们可以对每个设备进行迭代，来找到我们期望的某些达到要求的设备。但是这种迭代操作执行起来有些繁琐，因此CUDA runtime提供了一种自动方式来执行这个迭代操作。首先，找出希望设备拥有的属性并将这些属性填充到一个cudaDeviceProp结构。1234cudaDeviceProp prop;memset(&amp;prop, 0 , sizeof(cudaDeviceProp));prop.major = 1;prop.minor = 3; 之后，将该结构传递给cudaChooseDevice()，这样CUDA runtime运行时将查找是否存在某个设备满足这些条件，并返回一个设备ID，我们可以将这个设备ID传递给cudaSetDevice()。随后，所有的设备操作都将在这个设备上执行。 12345678910111213141516#include &lt;iostream&gt;using std::cout;using std::endl;int main()&#123; cudaDeviceProp prop; memset(&amp;prop, 0 , sizeof(cudaDeviceProp)); prop.major = 1; prop.minor = 3; int dev; cudaGetDevice(&amp;dev); cudaChooseDevice(&amp;dev, &amp;prop); cout&lt;&lt;"ID:"&lt;&lt;dev&lt;&lt;endl;&#125; 第四章 CUDA C并行编程4.1 本章目标 了解CUDA在实现并行性时采用的一种重要方式。 用CUDAC编写第一段并行代码 4.2 CUDA并行编程4.2.1 矢量求和运算假设有两组数据，将这两组数据中对应的元素两两想加，并将结果保存在第三个数组中。 基于CPU的矢量求和 123456789#define N 10void add(int *a, int *b, int *c)&#123; int tid = 0; //这是第0个CPU，因此索引从0开始 while(tid&lt;N)&#123; c[tid] = a[tid] + b[tid]; tid += 1; // 由于只有一个CPU，因此每次递增1 &#125;&#125; &emsp;&emsp;上面将代码特意写成方便修改为并行代码的形式，如，在双核处理器上，tid的设置可以分别为0和1，tid递增的大小可以改为2等。这就相当于在两个CPU上，一个执行奇数位想加，一个执行偶数位想加。 基于GPU的矢量求和 首先给出mian()函数： 123456789101112131415161718192021222324252627282930#define N 10int main()&#123; int a[N],b[N],c[N]; int *dev_a, *dev_b, *dev_c; //在GPU上分配内存，注意这里要知道为什么使用void** cudaMalloc( (void**)&amp;dev_a, N*sizeof(int)); cudaMalloc( (void**)&amp;dev_a, N*sizeof(int)); cudaMalloc( (void**)&amp;dev_a, N*sizeof(int)); ...//创建a，b数组并赋值 //将数组a，b复制到GPU cudaMemcpy(dev_a, a, N*sizeof(int),cudaMemcpyHostToDevice); cudaMemcpy(dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice); add&lt;&lt;&lt;N,1&gt;&gt;&gt;(dev_a, dev_b, dev_c); //将数组c从GPU复制到CPU cudaMemcpy(c, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost); ...//显式结果 //释放GPU上分配的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0;&#125; 看一下核函数的调用：1add&lt;&lt;&lt;N,1&gt;&gt;&gt;(dev_a, dev_b, dev_c); 尖括号中的两个数值将传递给runtime，作用是告诉runtime如何启动核函数： 第一个参数：表示设备在执行款核函数时使用的并行线程块的数量。 参数二：需要多少个线程格（Grid）（一格表示N个线程块的集合） 我们将每个并行执行环境都称为一个线程块（Block），对于此例，将有N个线程块在GPU上运行（N个运行核函数的副本）。 问题：如何在代码中知道当前正在运行的是哪一个线程块？ 回答：利用变量blockIdx.x 。 这是一个内置变量，在CUDA runtime中已经预先定义了这个变量，无需在代码中声明，该变量中包含的值就是当前执行设备代码的线程块的索引。 接下来是GPU版本的add()函数： 123456__global__ void add(int *a, int *b, int *c)&#123; int tid = blockIdx.x; //计算机该索引处的数据 if(tid &lt; N) c[tid] = a[tid] + b[tid];&#125; 当启动核函数时，我们将并行线程块的数量指定为N。这个并行线程块集合就称为一个“线程格（Grid）”。因此，此例表示我们想要一个一维的线程格，其中每个线程格包含N个线程块，每个线程块的blockInx.x的值都是不同的，cuda会为每个设备代码副本提供不同的blockInx.x。 需要注意的一点是：在启动线程块数组时，数组每一维（N）的最大数量不能超过65535。这是一种硬件限制，如过启动的线程块数量超过了这个限制，那么程序将运行失败。 4.2.2 一个有趣的示例绘制Julia集的曲线 Julia集算法：通过下面的迭代等式对复平面中的点求值。如果在计算某个点时，迭代等式的计算结果是发散的，那么这个点就不属于Julia集合。 Z_{n+1} = Z_n^2 + C 基于CPU的Julia集 先看main函数，它通过工具库创建了一个大小合适的位图图像，接着，将一个指向位图数据的指针传递给了核函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//julia.cpp#include&lt;iostream&gt;#include "opencv2/core/core.hpp"#include "opencv2/highgui/highgui.hpp"using namespace cv;void kernel(Mat&amp; M);int julia(int x, int y);//定义一个通用结构来保存复数值,r为实部，i为虚部struct cuComplex&#123; float r; float i; cuComplex(float a , float b ):r(a),i(b)&#123;&#125; float magnitude2()&#123;return r*r+i*i;&#125; cuComplex operator*(const cuComplex&amp; a)&#123; return cuComplex(r*a.r-i*a.i, i*a.r + r*a.i); &#125; cuComplex operator+(const cuComplex&amp; a)&#123; return cuComplex(a.r+r, a.i+i); &#125;&#125;;int DIM = 800;int main()&#123; cv::Mat M(DIM,DIM,CV_8UC1); kernel(M); imshow("Test",M); //窗口中显示图像 imwrite("E:/灰度图.jpg",M); //保存生成的图片 waitKey(0); //等待按任意键后窗口自动关闭 getchar(); return 0;&#125;void kernel(Mat&amp; M)&#123; for (int y=0;y&lt;M.rows;y++)//遍历每一行每一列并设置其像素值 &#123; for (int x=0;x&lt;M.cols;x++) &#123; int juliaValue = julia(x,y); M.at&lt;uchar&gt;(x,y)=155*juliaValue+100; &#125; &#125;&#125;//判断函数，如果该点属于集合返回1，否则返回0int julia(int x, int y)&#123; const float scale = 1.5; float jx = scale*(float)(DIM/2 - x)/(DIM / 2); float jy = scale*(float)(DIM/2 - y)/(DIM / 2); cuComplex c(-0.8, 0.154); cuComplex a(jx,jy); int i = 0; for(i = 0; i&lt;200; i++)&#123; a = a*a+c; if(a.magnitude2() &gt; 1000) return 0; &#125; return 1;&#125; 下面是kernel核函数对将要绘制的所有点进行迭代，并在每次迭代时调用julia来判断该点是否属于Julia集（“是”则涂红色，“否”则涂黑色）。 该函数首先将像素坐标转换为复数空间的坐标，为了将复平面的原点定位到图像中心，代码将像素位置移动了MID/2，然后，为了确保图像的范围为-1.0到1.0，我们将图像的坐标缩放了DIM/2倍。在计算处复空间中的点之后，需要判断这个点是否属于Julia集。通过迭代判断（本示例迭代200次，在每次迭代完成后，都会判断结果是否超过某个阈值），如果属于集合，就返回1，否则，返回0。最后运行指令g++ julia.cpppkg-config —cflags —libs opencv-o julia生成的效果图如下： 基于GPU的Julia集 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include&lt;iostream&gt;#include &quot;opencv2/core/core.hpp&quot;#include &quot;opencv2/highgui/highgui.hpp&quot;using namespace cv;__global__ void kernel(unsigned char* ptr);__device__ int julia(int x, int y);const int DIM = 800;struct cuComplex&#123; float r; float i; __device__ cuComplex(float a , float b ):r(a),i(b)&#123;&#125; __device__ float magnitude2()&#123;return r*r+i*i;&#125; __device__ cuComplex operator*(const cuComplex&amp; a)&#123; return cuComplex(r*a.r-i*a.i, i*a.r + r*a.i); &#125; __device__ cuComplex operator+(const cuComplex&amp; a)&#123; return cuComplex(a.r+r, a.i+i); &#125;&#125;;int main()&#123; cv::Mat M(DIM,DIM,CV_8UC1); unsigned char bitmap[DIM][DIM]; unsigned char *dev_bitmap; //定义一定char二维数组，用来存储GPU传过来的结果 cudaMalloc(&amp;dev_bitmap, DIM*DIM); dim3 grid(DIM,DIM); kernel&lt;&lt;&lt;grid,1&gt;&gt;&gt;(dev_bitmap); cudaMemcpy(bitmap,dev_bitmap, DIM*DIM , cudaMemcpyDeviceToHost); for (int y=0;y&lt;M.rows;y++) //遍历每一行每一列并设置其像素值 &#123; for (int x=0;x&lt;M.cols;x++) &#123; M.at&lt;uchar&gt;(x,y)=bitmap[y][x]; //M.at&lt;uchar&gt;(x,y)=juliaValue+100; &#125; &#125; imshow(&quot;Test&quot;,M); //窗口中显示图像 waitKey(0); getchar(); return 0;&#125;__global__ void kernel(unsigned char* ptr)&#123; int x = blockIdx.x; int y = blockIdx.y; int offset = x+ y*gridDim.x; int juliaValue = julia(x,y); ptr[offset] = 255*juliaValue;&#125;__device__ int julia(int x, int y)&#123; const float scale = 1.5; float jx = scale*(float)(DIM/2 - x)/(DIM/2); float jy = scale*(float)(DIM/2 - y)/(DIM/2); cuComplex c(-0.8, 0.156); cuComplex a(jx,jy); int i = 0 ; for(i = 0; i&lt;200; i++)&#123; a = a*a +c; if(a.magnitude2() &gt; 1000) return 0; &#125; return 1;&#125; 这里需要注意的是，在程序中指定了多个并行线程块来执行函数kernel。并且，使用了一种新的类型来声明了一个二维的线程格：1dim3 grid(DIM, DIM); 类型dim3并不是标准C定义的类型，它可以表是一个三维数组，至于为什么不直接用二维数组，CUDA开发人员主要是为了日后的扩展，所以用三维数组来表示二维数组，数组的第三维默认为1。下面的代码将线程块grid传递给CUDA运行时：1kernel&lt;&lt;&lt;grid,1&gt;&gt;&gt;(dev_bitmap); 代码中还使用了修饰符__device__，这代表代码将在GPU而不是主机上运行，由于这些函数已声明为__device__，因此只能从其他__device__函数或者__global__函数中调用它们。 通常，我们将在GPU上启动的线程块集合称为一个线程格。从名字的含义可以看出，线程格既可以是一维的线程块集合，也可以是二维的线程块集合。核函数的每个副本都可以通过内置变量blockIdx来判断哪个线程块正在执行它。塌秧，还可以通过内置变量gridDim来获得线程格的大小。 第五章 线程协作5.1 本章目标 了解CUDA C中的线程 了解不同线程之间的通信机制 了解并行执行线程的同步机制 5.2 并行线程块的分解当启动核函数时，我们会指定第一个参数的值，也就是指定多个并行副本，我们将这些兵行副本称为线程块（Block）。尖括号中的第二个参数表示CUDA运行时在每个线程块中创建的线程数量，因此，总共启动的线程数量可按下面的公式计算： N个线程块 \times M个线程每线程块 = N个并行线程5.2.1 矢量求和：重新回顾使用线程块中的并行线程，能够完成一些并行线程块无法完成的工作。 1.使用线程实现GPU上的矢量求和（即在一个线程块内设置多条线程）相较于之前的矢量求和，需要修改两个地方，第一是将下面的代码#1式改成代码#2式：123add&lt;&lt;&lt;N,1&gt;&gt;&gt;(dev_a, dev_b, dev_C);add&lt;&lt;&lt;1,N&gt;&gt;&gt;(dev_a, dev_b, dev_c); 第二是修改索引方式，有限现在只有一个线程块，所以不能再用blockIdx来获取索引了，而应使用线程索引，如下所示：1int tid = threadIdx.x; 完整的代码如下所是：12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;const int N = 10;/*#define N 20*/__global__ void add(int * dev_a, int* dev_b, int* dev_c)&#123; int tid = threadIdx.x; //注意此处使用的是线程索引 if(tid&lt;N) dev_c[tid] = dev_a[tid] + dev_b[tid];&#125;int main()&#123; int a[N]; int b[N]; int c[N]; for(int i=0;i&lt;N;i++) a[i]=i; for(int i=0;i&lt;N;i++) b[i]=i; int* dev_a; int* dev_b; int* dev_c; cudaMalloc(&amp;dev_a, sizeof(int)*N); cudaMalloc(&amp;dev_b, sizeof(int)*N); cudaMalloc(&amp;dev_c, sizeof(int)*N); cudaMemcpy(dev_a,a,sizeof(int)*N, cudaMemcpyHostToDevice); //第一个参数为目的地址，第二个为原地址，第三个为空间大小 cudaMemcpy(dev_b,b,sizeof(int)*N, cudaMemcpyHostToDevice); add&lt;&lt;&lt;1,N&gt;&gt;&gt;(dev_a,dev_b,dev_c); //线程块数为1，每块内的线程数为N cudaMemcpy(c, dev_c, sizeof(int)*N, cudaMemcpyDeviceToHost); for(auto x:c) std::cout&lt;&lt;x&lt;&lt;std::endl;&#125; 2.在GPU上对更长的矢量求和之前在第四章我们提到，由于硬件原因，线程块的数量不能超过65535。 同样，对于启动核函数时每个线程块中的线程数量，也有一定的限制。具体来说，最大的线程数量不能超过设备属性结构中maxThreadsPerBlock域的值。对于很多图形处理器而言，这个限制值是每个线程块512个线程（目前GTX 980Ti为1024个）。通过下面的代码可以获得当前机器上的最大线程数：12345678910111213#include &lt;iostream&gt;int main()&#123; cudaDeviceProp prop; int count; cudaGetDeviceCount(&amp;count); for(int i =0 ; i&lt; count; i++)&#123; cudaGetDeviceProperties(&amp;prop, i); std::cout&lt;&lt;count&lt;&lt;std::endl; std::cout&lt;&lt;prop.maxThreadsPerBlock&lt;&lt;std::endl; &#125;&#125;//output：1024 为了通过并行对长度大于1024的矢量进行相加，必须将线程和线程块结合起来才能实现，因此仍然需要改动两个地方：核函数中的索引计算方法和核函数的调用方式：（使用多个线程块，并且每个线程块包含多个线程） 首先是修改索引计算方法：12int tid = threadIdx.x + blockIdx.x * blockDim.x; 在上面的赋值语句中使用了一个新的内置变量，blockDim。对于所有线程块来说，这个变量是一个常数，保存的是线程块中每一维的线程数量。（回顾第四章，在gridDim中保存了一个类似的值，即在线程格中每一维的线程块数量。但要知道，gridDim是二维的，blockDim是三维的，只是很少用的高维索引值） 另一处修改是核函数调用本身，为了保证最终启动的线程数量不少于预期量，可以通过一种常见的技术来对需要启动的线程块数量进行向上取整，如下所示：1add&lt;&lt;&lt; (N+127)/128, 128 &gt;&gt;&gt; (dev_a, dev_b, dev_c); 上面的代码当N不是128的整数倍时，会启动过多的线程，这时候，判断语句if (tid&lt;N)就表现处作用了，它可以确保进行计算的线程的不会对越过数组边界的内存进行读取或写入：12if (tid &lt; N) c[tid] = a[tid] + b[tid]; 3.在GPU上对任意长度的矢量求和]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DenseNet-xxx]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-DenseNet%2F</url>
    <content type="text"></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>网络模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【置顶】基于深度学习的目标检测]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[前言下面对每篇文章及模型的亮点和优势进行了简单总结, 点击模型标题可以跳转至相关论文解读 (都是按照自己的理解记录的, 难免存在纰漏或者词不达意的地方, 望谅解. ) 基于深度学习的目标检测发展轨迹下面是自RCNN以来有关目标检测的文章，其中，加粗的部分为具有标志性意义的检测模型。 RCNN (CVPR, 2013, 必读) $\longrightarrow$ OverFeat (ICLR, 2014, 必读) $\longrightarrow$ MultiBox (CVPR, 2014) DCN (ICCV, 2017)Mask R-CNN(ICCV, 2017)Couple Net (ICCV, 2017) OHEM (CVPR, 2016)Focal Loss (ICCV, 2017) Speed-Accuracy TradeOff (CVPR, 2017) Cascade R-CNN (CVPR, 2018) RFB Net (ECCV, 2018) 经典模型结构 ResNet (CVPR, 2016) Inception系列V1-V4 FCN (CVPR, 2015) 模型 特点 参数量 备注 AlexNet VGGNet Inception ResNet 其他论文 Group Normalization (ECCV, 2018) Non-local NN (CVPR, 2018) Trick NMS Soft NMS (ICCV, 2017) [Learning NMS(CVPR, 2018)] Softer NMS (Arxiv, 2018) RCNN (CVPR, 2014)本文是一种工程上的艺术成就, 成功的将CNN运用到了目标检测任务当中, 开启了CNN统治目标检测领域的时代.(1) 利用SS (Selective Search)提取候选区域框:本篇文章利用SS(Selective Search) 算法首先生成大约2000个候选区域框 (2) 将CNN用于目标检测任务:CNN拥有十分强大的特征提取能力, 并且无需人为设计特征算子, 对提取出来的每个候选区域框进行CNN计算, 获取到固定长度的特征向量 (3) 利用SVM分类器对候选框分类:训练SVM分类器, 对候选区域框的特征向量进行分类. (4) 使用回归其精细修正候选框位置 注: 以上几个步骤是独立训练的, 这也是RCNN后续改进的空间 OverFeat (ICLR, 2014)(1) Multi-Scale Classification:在分类任务上, 虽然训练时采用和AlexNet相同的multi crop方法, 但是在预测阶段没有使用AlexNet的crop投票策略, 而是提出了Multi-Scale Classification方法, 一句话概括就是 对整个图片以不同的尺寸, 并且对每一个location进行模型预测 (2) 利用了全卷积的思想代替全连接降低了滑动窗口的计算代价, 同时支持任意尺寸的图片输入 (3) 可以用同一个模型完成分类, 定位, 检测任务:同一个模型, 只需要用回归层替换分类层, 即可完成目标定位任务, 同时利用了贪心策略来融合最终的定位结果 MultiBox-CVPR2014(1) 将目标边框检测转化为回归问题:将物体检测问题定义为输出多个bounding box的回归问题. 同时每个bounding box会输出关于是否包含目标物体的置信度, 使得模型更加紧凑和高效 (2) 通过损失函数将检测器训练过程整合到神经网络内部:将训练bounding box检测器作为整个网络训练过程的一部分, 也就是说在损失函数中包含了关于bounding box的损失项. 通过联合训练, 不仅利用了神经网络强大的特征表示能力, 而且将检测器的训练集成到了网络中 (3) 无类别监督训练, 使得边框推荐复杂度与类别无关, 易于扩展作者将本文的目标边框检测器在无监督的样本下训练, 由于本方法主要完成的功能就是画框, 并不会输出框中包含的物体类别, 因此训练的时候无需知道样本的类别信息. 这也使得该方法的计算复杂度与类别信息几乎无关, 可以轻易的推广到未知的类别当中. (当然也可以进行相关类别的训练, 对每个类别都训练一个检测器, 模型的总参数会随着类别数线性增加) SPPNet-ECCV2014(1) 提出了一种新的池化方法—-空间金字塔池化SPP: 可以接受任意尺寸的输入图片,并生成固定长度的表征向量 可以进行多尺度的联合训练, 提升模型精度 这种池化方法是比较general的, 可以提升不同模型架构的性能(分类任务) (2) 将SPP用于目标检测, 并且使用了先求卷积特征图谱, 后取区域的的策略(并不是首次提出):大大提升了模型训练和预测的速度(在预测阶段, 比RCNN快24~102倍, 同时取得了更好的精度). PS:注1: 在特征图谱上使用检测方法不是该文首次提出, 而SPP的贡献在于结合了deep CNN结构强大的特征提取能力和SPP的灵活性, 使得精度和速度同时提高注2: 相比于RCNN, SPPNet使用了EdgeBoxes( $0.2s/img$ )的方法来进行候选区域推荐, 而不是Selective Search( $1\sim 2s/img$ )注3: SPPNet在ILSVRC2014的目标检测任务上取得第二名, 在图片分类任务上取得第三名 MR-CNN5 DeepBox6 AttentionNet7 FastRCNN大幅提升了RCNN的训练时间和预测时间, 并且在PASCAL VOC 2007 上测试的准确率相差无几. (1) 直接在特征图谱上获取候选框:在用SS算法得到候选框的坐标以后, 将整张图片直接送入卷积网络, 然后, 将所有候选框的坐标映射到特征图谱上, 直接从特征图谱上获取到候选框的特征子图. (2) 提出RoI Pooling层:受到SPPNet的启发, FastRCNN 提出了一种RoI Pooling层, 它可以看作是SPP层在单一pyramid level上的一种特殊情况. RoI Pooling会将不同size的候选区域特征图片转化成固定尺寸的特征向量, 突破了全连接层对输入尺寸的限制. (3) 其他变动: 用softmax分类器替换了svm分类器 用Smooth L1损失替换了L2损失 在全连接层用SVD奇异值矩阵分解来降低计算成本 在训练SVM的时候不需要额外的硬盘存储特征 DeepProposal9#10 FasterRCNN(RPN)(1) Region Proposals Network:抛弃了外部的候选区域推荐算法, 提出RPN网络结构, 将候选区域推荐过程整合到神经网络里面, 至此, 目标检测中的所有过程都被统一到了一个网络中, 可以进行完全的端到端训练. RPN最终推荐的候选区域个数为 $W\times H \times k$ , $W\times H$ 为卷积特征图谱size, $k$ 为anchor boxes的数量. (2) 共享计算:RPN网络与前面的卷积层共享同样的卷积计算结果, 加快了模型速度. 提高了模型的精度. (3) 其他要点: 在预测物体位置时, 不直接在图片的坐标系中预测, 而是专为基于anchor box的位移预测, 这使得问题简化, 同时使得网络更容易学习 12 YOLO v1(1) 将检测问题看做是回归问题对于给定的输入图像, YOLO会使用一个单一的网络 同时 给出bounding box的预测结果和对应的类别概率. (2) 没有Region Proposal的过程YOLO将输入图片划分成 $S\times S$ 的网格, 如果某个物体的中心落在了某个cell里, 那么这个cell就负责该物体的检测. PS:注一: YOLO中采用 $S\times S$ 的网格划分来确定候选框, 这实际上是一种很粗糙的选框方式, 同时也导致了YOLO在面对小目标物以及群落目标物时, 性能较差.(因为YOLOv1的同一个cell无法预测多个目标, 也就是说YOLOv1理论上最多检测出49个物体). G-CNN13 AZNet14 Inside-OutsideNet(ION)15 HyperNet CVPR 1616 OHEM-CVPR2016提出了一种在线的难样例挖掘算法:作者根据每个RoIs的loss的大小来决定哪些是难样例, 哪些试试简单样例, 通过这种方法, 可以更高效的训练网络, 并且可以使得网络获得更小的训练loss. 同时, OHEM还具有以下两个优点: 消除FastRCNN系列模型中的一些不必要这参数 , 这些参数大多都是为了解决难样例问题服务的, 在使用OHEM以后, 不仅无需在对这些超参数进行调优, 同时还能获得更好的性能表现. OHEM算法可以与其他多种提升模型精度的trick相结合, 对于大多数模型(RCNN系列), 在使用了OHEM以后, 都能够获得精度上的提高, 可以看做是一种普适性的提升精度的方法. 注: 在实现OHEM上, 作者为了提升速度和效率, 特意设计了两个RoI网络, 以减少无用的计算. CRAFT18 MultiPatNet(MPN)19 SSD(1) 在不同尺度的feature map上进行预测: YOLO的网格划分法精度较低, 但是速度很快, 而Faster的anchor box法, 精度很高, 但是速度很慢, SSD同时考虑了这两种方法的优劣, 提出了在不同层的feature map上面进行anchor box选取的方法, 并在这些不同尺度的feature map上面进行物体类别检测和box检测. (这一点不同于OverFeat和YOLO, 它们只会在同一个feature map上面进行分类预测和box回归). (2) 添加了一些额外的卷积层来进行预测任务:在对不同尺度的feature map进行预测时, SSD使用了额外的层进行预测. 在这些层上的每一个location, 都会产生响应的box (对于特征图谱的每一个像素点, 都会产生一定数量的anchor box), 并对box进行预测和回归 (3) 默认box和宽高比:在每一个feature map的cell里面, 预测默认框相对于cell的偏移量, 同时预测该box属于每个类别的score. 具体来说, 对于每一个cell(location), 都会有 $k$ 个默认box, 对于这 $k$ 个默认box中的每一个box, 都会计算 $c$ 个类别score和4个相对偏移量. 因此, 对于每一个location, 需要有 $(c+4)k$ 个输出, 也就是需要$(c+4)k$ 个卷积核. 又因为特征图谱的大小为 $mn$, 所以最终的输出为 $(c+4)kmn$, 其中 $kmn$ 为box的数量, $(c+4)$ 为每个box带有的值. (4) 使用了数据增广, 难样例挖掘, atrous算法等trick大幅度提升精度和速度这个其实算不上亮点, 只不过作者确实使用这些技术提升性能不少 GBDNet21 CPF22 MS-CNN23 R-FCN (NIPS, 2016)(1) 利用position sensitive score map将目标位置信息融合进RoI在一般情况下, 分类任务具有平移不变性, 而检测任务却要求对目标的平移做出正确响应. 在Faster RCNN类的方法中RoI pooling之前都是卷积, 具有平移不变性, 但是一旦经过RoI pooling 之后, 后面的网络结果就不再具备平移不变性了. 因此, 本文提出了position sensitive score map来将目标位置的信息融合进RoI. (2) 让更多的层共享计算对于Faster RCNN等基于感兴趣区域的检测方法来说, 实际上是 分成了几个subnetwork, 第一个用来在整张图上做比较耗时的conv, 这些操作与region无关, 是计算共享的. 第二个subnetwork是用来产生候选区域(如RPN), 第三个subnetwork是用来分类或者进一步对box进行回归的, 这个subnetwork和region是有关系的, 衔接在这个subnetwork和前两个subnework中间的就是RoI pooling. 本文希望将耗时的卷积都尽量移到前面共享的subnetwork上面去, 因此与FasterRCNN相比(前91层共享, RoI pooling之后, 后10层不共享)不同, 将ResNet所有的101层都放在的前面共享的subnetwork中, 最后用来进行prediction的卷积只有1层, 大大减少了计算量. Speed-Accuracy TradeOff (CVPR, 2017)本文实现了一个灵活统一的目标检测框架, 并对三个主流的目标检测模型做了详细客观的分析和讨论通过该框架, 本文对目前主流的各个模型(Faster, R-FCN, SSD)影响精确度和速度的各个因素展开了详细的分析和讨论, 以此希望能够帮助从业者在面对真实应用场景时, 能够选择适当的模型来解决问题. 同时, 本文还发现了一些新的trick, 使得可以在保持精度的前提下, 提升模型的速度. PVANET25 DeepID-Net26 NoC27 DSSD(1) 利用反卷积模块向特征图谱中添加更多的上下文信息主要是对SSD的一点改进, SSD使用了不同阶段的卷积特征图谱进行目标检测, 而DSSD受到人体姿态识别任务的启发, 将这些不同阶段的卷积特征图谱通过反卷积模块连接起来, 然后再进行目标检测的预测任务. (2), 预测模块采用Residual模块这个不算是亮点, 不过也是改动之一, 基本来说就说原始的SSD是直接在特征图谱预测结果并计算损失的, 而DSSD在预测之前会先经过一个Residual模块做进一步的特征提取, 然后在进行预测. TDM29 Feature Pyramid Net(FPN)提出了多尺度的特征金字塔结构:将最后一层特征图谱进行不断尽快上采样, 并与每一个金字塔阶级的特征图谱进行加法合并操作, 得到新的表征能力更强的不同金字塔层次的特征图谱, 然后将RoI按照尺寸分别映射到这些特征图谱上, 再在每个特征图谱上进行类别和位置预测. 可以直观感受到, 这种多尺度的特征图谱在面对不同尺寸的物体时, 具有更好的鲁棒性, 尤其是在面对小型物体时. 同时, 这种特征金字塔结构是一种通用的特征提取结构, 可以应用到不同的网络框架中, 显著提高(5~8%)模型的召回率(因为提出了更多不同尺度, 不同特征信息的anchor box), 并且可以广泛提高(2~3%)模型的mAP. YOLO v2YOLO v2 实际上没有提出任何新的想法或者点子, 它的主要贡献在于尝试了当时几乎所有的可以提升模型性能的方法, 并且通过大量实验找出了一个相对比较好的参数组合.(作者在使用这些方法时有一个基本原则—-保持检测速度, 所以YOLOv2最终不仅精度提升, 其检测速度依然很快) (1) 使用了anchor box思想:YOLO v1是基于每个cell的 $B$ 个bounding box来预测物体的, 因此同一个cell只能预测出一种物体, 如果有两个物体的中心都落在了同一个cell里面, 则不能全都检测出来. 而 YOLOv2 结合了anchor box的思想 (不同尺寸和大小的anchor box, 可以检测到不同的物体) , 同时将网络的输入调整到416×416, 经过多个卷积层和池化层以后, 得到了13×13 (5个池化层,缩小32倍)的feature map, 然后在这个feature map上面使用anchor box(作者选了k=5种), 最终会生成 $13\times 13\times 5 = 845$ 个候选区域框(相比于YOLO v1的98个, 多多了). 未使用anchor box: 69.6mAP, 81% recall 使用anchor box: 69.2mAP, 88% recall 下降的原因, 个人觉得是YOLO在采用anchor box获取候选框的同时, 依然采用YOLOv1的训练方法, YOLOv2的损失函数是一个非常复杂的形式, 导致其在更新参数时很容易顾不过来, 因此其出错的概率也相应提升. (2) 在每个卷积层之后都是用了BN (3) 网络舍弃了全连接层, 采用全卷积FCN: (4) 使用了跨层连接借鉴了ResNet恒等映射的思想. 其中一个好处就是使得梯度更容易传播. RON32 DCN-ICCV20171) 引入了可以自调节感受野大小的deformable convolution和deformable RoI 模块该模块通过额外学习一组采样偏移量来决定卷积操作和RoI pooling操作的采样位置, 通过这种方式, 是的网络模型可以根据输入的图谱自动调节感受野的大小的分布. 2) 上面的两种deformable模块均可以无痛的添加到现有模型中由于deformable convolution和deformable RoI 模块并不会改变原始的输入输出大小, 因此可以很轻易的替换到现有网络中, 并且可以有其他多种提升精度的trick想叠加, 在多个视觉任务上(检测, 分割)都表现出色. DeNet Couple Net (ICCV, 2017)在进行区域分类时, 同时使用了全局信息,上下文信息和局部信息综合判断提出了一个新颖的全卷积网络, 并称之为CoupleNet, 它可以在目标检测中结合使用全局和局部信息. 具体来说, CoupleNet会将由RPN网络产生的候选区域送入到coupling module中, 该模块包含两个分支. 第一条分支利用position-sensitive RoI pooling来捕获物体的局部信息, 另一条分支利用RoI pooling对全局上下文信息进行编码. 接着, 我们设计了不同的coupling策略和归一化方法来使用这些不同分支格子的优势. Focal Loss-ICCV2017(1) 分析并指出了One Stage方法精度不高的原因: 极度不平衡的正负样本比例: anchor是一种类似sliding windows的选框方式, 这会使得正负样本的比例接近1000:1, 而且绝大部分负样本都是easy example. 梯度优化过程被easy example过度影响: 这些easy example的loss虽然不高, 但由于数量众多, 最终合起来会对loss有很大的贡献, 从而导致优化的时候过度关注这些easy example, 这样会收敛到一个不够好的结果. (2) 提出了解决正负样本比例和easy example 问题的Focal loss: FL(p_t) = \begin{cases} -(1-p_t)^{\gamma}log(p_t) & 当y=1 \\ -p_t^{\gamma}log(1-p_t) & 当y=0 \end{cases}核心思想很简单, 就是在优化过程中逐渐减低那些easy example的权重, 这样会使得训练优化过程对更有意义的样本有更高的偏置. 所谓easy example指的就是那些预测概率与真实概率十分相近的样本, 这些样本已经被网络很容易切正确的分类了, 所以应该适当减少他们的loss以降低他们对参数更新的影响程度. 以上面的公式为例, 当真实标签为1时, 如果预测概率(假设二分类) $p_t$ 接近于1, 则此样本是easy样本, 因此, 前面的 $(1-p_t)^{\gamma}$ , 就会非常小, 起到了抑制简单样本的作用. 注1: $\gamma$ 的值越大, 则简单样本对于loss的贡献程度越小, 当 $\gamma = 0$ 时, 会退化到普通的交叉熵函数. 注2: 文中在使用 $\gamma$ 参数的情况下, 还是用了另一个参数 $\alpha$ ,如下所示: FL(p_t) = \begin{cases} -\alpha (1-p_t)^{\gamma}log(p_t) & 当y=1 \\ -(1-\alpha) p_t^{\gamma}log(1-p_t) & 当y=0 \end{cases}在经过一系列调参之后, 得到 $\alpha=0.25, \gamma = 2$ 为最优组合. 可以看到, 加在正样本前面的 $\alpha$ 要更小, 个人猜测这是因为使用了Focal Loss之后, 原本在数量上不在优势的前景区域或许在对loss的贡献度上反超了后景区域, 因此, 需要对前景区域赋予更低的权重. (3) 基于Focal Loss设计并实现了RetinaNet PS:注一: 为什么Focal Loss没有用在Two Stage方法上面? 这是因为以RCNN为代表的一系列Two Stage会在区域候选推荐阶段采用两个问题来降低正负样本比例和easy example问题带来的影响: 采用NMS算法将物体位置候选框降低到一到两千个，更重要的是，这一到两千个可能位置并不是随机选取的，它们移除了大量的易分类负样本（背景框） 采用了biased-minibatch的采样策略, 比如，保证正样本和负样本的比例为1：3进行训练（这其实相当于起到了 $\alpha$ 因子的作用 36 Mask R-CNN (ICCV,2017)1) 提出了一个简单,灵活,通用的实例分割模型框架MaskRCNN 在 FasterRCNN 的基础上进行改进, 在模型的head部分引入了一个新的mask预测分支, 在训练阶段, 该分支会与其他分支并行执行, 在测试阶段, 虽然不是并行执行, 但是利用 NMS 减少了需要计算的候选框个数, 因此 MaskRCNN 模型整体增加的额外开销较小. 2) 提出了RoI Align来解决 RoI 与 pooling后的特征图谱之间的不对齐问题Fast/FasterRCNN 原始的 RoIPool 操作在进行池化时, 会进行两次粗糙的量化操作, 这使得池化后的特征图谱与 RoI 中的信息不能很好的对其, 对于像素级任务实例分割来说, 这种非对齐会使得模型性能大大降低, 因此 MaskRCNN 提出用基于双线性插值法的 RoI Align 代替 RoI Pool, 以此来解决非对齐问题. https://www.zhihu.com/people/big-big-stone/posts DSOD38 SMN39 YOLO v340 SIN41 STDN42 RefineDet结合了one-stage方法和two-stage方法各自的优势, 提出了一个基于single-shot的检测模型:模型主要包含两大模块, 分别是anchor精化模块和物体检测模块. 网络采用了类似FPN的思想, 通过 Transfer Connection Block 将特征图谱在两个模块之间传送, 不仅提升了的精度, 同时还在速度方面取得了与one-stage方案相媲美的表现 MegDetDA Faster R-CNNSNIPRelation-Network Cascade R-CNN (CVPR, 2018)本文针对检测问题中正负样本区分的 IoU 阈值选择问题提出了一种新的目标检测框架, Cascade R-CNN周所周知, 在 two-stage 的目标检测模型当中, 需要设置 IoU 阈值来区分正样本和负样本, 通常, 阈值选的越高, 正样本的框就与真实框越接近, 但是这样就会使得正样本的数量大大降低, 训练时容易产生过拟合问题, 而如果阈值选的较低, 就会产生大量的假正例样本. 根据经验和实验证明可知, 当输入的 proposals 和真实框的 IoU 的值, 与训练器训练时采用的 IoU 的阈值比较接近的时候, 训练器的性能会比较好, 为此, 作者提出了一种级联式的阈值训练方法, 先在较低的阈值上训练检测器, 得到具体更高 IoU 的候选框输出, 然后在此基础上进行训练, 不断提升 IoU 的阈值, 这样一来, 最终生成的候选框质量会变得更高 (与真实框的 IoU 更大). 作者提出这种框架的启发来自于图1(c), 整体来说, 输入的 proposals 的 IoU 在经过检测器边框回归以后, 其输出的边框与真实框会有更大的 IoU, 因此可以将这个具有更大 IoU 的框作为下一个检测器的输入, 同时调高训练时的 IoU, 进而得到质量更高的框 RFB Net (ECCV, 2018)本文从感受野大小的角度出发, 提出了 RFB 模块, 可以融合多个感受野特征, 进而提升轻量级网络(SSD)的特征表达能力相比于不断增加模型复杂度(深度,宽度)来增强特征的表达能力, 本文通过一种人工设计的机制来增强轻量级模型的特征表达能力, 以期获得一种既快又好的检测模型. Group Normalization-ECCV2018针对BN对batch size的依赖问题, 提出了一种新的通用型归一化方法提出了一个用于替代BN的简单算法, 称之为GN(Group Normalization). GN将输入图谱的通道分成不同的组, 并且计算每一组的mean和variance, 然后将其进行归一化. GN的计算复杂度与batch size 的大小是相互独立的, 并且它的准确度在不同范围内的batch size下仍然是稳定的. 并且在整体表现和不同任务上的效果均强于其他类型的归一化方法(LN,IN等) SoftNMS (ICCV, 2017)提出了一种NMS的变体, 通过利用该变体, 基本上可以提升任何模型的检测准确率作者们提出了一种新式的NMS算法, 并且利用该算法, 可以普遍提高当前现有模型的召回率(尤其是面对重叠程度大的物体), 同时, 由于可以不增加复杂度的情况下直接用该算法替换传统NMS算法, 因此, 在替换SoftNMS时, 无需更改模型的任何参数, 也无需重新训练模型, 就可以达到提升召回率的作用. (对mAP的提升大约为1%左右) Non-local Neural Networks (CVPR, 2018)1) 提出了 non-local operations 来解决 CNN 网络中的 long-range dependencies 问题传统 CNN 的卷积操作由于输出神经元只会与输入图谱上的一部分区域有关系, 因此, 在面对那些 long-range dependencies 的时候, 往往不能捕获到足够的信息来表征数据, 为此, 作者提出了 non-locl operations, 其相当于构造了一个和特征图谱尺寸一样大的卷积核, 从而可以维持更多信息. 2) non-local module 可以作为一种通用的模块应用在各项任务上作者通过实验证明, non-local 的有效性不仅仅局限于某一类特殊任务(如视频分类), 同时还可以轻易的整合到其他现有模型中, 如将其整合到 MaskRCNN 中, 可以当做是一种 trick 来提供 MaskRCNN 在目标检测/分割, 姿态识别等任务上的性能表现. SofterNMS (Arxiv, 2018)提出了一种新的边框回归损失函数和NMS算法作者提出了一种 基于KL散度的边框回归损失函数, 可以同时学习到边框的形变量和位置变化量. 最终产生的位置变化量会与位置的精确度有很强的联系, 然后将其使用在本文提出的 新的NMS 算法上, 以此提高准确度. 参考文献： https://github.com/hoya012/deep_learning_object_detection Deep Learning for Generic Object Detection: A Survey]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception V1 (GoogLeNet)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-InceptionV1%2F</url>
    <content type="text"><![CDATA[文章: Going Deeper with Convolutions作者: Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich备注: Google, Inception V1 核心亮点摘要文章提出了一个深度卷积神经网络结构，并取名为Inception。该模型最主要的特点在于提高了网络内部计算资源的利用率。在保证计算负载不变的前提下，通过人工设计提升了网络的深度和宽度。该模型基于Hebbian原理和多尺度处理的intuition来提高性能。关于该模型的一个实例正是提交在ILSVRC14上的GoogLeNet，一个22层深的深度网络，主要针对分类和检测任务。 介绍简要介绍了深度学习和神经网络技术近年来在图像分类和目标检测任务的发展。文章主要关注针对计算机视觉的高效深度神经网络，取名为Inception，名字来自于NIN。在文章中，“deep”具有两层含义：第一，指代文章新提出的Inception module，第二是指网络的深度。通常情况下，可以将Inception model视作NIN的“逻辑顶点”。 相关工作略 动机和High Level的考虑提高深度卷积网络最直接的方式就是增加它们的size，包括网络的深度（层数）和宽度（每层的神经元个数），这对于高质量的网络结构来说是ok的，尤其是在拥有大量优质数据的情况下。但是，这种方法存在这两个主要的缺点： 更大的size通常意味着更多的参数，这会使得网络更容易过拟合，尤其是在数据标签有限的情况下。由于获得大量优质数据具有一定难度，因此这往往会称为一个主要的瓶颈。 第二个缺点就是更大的size往往需要消耗更多的计算资源 文章认为解决以上问题的一个经济可行的办法是将全连接层置换成稀疏连接结构，甚至是在卷积内。 目前的硬件结构在面对非均匀分布的稀疏数据结构时，计算效率很低。 为此，文章希望找到一个新的结构，可以更高效的处理稀疏矩阵的运算。 文章通过多个实验验证了Inception模型在面对图像分类和检测问题时，可以取得十分好的效果。但是，对于Inception model是否能够成为其他领域任务的指导原则，还尚未有定论，需要更多的验证和实验才能说明。 框架细节Inception模型的一个核心思想在于找到 卷积网络中的最优局部稀疏结构可以在多大程度上被稠密组件近似和覆盖 。需要注意，由于假设了平移不变性，因此本文的模型将从卷积模块中开始建立，本文所需要做的就是找到一个局部最优结构，然后将这些结构在空间上组合起来。 为了避免path-alignment问题，现在滤波器大小设值为1×1,3×3,和5×5。 由于pooling层的重要性，本文才采用了pooling层。 将上面的Inception模块叠加起来，形成一个整体的模型。 但是直接叠加会使得向量维度剧增，因此，通过1×1卷积来控制维度。 当max pooling层的stride为1时，并不会缩小输出的feature map的size，只会影响depth的值。 关于此结构的一个好处在于它可以提高神经元的个数，同时避免网络不受控制的提升计算机复杂度。 GooLeNet(Inception V1) GoogLeNet(也叫做Inception V1)的网络结构图细节如下： GoogLeNet网络结构明细表解析如下： 0、输入 原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。 1、第一层（卷积层） 使用7x7的卷积核（滑动步长2，padding为3），64通道，输出为112x112x64，卷积后进行ReLU操作经过3x3的max pooling（步长为2），输出为((112 - 3+1)/2)+1=56，即56x56x64，再进行ReLU操作 2、第二层（卷积层） 使用3x3的卷积核（滑动步长为1，padding为1），192通道，输出为56x56x192，卷积后进行ReLU操作 经过3x3的max pooling（步长为2），输出为((56 - 3+1)/2)+1=28，即28x28x192，再进行ReLU操作 3a、第三层（Inception 3a层） 分为四个分支，采用不同尺度的卷积核来进行处理 （1）64个1x1的卷积核，然后RuLU，输出28x28x64 （2）96个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x96，然后进行ReLU计算，再进行128个3x3的卷积（padding为1），输出28x28x128 （3）16个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x16，进行ReLU计算后，再进行32个5x5的卷积（padding为2），输出28x28x32 （4）pool层，使用3x3的核（padding为1），输出28x28x192，然后进行32个1x1的卷积，输出28x28x32。将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x256 3b、第三层（Inception 3b层） （1）128个1x1的卷积核，然后RuLU，输出28x28x128 （2）128个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x128，进行ReLU，再进行192个3x3的卷积（padding为1），输出28x28x192 （3）32个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x32，进行ReLU计算后，再进行96个5x5的卷积（padding为2），输出28x28x96 （4）pool层，使用3x3的核（padding为1），输出28x28x256，然后进行64个1x1的卷积，输出28x28x64。 将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480 第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与3a、3b类似，在此就不再重复。 要点整理说明一下GoogLeNet采用多个卷积核的动机NIN网络和Inception Module这类结构非常看中模型在局部区域的拟合能力。它们认为：一张图像通常具有总体特征和细节特征这两类特征，一般小卷积核能够更好的捕捉一些细节特征，随着深层网络的小卷积不断计算下去，总体特征也会慢慢的被提炼出来，但是这样存在一个问题，那就是在如果只采用小卷积，那么网络结构的前段一般只有细节特征，后段才慢慢有一些总体特征，而我们希望这两方面的特征总是能够一起发挥作用，因此，上面的两种模型考虑采用更多不同尺寸的卷积核来提取特征，并把这些特征连接起来，一起送到后面的网络中去计算，使得网络可以获取到更多的特征信息。 Inception模块还采用了一个stride为1的max pooling层，它的主要功能是减少空间大小，降低过拟合风险（只选最大值，所以值的范围变少了）。 Inception中为什么使用1×1卷积？关于Inception Module，有一种很直接的做法就是将1×1,3×3,5×5卷积和3×3 max pooling直接连接起来，如下面的左图所示，但是这样的话就有个问题，那就是计算量增长太快了。 为了解决这个问题，文章在3×3和5×5的卷积之前，3×3max pooling之后使用了1×1卷积，使其输出的feature map的depth降低了，从而达到了降维的效果，抑制的过快增长的计算量。 1×1卷积的作用是什么？1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如，上一层的输出为 100x100x128，经过具有256个通道的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256，其中，卷积层的参数为128x5x5x256= 819200。而假如上一层输出先经过具有32个通道的1x1卷积层，再经过具有256个输出的5x5卷积层，那么输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256= 204800，大约减少了4倍 1×1的卷积核，在一定程度上可以实现全连接层： 具体的操作是，输入是224x224x3 的图像，假设经过变换之后最后一层是[7x7x512]的，那么传统的方法应该将其展平成为一个7x7x512长度的一层，然后做全连接层，假设全连接层为4096×1000层的（假设有1000个分类结果）。 那么用1×1卷积核怎么做呢，因为1×1卷积核相当于在不同channel之间做线性变换，所以： 先选择7×7的卷积核，输出层特征层数为4096层，这样得到一个[1×1×4096]层的 然后再选择用1×1卷积核，输出层数为1000层，这样得到一个[1×1×1000]层的 用卷积层代替全连接层的好处这样做其实有非常多的好处，比如上面的例子中输入是224x224x3 的图像，如果此时图像变得更大了，变成384x384大小的了，那么一开始按照32作为步长来进行卷积操作，最后还按照这个网络结构能得到一个[6×6×1000]层的，那么前面那个[6×6]有什么用呢，这个代表每一个位置上，其属于1000个分类结果中的打分，所以这在图像分割等领域等领域有着非常重要的作用【之前一篇论文就是用的这种方法Fully Convolutional Networks for Semantic Segmentation】。 Auxiliary Classifier当时Inception网络还是太深了，不好训练，因此网络中还加了两个侧枝，通过中间层的feature map，来得到预测结果（有了ResNet的shortcut以后，这种侧枝用的比较少了）。 为什么使用侧枝？为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。也就是说在测试的时候，只会用最后的softmax结果作为分类依据。 Inception Model有没有使用全连接层？在两个侧枝使用了卷积+FC+FC+SoftmaxActivation的结构，在最后一层使用了全局平均池化+FC+SoftmaxActivation的结构。 LRN：LocalRespNorm]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
        <tag>网络模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++创建对象时new与不new的区别]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E6%97%B6new%E4%B8%8E%E4%B8%8Dnew%2F</url>
    <content type="text"><![CDATA[C++在创建对象的时候可以采用两种方式：（例如类名为Test） Test test 或者 Test* pTest = new Test()。这两种方法都可以实例化一个对象，但是这两种方法有很大的区别，区别在于对象内容所在的内存空间不同，众所周知，内存的分配方式有三种（1）从静态存储区域分配。内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量，static 变量。（2） 在栈上创建。在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束后在将这些局部变量的内存空间回收。在栈上分配内存空间效率很高，但是分配的内存容量有限。（3） 从堆上分配的。程序在运行的时候用 malloc 或 new 申请任意多少的内存，程序员自己负责在何时用 free 或 delete 释放内存。那么当使用Test test给对象分配内存空间的时候，是分配在堆中的还是栈中的呢？ 在不使用new创建对象时，对象的内存空间是在栈中的，其作用范围只是在函数内部，函数执行完成后就会调用析构函数，删除该对象。 而使用new创建对象是创建在堆中的，必须要程序员手动的去管理该对象的内存空间。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIM指令速查及常用技巧]]></title>
    <url>%2Fz_post%2FLinux-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-VIM%E6%8C%87%E4%BB%A4%E9%80%9F%E6%9F%A5%E5%8F%8A%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[VIM 常用指令fa,i,r,o,A,I,R,O 进入编辑模式h,backspace 左移动 l,space 右移动 j 下移动 k 上移动0 移动到行首$ 移动到行末 1$表示当前行的行尾，2$表示当前行的下一行的行尾b 按照单词向前移动 字首e 按照单词向后移动 字尾w 按照单词向后移至次一个字首H 移动到屏幕最上 非空白字M 移动到屏幕中央 非空白字L 移动到屏幕最下 非空白字G 移动到文档最后一行gg 移动到文档第一行v 进入光标模式，配合移动键选中多行Ctrl+f 向下翻页Ctrl+b 向上翻页u 撤销上一次操作.. 回到上次编辑的位置dw 删除这个单词后面的内容dd 删除光标当前行dG 删除光标后的全部文字d$ 删除本行光标后面的内容d0 删除本行光标前面的内容y 复制当前行，会复制换行符yy 复制当前行的内容yyp 复制当前行到下一行，此复制不会放到剪切板中nyy 复制当前开始的n行 全选（高亮显示）：按esc后，然后ggvG或者ggVG 全部复制：按esc后，然后ggyG 全部删除：按esc后，然后dG 二、复制多行任务：将第9行至第15行的数据，复制到第16行 方法1：（强烈推荐）：9，15 copy 16 或 ：9，15 co 16由此可有：：9，15 move 16 或 :9,15 m 16 将第9行到第15行的文本内容到第16行的后面 ubuntu系统, 默认不支持系统剪切板与vim的交互, 需要先安装一个东西: sudo apt-get install vim-gnome 再set clipboard=unnamed 然后就可以使用ggyG了 选中指定行：方法3：把光标移到第9行 shift + v再把光标移动到第15行 Vim 有12个粘贴板依次编号为：0、1、2、…、9、a、”、+，其中 + 号为系统粘贴板，” 为临时粘贴板。系统剪切板中的内容可在其他程序中使用。上面的复制指令都可以配合剪切板进行操作。kj “nyw 复制当前单词到 n 号剪切板（双引号开始）“np 粘贴 n 号剪切板内容到当前位置后“+Y 复制当前行到系统剪切板“+nY 复制当前行往下 n 行到系统剪切板“+p 粘贴系统剪切板内容到当前位置后 “+yy // 复制当前行到剪切板“+p // 将剪切板内容粘贴到光标后面“ayy // 复制当前行到寄存器 a“ap // 将寄存器 a 中的内容粘贴到光标后面 p,P,. 粘贴ddp 当前行和下一行互换位置J 合并行Ctrl+r 恢复刚才撤销（u）的动作Ctrl+z 暂停并退出ZZ 保存离开xp 交换字符后面的交换到前面~ 更换当前光标位置的大小写，并光标移动到本行右一个位置，直到无法移动 Ctrl+e 向下滚动Ctrl+b 向上翻页b 按照单词向前移动 字首B 按照单词向前移动 字首 忽略一些标点符号e按照单词向后移动 字尾E 按照单词向后移动 忽略一些标点符号w 按照单词向后移至次一个字首W 按照单词向后移至次一个字首 忽略一些标点符号 H 移动到屏幕最上 非空白字M 移动到屏幕中央 非空白字L 移动到屏幕最下 非空白字 G 移动到文档最后一行gg 移动到文档第一行( 光标到句尾) 光标到局首{ 光标到段落开头} 光标到段落结尾nG 光标下移动到n行的首位n$ 光标移动到n行尾部n+ 光标下移动n行n- 光标上移动n行 zz将当前行移到屏幕中部，zb移到底部,zt 顶部 * 向下查找同样光标的字符# 向上查找同样光标的字符/code 查找 code 一样的内容，向后?code 查找 code 一样的内容，向前n 查找下一处N 查找上一处ma 在光标处做一个名叫a的标记 可用26个标记 (a~z)`a 移动到一个标记ad`a 删除当前位置到标记a之间的内容:marks 查看所有标记 :q 一般退出:q! 退出不保存:wq 保存退出:w filename 另存为 filename:jumps 历史编辑文档记录 ctrl+i，ctrl+o跳转位置, Ctrl+f 向文件尾翻一屏幕Ctrl+b 向文件首翻一屏幕Ctrl+d 向文件尾翻半屏幕Ctrl+u 向文件首翻半屏幕 i 在光标前I 在当前行首a 在光标后A 在当前行尾部o 在当前行下新开一行O 在当前行上新开一行r 替换当前字符R 替换当前行及后面的字符，直到按esc为止s 从当前行开始，以输入的文本替代指定数目的字符S 删除指定数目的行，并以输入的文本替代ncw,nCW 修改指定数目n的字符nCC 修改指定数目n的行 x 删除当前光标字符X 删除光标前字符nxnX dw 删除到下一个单词开头de 删除到本单词末尾dE 删除到本单词末尾包括标点在内db 删除到前一个单词dB 删除到前一个单词包括标点在内 ndw,nDW 删除光标开始及其后 n-1 个worddw 删除这个单词后面的内容dd 删除光标当前行dG 删除光标后的全部文字d$ 删除本行光标后面的内容d0 删除本行光标前面的内容ndd 删除当前行，以及其后的n-1行x 删除一个字符，光标后X 删除一个字符，光标前Ctrl+u 删除输入模式下的输入的文本 :split 创建新窗口Ctrl+w 切换窗口Ctrl-w = 所有窗口一样高Ctrl-w+方向键 多窗口视图切换 :args 列出当前编辑的文件名:next 打开多文件，使用 n(Next) p(revious) N(ext) 切换:file 列出当前打开的所有文件 替換（substitute） :[range]s/pattern/string/[c,e,g,i]]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《CUDA并行程序设计-GPU编程指南》]]></title>
    <url>%2Fz_post%2FCUDA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-CUDA%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1_GPU%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[前言部分——本书编排： 第一章：从宏观上介绍流处理器（streaming processor）的演变历史。 第二章：介绍并行编程的概念，建立基本认识。 第三章：详尽地讲解CUDA设备及与其紧密相关的硬件和架构。 第四章：介绍了如何在Windows、Mac和Linux等不同操作系统上安装和配置CUDA软件开发工具包。 第五章：介绍CUDA线程模型。 第六章：详细讲解了不同类型内存的工作机制。 第七章：详述了如何在若干任务中恰当地协同CPU和GPU。 第八章：介绍如何在应用程序中编写和使用多GPU。 第九章：对CUDA编程中限制性能的主要因素予以讲解。 第十章：介绍了CUDA软件开发工具包的示例和CUDA提供的库文件。 第十一章：关注构建自己的GPU服务器或者GPU集群时的几个问题。 第十二章：检视多数程序员在开发CUDA应用程序时易犯的错误类型。 第一章 超级计算机简史简介``]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HelloCUDA]]></title>
    <url>%2Fz_post%2FCUDA-CUDA%E7%A4%BA%E4%BE%8B%E5%AD%A6%E4%B9%A0-HelloCUDA%2F</url>
    <content type="text"><![CDATA[1234567891011121314//hellocuda.cu#include &lt;iostream&gt;#include "stdio.h"__global__ void kernel(void)&#123; printf("hello, cvudakernel\n");&#125;int main(void)&#123; kernel&lt;&lt;&lt;1,5&gt;&gt;&gt;(); cudaDeviceReset(); return 0 ;&#125; 在命令行执行12$nvcc hellocuda.cu -o hellocuda$./hellocuda 输出结果：12345hello, cvudakernelhello, cvudakernelhello, cvudakernelhello, cvudakernelhello, cvudakernel]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA示例学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中typeid实现原理和使用方法]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84typeid%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[先好好理解一下C++的typeid运算符到底是什么意思，再问“原理是什么”会比较好。先看这里学习typeid是什么意思：typeid operator针对题主给的例子：int i = 1;const char* name = typeid(i).name(); 这里的typeid(i)根本不需要做任何运行时动作，而是纯编译时行为——它使用变量i的静态类型直接就知道这是对int类型做typeid运算，于是可以直接找出int对应的std::type_info对象返回出来。 If expression is not a glvalue expression of polymorphic type, typeid does not evaluate the expression, and the std::type_info object it identifies represents the static type of the expression. Lvalue-to-rvalue, array-to-pointer, or function-to-pointer conversions are not performed.此时的typeid运算符就跟sizeof运算符的大部分情况一样，只需要编译器算出表达式的静态类型就足够了。算出表达式的静态类型是C++编译器的基本功能了，类型检查、类型推导等许多功能都依赖它。而当typeid运算符应用在一个指向多态类型对象的指针上的时候，typeid的实现才需要有运行时行为。If expression is a glvalue expression that identifies an object of a polymorphic type (that is, a class that declares or inherits at least one virtual function), the typeid expression evaluates the expression and then refers to the std::type_info object that represents the dynamic type of the expression. If the glvalue expression is obtained by applying the unary operator to a pointer and the pointer is a null pointer value, an exception of type std::bad_typeid or a type derived from std::bad_typeid is thrown.实际实现的时候，通常是在类的vtable里会有个slot保存着指向该类对应的std::type_info对象的指针。要形象的理解的话，请参考我在另一个回答里画的图：为什么bs虚函数表的地址（int）(&amp;bs)与虚函数地址（int）(int*)(&amp;bs) 不是同一个？ - RednaxelaFX 的回答可以看到Clang++在LP64上用的vtable布局，不禁用RTTI时，在-8偏移量上的slot就是存typeinfo指针的。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image-Generation-from-Scene-Graphs]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Image_Generation_from_Scene_Graphs%2F</url>
    <content type="text"><![CDATA[本篇论文解读的排版主要参见原文的格式，针对原文中的每一个小节进行展开，有的是对原文的一个提炼和简单概括，有的是对原文中一些要点的补充和说明。 摘要&emsp;&emsp;近几年来（至2018），针对某些特定目标（花，鸟等）的图片生成已经取得了令人激动的研究成果，但是当文本描述中包含多个物体和物体之间的关系时，仍然具有一些困难。 为了克服这一点，本文提出了从场景图来生成图片的方法，该方法可以推理出具体的物体和物体之间的关系。 本文的模型使用“图卷积层”（graph convolution）来处理输入的“图”（graph），通过预测物体的bounding boxes和segmentation masks来计算“场景布局”（scene layout），然后利用“级联精细化网络”（cascaded refinement network）将“场景布局”转换成一张图片输出。在训练网络时，通过对抗式的训练一对儿discriminators来确保生成图片的真实感。 本文在Visual Genome和COCO-Stuff数据集是验证了以上模型的有效性，结合高质量的生成图片、消融实验和人工调研的方法证明了本文提出的模型可以生成含有多个物体的复杂图片。 介绍——IntroductionWhat I cannot create，I do not understand —— Richar Feynman &emsp;&emsp;要想让计算机生成图片，就需要令其对图片有更深刻的理解。 &emsp;&emsp;为了达到以上目标，目前在text to image synthesis领域已经有许多的工作成果。这些模型可以在limited domains内产生十分惊人的结果，但是当文本信息变得复杂起来时，其生成出来的图片就不尽人意了。 &emsp;&emsp;句子通常都是由一个单词接一个单词组成的线性结构，但是，一个复杂的句子，其内部携带的信息，通常需要由基于物体的“场景图”在具体表示，“场景图”中包含物体和物体之间的关系。“场景图”作为表征图片和文本的强有力的工具，常常被用于语义图片检测、提高和评价图片描述领域中。也有关于将自然语言或图片转换成“场景图”的研究。 22,1，31,47,32,36，57,58 &emsp;&emsp;本篇文章的主要研究目的是在“场景图”约束条件下，生成带有多个物体和物体之关系的复杂图片。这一任务也带来了许多新的挑战。首先，必须要找到可以处理场景图的输入方法，对此本文使用了“graph convalution network”，它可以沿着“场景图”的“边”将信息传递。处理完“图”以后，还必须建立图结构的输入与二维图片的输出之间的联系。为此 ，本文通过预测图中所有物体的bounding boxes和segmentation masks构建了“场景布局（scene layout）”。得到“布局”以后，就需要生成图片，本文使用了“cascaded refinement network（CRN）”，它可以不断增大空间尺寸，生成图片。 最后，我们必须确保生成的图片具有一定的真实感并且包含多个可辨识的物体，为此我们针对image patches和generated objects训练了一对儿discriminator网络。另外，所有的模型可以进行端到端的联合训练。 &emsp;&emsp;我们在2个数据集上进行了实验：Visual Genome（提供人工标注的场景图）和COCO-stuff（从真实的物体位置生成场景图）。两个数据集的生成结果都证明了本文提出的方法的可以生成包含多个物体并且反映它们之间关系的复杂图片。同时，还进行了综合的消融实验来验证本文提出的模型中每一部分的有效性。 相关工作——Related Work&emsp;&emsp;生成图片模型 Generative Image Models 目前，生成模型主要可分为三类：Generative Adversarial Networks（GANs）、Variational Autoencoders（VAE）和基于像素的似然法autoregressive approaches。 [12,40,24,38,53] &emsp;&emsp;条件图片生成 Conditional Image Synthesis 通过在生成图片时向GAN网络添加条件的方式来控制最终输出图片的结果。由两种不同方法：一是将条件作为附加信息同时送入到generator和discriminator中，二是强制让discriminator去预测图片的label。本文选择后者。 [10,35,37,42,59,41,43,6,9,21,4,5,20,27,28,55,56,22] &emsp;&emsp;场景图 Scene Graph 表现为有向图，它的结点是物体，边是物体之间的关系。场景图多倍用于图片检索、图片描述评价等，有些工作也场景从文本或图片中生成场景图 [1,47,32,36,57,58,26] &emsp;&emsp;针对图的深度学习 Deep Learning on Graphs 有的工作是对图进行embedding学习，类似于word2vec，但这与本文的方法不同，因为本文在进行一次前向计算时，传过来的图都是新的。与本文方法更相关的是Graph Neural Networks，它可以对任意的图进行处理。 [39,51,14,34,11,13,46,8,49,48,7,19,29,54,2,15,25] 方法——Methond&emsp;&emsp;我们的目标是得到一个模型，该模型的输入描述物体和它们之间关系的“场景图”，输出是基于该场景图的图片。主要的挑战和困难有三个方面：一、必须找到可以处理“场景图”输入的方法;二、确保生成的图片可以真实反映出场景图中缩描述的物体;三、确保生成的图片具有真实感。 &emsp;&emsp;如图2所示，本文通过“image generation network f”将场景图转换成图片。该网络的inputs是场景图 $G$ 噪声变量 $z$ ，ouputs是 $\hat I = f(G,z)$ 。 图2 &emsp;&emsp;场景图经过“图卷积网络”后，会得到每个物体的embedding vectors，如图2和图3所示，每一层“图卷积层”都会沿着图的边将信息混合在一起。 图3 &emsp;&emsp;本文利用从图卷积网络中得到的object embedding vectors来预测每个物体的bounding boxes和segmentation masks。将它们结合起来形成一个“场景图 scene layout”，如图2中心所示，场景布局相当于是场景图和图片中间媒介。 &emsp;&emsp;最终将布局送入到CRN中，生成图片，如图2右边所示，CRN中会不断将布局的尺寸放大，指定生成新的图片为止。本文训练的生成器是CRN网络 $f$ 和两个分辨器 $D_{img}$ 和 $D_{obj}$ ，它们可以确保图片的真实感以及图片中物体的可识别力。关于这部分的详细介绍可以查看后文以及附加材料中的内容。 &emsp;&emsp;场景图 Scene Graphs 给定一个物体类别集合 $C$ 和一个关系集合 $R$ 。一个场景图可以用一个元组 $(O,E)$ 表示，其中 $O \subseteq C$ ， $E \subseteq O \times R \times O$ 。在处理的第一阶段，使用学习好的embedding layer将场景图中的结点和边转换成一个dense vector，就像语言模型中的那样。 &emsp;&emsp;图卷积网络 Graph Convolution Network 为了实现端到端的处理，本文需要一个可以对场景图进行处理的神经网络模型，为此，采用了由若干图卷积层构成的图卷积网络。 本文的图卷积网络与传统的卷积网络的工作方式类似：给定一个input graph，它每个结点和边的vecotrs维度为 $D_{in}$ ，然后经过一层图卷积层以后，就会生成一个新的vector，其维度为 $D_{out}$ 。（输出结点的值是关输入结点周围像素的函数）。 具体来说，对于所有的 $o_i \in O , (o_i,r,o_j) \in E$ ，给定输入向量 $v_i,v_r \in R^{D_{in}}$ 都会计算出输出向量 $v_i^{‘} , v_r^{‘} \in R^{D_{out}}$ 。 对于所有的结点和边，都会使用3个函数： $g_s , g_p , g_o$ ，其接受的输入为一个向量的三元组 $(v_i, v_r, v_j)$。&emsp;计算边的输出向量时，直接使用 $v_r^{‘} = g_p(v_i, v_r, v_j)$ 。而更新结点的值时较为复杂，因为结点往往连接了很多条边。对于每条始于 $o_i$ 的结点，都利用 $g_s$ 去计算候选向量（candidate vector），收集到所有的候选向量以后，将其放置于集合 $V_i^s$ 中。用 $g_o$ 以同样的方式处理止于 $o_i$ 的边。公式表示如下： V_i^s = {g_s(v_i, v_r, v_j) : (o_i, r, o_j) \in E}V_i^o = {g_o(v_j, v_r, v_i) : (o_j, r, o_i) \in E}然后再利用公式 $v_i^{‘} = h(V_i^s \cup V_i^o)$ 计算得到物体 $o_i$ 的输出向量 $v_i^{‘}$ （ $h$ 为池化操作）。有关计算的例子可以看图3。在本文中，函数 $g_s , g_p , g_o$ 的实现采用了一个单一网络，该网络会将输入向量连接起来，然后送到一个多层感知机（MLP）当中。pooling 函数 $h$ 会将输入结果进行平均值池化，然后送到MLP当中。 图4 &emsp;&emsp;场景布局 Scene Layout 为了生成图片，本文利用object embedding vectors去计算场景布局，该布局给出了要生成的图片的2D结构。本文利用图4中的object layout network来预测每个物体的bounding boxes和 segmentation masks，进而生成场景布局。 object layout networks接受形状为 $D$ 的embedding vector $v_i$ ，并把它送入到一个 mask regression network中去预测形状为 $M \times M$ 的soft binary mask $\hat m_i$ ，同时也送到一个 box regression network中去预测bounding box $\hat b_i = (x_0, y_0, x_1, y_1)$ 。 我们将 embedding vectors $v_i$ 和 mask $\hat m_i$ 逐个元素相乘，得到一个masked embedding ， 其shape为 $D \times M \times M$ ，然后，再利用双线性插值法结合物体的bounding box得到一个object layout。将所有的object layout相加，最终得到scene layout。在训练阶段，我们使用ground-truth bounding boxes来计算scene layout，在测试阶段我们使用预先预测好的bounding boxes进行计算。 &emsp;&emsp;级联精细化网络 Cascaded Refinement Network 在给定场景布局以后，本文使用CRN来根据场景布局生成图片。一个CRN网络包含了一系列的convolutional refinement modules，modules之间的spatial resolutoin会不断变大（double），最终达到预定义的图片大小。 每个module都以scene layout（downsampling到当前module接受的大小）和前一层module的输出结果。 这两部分输入沿着channel连接在一起,送到2层3×3的卷积层里，然后利用最近邻插值对结果进行upsampling，之后继续传送到下一个module中。第一个module以scene layout和高斯噪声 $z \sim p_z$ 作为输入。把从最后一个module得到的结果再送到两个final convolution layers中去，生成最终的图片。 &emsp;&emsp;分辨器 Discriminators 本文训练了两个分辨器 $D_{img}$ 和 $D_{obj}$ patch-based image discriminators $D_{img}$ ：确保生成图片的overall appearance是realistic的。利用全卷积网络实现。 object discriminator $D_{obj}$ ：确保图片中的每个物体都是recognizable并且realistic的。分别利用辅助分类器 auxiliary classifier和全卷积网络实现。 &emsp;&emsp;训练 Training 本文将generation network $f$ 和 $D_{img} , D_{obj}$ 联合训练。generation network的训练目标是minimize下面的6个损失函数的权重和： $Box \ loss\ \ L_{box} = \sum_{i=1}^n ||b_i - \hat b_i||_1$ ：计算真实box和预测box之间的L1范式 $Mask\ loss\ \ L_{mask}$ ：计算真实mask和预测mask之间基于像素的交叉熵 $Pixel\ loss\ \ L_{pix} = ||I - \hat I||_1$ ：真实图片和生成图片之间的L范式 $Image\ adversarial\ loss\ \ L_{GAN}^{img}$ ：针对 $D_{img}$ 的损失函数 $Object\ adversarial\ loss\ \ L_{GAN}^{obj}$ ：针对 $D_{obj}$ 的损失函数，确保物体的realistic $Auxiliarly\ classifier\ loss\ \ L_{AC}^{obj}$ ：针对 $D_{obj}$的损失函数，确保物体的recognizable &emsp;&emsp;实现细节 Implementation Details 本文对所有的scene graphs都进行了数据增强，并且添加了特殊的图片间的relationships，可以把每个真实物体与图片物体进行连接，确保所有的scene graphs都是连通的。我们使用Adam训练所有的模型，学习率设置为 $10^{-4}$ ， batch size 设置为32, 迭代次数为一百万次，使用单个Tesla P100训练了3天。 对于每一次minibatch，我们首先更新 $f$ ，而后更新 $D_{img}$ 和 $D_{obj}$ 。对于所有的graph convolution 本文使用ReLU作为激活函数，对于CRN和discriminators 使用Leaky ReLU作为激活函数，同时使用了batch normalization技术。 实验&emsp;&emsp;在实验中，我们将证明本文提出的方法可以生成复杂的图片，并且正确反应场景图中的物体和物体之间的关系。 数据集&emsp;&emsp;COCO 使用2017 COCO-Stuff 数据集，该数据集共有80个物体类别，40K的训练集和5K的验证集，所有的图片标注都具有bounding boxes和segmentation masks 。利用这些标注，本文建立了2D平面上的场景图，总共包含6中人工设定的关系：左边，右边，上边，下边，里面，外面。我们忽略了图片中占图片比例小于2%的物体，使用的图片包含3～8个物体。将COCO val分为val和test两部分。最终，我们得到了24972张训练图片，1024张val图片，2048张test图片。 &emsp;&emsp;Visual Genome 本文使用VG 1.4数据集，它包含108077张图片，并且具有标注好的场景图。将其中的80%用作训练集，10%分别用作val和test，本文仅仅使用在训练集中出现次数大于2000次的物体和大于500次的关系，最终，我们得到的训练集具有178种物体和45种关系类型。我们忽略图片中的小物体，并且使用图片中具有3～30个物体和至少一种关系类型的图片，最终我们得到了62565张图片作训练集，5506张val和5088张test，平均每张图片包含10个物体和5种关系类型。由于VG数据集没有提供segmentation masks标注，所以在使用VG数据集时，我们忽略mask loss 。 定性结果——Qualitative Results&emsp;&emsp;由本文提出的模型生成的图片示例如图5,6所示 图5 图6 消融实验&emsp;&emsp;在消融实验中，如表1所示，我们验证了模型每一部分对最终图片质量的重要性和必要性。文本使用 $inception\ score^2$作为衡量生成图片好坏的标准。 表1 表1 我们测试了以下几种不同的消融模型： &emsp;&emsp;无图卷积 no gconv ：去掉图卷积层，因此boxes和masks会直接从原始的object embedding vectors预测而来。 &emsp;&emsp;无关系 no relationships ：使用图卷积层，但是忽视场景图中的所有“边”，即关系信息。 &emsp;&emsp;无分辨器 no discriminators ：去掉分辨器 $D_{img}$ 和 $D_{pix}$ ，依靠像素回归损失函数 $L_{pix}$ 来引导图片的生成。 &emsp;&emsp;去掉一个分辨器 omit one of the Discriminators ：仅去掉其中一个分辨器 &emsp;&emsp;GT Layout ：除了消融实验外，本文还使用了GT layout来代替 $L_{box} 和 $L_{mask}$ 损失函数。 物体定位 Object Localization&emsp;&emsp;除了关注生成图片的质量外，我们还对本文模型预测到的bounding boxes进行了分析。在表2中，我们展示了object的召回率和2种交并比的分值。另一个评价标准就是多样性。 表2 用户调研 User Studies&emsp;&emsp;作者找来了数名志愿者，让他们根据以下两个评价标准对本文模型的生成结果和StackGAN的结果进行评价。 Caption Matching Object Recall]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>图片生成 image generation</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的复杂度分析]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/31575074 https://zhuanlan.zhihu.com/p/31575074 https://blog.csdn.net/dcrmg/article/details/79652521 https://blog.csdn.net/laolu1573/article/details/79196160 时间复杂度时间复杂度即模型的浮点数运算次数, 可用 FLOPs (FLoating-point OPerations) 来衡量. 单个卷积层的时间复杂度Time \sim O(M^2 K^2 C_{in} C_{out}) $M$ : 每个卷积核输出的特征图谱的边长 $K$ : 每个卷积核的边长 $C_{in}$ : 每个卷积核的输入通道数 $C_{out}$ : 本卷积层具有的卷积核个数, 也即输出通道数 注一: 为了简化表达式, 这里假设输入和输出的特征图谱都是正方形注二: 同样为了简化表达式, 这里省略了偏置项. 网络整体的时间复杂度Time \sim O \Big(\sum_{l=1}^D M^2K^2C_{l-1}C_l \Big) $D$ : 神经网络具有的卷积层层数, 也就是网络的深度 $l$ : 神经网络的第l层卷积层 $C_l$ : 神经网络第l层卷积层的输出通道数, 也就是该层的卷积核个数 示例: 用Numpy实现简单二维卷积: 1234567891011121314def conv2d(img, kernel): height, width, in_channels = img.shape kernel_height, kernel_width, in_channels, out_channels = kernel.shape out_height = height - kernel_height + 1 out_width = width - kernel_width + 1 feature_maps = np.zeros(shape=(out_height, out_width, out_channels)) for oc in range(out_channels): # Iterate out_channels (# of kernels) for h in range(out_height): # Iterate out_height for w in range(out_width): # Iterate out_width for ic in range(in_channels): # Iterate in_channels patch = img[h: h + kernel_height, w: w + kernel_width, ic] feature_maps[h, w, oc] += np.sum(patch * kernel[:, :, ic, oc]) return feature_maps 空间复杂度空间复杂度 (访存量) 包含两部分: 总参数量 + 各层的输出特征图谱 参数复杂度: 模型所有带参数的层的参数总量: $O(K^2C_{l-1}C_{l})$ 特征图谱复杂度: 实时运行过程的每层计算出的图谱大小: $O(M^2C_l)$ Space \sim O\Big( \sum_{l=1}^D K_l^2 C_{l-1} C_l + \sum_{l=1}^D M^2 C_l \Big)注意, 上面的参数复杂度没有带偏置项, 因为这只是计算复杂度, 而不是精确的参数个数. 复杂度对模型的影响复杂度的优化1×1 卷积降维同时优化时间复杂度和空间复杂度 用两个 3×3 卷积 (18个参数) 替代 5×5卷积 (25个参数) 使用 N×1 和 1×N 卷积级联替代 $N×N$ 卷积.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树结构知识点总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-%E6%A0%91%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[注：本文的对各个概念的定义不一定采用常用的标准定义，原因有二： 常用的定义描述在百度百科上即可查到 更多的是想通过自己的理解，用简短的几个关键词语或句子，来点明核心要点 二叉树基本概念和性质二叉树的定义：二叉树中的每个节点至多有2棵子树，并且子树有左右之分，其次序不能任意颠倒。 二叉树的性质： 对于非空二叉树：$N_0 = N_2 +1$ 在于非空二叉树，第k层上的节点数不超过：$2^{k-1}$ 高为h的二叉树，总节点数不超过：$2^h-1$ 具有N个节点的完全二叉树，其高度为： $\lceil log_2(N+1) \rceil$ 或 $\lfloor log_2{N} \rfloor +1$ 对于完全二叉树，如果各个节点按照顺序存储(从 1 开始)，则节点之间编号具有一定关系： 节点 $i$ 的父节点为 $\lfloor \frac{i}{2} \rfloor (i&gt;1)$ $i$的左右子树分别为：$2i$ 和 $2i+1$ （如果$2i/2i+1 \geq N$，则无左/右子树 给定N个节点，能够成$h(N)$ 种不同的二叉树：$h(N)=…$ 设有$i$个枝点，$I$为所有枝点的道路长度总和，$J$为叶的道路长度总和$J=I+2i$ 注意： 二叉树不是度为2的树。 度为2的树至少要有3个节点，而二叉树可以为空;度为2的树的左右子树是相对而言的，当只有一个孩子时，就无须分左右 满二叉树与完全二叉树满二叉树： 除叶子节点外的所有节点均有两个子节点 完全二叉树： 最后一个不满的“满二叉树”，最后一层所有节点集中在左边 二叉树的遍历先根遍历：根节点、左子树、右子树递归实现12345void preOrder(Tree t)&#123; visit(t-&gt;value); //访问根节点 if (t-&gt;left) preOrder(t-&gt;left); //访问左孩子 if (t-&gt;right) preOrder(t-&gt;right); //访问右孩子&#125; 非递归实现对于任一节点，其可看做是根节点，因此直接访问，访问后，若其左孩子不为空，则按相同规则访问其左子树，当访问完左子树之后，再访问其右子树： 对于任一节点P： 访问节点P，并将P入栈 如果P的左孩子不为空，则令P = P-&gt;left，并转向第一步。若为空，则将P出栈，并令P = P-&gt;right，然后转向第一步。 直到P为nullptr并且栈为空时，结束循环 123456789101112131415vector&lt;TreeNode*&gt; preOrder;if(pRoot == nullptr) return preOrder;stack&lt;TreeNode*&gt; s_node;TreeNode* P = pRoot;while(!s_node.empty() || P!=nullptr)&#123; while(P!=nullptr)&#123; preOrder.push_back(P); // visit P s_node.push(P); P = P-&gt;left; &#125; if(!s_node.empty())&#123; P = s_node.top(); s_node.pop(); P = P-&gt;right; // go to right child &#125;&#125; 中根遍历：左子树、根节点、右子树递归1234567void in_order(TreeNode* pRoot)&#123; if(pRoot != nullptr)&#123; in_order(pRoot-&gt;left); visit(pRoot); in_order(pRoot-&gt;right); &#125;&#125; 非递归对于任一节点，优先查看其左孩子，而左孩子节点又可以看作是一个根节点，则继续优先查看其左孩子，直到遇到左孩子节点为空的根节点才进行访问。然后再转向查看其右孩子，右孩子可看作是一个根节点，继续按上面的规则循环： 对于任一节点P： 若其左孩子不为空，则就P入栈并将P的左孩子置为当前的P，然后继续查看当前P的左孩子，直到为空； 经过上一步后，P指向了空的左孩子，因此取栈顶元素并进行出栈操作，同时访问该栈顶节点，然后将P置为栈顶节点的右孩子（无需判断右孩子是否为空，若为空则下一次循环会自动继续取栈顶） 知道P为nullptr并且栈为空时循环结束 1234567891011121314vector&lt;TreeNode*&gt; inOrder;if(pRoot == nullptr) return inOrder;stack&lt;TreeNode*&gt; stack_node;TreeNode* P = pRoot;while(!stack_node.empty() || P !=nullptr)&#123; while(P!=nullptr)&#123; stack_node.push(P); P = P-&gt;left; &#125; if(!stack_node.empty())&#123; P = stack_node.top(); stack_node.pop(); inOrder.push_back(P); // visit P P = P-&gt;right;&#125; 后根遍历：左子树、右子树、根节点递归1234567void in_order(TreeNode* pRoot)&#123; if(pRoot != nullptr)&#123; in_order(pRoot-&gt;left); in_order(pRoot-&gt;right); visit(pRoot); &#125;&#125; 非递归后序遍历的非递归实现是最难的一种。因为在后序遍历中，要保证左孩子和右孩子都已被访问，并且左孩子在右孩子之间访问，最后才能访问根节点。有两种思路： 思路一：思路二：二叉排序树（Binary Sort Tree, BST）基本概念和性质定义： 也叫二叉查找树或有序二叉树。当树不为空时，该树具有如下性质： 左子树上的所有节点值，均小于其根节点的值 右子树上的所有节点值，均大于其根节点的值 左、右子树也分别为二叉排序树 没有键值相等的节点 性质： 对二叉排序树进行中根遍历，即可得到一串有序数列（从小到大） 时间复杂度： 插入与查找的复杂度均为$O(logn)$，但在最坏情况下为$O(n)$（原因在于树不一定是平衡的） 平衡二叉书(AVL树, 名字来源于其发明者 Adelson-Velsky 和 Landis)]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的封装、继承、多态、重载、重写基本概念解析]]></title>
    <url>%2Fz_post%2FCpp-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-Cpp%E4%B8%AD%E7%9A%84%E5%A4%9A%E6%80%81%E3%80%81%E9%87%8D%E8%BD%BD%E3%80%81%E9%87%8D%E5%86%99%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[面向对象的三个基本特征封装继承多态实现多态具有两种方法：重载overload（利用参数列表）和覆盖override（利用virtual关键字） C++中的override关键字从C++11起，引入了新的关键字override，主要用于紧随成员函数声明或定义内成员函数的声明器之后使用。 在成员函数声明或定义中，override确保该函数为虚并覆写（overrride）来自基类的虚函数。override是在成员函数声明器后使用时才拥有特殊含义的标识符，其他情况下不是保留的关键字。 123456789101112struct A&#123; virtual void foo(); void bar();&#125;;struct B : A&#123; void foo() const override; // 错误： B::foo 不覆写 A::foo（签名不匹配） void foo() override; // OK ： B::foo 覆写 A::foo void bar() override; // 错误： A::bar 非虚&#125;; C++中的final关键字指定派生类不能覆写虚函数，或类不能被继承。 在虚函数声明或定义中使用时，final确保函数为虚且不可被派生类覆写，否则程序生成编译时错误。 在类定义中使用时，final指定此类不可出现于另一类的定义的 base-specifier-list 中（换言之，不能从它派生出其他类），否则程序生成编译时错误。 final是在用于成员函数声明或类头部时有特殊含义的标识符，其他语境中它非保留关键字，可用于命名对象或函数。 1234567891011121314151617181920struct Base&#123; virtual void foo();&#125;;struct A : Base&#123; void foo() final; // A::foo 被覆写且是最终覆写 void bar() final; // 错误：非虚函数不能被覆写或是 final&#125;;struct B final : A // struct B 为 final&#123; void foo() override; // 错误： foo 不能被覆写，因为它在 A 中是 final&#125;;struct C : B // 错误： B 为 final&#123;&#125;;]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入理解TensorFlow架构设计与实现原理》]]></title>
    <url>%2Fz_post%2FTensorFlow-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3TF%2F</url>
    <content type="text"><![CDATA[第一章 TensorFlow系统概述人工智能和深度学习的热潮将Tensoflow推向了很高的地位，本章主要是作为引子，来对TF进行一个概述 1.1 简介1.1.1 产生背景近年来深度学习在图像、视觉和语音领域的突破，促使了各种深度学习框架的诞生 1.1.2 独特价值TF能在众多开源框架中杀出重围，除了Google的背书以外，还少不了以下独特价值： 运算性能强劲：TF1.0利用线性代数编译器XLA全方位的提升了计算性能，XLA帮助TF在CPU、GPU、TPU、嵌入式设备等平台上更快速的运行机器学习模型的训练与推理任务。同时，还提供了大量针对不同软硬件环境的优化配置参数 框架设计通用：TF既提供高层封装API（如Slim、Keras、TF Layers等），又提供底层原生API 支持生产环境部署 语言借口丰富：支持python、C、C++、java、Go等 端云协同计算：支持同时在云侧和端侧运行（移动设备等终端） 1.1.3 版本变迁迭代更新非常快，慢慢走向成熟 1.1.4 与其他主流深度学习框架对比起初，TF的内存消耗和计算速度一直是短板。但是，随着XLA和RDMA等特性的发布，TF的性能在绝大多数情况下都不输于其他深度学习框架。 TF的灵活性导致了学习成本也相应较高，API过于丰富。（不过，可以使用Keras、TF Layers来解决此问题） 1.2 设计目标TF的设计目标并非局限一套深度学习库，Google希望其成为一套面向多种应用场景和编程范式、支持异构计算平台、具备优异性能与可伸缩性的通用人工智能引擎。 1.2.1 灵活通用的深度学习库TF的灵活性主要体现在以下几个方面： 算子定义：粒度更细，数量更多 编程范式：支持声明式编程，将模型的定义和执行解耦 runtime框架 多语言支持 1.2.2 端云结合的人工智能引擎TF对云计算场景的支持是其竞争力的基础，主要体现在以下方面： 提供多种标准化的安装包、构建脚本和容器化封装，支持在不同Linux发行版以及Windows Server等服务器上部署 支持对接多种常见的公有云和私有云服务 兼容若干种常见的高性能计算与通信硬件 灵活的runtime框架设计，既提供标准且易用的PS-worker分布式模式，也允许用户自由开发针对特定环境需求的分布式框架 TF在端侧方面也毫不逊色，主要体现在以下几个方面 推理（预测）态代码能够运行于多种主流的终端平台 通过XLA AOT（ahead of time）编译技术及其他软硬件解耦设计，简化对接 提供量化参数和低精度代数等算法层机制 提供模型与框架一体化的精简版runtime平台 1.2.3 高性能的基础平台软件TF的高性能设计体现在它对高端和专用硬件的深入支持。 1.3 基本架构1.3.1 工作形态TF采用了库模式，其工作形态是有用户编写主程序代码，调用Python或其他语言函数库提供的借口以实现计算逻辑。 1.3.2 组件结构结构示意图可查看书上p13 构成TF的主体使其运行时核心库。 对于普通的python应用层开发者而言，这个核心库就是值通过pip命令等方式安装TF之后，部署到site-packages或类似目录中的动态链接库文件。 生成这个库的C++源代码大致分为3个层次：分布式运行时、公共运行时和算子核函数。其中，公共运行时实现了数据流图计算的基本逻辑，分布式运行时在此基础上实现了数据流图的跨进程协同计算逻辑，算子核函数则包含图上具体操作节点的算法实现代码。 第二章 TensorFlow环境准备2.1 安装可查看官方文档 有一点需要注意：为了保证软件对操作系统和硬件平台的通用性，Google官方发布的TensorFlow whl包没有使用过多的编译优化选项，如 XLA、AVX、SSE等，如果想要打开这些编译优化选项来提升TF的计算性能，那么必须使用源代码编译安装的方式。 2.2 依赖项2.2.1 Bazel软件构建工具Bazel是Google开源的一套软件构建工具，功能定位与CMake、GNU Autotools和Apache Ant等类型，但具有一些独特的优势，如下所示： 多语言支持：C++、Java、Python等 高级构建语言 多平台支持 可重现性 可伸缩性 Bazel使用工作空间、包和目标三层抽象组织待构建的对象 工作空间（workspce）： 包（package）： 目标（target）： 2.2.2 Protocal Buffers 数据结构序列化工具2.2.3 Eigen线性代数计算库2.2.4 CUDA统一计算设备架构CUDA是NVIDIA公司退出的一种用于并行计算的软硬件架构，发布于2007年。该架构以通用计算图形处理器（GPGPU）作为主要的硬件平台，提供一组用于编写和执行通用计算任务的开发库与运行时环境。 CUDA作为软件依赖项提及时，往往指的是CUDA架构中的软件组件，即NVIDIA驱动程序和CUDA Toolkit。除了基本的CUDA开发库和编译器外，CUDA工具包还包括cuBLAS、cuFFT、cuSOLVER、cuDNN等高级算法库，以及IDE、调试器、可视化分析器等开发工具，其中部分组件需要独立安装。 在CUDA架构中，不同层次的软件组件均为开发者提供编程接口，以适应不同类型软件的开发需求： NVIDIA驱动层的开发接口（即cu开头的函数，也称为CUDA Driver API）较为底层，暴露了GPU的若干内部实现抽象。这种接口能够对GPU的运行时行为进行细粒度控制，有助于提升程序的运行时效率，但缺点在于开发过程烦琐。一般的GPU应用程序不会直接使用这一层接口，然而TF内部的GPU计算引擎——StreamExecutor为了追求性能，选择使用这一层接口实现GPU任务调度和内存管理等功能 CUDA开发库的API（即以cuda开头的函数，也称为CUDA Runtime API）是CUDA架构中使用最为广泛的接口，功能涵盖GPU设备管理，内存管理，时间管理以及图形处理相关的逻辑。 cuBLAS、cuDNN等高级算法库：提供了面向通用计算（如线性代数）或领域专用计算（如神经网络）需求的高层次接口。在这个层次，GPU设备的很多技术细节已被屏蔽，开发者可以专注于算法逻辑的设计与实现。TF面向NVIDIA GPU的计算类操作大多基于cuBLAS和cuDNN接口实现。 对于TF而言，CUDA工具包是不受Bazel管理的外部依赖项，因此，用户如果想要使用NVIDIA GPU加速深度学习时，需要事先安装带有NVIDIA驱动程序的CUDA工具包即cuDNN库 2.3 源代码结构2.3.1 根目录TF源码的组织复合Bazel构建工具要求的规范。其根目录是一个Bazel项目的工作空间。 2.3.2 tensorflow目录TF项目的源码主体位于tensorflow目录，该目录下的源文件几乎实现了TF的全部功能，同时体现了TF的整体模块布局。 2.3.3 tensorflow/core 目录TF核心运行时库的源代码位于tensorflow/core目录 2.3.4 tensorflow/python 目录TF Python API的源码位于tensorflow/python目录 2.3.5 安装目录pip命令会将TF运行时所需的Python文件、动态链接库以及必要的依赖项复制到当前Python环境的site-packages或dist-packages目录中，其中TF软件本身的的运行时代码会被部署到tensorflow子目录，这一目录具有与源码tensorflow目录相似的组织结构。二者的不同点在于以下几点： 安装目录中只包含每个模块的Python语言接口文件，不再包含C++源码。所有使用到的C++源码已被编译到了python子目录下的动态链接库文件中（在Linux下为_pywrap_tensorflow_internal.so）。如果某个模块未提供Python API，那么相应的子目录不会在安装目录中出现 安装目录中的python/ops子目录比同名的源代码子目录增加了一系列名称有gen_开头的Python接口文件。这些文件是TensorFLow编译脚本自动创建的，旨在为C++核心库的一部分数据流图操作提供Python编程接口 安装目录比源代码目录多出一个include子目录。这个目录包含了TensorFLow本身以及Protocol Buffers、Eigen等依赖库的C++头文件，允许用户通过编程方式使用核心库的功能。 第三章 TensorFlow基础概念3.1 编程范式：数据流图TF采用了更适合描述深度神经网络模型的声明式编程范式，并以数据流图作为核心抽象。 优势（相比更广泛的命令式编程范式）： 代码可读性强 支持引用透明 提供预编译优化能力 3.1.1 声明式编程与命令式编程二者的最大区别在于：前者强调“做什么”， 后者强调“怎么做”。 声明式编程：结构化、抽象化，用户不必纠结每个步骤的具体实现，而是通过下定义的方式描述期望达到的状态。声明式编程比较接近人的思考模式；程序中的变量代表数学符号或抽象函数，而不是一块内存地址，程序的最终输出仅依赖于用户的输入数据，计算过程不受内部和外部状态影响。 命令式编程：过程化、具体化、用户告诉机器怎么做，机器按照用户的指示一步步执行命令，并转换到最终的停止状态。命令式编程起源于对汇编语言和机器指令的进一步抽象，本身带有明显的硬件结构特征。它通过修改存储器的值、产生副作用的方式实现计算。这里的副作用是值对外部环境产生的附加影响。 编程是一种输入到输出的转换机制，这两种范式提供了截然不同的解决方案： 声明式编程：程序是一个数学模型，输入是自变量，输出是因变量，用户设计和组合一系列函数，通过表达式变换实现计算。 命令式编程：程序是一个有穷自动机，输入是起始状态，输出是结束状态，用户设计一系列指令，通过指令的执行完成状态转换。 适用范围： 声明式编程：DL、AI 命令式编程：交互式UI、OS 3.1.2 声明式编程在DL应用上的优势 代码可读性强 &emsp;&emsp;以目标为导向，更接近于数学公式或人类的思维方式 支持引用透明 &emsp;&emsp;引用透明是指：如果一个函数的语义同他出现在程序中的上下文无关，则称它是引用透明的。关于引用透明的一个推论是：函数的调用语句可以被它的返回值取代，而不影响程序语义。因此，用户可以选择执行任意的模块组合（子图），以得到不同模型结构的输出结果。 提供预编译优化能力 &emsp;&emsp;TF需要实现编译得到完整的数据流图，然后根据用户选择的子图、输入数据进行计算。因此，声明式编程能够实现多种预编译优化，包括无依赖逻辑并行化、无效逻辑移除、公共逻辑提取、细粒度操作融合等。 3.1.3 TensorFlow数据流图的基本概念TF的数据流图是一个 有向无环图 。 图中的节点代表各类操作（opertion），具体包括数学运算、数据填充、结果输出和变量读写等操作，每个节点上的操作都需要分配到具体的物理设备（CPU、GPU）上执行。 图中的有向边描述了节点间的输入、输出关系（也就是各个操作的输入和输出），边上流动（flow）着代表高位数据的张量。 1.节点前向图中的节点统一称为操作，它们根据功能可以分为以下3类： 数学函数或表达式 存储模型参数的变量（variable） 占位符（placeholder） 后向图中的节点同样分为三类： 梯度值 更新模型参数的操作 更新后的模型参数 2.有向边数据流图中的有向边用于定义操作之间的关系，它们分为两类：一类用来传输数据，绝大部分流动着的张量的变都是此类，简称数据边；另一类用来定义控制依赖，通过设定节点的前置依赖决定相关节点的执行顺序，简称控制边。 3.执行原理声明式编程的特点决定了在深度神经网络模型的数据流图上，各个节点的执行顺序并不完全依赖于代码中定义的顺序，而是与节点之间的逻辑关系以及运行时库的实现机制相关。 抛开运行时库内部的复杂实现，数据流图上节点的执行顺序参考了拓扑排序的设计思想，其过程可以简述为以下4个步骤： 以节点名称作为关键字、入度作为值，创建一张散列表，并将次数据流图上的所有节点放入散列表中。 为此数据流图创建一个可执行节点队列，将散列表中入度为0的节点加入到该队列，并从散列表中删除这些节点 依次执行该队列中的每一个节点，执行成功后将此节点输出指向的节点的入度值减1，更新散列表中对应节点的入度值 重复步骤2和步骤3，直到可执行节点队列变为空 3.2 数据载体：张量TF提供Tensor和SparseTensor两种张量抽象，分别表示稠密数据和系数数据。后者旨在减少高维稀疏数据的内存占用。 3.2.1 张量：Tensor与数学和物理学中的张量不同，在NumPy或TF中，通常使用多维数组的形式描述一个张量，数组的维数表示对应张量的阶数。张量的阶数决定了其描述的数据所在高维空间的维数，在此基础上，定义每一阶的长度可以唯一确定一个张量的形状。 TF中的张量形状用python中的列表表示，列表中的每个值依次表示张量各阶的长度（如图片的张量：[128,128,3]）。 TF的张量在逻辑定义上是数据载体，但在物理实现时是一个句柄，它存储张量的元信息以及指向张量数据的内存缓冲区指针。这样设计是为了实现 内存复用。在某些前置操作（生产者）的输出值被输入到多个后置操作（消费者）的情况下，无须重复存储输出值。 1.创建一般情况下，用户不需要使用Tensor类的构造方法直接创建张量，而是通过操作间接创建张量，如constant和add操作等： 1234567import tensorflow as tfa = tf.constant(1.0)b = tf.constant(1.0)c = tf.add(a,b)print([a,b,c])//output: [&lt;tf.Tensor....&gt;,&lt;...&gt;,&lt;...&gt;] 没有执行会话，所以不会输出值，而是输出abc的类型 2.求解3.成员方法Tensor具有eval，get_shape等成员方法 4.操作TF为Tensor提供了abs,add,reduce_mean等大量操作 5.典型用例见书p44 3.2.2 稀疏张量：SparseTensorTF提供了专门用于处理高维稀疏数据的SparseTensor类。该类以键值对的形式表示高维稀疏数据，包含indices、values和dense_shape三个属性。 1.创建在TF中创建稀疏张量时，一般可以直接用SparseTensor类的构造方法，如下：123import tensorflow as tfsp = tf.SparseTensor(indices=[[0,2],[1,3]], values=[1,2],dense_shape=[3,4]) 2.操作TF为稀疏张量提供了一些专门的操作，方便用户处理。 3.典型用例见书p46 模型载体：操作TF中每个节点均对应一个具体的操作。因此，操作是模型功能的实际载体，数据流图主要有以下三种节点： 计算节点：对应的是无状态的计算或控制操作，主要负责算法逻辑表达式或流程控制 存储节点：对应的是有状态的变量操作，通常用来存储模型参数 数据节点：对应的是特殊的占位符操作，用于描述待输入数据的属性 3.3.1 计算节点：OperationOperation类定义在tensorflow/python/framework/ops.py文件中，提供了获取操作的名称、类型、输入张量、输出张量等基本属性的方法。 对于无状态节点，其输出有输入张量和节点操作共同决定 3.3.2 存储节点：Variable存储节点作为数据流图中的有状态节点，其主要作用是在多次执行相同数据流图时存储特定的参数，如深度学习或机器学习的模型参数。 对于有状态的节点，其输出除了跟输入张量和节点操作有关之外，还会受到节点内部保存的状态值的影响。 1.变量TF中的存储节点抽象是Variable类 2.变量操作每个变量对应的变量操作对象在变量初始化时构造，变量支持两种初始化方式： 初始值。根据用户输入或采用缺省值初始化 VariableDef。使用Protocol Buffers定义的变量完成初始化 3.read节点通过解释read节点的实现原理，加深对于变量、变量操作和变量值的理解。 3.3.3 数据节点：Placeholder数据流图本身是一个具有计算拓扑和内部结构的“壳”。在用户向数据流图填充数据前，图中并没有真正执行任何计算。当数据流图执行时，TF会向数据节点填充（feed）用户提供的、复合定义的数据。 TF的数据节点有占位符操作（placeholder Operation）实现，其对应的操作函数是tf.placeholder。针对稀疏数据，TensorFlow也提供了稀疏占位符操作（sparse placeholder operatin），其操作函数是tf.sparse_placeholde。 3.4 运行环境：会话]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[报错module 'tensorflow' has no attribute 'FIFOQueue']]></title>
    <url>%2Fz_post%2FTensorFlow-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95-no_attribute_FIFOQueue%2F</url>
    <content type="text"><![CDATA[报错原因可能是因为当前路径下存在有与tensorlfow官方库相冲突的文件名，解决办法有2个。 1、更改掉有冲突性质的名字这里如果你回忆一下在创建了哪个文件以后产生报错，然后将那个文件的名字更改一下就行了。以我自己为例，我这里创建了一个queue.py的文件，然后运行时就报这个错误了，并且不只是这个文件，在当前路径下的其他py文件也不能正常运行，但是如果换一个文件夹路径，运行其他文件夹下的py文件是没有问题的。所以这里我把queue.py改名成了q.py（其他名字也行），然后在运行，就不会报错了 2、tensorlfow本身问题如果你不管在什么路径下运行任何还有tensorflow代码的文件，都会报错的话，建议使用指令重装TF： 123pip3 uninstall tensorflow-gpu#卸载pip3 install --upgrade tensorflow-gpu #安装]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>踩坑记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StackGAN---ICCV2017]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-StackGAN%2F</url>
    <content type="text"><![CDATA[本篇论文解读的排版主要参见原文的格式，针对原文中的每一个小节进行展开，有的是对原文的一个提炼和简单概括，有的是对原文中没有详细介绍的技术的补充和说明。 摘要&emsp;&emsp;要想从给定的文字描述中生成一幅高质量的图片，是一件十分具有挑战性的事情。而现有的方法（截止至2017年）生成的图片，虽然可以表现出文字中的一些关键信息，但是它们都缺少了很多局部细节。这篇文章提出了一个“堆叠式的生成式对抗神经网络——Stacked Generative Adversarial Networks（StackGAN）”。它可以在根据给定的文本信息，生成像素为256×256的高质量图片。具体的做法是利用“分治”的思想，将一个复杂的问题分解成更易解决的多个小问题。这篇文章将生成图片的过程分为了2个阶段： Stage-1：根据给定的文本信息，生成64×64大小的图片草图，着重于描绘图片中主要目标的轮廓和颜色 Stage-2：结合64×64的图片草图和文本信息，生成256×256的更高质量的图片，这个过程不仅修正了第一阶段中的错误信息，同时还反映了的图片中更多的细节。 &emsp;&emsp;另外，为了提高训练时的稳定性和生成图片的多样性，本文还提出了一种新颖的“条件数据增强技术”，可以确保训练过程的平缓（ 不懂 ） 介绍&emsp;&emsp;作者简要介绍了有关“图像生成”和“GAN”的研究现状和存在的困难，然后阐述了本文的3点主要贡献： 提出了一个“堆叠式的生成式对抗神经网络——StackGAN”。该网络是第一个能够生成256×256图片大小的神经网络模型 提出了一个新颖的“条件数据增强技术”。以此来稳定网络训练过程，同时提高了生成图片的多样性 大量的实验和高质量的实验结果表明本文设计的模型框架是有效的，这对以后的研究和发展具有一定的帮助作用 相关工作 VAE PixelRNN energy-based GAN $S^2$-GAN … 堆叠的生成式对抗神经网络——StackGAN&emsp;&emsp;为了生成高质量的图片，本文提出了一个简单但是十分有效的网络模型，它将生成过程分成了两个阶段： Stage-1：根据给定的文本信息，生成64×64大小的图片草图，着重于描绘图片中主要目标的轮廓和颜色 Stage-2：结合64×64的图片草图和文本信息，生成256×256的更高质量的图片，这个过程不仅修正了第一阶段中的错误信息，同时还反映了的图片中更多的细节。 正文之前——Preliminaries&emsp;&emsp;作者在这里简单介绍了GAN和Conditional GAN的核心原理。 GAN：由两个“子模型”组成，这两个“子模型”不断交替训练，它们的训练目标恰好相反，仿佛实在彼此对抗一般。生成模型G的目标是习得能够代表真实数据分布 $p_{data}$ 的生成概率分布 $p_g$ ，而分辨模型D的目标则是要习得能够准确分辨出 $p_{data}$ 和 $p_g$ 的二分类器。它们之间的关系就像是一场“two-player min-max game”一样，其目标方程可以如下表示： \min_G \max_D V(D,G) = E_{x\sim p_{data}}[logD(x)] + E_{z\sim p_z}[log(1-D(G(z)))] Conditional GAN：是GAN的一种扩展，它令生成器和分辨器均接受一个条件向量 $c$ ，分别写成 $G(z,c)$ 和 $D(x,c)$ ，如此一来，生成器就可以根据不同的条件 $c$ 来限定生成的图片。（ $c$ 通常为图片标签或者图片描述） 条件增强技术——Conditioning Augmentation&emsp;&emsp;如图1所示，文本描述信息 $t$ 首先会通过一个编码器进行编码，生成一个text embedding: $\phi_t$ 。 在之前的工作中，会将text embedding非线性的传送到生成器当中去。但是，由于text embedding通常维度很高（100以上），所以当我们的数据十分有限时，text embedding表现出不连续，稀疏等特点，这对训练生成器来说是不利的。为了缓和这个问题，本文不再将text embedding直接送入的生成器中进行训练，而是随机的从一个独立高斯分布 $N(\mu(\phi_t), \Sigma(\phi_t))$ 当中进行采样，生成新的变量 $\hat{c}$ 。利用该技术，可以从一小部分数据中生成更多的训练数据（pairs）。 &emsp;&emsp;另外，为了更进一步的增强训练时的平滑性，同时为了抑制过拟合，本文还在目标函数中增加了下面的正则惩罚项(KL离散度——KL divergence)： D_{KL}(N(\mu(\phi_t), \Sigma(\phi_t)) || N(0,I))（引入随机性变量 $\hat z$ 的原因，可以考虑是因为同样的特别描述往往可以对应到不同的图片） Stage-1 GAN&emsp;&emsp;通过预训练好的编码器对给定的图片描述进行编码，生成text embedding: $\phi_t$ 。利用Conditioning Augmentation技术根据 $\phi_t$ 对 $N(\mu_0(\phi_t),\Sigma_0(\phi_t))$ 采样得到 $\hat c_0$ 。 最后，利用 $\hat c_0$ 和随机向量 $z$ 进行第一阶段的生成，得到 $G(z,\hat c_0)$ 。 在Stage-1的训练阶段，模型通过交替训练以下2个目标函数来分别优化生成器和分辨器： 生成器， $min(L_{G_0})$ ：L_{G_0} = E_{z\sim p_z,t\sim p_{data}}[log\bigl(1-D_0(G_0(z,\hat c_0), \phi_t)\bigr)] + \lambda D_{KL}\bigl( N(\mu_0 (\phi_t), \Sigma_0(\phi_t)) || N(0,I_0) \bigr) 分辨器， $max(L_{D_0})$ ， $D(I,\phi_t)$ 代表图片 $I$ 在 $\phi_t$ 条件下是真实图片的概率：L_{D_0} =E_{(I_0,t)\sim p_{data}} [logD_0(I_0,\phi_t)] + E_{z\sim p_z,t\sim p_{data}}[log \bigl(1-D_0(G_0(z,\hat c_0), \phi_t) \bigr)] 上面公式中： $I_0$ ：来自真实分布 $p_{data}$ 的图片 $t$ ：来自真实分布 $p_{data}$ 的图片描述 $\phi_t$ ：t经过预训练好的编码器编码后得到的text embedding $z$ ：噪声，从 $p_z$ 分布中随机采样而来（本文用的是高斯分布） $\lambda$ ：惩罚项的权重，本文采用 $\lambda = 1$ $N(\mu_0 (\phi_t), \Sigma_0(\phi_t))$ ：用于生成 $\hat c_0$ 的高斯分布，其期望值和方差都是通过神经网络学习出来的 模型架构 ： &emsp;&emsp;对于生成器 $G_0$ 来说，要想得到文本条件变量 $\hat c_0$ ，首先要将text embedding $\phi_t$ 送到一个全连接层中以此来生成 $\mu_0$ 和 $\sigma_0$ （ $\sigma_0$ 是 $\Sigma_0$ 对角线上的值），得到高斯分布 $N(\mu(\phi_t),\Sigma(\phi_t))$ 。然后从高斯分布中采样得到文本条件变量 $\hat c_0 = \mu_0 + \sigma_0 \cdot \epsilon$ ， $\hat c_0$ 的维度为 $N_g$ 。 这里，“ $\cdot$ ”代表点乘，$\epsilon \sim N(0,I)$ 。之后，将 $\hat c_0$ 和噪声向量 $z$ 连接起来，它们经过一些列“升维块”（upsampling block）之后，会生成大小为 $W_0 \times H_0$ 的图片。 &emsp;&emsp;对与分辨器 $D_0$ 来说，首先利用全连接层将 $\phi_t$ 压缩到 $N_d$ 维，然后，将其在空间上进行复制，形成一个 $M_d \times M_d \times N_d$ 的张量。 同时，将图片送到一系列“降维块”（downsampling block）中，使其具有 $M_d \times M_d \times N_{filter}$ 的空间结构，然后将图片的tensor和文本的tensor沿着channel的维度连接的一起，然后将其送到 $1 \times 1$ 的卷积层当中，联合学习图片和文本之间的关系特征。最后，将特征传送到输出为一个节点的全连接层，得到当前图片与文本属于真实数据的概率。 Stage-2 GAN&emsp;&emsp;从Stage-1 GAN中得到的低分辨率图像通常会缺少一些局部细节，有时候还会造成主要目标物不同程度的形变。另一方面，有些存在于文本中的重要信息，也可能被忽视。 因此，本文的Stage-2 GAN在Stage-1的基础上进行构建。它将Stage-1返回的低分辨率图片和图片描述的text embedding作为GAN的条件，使之返回的结果不仅能修正Stage-1中的错误信息，同时还可以补充在Stage-1中没有捕捉到的信息。 &emsp;&emsp;当选取Stage-1低分辨率结果 $s_0 = G_0(z,\hat c_0)$ 和 高斯变量 $\hat c$ 作为GAN的条件时，生成器和分辨器的目标函数如下所示： 生成器， $min(L_G)$ ：L_G = E_{z\sim p_z,t\sim p_{data}}[log\bigl(1-D(G(s_0,\hat c), \phi_t)\bigr)] + \lambda D_{KL}\bigl( N(\mu (\phi_t), \Sigma(\phi_t)) || N(0,I) \bigr) 分辨器， $max(L_D)$ ， $D(I,\phi_t)$ 代表图片 $I$ 在 $\phi_t$ 条件下是真实图片的概率：L_D =E_{(I,t)\sim p_{data}} [logD(I,\phi_t)] + E_{z\sim p_z,t\sim p_{data}}[log \bigl(1-D(G(s_0,\hat c), \phi_t) \bigr)] &emsp;&emsp;与第一阶段的GAN不同的是，在Stage-2中，本文提出了一个假设，那就是作为Stage-1条件之一的随机变量 $z$ ，可以确保Stage-1的生成结果具有多样性。在这样的假设下，本文在Stage-2阶段并不使用 $z$ 作为条件，而是采用Stage-1的生成结果 $s_0$ 作为条件。 高斯条件变量 $\hat c$ 和 $\hat c_0$ 分别作为Stage-2和Stage-1阶段的CA（Contioning Augmentation），它们共享同一个text embedding—— $\phi_t$ ， $\phi_t$ 由同一个预训练的编码器生成。 但是，Stage-1和Stage-2的CA会通过不同的全连接层，因此，它们生成的关于 $\phi_t$ 的均值和方差不同。 通过这种方式，Stage-2可以学习到被Stage-1所忽略的一些有用的信息。 模型架构： &emsp;&emsp;对于生成器，本文利用残差模块（residual blocks）将Stage-2的生成器设计成一个“编码-解码网络”（encoder-decoder network）。 首先，根据给定的text embedding $\phi_t$ 生成维度为 $N_g$ 的文本条件向量 $\hat c$ ，然后对其进行复制，使之形成形状为 $M_g \times M_g \times N_g$ 的张量。 同时，将Stage-1的结果 $s_0$ 送到若干个“降维模块”（downsampling block）中，直至其size变为 $M_g \times M_t$ 为止。然后将文本特征和图片特征沿着channels连接到一起，并将其送到若干个“残差模块”中去，目的是为了学习到图片和文本交织在一起的多模态表征。最终，通过一系列的“升维模块”（upsampling block），也就是解码器（decoder），生成size为 $W \times H$ 的高分辨率图片。 &emsp;&emsp;对于分辨器，它的结构与Stage-1中的结构相似，只不过由于接受的图片size变大了，所以需要更多的“降维模块”（downsampling block）。为了让分辨器更好的学到图片和文本之间的联系，本文采用了matching-aware discriminator，而非vanilla discriminator，具体可以参考才论文中的参考文献部分。 在训练阶段，正反例构成如下： 正例：真实的图片和与之对应的文本描述 反例：1、真实的图片和不相匹配的文本描述&emsp; 2、生成器生成的图片和与之对应的文本描述 实现细节&emsp;&emsp;“升维模块”由 $3 \times 3 stride 1$ 的卷积层后接最近邻upsampling组成。除了最后的卷积层外，其他卷积层都使用了Batch Normalization和ReLU激活函数。 “残差模块”由 $3 \times 3 stride 1$ 的卷积层、Batch Normalization和ReLU激活函数组成。 在 $128 \times 128$ 的StackGAN模型中，使用了2个“残差模块”，而在 $256 \times 256$ 的StackGAN模型中，使用了4个“残差模块”。 “降维模块”由 $4 \times 4 stride 2$ 的卷积层、Batch Normalization（除第一层）和LeakyReLU组成。&emsp;&emsp;默认情况下，$N_g = 128$ , $N_z = 100$, $M_g = 16$, $M_d = 4$, $N_d = 128$, $W_0 = H_0 = 64$ , $W = H = 256$ 。在训练时，首先固定Stage-2 ，同时迭代训练Stage-1 GAN的 $D_0$ 和 $G_0$ 600 epochs。然后再固定Stage-1 ，同时迭代训练Stage-2 GAN的 $G$ 和 $D$ 600epochs。所有的网络训练时都使用ADAM优化算法，batch size为64,初始的学习率为0.0002 ，学习率每经过100epochs都会衰减成原来值的一半。 实验&emsp;&emsp;为了验证上面提出的模型和方法的有效性，本文进行了大量的高质量实验。本文采用的对照方法是目前（2017）最有效的两个图片生成方法：GAN-INT-CLS和GAWWN 。 测试这两个方法时使用的代码来自于各自的作者。 除此以外，文本还设计了多个baseline models来验证模型整体的设计和每个部分对模型的重要程度。对于第一个baseline，本文直接训练Stage-1 GAN来生成64×64和256×256大小的图片，来探究本文提出的stack结构和CA技术是否对最终结果有帮助。然后修改本文的StackGAN，令其分别生成128×128和256×256大小的图片，来验证在生成更大的图片时，本文的模型是否可以生成取得更好的图片质量。另外，我们还验证了是否有必要在每个Stage都将图片描述作为约束条件之一。 数据集和评价标准&emsp;&emsp;CUB数据集包含200种鸟类，共11788张图片。由于该数据集中80%的鸟的大小与图片大小的比例小于0.5,因此我们需要对其进行剪裁预处理，确保所有的图片中鸟占总大小的比例在0.75以上。Oxford-102数据集包含102种花类，共8189张图片。为了证实本文方法的有效性，同时还对一个更具挑战性的数据集——MS COCO数据集进行了实验， MS COCO数据集中的图片具有不同的背景，同时每张图片会有多个不同的物体， 它具有80k张图片作为训练集，40k张图片作为作为验证集。 COCO中的每张图片具有5条描述，CUB和Oxford-102中的每张图片具有10条描述。 &emsp;&emsp; 评价标准： 目前，关于图片生成还没有很适合的权威的评价标准，在这里，本文采用的是最近比较流行的“inception score”来对生成图片的质量进行评价，公式如下： I = exp(E_x D_{KL}( p(y|x) || p(y)))其中， $x$ 代表生成的图片， $y$ 代表Inception model预测的图片标签。该评价公式背后包含的隐层含义是： 好的模型应该能生成多种多样并且有意义的图片 。因此，边缘分布 $p(y)$ 和条件分布 $p(y|x)$ 之间的KL散度应该越大越好。对于MC COCO数据集，本文直接使用预训练好的模型进行评价，而对于CUB和Oxfor-102数据集，本文先对其进行fine-tune迁移学习，训练出不同的模型，然后，再分别进行评价。评价时，对每个模型都需要大量的样本参与（随机挑选30k以上个生成模型产生的样本）。 &emsp;&emsp;尽管inception score可以表现出类似于人的对“高质量图片”的感知能力，但是它却不能准确反应出生成的图片和图片描述信息之间的相关联系。因此，本文还进行人工评价。本文随机选取了CUB和Oxfor-102测试集中的50条图片描述信息，从MS COCO测试集中随机选取了4k条图片描述。 对于每条描述信息，对应的生成模型都会产生5张图片， 然后让10个用户通过不同的方法对这些图片进行排序。最后用平均排序结果作为最终的人工评价。 定性/定量结果——Quantitative and qualitative results&emsp;&emsp;本文在3个数据集上与现有的两种stage of art方法进行比较，inception scores和平均人工排序结果如下表所示。可以看出，本文的Stack GAN方法在三个数据集上全都取得了最好的结果。 &emsp;&emsp;从下面的图中，也可以看出本文提出的方法，可以生成更加生动的图片，同时生成的图片也具有多样性，而非单纯的“记忆”了训练集中的数据。 成分分析——Component analysis&emsp;&emsp;在本小节中，我们会利用baseline mdoels来分析StackGAN每个成分的作用。下表展示了不同baseline models的inception socres StackGAN的设计： 如表中前四行所示，如果令Stage-1 GAN直接生成256×256的图片，inception scores会大大下降，并且在不使用CA的情况下，甚至无法生成合理的具有含义的图片。即使在使用CA的情况下可以生成含有一定意义的图片，它的图片质量也远不如Stack GAN生成的图片质量，详情如下图所示。这说明了本文提出了Stack结构是有效的。 另外，当把生成图片的分辨率降到128×128以后，inception socres的值从3.70降到了3.35。需要注意的是，inception model接受的图片大小为299×299,所以它在计算inception scores之前，会先将所有输入图片放缩到规定大小后再送入网络进行计算。因此，如果本文的模型仅仅是将128×128的图片放大到256×256，那么，最终计算出的inception scores就不会产生差异（多个线性放缩的叠加会退化成一个放缩计算），这就说明了本文的模型在生成256×256的图片时，相较于128×128的图片，确确实实生成了更多有效的信息。对于256×256的Stack GAN来说，如果仅仅只在Stage-1使用图片描述作为约束条件，inception scores的值会从3.70降到3.45，这说明将Stage-2阶段确实可以捕捉到Stage-1所忽略的文本信息，从而有助于生成高质量的图片。 条件增强——Conditioning Augmentation： 本文同时还探究了CA技术的有效性，在不使用CA技术时，inception scores的值明显下降，并且，在训练时还容易崩溃，导致生成的图片难以识别。而在使用CA时，不仅能提成inception scores的值，还能生成更加多样的鸟（如姿态、脸的朝向等）。 语句嵌入插值——Sentence embedding interpolation： 为了更进一步的证明我们的模型可以学习到更加光滑的潜在的数据信息，本文通过对text embedding线性插值的方式来进行验证。首先固定住噪声变量 $z$ ，这样一来生成的图片仅受图片text embedding约束的影响。下图中第一行的图片使用的text embedding是由我们自己可以制作的句子生成的，这些句子仅仅包含一些简单的颜色描述。结果显示，生成的图片可以根据不同的text embedding生成与之对应的鸟，可以反映出text embedding中描述的颜色，同时还能保持合理的外形。第二行展示了一些更加复杂句子生成的图片，这些句子包含更多的描述信息，用这些句子生成的图片中的鸟，它们的主要颜色会从红色慢慢变成蓝色，而翅膀的颜色会从黑色慢慢的变成棕色。 总结&emsp;&emsp;在本篇文章中，作者提出了一个Stack GAN模型框架，同时结合CA技术，来生成具有照片真实度的图片。 该模型将text-to-image synthesis任务变成了一个“草图——细化”的过程。 Stage-1 GAN主要负责输出图片的“草图”，它侧重于输出图片的基本颜色搭配和主要目标的轮廓信息。 Stage-2 会修正Stage-1中的一些错误，同时很会再次根据图片描述来补充Stage-1中遗漏的信息，从而生成更高质量的图片。 大量的实验表明，本文提出的方法是有效的。 同时，与现有的最领先的方法进行比较后，本文的模型可以生成更高分辨率的图片，同时包含更多的信息和细节。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>图片生成 image generation</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Linux命令行与shell脚本编程大全》]]></title>
    <url>%2Fz_post%2FLinux-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Linux%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Eshell%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深度学习与计算机视觉》]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%2F</url>
    <content type="text"></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy和tensorflow中的关于参数axis的辅助理解方法]]></title>
    <url>%2Fz_post%2FPython-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86-numpy%E5%92%8Ctensorflow%E4%B8%AD%E7%9A%84%E5%85%B3%E4%BA%8E%E5%8F%82%E6%95%B0axis%E7%9A%84%E8%BE%85%E5%8A%A9%E7%90%86%E8%A7%A3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一句话总结：固定除axis之外的其他维度，在axis指定的维度上进行函数操作，轴的计数按shape的形状从左往右为0轴，1轴，2轴…（详解请看下面的推导） 首先声明：axis的默认值不是0，这一点我发现很多博客文章都搞错了。所以一定要知道，axis的默认值不是0，0代表0轴，而默认值是将整个shape拉伸成一个一维向量，在整个向量上求解过 axis的默认值不是0当给axis赋值为0时，和采取默认值时的表现是完全不同的，从下面的代码就可以看出。 1234567891011121314&gt;&gt;&gt; z #大小为2×3×4的数组array([[[ 2, 3, 4, 8], [ 3, 1, 4, 1], [ 6, 3, 2, 6]], [[10, 2, 45, 2], [ 2, 4, 5, 10], [22, 4, 4, 1]]])&gt;&gt;&gt; np.sum(z,axis=0) # axis=0array([[12, 5, 49, 10], [ 5, 5, 9, 11], [28, 7, 6, 7]])&gt;&gt;&gt; np.sum(z) #axis不指定，取默认值154 理解axis参数的作用刚开始学习numpy和tensorflow的朋友经常遇到类似下面这样的一些函数： 123456789101112#pythonx=[[1,2],[5,1]]x=np.array(x)z1=np.max(x,axis=0)z2=np.max(x,axis=1)#tensorflowx=tf.constant([[1.,2.],[5.,2.]]) x=tf.shape(x) z1=tf.reduce_max(x,axis=0)#沿axis=0操作 z2=tf.reduce_max(x,axis=1)#沿axis=1操作 类似的还有argmax，sum等等函数，它们都含有一个名为axis的参数，那这个参数到底是什么意思呢？一句话总结就是：沿着axis指定的轴进行相应的函数操作 直接看这句话可能看不懂，下面用一个最简单的例子来说明一下。 123456789101112import numpy as np#首先，创建一个2×3维的numpy的array数组x=[[2,3,4],[1,2,5]]x=np.array(x)#然后，计算不同参数下np.max的输出print(np.max(x))# 5print(np.max(x,0))# [2,3,5]print(np.max(x,1))# [4,5] 可以看到，如果不知道axis，那么默认就是取得整个数组的最大值，这相当于把多维数组展开成一维，然后找到这个一维数组里的最大值。而当axis=0时，直观上来看就是取得每一列的最大值，源数组总共为2行3列，所以最终的输出包含3个元素。当axis=1时，就相当与是取每一行的最大值。 上面的理解方式在二维数组还比较直观，但是如果数组达到3维4维甚至更高维时，就不能简单的从行列角度出发去理解了，这时应该考虑从“轴”的角度来看。首先，明确一点，“轴”是从外向里的，也就是说，最外层的是0轴，往内一次是1轴，2轴… 。 具体可以看下面的例子： 12345678910&gt;&gt;&gt; zarray([[[ 2, 3, 4, 8], [ 3, 1, 4, 1], [ 6, 3, 2, 6]], [[10, 2, 45, 2], [ 2, 4, 5, 10], [22, 4, 4, 1]]])&gt;&gt;&gt; z.shape(2, 3, 4) 可以看到，这是一个2×3×4的三位数组，其中0轴对应第一维（2），1轴对应第二维（3），2轴对应第三维（4）。当我们指定了函数按某一轴来计算时，函数的输出数组的shape就是去掉当前轴的shape，如下所示。 123456&gt;&gt;&gt; np.max(z,axis=0).shape(3, 4)&gt;&gt;&gt; np.max(z,axis=1).shape(2, 4)&gt;&gt;&gt; np.max(z,axis=2).shape(2, 3) 而对于输出数组的每一个元素output[i][j]的值，实际上就是z[i][...][j]集合中的最大值，如下面的代码所示。其中当axis=0时，输出数组output的shape为3×4，其中output.[2][3]的值，实际上就是z[0][2][3],z[1][2][3]的最大值，也就是（6，1）中的最大值，即为output.[2][3]=6。 再如axis=1时，输出数组output的shape为2×4，其中output.[1][2]的值，实际上就是z[1][0][2],z[1][1][2],z[1][2][2]中的最大值，也就是（45，5，4）中的最大值，即为output.[1][2]=45]。 12345678910&gt;&gt;&gt; np.max(z,axis=0)array([[10, 3, 45, 8], [ 3, 4, 5, 10], [22, 4, 4, 6]])&gt;&gt;&gt; np.max(z,axis=1)array([[ 6, 3, 4, 8], [22, 4, 45, 10]])&gt;&gt;&gt; np.max(z,axis=2)array([[ 8, 4, 6], [45, 10, 22]]) 数学公式总结用形式化的数学语言总结上面的过程就是：对于大小为[i,j,k]的输入数组z，假设axis=0，那么输出矩阵output的大小就为[j,k]，并且output的每一个元素的计算方式如下： x^{y^z}=(1+{\rm e}^x)^{-2xy^w}output[j,k]=\max_{i}(z[i,j,k])如果axis=1，那么输出矩阵output的大小就为[i,k]，并且output的每一个元素的计算方式如下： output[i,k]=\max_{j}(z[i,j,k])对于4维，5维甚至无限维的情况，计算方法是一样的，你不妨自己推导一下，如果有任何问题，欢迎可以在评论中留言。 另外，对于其他的sum，argmax等等函数中的计算方法也是一样的，只需要把函数max换成对应的函数即可，如下所示： sum： output[j,k]=\sum_{i}(z[i,j,k])argmax: output[j,k]=argmax_{i}(z[i,j,k])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github源码：DenseCap]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-DenseCap%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本篇博文是对论文DenseCap的源码实现，作者是斯坦福的Justin Johnson项目地址：https://cs.stanford.edu/people/karpathy/densecap/源码地址：https://github.com/jcjohnson/densecap论文地址：http://arxiv.org/abs/1511.07571 注意事项：源码是15年写的，所以使用的是比较老版本的cuda和cudnn（8.0 v5.1），并且作者也没有在继续更新代码了，所以如果你想成功运行起来的话，尽量不要用太高版本的cuda，否则可能会出现文件丢失错误（libcudnn (R5) not found in library path.） 安装安装以下依赖： 123456$luarocks install torch$luarocks install nn$luarocks install image$luarocks install lua-cjson$luarocks install https://raw.githubusercontent.com/qassemoquab/stnbhwd/master/stnbhwd-scm-1$.rockspec$luarocks install https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/torch-rnn-scm-1.rockspec （可选）安装GPU相关依赖（如果你不使用GPU跑代码，可以不装这里） 123$luarocks install cutorch$luarocks install cunn$luarocks install cudnn 下载预训练模型在命令行中键入下面的指令，运行脚本下载预训练模型（注意，下面的脚本文件在github上的项目代码里，所以你要先把github上的源代码下载下来，然后进入到项目目录里面）1$sh scripts/download_pretrained_model.sh 用图片来测试模型源码中自带了一张大象的图片，你可以用下面的指令来对大象图片进行测试，如果你想测试自己的图片，把图片放到项目中的imgs文件里，然后修改指令后面的图片名称为你自己图片的名称就可1$th run_model.lua -input_image imgs/elephant.jpg 如果你没有GPU，记得要加上-gpu -1指令来告诉模型在cpu上指令（CPU上的指令速度较慢，我自己的执行情况是：GTX980Ti：0.3s 酷睿i5/7：5～10min） 以上指令会生成vis/data文件夹，这就是模型的运行结果，可以用下面的方式查看结果， 12$cd vis$python -m SimpleHTTPServer 8181（或者python -m http.server 8181) 然后，在浏览器中打开http://localhost:8181/view_results.html. 当然，如果你想一次运行数张图片，可以使用下面的指令，该指令会将指定路径下的图片全部执行1$th run_model.lua -input_dir /path/to/my/image/folder 问题：我遇到了以下问题，这里列出我自己的解决方法，如果你还遇到了其他不同的问题，可以留言，我会尽快答复你 问题1：cutorch问题提示找不到cutorch，或者其他什么相关的错误 解决办法：重新安装cutorch1$luarocks install cutorch 不幸的是，这个解决方法对我并没有用，我最后发现是因为代码运行的cutorch版本是5.1，而由于此时我安装了高版本的cuda（9），所以在使用上面的指令安装时，安装的是cutorch 5.2，所以提示找不多5.1的cutorch，最后，我重新换回了的cuda8.0，并重新安装cutorch，解决了问题，切换cuda版本的方法可以看这里：https://blog.csdn.net/ksws0292756/article/details/80120561 问题2：libcudd.5.so.5 找不到主要原因还是cuda和cudnn的版本问题，我切换了cuda和cudnn的相关版本，换到cuda8.0和cudnn_v5.1以后， 解决了问题]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图片描述 image captioning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FasterRCNN-NIPS2015]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-NIPS2015-FasterRCNN%2F</url>
    <content type="text"><![CDATA[文章:作者: 背景介绍核心亮点论文细节Region Proposals Networks 原文中, 作者用ZF model为例对RPN网络进行讲解, ZF model的最后一层卷积层Conv5具有256个卷积核, 也就是说它生成的特征图谱的shape为 $W\times H\times 256$, RPN网络的第一层可以认为是一个卷积层, 卷积核的尺寸为 $n\times n$ (文中使用的是 3×3), 卷积核的个数为 256 (维持通道数不变). 利用该层的卷积层对Conv5的特征图谱操作, 最终生成的特征图谱大小仍然为 $W\times H\times 256$ (只不过此时,图谱中的每个点都会原图谱中的 $n\timesn$ 个点相关联). 对于这个图谱中的每个点, 我们认为它是一个anchor, 可以将它看做是一个元素个数为256的1维向量 (因为通道数为256). 然后, 对于每一个anchor, 都会分配 $k$ 个anchor boxes. 每个anchor box都要分前景后景, 也就是分是否包含物体, 同时, 每一个anchor box还要输出预测物体相对于anchor的偏移坐标量. 训练的时候会选择128个postive anchor boxes 和128个negative anchor boxes. 实际上, RPN网络由两部分构成: 一个卷积层, 一对全连接层分别输出分类结果(cls layer)以及坐标回归结果(reg layer). 实现上: 利用了一个卷积核大小为 $n\times n$ 的卷积层, 后接两个 $1\times 1$ 的卷积层.(分别用于回归和分类) 损失函数:L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^* ) + \lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^* )RPN和FastRCNN共享卷积参数原文为了使RPN和FastRCNN共享卷积参数, 讨论了三种不同的训练策略: Alternating training: 先训练RPN, 然后用RPN产生的候选区域来训练Fast RCNN, 之后, FastRCNN更新参数以后, 继续用来训练RPN. 该策略是本篇paper所有的实验使用的训练方法 Approximate joint training: 如果anchor box有两个物体重叠了? 怎么处理????bounding box 回归的时候, 为什么不直接对坐标回归, 而是采用偏移量和缩放度?]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《TensorFlow实战》]]></title>
    <url>%2Fz_post%2FTensorFlow-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TF%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastRCNN-ICCV2015]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-ICCV2015-FastRCNN%2F</url>
    <content type="text"><![CDATA[文章:作者: 背景介绍核心亮点论文细节RoI PoolingRoI Pooling的前向传播过程如下: 对于任意给定的feature map的候选区域框, 将其划分成指定的网格大小, 然后执行max pooling操作. 这样, 对于任意size的输入, 都可以获得固定长度的输出. RoI Pooling 在反向传播计算梯度时, 可以看做是分别对每个候选区域框计算max pooling 的梯度, 然后将所有候选区域框的梯度累加, 其过程及公式如下: \frac{\partial L}{\partial x_i} = \sum_r \sum_j [i = i^*(r,j)]\frac{\partial L}{\partial y_{rj}}式中, $x_i$ 代表RoI Pooling前特征图上的像素点, $y_{rj}$ 代表pooling后的第 $r$ 个候选区域的第 $j$ 个点, $i^(r,j)$ 代表点 $y_{rj}$ 像素值的来源(最大池化的时候选出的最大像素值所在点的坐标). 由此可以看出, 只有当池化后某个点的像素值在池化过程中采用了当前点 $x_i$ 的像素值 (即满足 $i = i^ (r,j)) 时, 才在 $x_i$ 处回传梯度. 为什么RoI Pooling比SPP效果好?SPP的Pooling方式是组合不同划分粒度下feature map的max pooling. 这种方式就会导致网络在反向传播的时候, 不容易通过SPP层反向传播. 所以SPPNet的作者就没有设计对应的传播规则 (这里与Inceptin不同, Inception是卷积核连接, 梯度更新是互相独立的, 按照正常计算即可) , 而是直接在最后两个全连接层上进行fine-tune, 虽然最后也取得了不错的成果, 但是Roos认为, 虽然离输入层较劲的前几层卷积层是比较generic和task independent的, 但是靠近输出层的卷积层还是很有必要进行fine-tune的, 他也通过实验证实了这种必要性, 于是他简化了SPP的Pooling策略, 用一种更简单粗暴的Pooling方式来获得固定长度的输出向量, 同时也设计了相应的RoI Pooling的反向传播规则, 并对较深的基层卷积层进行了fine-tune, 最终取得了不错的效果. 损失函数分类损失, 对于真实类别 $u$ 的损失函数如下: L_{cls}(p,u) = -log p_ubounding box回归: L_{loc}(t^u,v) = \sum_{i\in \{x,y,w,h\}} smooth_{L_1}(t_i^u - v_i)smooth_{L_1}(x) = \begin{cases} 0.5x^2 & \text{if } |x| < 1 \\ |x| - 0.5 & \text{otherwise} \end{cases}相比于 $L_2$ 损失, $L_1$ 损失对于离异值更加鲁棒, 当预测值与目标值相差很大时, 梯度很容易爆炸, 因为梯度里面包含了 $(t_i^u - v_i)$ 这一项, 而smooth L1 在值相差很大是, 其梯度为 $\pm 1$ ( $L_1$ 在 $x$ 绝对值较大时, 是线性的, 而 $L_2$ 是指数的, 很容易爆炸) 联合任务损失函数: L(p,u,t^u,v) = L_{cls}(p,u) + \lambda[u\ge 1]L_{loc}(t^u,v)Truncated SVD 截断式奇异矩阵分解对于整张图片的分类问题来说, 花费在全连接上的计算时间相比于在卷积层上的计算时间来说, 要小很多. 但是, 与之相反的, 对于目标检测问题来说, 需要处理的RoI数量很大, 并且前向计算的时间有几乎一半都花费在了全连接层的计算上, 因此, 使用truncated SVD技术来进行加速. 对于一个权重矩阵为 $u\times v$ 的全连接层来说, 该矩阵可以被近似的因式分解为: W \approx U \sum_t V^T式中, $U$ 是一个 $u\times t$ 的矩阵, $\sum_t$ 是一个 $t\times t$ 的对角矩阵, 包含着矩阵 $W$ 的值最大的 $t$ 个奇异值]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask R-CNN (ICCV, 2017)]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-MaskRCNN-ICCV2017%2F</url>
    <content type="text"><![CDATA[文章: MaskRCNN作者: Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick备注: FAIR, ICCV best paper 核心亮点1) 提出了一个简单,灵活,通用的实例分割模型框架MaskRCNN 在 FasterRCNN 的基础上进行改进, 在模型的head部分引入了一个新的mask预测分支, 在训练阶段, 该分支会与其他分支并行执行, 在测试阶段, 虽然不是并行执行, 但是利用 NMS 减少了需要计算的候选框个数, 因此 MaskRCNN 模型整体增加的额外开销较小. 2) 提出了RoI Align来解决 RoI 与 pooling后的特征图谱之间的不对齐问题Fast/FasterRCNN 原始的 RoIPool 操作在进行池化时, 会进行两次粗糙的量化操作, 这使得池化后的特征图谱与 RoI 中的信息不能很好的对其, 对于像素级任务实例分割来说, 这种非对齐会使得模型性能大大降低, 因此 MaskRCNN 提出用基于双线性插值法的 RoI Align 代替 RoI Pool, 以此来解决非对齐问题. 摘要本文提出了一个简单, 灵活, 通用的目标实例分割框架. 本文的方法可以有效的检测图片中的物体, 同时为每个实例生成一个高质量的掩膜, 我们将其称为 Mask RCNN, 它是从 Faster RCNN扩展而来的, 添加了一个新的分支来并行预测物体掩膜. MaskRCNN可以轻易的进行训练, 并且相对于FasterRCNN只增加了很少的开销, 可以在5fps下运行. 不仅如此, MaskRCNN可以轻易的泛化到其他任务中, 如人体姿态识别. 本文的模型在COCO的三个系列任务都, 都取得了最好的效果. 背景介绍本文意在提出一种通用的实例分割模型框架-MarkRCNN, 该模型扩展自FasterRCNN, 在FasterRCNN模型中的每一个RoI上, 添加一个与检测分支平行运行的掩膜预测分支, 如图1所示. 掩膜分支(mask branch) 是一个小型的FCN网络, 它应用在每一个RoI上, 以pixel-to-pixel的方式来预测一个分割掩膜. Mask RCNN易于实现, 且增加的额外开销很小, 并且具有很大的灵活性, 是一个通用的实例分割模型框架. 在FasterRCNN中使用的RoI pooling是一种针对目标检测任务的粗糙的pooling方法, 会造成一定程度上的不对齐结果, 为了克服这一点, 本文提出了RoIAlign, 用于保留准确的空间位置, RoIAlign可以将掩膜的精确度才提高10%~50%. 另外, 本文发现, 将二值掩膜预测和类别预测任务分开独立进行是非常重要的一步, 因此, 我们为每一个类别都会单独进行mask预测, 避免了不同类别之间的冲突. 这一点与FCN不同, FCN会对一个像素点进行多类别的分类.我们的模型在GPU上的运行速度大约为200ms/frame, 在8-GPU的单机上训练时, 需要1到2天的时间. Faster RCNN简单回顾一下Faster RCNN, 它包含两个阶段, 第一个阶段, 是RPN结构, 用于生成候选框集合. 第二个阶段, 本质上就是一个Fast RCNN, 利用RoI pooling从RoI中提出固定尺寸的特征, 然后进行分类任务和边框回归任务. 这两个阶段使用的特征图谱是共享的, 都来自backbone网络. Mask RCNNMask RCNN在概念上来说非常简单: FasterRCNN对于每个候选框来说都有两个输出分支, 一个class label和一个bounding-box offset, 对此在MaskRCNN中我们添加了第三个分支用于输出掩膜. 虽然这是一个看起来很自然的想法, 但是额外增加的掩膜分支和class, box分支并不相同, 它需要物体更加精确的空间位置. 因此, 我们还引入了一个在MaskRCNN中非常关键的元素:RoIAlign, 用于进行像素对其. 来弥补FasterRCNN RoI pooling的粗糙映射造成的位置偏移问题.Mask R-CNN 使用了与 Faster R-CNN相同的two-stage结构, 第一阶段使用了相同的RPN网络, 第二阶段, 在执行class分类的box回归任务的 同时(并行), MaskRCNN会为每一个RoI生成一个二值掩膜. 这一点与许多现有系统不同, 它们都是在mask预测结果的基础上进行分类任务的. 我们的灵感来自于Fast RCNN中class任务和box任务的并行执行.Formally, 在训练阶段, 我们在每一个采样的RoI上定义一个multi-task loss如: $L = L_{cls}+L_{box}+L_{mask}$. 前两个loss和Fast RCNN相同, 第三个分支对于每一个RoI的输出维度为 $Km^2$, 代表这分辨率 $m\times m$ 下的 $K$ 个二值掩膜, 每一个掩膜对应了一个类别(共 $K$ 个类别). 为此, 我们使用了 per-pixed sigmoid, 并且将 $L_{mask}$ 定义为平均二值交叉熵. 对于一个与真实类别 $k$ 相关联的RoI, $L_mask$ 只在第 $k$ 个mask上有定义(其他mask不计入loss).我们对 $L_{mask}$ 的定义使得网络模型可以为每个class生成mask, 避免了不同类别之间的竞争冲突. 并且利用分类分支的结果标签来选择对应的mask进行计算. 这样做可以使得mask预测任务和class预测任务decouple.(与现有FCNs模型不同). Mask Representation一个mask代表了一个物体的空间布局. 因此, 和 class labels 或者 box offsets 不同(输出维度较小), 提取mask的空间结构时可以通过像素到像素卷积的方式解决.具体来说, 我们使用一个FCN从每个RoI中预测 $m\times m$ 的mask. 这使得mask分支中的每一层都维护着 $m\times m$ 的物体空间布局矩阵, 而不用将其转换成向量形式(缺少空间信息). 和之前的工作(依赖于fc层预测mask)不同的是, 本文的全卷积表征需要的参数更少, 同时通过实验证明, 更加精确. 这种 pixel to pixel 的机制需要我们的RoI feature能够精确与真实物体对齐, 保持其空间位置和结构信息. 为此, 本文提出了RoIAlign来代替RoiPool.(这个替换很重要) RoIAlignRoIPool可以从每个RoI中提取到一个较小的固定的feature map(如, 7×7). RoIPool首先会将浮点型的feature map坐标离散化成输出size对应的整数, 然后根据每个bin内的像素数值进行pooling操作. 但是这种pooling方式会引入RoI与提取后的feature map之间的misalignments, 这种misalignments对于分类任务来说或许影响不大, 但是对于predicting pixel-accurate mask任务来说就会造成很大的负面影响.为了解决这个问题, 我们提出了RoIAlign层, 移除了RoIPool粗糙的量化计算, 将输出的feature map与输入的RoI对齐. RoIAlign的原理很简单: 避免在RoI边界或者bins上执行任何量化计算(即, 我们使用 $x/16$, 而不是 $[x/16]$). 我们利用双线性插值法来计算每个位置上具体的像素值, 并且对计算结果整合(max或者average). 具体计算细节如图3所示. 我们注意到, 在不使用任何量化计算以后, 计算结果对于具体的采样点和采样数量的多少都不再那么敏感了. RoI Pooling存在两次量化过程: 将候选框边界量化为整数坐标值 将量化后的边界区域分割成 $k\times k$ 个bins, 并对每一个单元的边界量化 可以看出, 上面的量化操作是一种很粗糙的Pooling方式, 由于feature map可以看做是对原图特征的高度概括信息, 所以feature map上的细微差别映射回原图时, 往往会导致产生很大的像素位移差. 故此, 提出了RoI Align的解决思路: 取消量化操作, 使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值, 从而将整个特征聚集过程转换为一个连续的操作. 其具体流程如下: 遍历每一个候选区域, 保持浮点数边界不做量化 将候选区域分割成 $k\times k$ 个bins, 每个bins的边界也不做量化 在每个bins中计算固定四个坐标的位置, 使用双线性插值的方法计算出这四个位置的值, 然后进行最大化操作. 网络结构(Network Architecture)为了说明本文方法的通用性, 我们利用多个模型结构初始化Mask RCNN. For clarity, we differentiate between: 1) 用于提取整个图片特征的backbone卷积结构, 2) 用于执行bounding-box recognition(cls,reg)任务和 mask prediction 任务的 network head.我们将backbone architecture 命名为术语: network-depth-features. 我们测试了 50和101层的 ResNet, ResNeXt 网络. 原始的 FasterRCNN 在使用ReNets时, 利用的是第4段卷积块的最后一层卷积层的输出, 我们称为 C4. 这个backbone, 即 ResNet-50-C4 是很多模型中的一个常用的选择.我们同时还使用了另一个更有效backbone—-Feature Pyramid Network(FPN). FasterRCNN和FPN在提取RoI feature的level上有所区别, 但是其余部分基本和ResNet相同. 使用 ResNet-FPN backbone 用于特征提取可以获得极大的性能提升(both acc and speed).对于 network head, 我们根据之前的工作添加了一个全卷积的mask prediction 分支. Specifically, 我们将基于ResNet和FPN的 FasterRCNN 的 box heads进行扩展, 具体细节如图4所示. ResNet-C4 的 head 包含 ResNet 的第5个卷积块(即 9层的res5, 计算密集型). 对于FPN来说, 它的backbone就已经包含了res5, 因此可以具有效率更高的head(filers更少).我们注意到本文的mask分支具有一个很简单的结构. 其他更加复杂设计将会在以后的工作的进一步提高模型的性能. 实现细节(Implementation Details)我们使用Faster RCNN原文提供的超参数.(无需调参也说明了MaskRCNN的鲁棒性很高) Training: 和FastRCNN一样, 如果RoI与真实框的IOU大于0.5, 就会被认为是正样本, 否则认为是负样本(这里与FasterRCNN不同). $L_{mask}$ 只会计算正样本上的掩膜损失. mask target 是 RoI 和 真实mask之间的交集(注意不是直接根据真实mask计算损失). 我们使用 image-cengric (FastRCNN), 图片会被resized成scale(shorter edge) 为800 像素. minibatch为 2 img/GPU, 每个img含有N个sampled RoIs. 正负样本比例为 1:3. 对于 C4 backbone来说, N为 64, 对于FPN来说, N为512.(因为FPN效率更高, 所以可以在一张图片上计算更多的RoIs). 在8PGUs上训练(即有效minibatch为16). lr为0.02, 每120k 迭代(iteration)会缩小10倍. weight decay为0.0001, momentum为0.9. 在使用ResNeXt时, one img/GPU, lr初始为0.02, 其他相同.FPN的anchor具有 5 scales 和 3 aspect ratios. 为了方便进行消融实验, RPN是被单独训练的, 并且没有与Mask RCNN共享了卷积特征(可共享, 只是为了方便没有共享). 对于本文中的每一项, RPN和MaskRCNN都具有相同的backbones. Inference: 在测试阶段, proposal的数量为300 for C4 backbone, 1000 for FPN. 使用NMS选择了最高score的100个boxes, 对其应用mask branch, 虽然这里没有采用训练时的并行侧率, 但是它可以加速预测速度, 同时能够提高精度(因为只对更少的100个box使用了mask branch, 而训练阶段虽然是并行的, 但是进行mask branch的box更多一些). mask branch 可以对每个RoI预测出 $K$ 个maks, 但是我们只会使用第 $k$ 个mask来计算损失. 我们会将 $m\times m$ 的浮点类型的mask放缩到RoI的尺寸大小, 然后依据一个阈值(0.5)对mask的像素值进行二值化操作. 由于我们只对100个top score box执行mask branch , 因此在模型预测时, MaskRCNN相比于FasterRCNN, 只增加了很小的开销. 实验: 实例分割(Instance Segmentation)使用了 COCO 数据集, 采用 AP(averaged over IoU thresholds) 评价标准, $\text{AP}_{50}$, $\text{AP}_{75}$, $\text{AP}_{S}$, $\text{AP}_{M}$, $\text{AP}_{L}$. 如表1所示, MarkRCNN 的性能超过 COCO2015 和 COCO2016的实例分割冠军 MNC 和 FCIS.(并且 MaskRCNN 没有使用 multi-scale train/test, horizontal flip test, OHEM 等 trick, 言外之意 MaskRCNN 的性能可以进一步利用这些 trick 提高) Table2 显示了对 MaskRCNN 的消融实验分析结果. Table3 显示了 MaskRCNN 与当前的 state of art 的目标检测方法在 COCO 数据集上的表现. MaskRCNN for Human Pose Estimation: 附录A: Experiments on Cityscapes 附录B: Enhanced Results on COCO下表显示了 MarkRCNN 被各种 trick 增益后的性能表现]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CVPR2014-RCNN%2F</url>
    <content type="text"><![CDATA[文章: 作者: 背景介绍核心亮点关键技术论文细节Selective Search 算法基本思路如下: 使用一个分割手段, 将图像分割成小区域 查看现有小区域, 合并可能性最高的两个区域, 重复知道整张图像合并成一个区域位置. 优先合并以下区域: 颜色(颜色直方图)相近的 纹理(梯度直方图)相近的 合并后总面积小的 合并后, 总面积在其BBox中所占比例大的 输出所有存在过的区域, 即所谓的候选区域]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第十二章～第十三章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter12_13%2F</url>
    <content type="text"><![CDATA[第一章 预备知识第二章 开始学习C++第三章 处理数据第四章 复合类型第五章 循环和关系表达式第六章 分支语句和逻辑运算符第七章 函数——C++的编程模块第八章 函数探幽第八章 函数探幽第八章 函数探幽第八章 函数探幽第十二章 类和动态内存分配动态内存和类复习示例和静态成员 所有的对象都共用一个静态成员副本。 不能在类声明中初始化静态成员变量 ，这是因为声明描述了如何分配内存，但并不分配内存。p428 对于静态类成员，可以在类声明之外使用单独的语句来进行初始化，这是因为静态类成员是单独存储的，而不是对象的组成部分。注意下面的初始化语句，指出了类型，并使用了作用域运算符，但没有使用关键字static。p428 1int StringBad::num_strings = 0; 在构造函数中使用new来分配内存时，必须在相应的析构函数中使用delete来释放内存。如果使用new[]来分配内存，则应使用delete[]来释放内存。p429 特殊成员函数 C++提供了一些特殊的成员函数，它们会在一定条件下自动创建。p432 默认构造函数，如果没有定义构造函数; 默认析构函数，如果没有定义; 复制构造函数，如果没有定义; 赋值运算符，如果没有定义; 地址运算符，如果没有定义。 C++11提供了另外两种特殊成员函数：移动构造函数和移动赋值运算符。这将在第十八章讨论。 其中，隐式地址运算符返回调用对象的地址（即this指针的值），这与我们的初衷一致。主要引起问题的是复制构造函数和赋值运算符。p432 复制构造函数 用于将一个对象复制到 新创键 的对象中。也就是说，它用于初始化过程中，而不是常规的复制过程中。类的复制构造函数原型通常如下：Class_name(const Class_name&amp;); p433 何时调用复制构造函数：每当程序产生了对象副本时，都将使用复制构造函数，如用赋值语句初始化，函数按值传递等。具体地说，当函数按值传递对象或者函数按值返回对象时，都将使用复制构造函数。因此，进行传递时，多用引用，可以减少调用复制构造函数的时间和存储新对象的空间。 默认复制构造函数的功能：默认的复制构造函数逐个复制非静态成员，复制的是成员的值（按值复制，浅复制），如果成员本身就是类对象，则将使用这个类的复制构造函数来复制成员对象。静态函数不受影响，因为它们属于整个类。 复制构造函数容易引起的问题 如果常规构造函数中设置了一个静态变量用于计录创建对象的个数，那么就应在复制构造函数中显示写出该逻辑，否则会导致计数结果不准确。p434 由于隐式复制构造函数是按值进行复制的。在这成员变量中含有指针时是十分危险的，因为这样依赖两个对象中的成员指针就会指向同一块内存，如果其中一个对象被释放后，其指针指向的内存块可能会导致不确定的错误。另外，程序有可能会因为两次释放同一块内存而导致程序终止。具体的错误取决于系统和相关实现。p435 深度复制。深度复制可解决上述的问题。复制时应当复制指针指向内容的副本，并将副本的地址赋给新的对象，这样一来，两个对象就是完全独立的。必须自定义复制构造函数的原因就在于，一些类成员是使用new初始化的、指向数据的指针，而不是数据本身。p435 赋值运算符容易引起的问题 赋值运算符的完整函数原型如下：Class_name &amp; Class_name::operator = (const Class_name&amp;);。它接受并返回一个指向类对象的引用。 赋值运算符的功能以及何时使用：将已有的对象赋给另一个对象时，将使用重载的复制运算符。（注意，初始化赋值时，不会调用赋值运算符重载，而是调用复制构造函数）。p436 赋值的问题：主要是由于浅复制造成的数据问题，由于赋值时是按值赋值的，导致指针变量会指向相同的地址。解决的方法是提供赋值运算符（进行深度复制）的定义，其实现与复制构造函数相似，但也有一些差别。p436 由于目标对象是已经存在的对象，所以它可能引用了以前分配的数据，因此函数应使用delete[]来释放这些数据。 函数应当避免将对象赋给自身，否则，给对象重新赋值前，释放内存操作就已经删除了对象内容。这一点可以通过程序逻辑实现：if(this == &amp;s) return *this; 函数应返回一个指向调用对象的引用。通过返回一个对象，函数可以想常规赋值操作那样，连续进行赋值。 改进后的新String类 下面两种方式分配的内存量相同，区别在于前者与类析构函数兼容，而后者不兼容。p438 12str = new char[1]; //与析构函数中的delete []str; 兼容str = new char; C++11空指针： 在C++98中，字面值0有两个含义：可以表示数字值零，也可以表示空指针，这使得阅读程序的人和编译器难以区分。C++11提供了更好的解决方案，引入新关键字nullptr，用于表示空指针。原来的表示依然合法，但建议使用nullptr。 p438 静态成员函数： 不能通过对象调用静态成员函数，也不能使用this指针。如果静态成员函数是在公有部分声明，则可以使用类名和作用域解析符来调用它。 其次，由于静态成员函数不与特定的对象相关联，因此只能使用静态数据成员，不能访问其他成员数据。p441 较早的get(char *, int)版本在读取空行后，字符串中第一个字符将是一个空字符。较新的C++标准则会返回false。p446 在构造函数中使用new时应注意的事项 使用new初始化对象的指针成员时，必须注意下面几项：p446 如果在构造函数中使用new来初始化指针成员，则应在析构函数中使用delete。 new和delete必须相互兼容。new对应于delete，new[]对应于delete[]。 如果有多个构造函数，则必须以相同的方式使用new，要么都带中括号，要么都不带。因为只有一个析构函数，所有的构造函数都必须与它兼容。然而，可以在一个构造函数中使用new初始化指针，而在另一个构造函数中将指针初始化为空（0或C++11中的nullptr），这是因为delete（无论是否带[]）可以用于空指针。 应定义一个复制构造函数，通过深度复制将一个对象初始化为另一个对象。它应分配足够的空间来存储复制的数据，并复制数据，而不仅仅是数据的地址。另外，还应该更新所有受影响的静态类成员。 应定义一个赋值运算符，通过深度复制将一个对象赋值给另一个对象。它应该检查自我赋值的情况，释放成员指针以前指向的内存，复制数据而不仅仅是数据的地址，并返回一个指向调用对象的引用。 有关返回对象的说明返回指向const对象的引用 返回的对象应该是函数参数传递进来的对象，并且是const的，所以需要返回const引用。p449 返回指向非const对象的引用 返回的对象应该是函数参数传递进来的对象，但是由于参数不是const的，所以返回非const（当然也可以返回const）。常见的两种情况是重载赋值运算符以及重载与count一起使用的&lt;&lt;运算符。p449 返回对象 如果返回的对象是被调用函数中的局部变量，则不应该按引用方式返回它，因为在调用函数执行完毕时，局部对象将调用其析构函数。p450 返回const对象 返回const对象，该对象将不能作为右值，此时可以避免一些不必要的错误。p450 使用指向对象的指针 利用new创建一个对象并令指针指向它，该对象会被分配到堆内存中，直到使用delete为止，该对象一直存在。p453 1String * favorite = new String; 使用对象指针时，需要注意几点：p454 使用常规表示法来声明指向对象的指针：String * glamour; 可以将指针初始化为指向已有的对象：String * second = &amp;string_first; 想要将创建一个新的对象，可以使用new来初始化指针：String * glamour = new String 可以通过间接访问运算符-&gt;来调用类的方法。 可以通过解除引用运算符（ * ）来活得对象。 定位new运算符： 定位new运算符可以在分配内存时指定内存的位置：Sting * p1 = new (buffer) String; 。但是在使用时要注意以下几点：p457 确保定义不同的对象时，二者使用的内存地址是不同的，且内存单元之间没有重叠 delete可以与常规new运算符配合使用，但不能与定位new运算符配合使用。有时需要通过显示调用析构函数来释放对象的内存。 对于使用定位new运算符创建的对象，应以与创建顺序相反的顺序进行删除。原因在于，晚创建的对象可能依赖于早创建的对象。另外，仅当所有对象（定位new创建的）都被销毁后，才能释放用于存储这些对象的缓冲区。 复习各种技术 p459 初始化列表 如果Classy是一个类，而mem1、mem2和mem3都是这个类的数据成员，则类构造函数可以使用如下的语法来初始化数据成员。该语法需要注意以下几点：p464 这种格式只能由于构造函数; 必须用这种格式来初始化非静态const数据成员（C++11之前）; 必须用这种格式来初始化引用数据成员。123Classy:Classy(int n, int m) :mem1(n), mem2(0), mem3(n*m+2)&#123; ...&#125; 第十三章 类继承13.1 一个简单的基类 从一个类派生出另一个类时,原始类称为基类,继承类称为派生类。p481 13.1.1 派生一个类 使用公有派生，基类的公有成员将成为派生类的公有成员。基类的私有部分只能通过基类的公有和 保护 方法访问。 class A : public B //A继承自B，且是公有继承 p483 13.1.2 构造函数：访问权限的考虑创建派生类对象时，程序首先会创建基类对象，这意味着 基类对象应该在程序进入派生类构造函数之前被创建 。C++使用成员初始化列表语法来完成这种工作。 派生类构造函数必须调用基类的构造函数，利用成员初始化列表语法来显式调用基类构造函数，如果没有显式调用，那么就会调用默认的基类构造函数。p48412345678910derived::derived(type1 x, type2 y) : base(x,y)&#123; //显式调用基类B的构造函数 ...&#125;derived::derived(type1 x, type2 y)&#123; //该代码与下面的等效 ...&#125;derived::derived(type1 x, type2 y) : base()&#123; ...&#125; 有关派生类构造函数的要点如下： 首先创建基类对象（在进入派生类构造函数之前就被创建） 派生类构造函数应通过 成员初始化列表 将基类信息传递给基类构造函数 派生类构造函数应初始化派生类新增的数据成员 释放对象的顺序与创建对象的顺序相反，即首先执行派生类的析构函数，然后自动调用基类的析构函数。p485 如果没有在成员初始化列表中提供基类构造函数，程序将使用默认的基类构造函数，成员初始化列表只能用于构造函数。p486 派生类与基类之间的特殊关系 当基类的方法不是私有的（可以是公有或保护），派生类可以使用基类的方法。p488 基类指针/引用可以在不进行显式类型转换的情况下指向/引用派生类对象（反之不行）。但是只能调用基类方法。p488 继承：is-a关系 公有继承是最常用的方式（另外还有私有和保护继承），它建立一种is-a-kind-of（是一种）的关系，术语简称is-a。is-a关系的派生类对象也是一种基类对象，凡是可以对基类执行的操作，都可以对派生类执行。p489 多态公有继承 实现多态公有继承的方式有以下两种：p490 在派生类中重新定义基类的方法 使用虚方法（关键字virtual只用与类声明的方法原型中，不用于方法定义） 如果要在派生类中重新定义基类的方法，通常应将基类方法声明为虚的。这样，程序将根据对象类型而不是引用或指针的类型来选择方法版本。为基类声明一个虚析构函数也是一种惯例。p493 在派生类方法中，标准技术是使用作用域解析符来调用基类方法。p496 静态联编和动态联编 将源代码中的函数调用解释为执行特定的函数代码块被称为函数名联编（binding）。在编译过程中进行联编被称为静态联编（static binding），又称为早期联编（early binding）。但是有时候，无法在编译时确定使用哪个函数块（如虚函数），所以，编译器必须生成能够在程序运行时选择正确的虚方法的代码，这被成为动态联编（dynamic binding），又称为晚期联编（late binding）。p502 指针和引用类型的兼容性 将派生类引用或指针转换为基类引用或指针被称为向上强制转换（upcasting）。如果不使用显式类型转换，则向下强制转换是不允许的。p502 隐式向上强制转换使基类指针或引用可以指向基类对象或派生类对象，因此需要动态联编。C++使用虚成员函数来满足这种需求。p503 虚成员函数和动态联编 编译器对非虚方法使用静态联编，对虚方法使用动态联编。因为虚方法是根据对象类型来选择的，而对象类型只有在运行时才能确定。非虚方法则是根据引用或指针的类型来选择方法，它们可以在编译时确定。p503 为什么有两种联编类型以及为什么默认为静态联编： 动态联编的好处是可以重新定义类方法，但是在运行阶段跟踪对象类型会产生一定的开销，这使得动态联编没有静态联编效率高，这也是选择静态联编为默认方式的原因。p503 虚函数的工作原理： C++规定了虚函数的行为，但将实现方法留给了编译器作者。通常，编译器处理虚函数的方法是，给每个对象添加一个隐藏成员。隐藏成员中保存了一个指向函数地址数组的指针。这种数组成为虚函数表（virtual function table，vtbl）。虚函数表中存储了为类对象进行声明的虚函数的地址。 派生类对象将包含一个指向独立地址表的指针。如果派生类提供了虚函数的新定义，该虚函数表将保存新函数的地址;如果没有重新定义虚函数，则保存函数原始版本的地址。调用虚函数时，程序将查看存储在对象中的vtbl地址，然后转向相应的函数地址表。p504 根据工作原理可以得出，使用虚函数时，在内存和执行速度方面有一定的成本（虽然非虚函数效率高，但不具备动态联编功能），包括：p505 每个对象都将增大，增大量为存储地址的空间; 对于每个类，编译器都将创建一个虚函数地址表（数组）; 对于每个函数调用，都需要执行一项额外的操作，即到表中查找地址。 有关虚函数注意事项 虚函数的一些要点：p505 在基类方法的声明中使用关键字virtual可使该方法在基类以及所有的派生类（包括儿子的儿子）中是虚的。 如果使用指向对象的引用或指针来调用虚方法，程序将使用为对象类型定义的方法，而不使用为引用或指针类型定义的方法。这称为动态联编或晚期联编。这种行为非常重要，因为这样基类指针或引用可以指向派生类对象。 构造函数不能是虚函数。创建派生类对象时，将调用派生类的构造函数，而不是基类的构造函数，然后，派生类的构造函数会使用基类的构造函数，这种顺序不同于继承机制。因此，派生类不急成基类的构造函数。p505 除非类不是基类，否则应该将析构函数声明为虚函数。p505 友元不能是虚函数，因为友元不是类成员，而只有成员才能是虚函数。p505 如果派生类没有重新定义函数，将使用该函数的基类版本。如果派生类位于派生链中，则将使用最新的虚函数版本。p506 重新定义继承的方法并不是重载，而是将基类的方法隐藏，也可以看作是重写。由此，得出两条经验规则：第一，如果重新定义继承的方法，应确保与原来的原型完全相同，但如果返回类型是基类引用或指针，则可以修改为指向派生类的引用或指针。第二，如果基类声明被重载了，则应在派生类中重新定义所在的基类版本。如果只定义类部分版本，则其他版本将被隐藏。另外，如果不需要修改，则新定义直接调用基类版本即可，如void derived::show() { const(base::show()); } 访问控制：protected 对于外部世界来说，保护成员的行为与私有成员相似，但对于派生类来说，保护成员的行为与公有成员相似。p507 对于数据成员最好采用私有访问控制，同时通过基类方法使派生类能够访问基类数据。对于成员函数来说，保护访问控制很有用，它让派生类能够访问公众不能使用的内部函数。 抽象基类 从多个类中抽象出它们的共性，将这些共性放在一个抽象基类（abstract base class，ABC）中，然后再从该ABC派生出这些类。ABC中有些方法不能直接实现，C++通过纯虚函数（pure virtual function）提供未实现的函数。纯虚函数声明的结尾处为=0，如下所示： 1virtual double Area() const = 0; // a pure virtual function 当类声明中包含纯虚函数时，则不能创建该类的对象，而只能作为基类使用。要成为真正的ABC，必须至少包含一个纯虚函数，原型声明中的=0是虚函数成为纯虚函数，一般纯虚函数没有定义，但C++允许纯虚函数有定义，即可以把所有派生类的某个共同操作作为纯虚函数的定义，然后在派生类重写该纯虚函数时调用。p509 13.7 继承和动态内存分配如果基类使用动态内存分配，并重新定义赋值和复制构造函数，那么将怎么影响派生类的实现呢？有以下几种情况： 13.7.1 派生类不使用new 如果基类使用了动态内存分配，而派生类未使用，那么就不需要为派生类定义显式析构函数、赋值和复制构造函数。 p516 13.7.2 派生类使用new 如果派生类使用了new，就必须为派生类定义显示析构函数、赋值和复制构造函数。p517 13.8 类设计回顾13.8.1 编译器生成的成员函数 默认构造函数 &emsp;&emsp;如果没有定义任何构造函数，编译器将定义默认构造函数。 复制构造函数&emsp;&emsp;如果程序没有使用（显式或隐式）复制构造函数，编译器将提供原型，但不提供函数定义。 复制运算符&emsp;&emsp;默认的赋值运算符用于处理同类对象之间的赋值。不要将赋值与初始化混淆了。如果语句创建新的对象，则是用初始化。如果语句修改已有对象的值，则是赋值。 13.8.2 其他的类方法 构造函数 构造函数不同于其他类方法，因为它创建新的对象，而其他类方法只是被现有的对象调用。这是构造函数不被继承的原因之一，继承意味着派生类对象可以使用基类的方法，然而，构造函数在完成其工作之前，对象并不存在。 析构函数 一定要定义显式析构函数来世放类构造函数使用new分配的所有内存，并完成类对象所需的任何特殊的清理工作。对于基类，即使它不需要析构函数，也应提供一个虚析构函数。 转换 使用一个参数就可以调用的构造函数定义了从参数类型到类类型的转换。 按值传递对象引用传递 返回对象和返回引用 有些类方法返回对象，有些返回引用，返回对象涉及生成返回对象的临时副本。优先返回引用，但函数不能返回在函数中创建的临时对象的引用。 使用const可以是用const来确保方法不修改参数。 注意，如果函数将参数声明为指向const的引用或指针，则不能将该参数传递给另一个函数，除非后者也确保了参数不会被修改。 13.8.3 公有继承的考虑因素 is-a关系 要遵循is-a关系。如果派生类不是一个特殊的基类，则不用使用公有派生。 什么不能被继承 构造函数是不能继承的，也就是说，创建派生类对象时，必须调用派生类的构造函数。 C++11新增了一种能够继承构造函数的机制，但默认是不能继承构造函数。 析构函数也是不能继承的。 赋值运算符是不能继承的。 赋值运算符 私有成员与保护成员 虚方法 析构函数 友元函数]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DenseCap---CVPR2016]]></title>
    <url>%2Fz_post%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-DenseCap%2F</url>
    <content type="text"><![CDATA[本篇论文解读的排版主要参见原文的格式，针对原文中的每一个小节进行展开，有的是对原文的一个提炼和简单概括，有的是对原文中涉及但是又没有详细介绍的技术的补充和说明。原文连接：https://cs.stanford.edu/people/karpathy/densecap/作者个人主页：https://cs.stanford.edu/people/jcjohns/PS：本篇博文不是对原文的简单翻译，论文中每一处涉及到的知识点以及论文中没有提及的技术细节，本文都会做一定的补充说明，如果还有什么看不懂的地方的话，可以留言一起讨论，我会尽量在24小时内回复。 (正文所有图片中的ksws0292756水印是我的CSDN博客) 这里输入题注 摘要&emsp;&emsp;这篇文章的主要工作是对图像的dense captioning。所谓dense captioning，就是要描述的对象不再是一幅简单的图片，而是要将图片中的许多局部细节都都用自然语言描述出来。这篇文章所做的工作可以说是object detection和image captioning的一般化，即当描述的语言是一个单词的时候，就可以看作是object detection，当描述的对象是整幅图片的时候，就成了普通的image captioning。这篇文章的主要贡献在于提出了一个Fully Convolutional Localization Network（FCLN）网络结构，该网络结构可以进行端到端式的训练，无需额外的候选区域生成模型（以及整合到网络内部），只需要进行一轮优化和前馈计算就可以得到输出结果。网络模型有三部分组成：卷积网络（Convolutional Network）、密集定位层（dense localization layer） 和RNN语言模型。 介绍&emsp;&emsp;本小节主要介绍了dense cationing任务的定义，以及相对应的object detection和image caotioning方面的研究。大家可以自己看一下原文 相关工作&emsp;&emsp;这里只给出重要的2篇论文（作者主要是在这两篇论文的几处上进行模型构建的），其他的可以参见原文 Faster R-CNNhttp://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networksDeep Visual-Semantic Alignments for Generating Image Descriptionshttps://cs.stanford.edu/people/karpathy/deepimagesent/ 模型总览目标：设计一个可以标定出感兴趣区域并且用自然语言描述其中内容的网络框架模型挑战与难点：在兼顾高效性和有效性的前提下，开发出一个可以支持端到端训练并且只需一次优化的模型 模型框架 卷积网络（Convalutional Network）&emsp;&emsp;作者采用了基于VGG-16的网络结构，包含13层卷积核为3×3的卷积层和4层池化核为2×2的最大池化层（原本的VGG是5层池化层，这里作者做了一些小改动，改为4层），因此，对于大小为$3×W×H$的图片，经过卷积网络后，输出结果是$C×W’×H’$的特征图谱，这里$C=512$，$W’=\lfloor\frac{W}{16}\rfloor$，$H’=\lfloor\frac{H}{16}\rfloor$，该特征图谱就是下一层Fully Convolutional Localization Layer的输入。 全卷积定位层（Fully Convolutional Localization Layer）输入和输出 输入 : 来自卷积网络的特征图谱$C×W’×H’$（size任意） 输出 : 输出B个候选区域的表征向量（定长），每个特征向量都包含下面三个关键信息： 候选区域的坐标：输出形式是一个$B×4$的矩阵，每行代表一个候选区域的坐标 候选区域的置信分数：一个长度为$B$的一维列向量，向量内每个元素都给出了候选区域的得分。得分越高说明越可能是真实区域 候选区域的特征：输出形式为$B×C×X×Y$的特征集合，这里B代表区域个数，$X×Y$表示特征图谱的大小（注意，这里的size已经是固定的），$C$代表特征的维度 &emsp;&emsp;这里额外说明一下，在CNN阶段我们不需要指定输入图片的大小（传统CNN分类任务由于FC全连接层的限制，使得输入图片的大小是固定的），因为这里我们关心的是图片的特征，而卷积层和池化层根本不care输出尺寸的多少，它们只负责拿到前一层的特征图谱（feature map）。&emsp;&emsp;但是为什么这里的输出必须是定长的向量呢？主要是因为后面RNN模型的制约，由于RNN模型接受的数据必须的定长的，所以在全卷积定位层（FCL）阶段的最后一步，我们需要使用双线性插值的方法来使输出成为定长的特征向量。 卷积锚点（Convolutional Anchors）&emsp;&emsp;这里的工作主要参考自Faster R-CNN。主要思想是借助一系列具有平移不变性的锚点（anchors）来预测候选区域的位置和大小，具体做法如下：&emsp;&emsp;对于大小为$W’×H’$的特征图谱来说，将图谱中的每一个像素点都做为一个锚点（anchor）（锚点数量为$W’×H’$个），将该点反向映射会原始图像$W*H$中，然后基于该锚点，画出不同宽高比和大小的若干个“锚箱”（anchor box）。下图所示是3个具有相同大小但是不同宽高比的锚箱示例（分别为1:1，1:2，2:1）。 &emsp;&emsp;如果采用Faster R-CNN的设置，即每个锚点对应3个不同的size取值（$128^2，256^2，512^2$）和3个不同的宽高比取值（1:1，1:2，2:1），因此，每个锚点对应的锚箱数量为$k=9$，在本文中采用的是$k=12$，具体对应多少个size和宽高比文中并没有给出。对于这$k$个锚箱，定位层（localization layer）会通过回归模型来预测相应的置信分数（score）和位置信息（scalars）。具体的计算过程是将特征图片作为输入，经过一个卷积核为$3×3$的卷积层（filter个数为256)，然后再经过一个卷积核为$1×1$卷积层（filter个数为$5k$，这里$k$代表anchor box的数量）,所以这一层的最终输出是$5k×W’×H’$的张量，包含了所有锚点对应的置信分数和位置信息。 边界回归（Box Regression）&emsp;&emsp;边界回归主要是对刚刚预测的候选区域的一次精修，进行边界回归的原因主要是当前的候选区域可能与真实区域并不是特别匹配，如下图所示： &emsp;&emsp;图中，绿色框代表真实区域，红色框代表目前的候选区域，我们可以看到，候选区域虽然可以判断出区域内存在物体（飞机），但是它的定位并不是很准取，这时候就可以利用box regression来对边框进行微调。核心思想是利用线性回归得到关于边框的四个位移参数$（t_x,t_y,t_w,t_h）$，然后通过下面的式子对候选区域的中点$（x,y）$和size$（w，h）$进行更新 x=x_a+t_xw_a$$$$ y=y_a+t_yh_a$$$$ w=w_aexp(t_w) $$$$h=h_aexp(h_w)有关box regression的详细讲解可以参考这篇论文：https://blog.csdn.net/zijin0802034/article/details/77685438（PS：这篇论文的讲解是基于R-CNN的，其中的符号表示与本文有些出入，如$t_x,t_y$在R-CNN中代表的是真实区域的中心坐标，看的时候注意一下各个符号都表达了什么，不要搞混了） 区域采样&emsp;&emsp;以图像大小为$W=720，H=540$，锚箱（anchor box）数量为$k=12$的情况为例，得到的候选区域的个数应该为$\lfloor\frac{720}{16}\rfloor×\lfloor\frac{540}{16}\rfloor×12=17820$（文章中写的是17280，我感觉应该是写错了）。为了降低成本，我们只取这些候选区域的子集来参与训练过程和测试过程，具体选取原则如下： 在训练阶段: 采用Faster R-CNN的方法，采集一个大小为$B=256$的minibatch来进行训练，在这$B$个候选区域中，有至多$B/2$个正样本，其余均为负样本。采集时，如果所有的候选区域中（这里为17280个）正样本的数量不足$B/2$个，那么就由负样本补充，所以，最终的minibatch中正样本的数量$B_P\le B/2$，而负样本的数量$B_N=B-B_P$。正样本和负样本的定义如下： 正样本：候选区域与一个或多个真实区域的面积相交部分大于70% 负样本： 候选区域与所有真实区域的面积相交部分小于30% 在测试阶段: 基于每个候选区域的置信分数，采用非极大抑制选取$B=300$个置信分数最高的候选区域 &emsp;&emsp;非极大抑制：这里的抑制就是忽略的意思，非极大抑制的意思就是忽略那些与具有最高score值的候选区域的相交面积大于设定阈值的其他候选区域。这样做的目的主要是为了减少重叠区域的输出，从而更精细化的定位目标位置。 &emsp;&emsp;经过以上操作，最终我们可以得到关于这B个候选区域的位置坐标和置信分数，表示为B×4和B×1的张量，这就是定位层（localization layer）的输出。 双线性插值（Bilinear Interpolaion） &emsp;&emsp;在经过采样后，我们得到的各个候选区域是具有不同大小和宽高比的矩形框。为了与全连接层（主要进行识别分类）和RNN语言模型的进行建立连接，我们必须将候选区域提取成固定大小的特征表示向量。对于这一问题，Faster R-CNN提出了感兴趣区域池化层（RoI pooling layer），具体方法是大小为$W’×H’$的卷积特征图谱进行划分，得到具有$X×Y$个小网格的网格图，然后根据最大池化的原理，将小网格内的像素最大值作为代表该网格的特征像素，最终可以得到定长为$X×Y$的特征向量。划分示意图如下所示。 &emsp;&emsp;RoI pooling layer需要两个输入：卷积特征图谱和候选区域坐标。但是在应用梯度下降时，该方法只能对特征图谱采用反向传播（BP）算法，而不能对候选区域坐标使用BP算法，为了克服这个缺点，在本文中，作者采用了双线性插值。&emsp;&emsp;具体来说，就是对于任意的特征图谱$U（C×W’×H’）$和候选区域，我们要将其放缩成大小为$（C×X×Y）$的特征图谱$V$，放缩过程按照如下步骤进行： 计算$V$到 $U$的反向投影坐标值，例如对于特征图谱$V$中的任意一点坐标$(x_{i,j}^V,y_{i,j}^V)$，投影到$U$中的坐标值为x_{i,j}=x_{i,j}^V*\frac{W'}{X}，y_{i,j}=y_{i,j}^V*\frac{H'}{Y}很容易看出，这里$x_{i,j}和y_{i,j}$的值均为浮点数，然而图像的像素坐标在计算机中必须为整数，所以这里坐标$(x_{i,j},y_{i,j})$对应的像素点是虚拟像素点，并不是$U$中实际存在的点。 按照双线性插值法，得到$U$中$(x_{i,j}^U,y_{i,j}^U)$坐标点的像素值，该像素值就是$V$中对应点的像素值$V_{c,i,j}$，计算公式如下V_{c,i,j}=\sum_{i'=1}^{W’}\sum_{j'=1}^{H'}U_{c,j',j'}k(i'-x_{i,j})k(j'-y_{i,j})，其中 ，k(d)=max(0,1-|d|) 利用上面的方法，计算$V$中所有像素点的坐标值，得到$C×X×Y$的特征图谱 &emsp;&emsp;对于上面的步骤可能理解起来不太直观，下面我们利用一个例子来帮助理解，我们假定源图谱U的大小为4×4，目的图谱V的大小为3×3，如下图所示 如果我们想要知道V中某点的坐标值，以V的中心点为例，我们先计算出V反向投影到U的坐标值$(x_{i,j},y_{i,j})$ x_{i,j}=1*\frac{4}{3}=1.333，y_{i,j}=1*\frac{4}{3}=1.333然后，利用上面的公式计算$V_{c,i,j}$的值 V_{c,i,j}=95*0.667*0.667+32*0.667*0.333+156*0.333*0.667+84*0.333*0.333=93.336\approx 93 最终，对于$B$个候选区域，我们会得到形式为$B×C×X×Y$的一个张量，这就是localization layer的最终输出。 识别网络（Recognition Network）&emsp;&emsp;识别网络以一个全连接的神经网络，它接受的是来自定位层的候选区域的特征矩阵（定长）。将每个候选区域的特征拉伸成一个一维列向量，令其经过两层全连接层，每次都使用ReLU激活函数和Dropout优化原则。最终，对于每一个候选区域，都会生成一个长度为$D=4096$的一维向量。&emsp;&emsp;将所有的正样本的存储起来，形成一个$B×D$形状的矩阵，将该矩阵传送到RNN语言模型中。另外，我们允许识别网络对候选区域的置信分数和位置信息进行二次精修，从而生成每个候选区域最终的置信分数和位置信息，这一次的精修与之前的box regression基本是一样的，只不过是针对这个长度$D$的向量又进行了一次box regression而已（在R-CNN论文中已经指出，理论上是可以通过迭代使用box regression来不断让候选区域无限逼近真实区域的，不过实现表明，对最终的结果提升并不大）。 RNN语言模型（RNN Language Model）&emsp;&emsp;将图片的特征图谱输入到RNN语言模型当中，从而获得基于图片内容的自然语言序列。基本方法是将识别网络的输出结果进行编码（每一个候选区域到对应一个编码），记为$x_{-1}=CNN（I）$，然后将该区域对应的真实描述$s_1,…,s_T$也进行编码，记为$x_1,…x_T$，这里，$x_i$就是对应的$s_i$的向量编码。于是，我们就得到了长度为T+2的单词向量序列$x_{-1},x_0,x_1,…,x_T$，其中$x_{-1}$代表这候选区域的图像信息，$x_0$是特殊的开始标志，$x_1,…x_T$代表每一个单词的向量编码，将这T+2长度的向量序列feed到RNN中，训练出一个预测模型。接着，在预测阶段，训练好的RNN语言模型的 输入是$x_{-1}$和$x_0$ （START token），然后根据公式$h_t,y_t=f(h_{t-1},x_t)$分别计算出隐藏状态$h_0$和单词向量$y_0$。这里，$y_t$是一个长度为$|V|+1$的向量，$V$代表词库的size，多出来的1是一个特殊的END标志，根据$y_0$预测出第一个word，然后将该word再作为下一层LSTM网络（RNN中的语言模型网络）的输入，预测出第二个word，一直 递归 的重复这个过程，直到输出的word是END标志为止。该预测过程可以用下面的公式和两张示意图表示。 x_{-1}=CNN(I)$$$$x_t=W_eS_t，t\in \{ 0...N-1 \} $$$$p_{t+1}=LSTM(x_t)，t\in \{ 0...N-1\}&emsp;&emsp;上式中，$x_{-1}$代表$CNN$生成的$D$维图像特征向量，并且它将作为整个$RNN$语言模型的初始输入，$S_t$代表RNN模型生成的一个个单词（word），其中$S_0$是一个特殊的开始标志，$p_{t+1}$代表第$t+1$个单词在整个单词表中的分布率，它是$p(S_{t+1}|I,S_0,…,S_t)$的简写形式，之后，选取$p_t$概率最大的元素作为句子中第$t$个单词的输出，如果概率最大的元素对应的是$END$标识符，则句子生成结束，迭代终止。 有关RNN模型生成图片描述的详细介绍可以参考下面两篇论文：Show and Tell: A Neural Image Caption Generatorhttps://arxiv.org/abs/1411.4555Deep Visual-Semantic Alignments for Generating Image Descriptionshttps://arxiv.org/abs/1412.2306 损失函数（Loss function）&emsp;&emsp;这篇文章训练时的损失函数有五个，如下图所示，首先是lacalization layer定位层的边框位置回归和置信分数两处损失函数，前者使用smooth L1 loss，后者使用binary logistic loss。损失函数的数学定义可以参考Fast R-CNN和Faster R-CNN里面的损失函数。&emsp;&emsp;接下来是Recognition Network的两处损失函数，该层和localization layer一样，也是边框位置和置信分数两个损失函数，最后是语言模型的损失函数，采用的取交叉熵（cross-entropy）损失函数。&emsp;&emsp;作者利用bathch size和sequence length对所有的损失函数都进行了归一化。经过不断测试，作者发现将后续区域边框的初始权重设为0.1，将图片描述的置信权重设为1.0，是比较高效率的初始化设置。文中并没有对损失函数给出详细定义，通过查阅相关论文后，得到了各个损失函数的详细定义如下： 置信度损失函数（binary logistic loss） : l(w,b)=-\sum_{i=1}^{m}lnP(y_i|x_i;w,b)$$$$P(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$$$$P(y=0|x)=\frac{1}{1+e^{w^Tx+b}}这里，$w$为矩阵，$b$为向量，$x_i$是输入的图像区域的特征图谱，$y_i$为期望的真实输出（is or not object） 边框位置回归损失函数（smooth L1 loss）: L_{loc}(t^u,v)=\sum_{i\in \{x,y,w,h\}}smooth_{L_1}(t_i^u-v_i) smooth_{L_1}(x)=\begin{cases} 0.5x^2& \text{if |x|]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>图片描述 image captioning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《TensorFlow实战Google深度学习框架》]]></title>
    <url>%2Fz_post%2FTensorFLow-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TF%E5%AE%9E%E6%88%98Google%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第九章～第十一章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter9_11%2F</url>
    <content type="text"><![CDATA[第一章 预备知识第二章 开始学习C++第三章 处理数据第四章 复合类型第五章 循环和关系表达式第六章 分支语句和逻辑运算符第七章 函数——C++的编程模块第八章 函数探幽第九章 内存模型和名称空间单独编译 头文件中常包含的内容： （不能将函数定义放在头文件中，容易出现重定义错误） p301 函数原型 使用#define或const定义的符号常量 结构声明 （结构声明不创建变量，只是告诉编译器如果创建该结构变量） 类声明 模板声明 内联函数 在同一个文件中只能将同一个头文件包含一次，利用下述C/C++技术可以避免多次包含同一个头文件。p302 1234#ifndef COORDIN_H_#define COORDIN_H_...#endif 多个库的链接： 不同的编译器可能会为同一个函数生成不同的修饰名称（取决于编译器设计人员），名称的不同将使链接器无法将一个编译器生成的函数调用与另一个编译器生成的函数定义匹配。在链接编译模块时，请确保所有对象文件或库都是由同一个编译器生成的。（如果有源代码，通常可以用自己的编译器重新编译源代码来消除链接错误）。p304 存储持续性、作用域和链接性 C++使用三种（在C++11中是四种）不同的方案来存储数据，这些方案的区别就在于数据保留在内存中的时间：p304 自动存储持续性 静态存储持续性 线程存储持续性（C++11） 动态存储持续性 作用域和链接 作用域（scope）描述了名称在文件（翻译单元）的多大范围可见。链接性（linkage）描述了名称如何在不同单元间共享。 自动变量的名称没有链接性，因为它们不能共享。p305 全局作用域是名称空间作用域的特例。 p305 自动存储持续性 C++11中的auto： 在C++11中，auto关键字用于自动类型推断。但在C语言和以前的C++版本中，auto用于显式的指出变量为自动存储（实际中很少很使用，因为默认就是自动存储类型）。在C++11中，这种用法不再合法。p307 函数及其中的变量存放于“栈”中——这是专门流出来的一段内存，栈的长度由具体的实现决定。p308 寄存器变量：在C++11中，关键字register的作用只是显示地指出变量是自动的。鉴于它只能用于原本就是自动的变量，使用它的唯一原因是，指出程序员想使用一个自动变量。保留该关键字的原因是避免使用了该关键字的现有代码非法。p309 静态持续变量 和C语言一样，C++也为 静态 存储持续性变量提供了3种链接性，这三种链接性都在整个程序执行期间存在，与自动变量相比，它们的寿命更长。p309 外部链接性（可在其他文件中访问） 内部链接性（只能在当前文件中访问） 无链接性（只能在当前函数或代码块中访问，与自动变量不同的是，就算不在函数中，变量也存在，只是不能访问） 由于静态变量的数目在程序运行期间是不变的，因此程序不需要使用特殊的装置（如栈）来管理它们。编译器将分配固定的内存块来存储所有的静态变量，这些变量在整个程序执行期间一直存在。另外，如果没有显式地初始化静态变量，编译器将把它设置为0。在默认情况下，静态 数组和结构将每个元素或成员的所有位都设置为0。p309 创建三种链接性的静态持续变量：p309 外部链接性：必须在代码块的外面声明 内部链接性：必须在代码块的外面声明，并使用static限定符 无链接性：必须在代码块内部声明，并使用static限定符123456789int global = 1000; //静态持续变量，外部链接性，作用域为整个文件static int one_file = 50; //静态持续变量，内部链接性，作用域为整个文件int main()&#123; ...&#125;void funct1(int n)&#123; static int count = 0; //静态持续变量，无链接性，作用域为局部 int llama = 0;&#125; 五种变量存储方式：p310 自动 寄存器 静态，无链接 静态，外部链接 静态，内部链接 关键字重载： 关键字的含义取决于上下文，static用于局部声明，以指出变量是无链接性的静态变量时，表示的是存储持续性。而用于代码块外的声明时，static表示内部链接性，因为位于代码块外的变量已经是静态持续性了。p310 静态变量的初始化： 静态变量有三种初始化方式：零初始化（变量设为零）、常量表达式初始化和动态初始化。 零初始化和常量表达式初始化被统称为静态初始化，这意味着在编译器处理文件时初始化变量，动态初始化意味着变量将在编译后初始化。p310 静态变量的初始化过程： 首先，所有静态变量都被零初始化，而不管程序员是否显式地初始化了它。接下来，如果使用常量表达式初始化了变量，且编译器仅根据文件内容（包括被包含的头文件）就可计算表达式，编译器将执行常量表达式初始化。必要时，编译器将执行简单计算。最后，剩下的变量将被动态初始化。 常量表达式并非只能是使用字面常量的算术表达式。（sizeof运算符也可以）p310 链接性为外部的变量通常简称为外部变量，也称全局变量，它们的存储持续性为静态，作用域为整个文件。p310 静态持续性、外部链接性 单定义规则（One Definition Rule，ODR）： 变量只能定义一次。为满足这种需求，C++提供了两种变量声明：p311 定义声明（简称定义）：为变量分配存储空间。 引用声明（简称声明）：不给变量分配存储空间，引用已有的变量。使用关键字extern12double up; //定义声明exterm int blem; //blem在别处定义 如果要在多个文件中使用外部变量，只需在一个文件中包含该变量的定义（单定义规则），但在使用该变量的其他所有文件中，都必须使用关键字extern声明它。p311 1234567//file01.cppextern int cats = 20; // 由于初始化，所以这里是定义而非声明int dogs = 22; //定义//即使去掉file01.cpp文件中的extern也无妨，效果相同。//file02.cppextern int cats; //使用extern且无初始化，说明使用的是其他文件的catsextern int dogs; //同上 静态持续性、内部链接性 将作用域为整个文件的变量声明为静态外部变量（内部链接性），就不必担心其名称与其他文件中的外部变量发生冲突123456789101112131415//file1int errors = 20;//file2int errors = 5;int main()&#123; cout&lt;&lt;errors; //报错，errors与file1中的外部变量重定义 ...&#125;//解决方法：file2static int errors = 5;int main()&#123; cout&lt;&lt;errors; // 输出5&#125; 静态存储持续性、无链接性 局部静态变量：虽然该变量只在该代码块中可用，但它在该代码块不处于活动状态时仍然存在。因此在两次函数调用之间，静态局部变量的值将 保持不变 。另外，如果初始化了静态局部变量，则程序 只在启动时进行一次初始化 。以后再调用函数时，将不会被再次初始化。p315 说明符和限定符 存储说明符（storage class specifier）：p317 auto（在C++11中不再是说明符） register static extern thread_local（C++11新增的） mutable：即使结构（或类）变量为const，其某个成员也可以被修改 cv-限定符（cv-qualifer）：p317 const：内存被初始化后，程序便不能再对它进行修改 volatile：即使程序代码没有对内存单元进行修改，其值也可能发生变化 在默认情况下全局变量的链接性为外部，但const全局变量的链接性为内部。因此，将一组常量放在头文件中，其他引用该头文件的文件都相当于自己定义了私有的常量，这就是能够将常量定义放在头文件中而不会重定义的原因。p318 如果处于某种原因，程序员希望某个常量的链接性为外部的，则可以使用extern关键字来覆盖默认的内部链接性，extern const int states = 50;，在这种情况下，必须在所有使用该常量的文件中使用extern关键字来声明它。p318 函数和链接性 C++不允许在一个函数中定义另一个函数，因此所有函数的存储持续性都自动为静态，即在整个程序执行期间都一直存在。p318 在默认情况下，函数的链接性为外部。即可以在文件间共享，使用extern来指出函数实在另一个文件中定义的（可选）。p318 可以使用关键字static将函数的链接性设置为内部，使之只能在一个文件中使用，必须同时在原型和函数定义中使用该关键字。p318 内联函数不受单定义规则的约束，这允许程序员能够将内联函数的定义放在头文件中。但是C++要求同一个函数的所有内联定义都必须相同。 p319 C++查找函数顺序：静态（在本文件中找）——外部（在所有的程序文件中找）——在库函数中找。因此如果定义了一个与库函数同名的函数，编译器优先使用程序员定义的版本（C++不推荐这样做）。p319 语言链接性 不同的语言采用了不同的链接性，为了解决这种问题，需要特别指定函数采用的链接性（默认为C++链接性）。p319 存储方案和动态分配 前面介绍的分配内存的5种方案（线程内存除外），它们不适用于C++运算符new分配的内存，这种内存被称为动态内存。动态内存由运算符new和delete控制，而不是由作用域和链接性规则控制。 p320 使用new运算符初始化： 1234567891011//如果要为内置的标量类型分配存储空间并初始化，可在类型名后面加上括号和初始值int *pi = new int(6);double *pd = new double(99.99);//如果要初始化常规结构或数组，需要用大括号的列表初始化，这要求编译器支持C++11.struct where &#123;double x; double y; double z;&#125;;where *one = new where&#123;2.5, 5.3, 6.2&#125;;int *ar = new int [4]&#123;2,4,6,8&#125;;//列表初始化也可以用于单值变量int *pin = new int&#123;6&#125;;double *pdo = new doubel&#123;99.99&#125;; new失败时，在最初的10年中，C++在这种情况下让new返回空指针，但现在将引发异常std::bad_alloc。p320 运算符new和new[]分别调用函数1和2，同样delete和delete[]调用3和4。p320 123456789101112void * operator new(std::size_t);void * operator new[](std::size_t);void * operator delete(void *);void * operator delete[](void *);//std::size_t是一个typedef，对应于合适的整型int *pi = new int;//该式会被转换为下式int *pi = new(sizeof(int));int *pa = new int[40];//同样，转换为下式int *pa = new(40*sizeof(int));delete pi;//同样，转换为下式delete(pi); 定位new运算符。p321 名称空间传统的C++名称空间 一些基本术语：p324 声名区域：变量可以进行声明的区域。对于全局变量，其声明区域为所在的文件，对于局部变量，其声明区域为所在的代码块。 潜在作用域：变量的潜在作用域从声明点开始，到其声明区域的结尾。因此潜在作用域比声名区域小。 作用域：变量对程序而言可见的范围。变量并非在其潜在作用域内的任何位置都可见，如被另一个嵌套声明区域中的同名变量隐藏。作用域小于潜在作用域。 新的名称空间特性 一个名称空间中的名称不会和另一个名称空间的相同名称发生冲突，利用新的关键字namespace可以创建名称空间：p325 12345678910111213141516171819202122namespce Jack&#123; double pail; void fetch();&#125;namespace Jill&#123; double fetch; int pal;&#125;//名称空间是开放的，可以重复使用namespace来将名称添加到名称空间中namespace Jack&#123; char * goose(const char*); //将goose添加到Jack名称空间（已有pail和fetch）&#125;//可以在另一个文件中使用namespce为函数原型写出定义namespace Jack&#123; void fetch()&#123; ... &#125;&#125;//使用作用域解析运算符来使用名称空间Jack::pail = 12.35;Jill::pal = 1;Jack::fetch(); 名称空间可以是全局的，也可以位于另一个名称空间中，但不能位于代码块中。保持，在默认情况下，在名称空间中声明的名称的链接性是外部的（除非使用了const）。p326 using声明和using编译指令： p326 using声明：using Jack::fetch 使特定的标识符可用（可以用在代码块中）。 using编译指令：using namespace Jack 使整个名称空间可用（可以用在代码块中，放在代码块中时，虽然它只在该代码块中可见，但是其作用域不是布局的）。p328 使用using编译指令和使用多个using声明是不一样的。假设名称空间和声明区域定义了相同的名称。如果试图使用using声明将名称空间的名称导入该声明区域，则这两个名称会发生冲突，从而出错。如果使用using编译指令将该名称空间的名称导入该声明区域，则局部版本将隐藏名称空间版本。p328 推荐使用using声明而不是using编译指令，因为前者更安全。在引入的名称有相同局部名称时，前者会发出错误提示，后者只会隐藏名称空间版本而不进行提示。p329 名称空间可以嵌套：1234567891011121314151617181920namespce elements&#123; namespce fire&#123; int flame; &#125; using Jill::fetch; using namepace Jack; float water;&#125;using namespace elements;using namespace elements::fire;//访问Jill::fetch，由于在elements中声明了Jill::fetch，所以以下两种名称空间都可用Jill::fetch;elements::fetch;using namespace elements;//这条编译指令与下面两条编译指令等价using namespace elements;using namespace Jack;namespace ele = elements; //给elements创建别名 名称空间示例名称空间及其用途 指导原则：p334 使用在已命名的名称空间中声明的变量，而不是使用外部全局变量。 使用在已命名的名称空间中声明的变量，而不是使用静态全局变量。 如果开发了一个函数库或类库，将其放在一个名称空间中。 仅将编译指令using作为一种将旧代码转换为使用名称空间的权益之计。 不要在头文件中使用using编译指令。首先，这样做掩盖了要让哪些名称可用，另外，包含头文件的顺序可能影响程序的行为。 导入名称时，首选使用作用域解析运算符或using声明的办法。 对于using声明，首选将其作用域设置为局部而不是全局。 第十章 对象和类过程性编程和面向对象编程 面向对象变成（OOP），首先从用户的角度考虑对象——描述对象所需的数据以及描述用户与数据交互所需的操作。p341 抽象和类类型是什么 指定基本类型完成了三项工作：p342 决定数据对象需要的内存数量 决定如何解释内存中的位（long与float位数相同，但含义不同） 决定可使用数据对象执行的操作或方法 10.2.2 C++中的类 类规范由两个部分组成：p342 类声明：以数据成员的方式描述数据部分，以成员函数（方法）的方式描述公有接口——提供了类的蓝图。 类方法定义：描述如何实现类成员函数——提供了类的实现细节。 类对象成员访问类型默认为私有private。结构体成员访问类型默认为公有public。p345 类和结构的区别： 实际上，在C++中，对结构进行了扩展，使之具有与类相同的特性。它们之间唯一的区别是，结构的默认访问类型是public，而类的默认访问类型是private。C++程序员通常使用类来实现类描述，而把结构限制为只表示纯碎的数据对象。（看上去类可以完美替代结构体，事实上也是这样，C++保留结构体的主要原因是为了向C兼容）。 实现类成员函数 定义成员函数时，使用作用域解析符（::）来标识函数所属的类。类方法可以直接访问类的组件（private和public均可，并且无需使用作用域解析符）。 p345 1234//无需使用public，因为在声明函数原型时已经指明了访问类型void Stock::update(double price)&#123; ...&#125; 内联方法： 方法定义位于类声明处的函数都将自动成为内联函数。类声明常常将短小的成员函数作为内联函数。内联函数的特殊规则要求在每个使用它们的文件中都对其进行定义，因此通常将内联定义放在定义类的头文件中。p347 12345678class Stock&#123; private: int shares; double share_val; void set_tot() &#123;total_val = shares*share_val;&#125; //自动成为内联函数 public: ...&#125; 类的每个新对象都有自己的存储空间，用于存储其内部变量和类成员。但是同一个类的所有对象共享同一组类方法，即每种方法只有一个副本。p348 使用类 要创建类对象，可以像基本类型一样声明类对象Stock kate，joe; //声明了2个对象kate和joe，也可以使用new为类对象分配存储空间。p349 修改实现 利用setf()控制输出格式，并将修改限定在实现文件中，以免影响程序的其他方面。p351 类的构造函数和析构函数声明和定义构造函数 构造函数没有返回类型。并且，构造函数的形参名称不能与类成员变量的形参名称完全相同，一种常见做法是在数据成员中使用m_前缀，或者用this指针this-&gt;company = company。p3531234567891011class Stock&#123; private: string m_company; ... public: Stock(const string &amp;company);更&#125;Stock::Stock(const string &amp;company)&#123; m_company = company;&#125; 使用构造函数 C++提供了两种使用构造函数来初始化对象的方式。p35412Stock garment = Stock(&quot;Furry&quot;); //显示调用构造函数Stock garment(&quot;Furry&quot;); //隐式调用构造函数，二者等价 默认构造函数 当且进党没有定义任何构造函数时，编译器会提供一个默认构造函数，它不接受任何参数，也不做任何操作。它可以使得下述语句正常运行：p354 1Stock cat; //隐式地调用了默认构造函数 如果为类定义了构造函数，程序员就必须为它显式提供默认构造函数，除非不使用无参数的对象声明Stock cat;，否则会报错。定义默认构造函数的方式有两种：p354 12Stock(const string &amp; company = &quot;default_company&quot;); //为所有参数提供默认值Stock(); //函数重载定义无参数的构造函数 隐式地调用默认构造函数时，不要使用圆括号：p355 1234Stock first(&quot;Furry&quot;); //隐式调用非默认构造函数Stock second(); //这是一条声明语句，指出second()是一个返回Stock对象的函数Stock third; //隐式调用默认构造参数Stock third = Stock(); //显式调用默认构造参数 接受一个参数的构造函数（或者其它的参数提供了默认值）允许使用赋值语法将对象初始化为一个值。p362 1Classname object = value; 带参数的构造函数也可以是默认的构造函数，只要所有参数都有默认值。但是只能有一个默认构造参数，也就是说，一旦所有参数都提供了默认值，就不能再声明无参数的构造函数，否则会产生二义性错误。 p433 析构函数 如果程序员没有提供析构函数，编译器将隐式的声明一个析构函数，析构函数没有返回类型，也没有参数，在声明时，需要在类型前加上波浪号：p355 1234567class Stock&#123; public: ~Stock(); //声明 &#125;Stock::~Stock()&#123; //定义&#125; 编译器调用析构函数的时机：p356 静态存储类对象：在程序结束时自动被调用 自动存储类对象：在程序执行完代码块时自动被调用 new创建的对象：当使用delete来释放对象内存时自动被调用 构造函数的另一种用法——赋值。语句1为初始化语句，语句2为赋值语句，构造函数会创建一个 临时 的对象，然后将该对象的值赋给已经存在的对象stock1，之后编译器会自动调用析构函数 删除该临时对象 。p361 123Stock stock1 = Stock(&quot;test1&quot;);stock1 = Stock(&quot;test2&quot;);//如果既可以通过初始化，也可以通过赋值来设置对象的值，则应采用初始化方式，通常这种方式的效率更高。 可以使用C++11的列表初始化方式来作用于类，前提是提供了相应的构造函数。p361 12Stock hot_tip = &#123;&quot;Plus&quot; ,100, 45.0&#125;;Stock jock&#123;&quot;Sport&quot;&#125;; 以上两个声明中，用大括号括起的列表与下面的构造函数匹配：1Stock::Stock(const std::string&amp; co, long n =0, double pr = 0.0); 另外，C++11还提供了名为std::initialize_list的类，可将其用作函数参数或方法参数的类型。这个类可表示任意长度的列表，只要所有的列表项的类型都相同或可转换为相同的类型。（在16章介绍）。 C++的成员函数如果不修改调用对象，则应将其声明为const，将const关键字放在函数的括号后面。（放在前面就变成了返回类型为const double了）p36212345678class Stock&#123; public: double show() const; //const成员函数声明&#125;double Stock::show() const&#123; //const成员函数定义&#125; this指针 this指针指向用来调用成员函数的对象（this被作为隐藏参数传递给方法）。一定要注意this是一个指向对象的指针，所以在使用时要按照指针的方式。p36412this-&gt;shares; //用间接成员运算符-&gt;引用对象的成员return *this; //返回this指向的对象 对象数组 利用对象数组可以创建同一个类的多个对象。p368 123456Stock mystuff[4]; //调用默认构造函数Stock stocks[4]=&#123; //为每个元素调用指定的构造函数 Stock(&quot;NanoSmar&quot;); Stock(); //显示调用默认构造函数 //stocks[2]和stock[3]未指明构造函数，将调用默认构成函数&#125; 初始化对象数组的方案是，首先使用默认构造函数创建数组元素，然后花括号中的构造函数将创建临时对象，然后将临时对象的内容复制到相应的元素中。因此，要创建类对象数组，则这个类必须有默认构造函数。p369 类作用域 C++类引入了一种新的作用域：类作用域。在类中定义的名称（如类数据成员名和类成员函数名）的作用域都为整个类，作用域为整个类的名称只在该类中是已知的，在类外是不可知的。要调用公有成员函数，必须通过对象访问。同样，在定义成员函数时，必须使用作用域解析符。 作用域为类的常量 直接在类中声明const常量是非法的，因为声明类只是描述了对象的形式，并没有创建对象。因此，在创建对象前，将没有用于储存值的空间。p371 实现“类的常量”的两种方式： 方式1：使用枚举，在类中声明一个枚举，用枚举为 整型常量 提供作用域为整个类的符号名称。 方式2：使用static，这将创建一个常量，该常量将于其他静态变量存储在一起，而不是存储在对象中。该常量被所有的类对象共享。123456class Bakery&#123; private: const int Months = 12; //非法，无法编译 enum &#123;Months = 12&#125;; //未提供枚举名，这种方式声明枚举并不会创建类数据成员，Months只是一个符号名称，在编译时，将用12来替换它。 static const int Months = 12; //C++98中，不能存储double常量，C++11消除了这种限制&#125; 作用域内枚举 传统的枚举如果两个枚举定义中的枚举量名称相同，则会发生冲突，C++利用类作用域的方法消除了这种冲突。p372 123456789//传统枚举量，产生冲突enum egg &#123;Small, Medium, Large, Jumbo&#125;;enum t_shirt &#123;Small, Medium, Large, Xlarge&#125;;//类作用域，不冲突。 也可以利用关键字struct代替class。enum class egg &#123;Small, Medium, Large, Jumbo&#125;;enum class t_shirt &#123;Small, Medium, Large, Xlarge&#125;;//使用时用枚举名和作用域解析符来限定枚举量：egg choice = egg::Large;t_shirt t_choice = t_shirt::Large; C++11还提高了作用域内枚举的类型安全，在有些情况下，常规枚举将自动转换为整型，如将其赋给int变量或用于比较表达式时，但作用域内枚举不能隐式地转换为整型。p372 枚举有某种底层整型类型表示，在C++98中，如何选择取决于实现，因此包含枚举的结构的长度可能随系统而异。对于作用域内枚举，C++11消除了这种依赖性。默认情况下，C++11作用域内枚举的底层类型为int。而常规枚举的底层类型依然随实现而异。另外，C++11提供了指定底层类型的语法。p3721enum class :short pizza &#123;Small,Medium,Large,XLarge&#125;; //:short将底层类型指定为short 抽象数据类型 类很适合描述ADT。公有成员函数接口提供了ADT描述的服务，类的私有部分和类方法的代码提供了实现，这些实现对类的客户隐藏。p373 第十一章 使用类运算符重载 要重载运算符，需使用被成为运算符函数的特殊函数形式。op必须是有效的C++运算符，不能虚构一个新的符号。p381123456789101112operatorop(argument-list)&#123;&#125;Stock::operator+(...)&#123;&#125;Stock::operator*(...)&#123;&#125;//当编译器发现了运算符的操作数是对应的对象是，会自动替换运算符为重载函数stock3 = stock1 + stock2; //左侧的操作数为调用对象，右侧为重载函数的参数stock3 = stock1.operator+(stock2); //用operaotr+重载函数替换“+” 计算时间：一个运算符重载示例添加加法运算符 对于连加或连乘，需要函数返回的是正确的对象类型。p38712t4 = t1 + t2 + t3;t4 = t1.operator+(t2.opertor+(t3)); //当operator+返回的函数类型复合其参数列表要求时，合法。 重载限制 重载的运算符不必是成员函数，但必须至少有一个操作数是用户定义的类型，这是为了防止用户为标准类型重载运算符。p387 使用运算符时不能违反运算符原来的语法规则，如双目不能重载成单目，同时，重载不会修改运算符的优先级。p387 不能创建新的运算符。p387 不能重载下面的运算符： p387 “sizeof”运算符 “.”成员运算符 “.* ”成员指针运算符 “::” 作用域解析运算符 “? :” 三目条件运算符 “typeid” 一个RTTI运算符 “const_cast” 强制类型转换运算符 “dynamic_cast” 强制类型转换运算符 “reinterpret_cast” 强制类型转换运算符。 “static_cast” 强制类型转换运算符 大多数运算符都可以通过成员或非成员函数进行重载，但下面 的运算符只能通过成员函数进行重载： p387 “=” 赋值运算符 “()” 函数调用运算符 “[]” 下标运算符 “-&gt;” 通过指针访问类成员的运算符 友元 除了private，public和protect控制的类访问权限外，C++提供了另外一种形式的方式权限：友元。通过让函数成为类的友元，可以赋予该函数与类的成员函数相同的访问权限。友元有3种： p391 友元函数 友元类 友元成员函数 创建友元 创建友元的第一步是将其原型 放在类声明中 ，并在原型声明前加上关键字friend：p391 123friend Time operator*(double m, const Time &amp; t);//可以解决2.85*time的乘法重载的问题，2.85不是Time对象，因此无法调用成员重载函数，需要借助友元非成员函数实现。//该声明意味着：1、虽然该函数是在类声明中声明的，但它不是成员函数，因此不能使用成员运算符来调用; 2、虽然该函数不是成员函数，但它与成员函数的访问权限相同。 编写友元函数的定义。因为它不是成员函数，所以无需使用类名::限定符，另外，定义时不要在函数头使用关键字friend。p392 常用的友元：重载&lt;&lt;运算符 cout是一个ostream对象，对于每种基本类型ostream类声明中都包含了相应的重载的operator&lt;&lt;()定义。因此，对于不同的基本类型，&lt;&lt;运算符在cout对象中可以表现出不同行为。p392 &lt;&lt;的第一种重载版本 如果直接通过类声明来重载operator&lt;&lt;()函数，那么在使用时就会像这样，time&lt;&lt;cout;，其中，time是Time类的实例，而cout是Time类重载函数的参数，为了看起来不那么迷惑，利用友元函数，使其第一个参数为ostream对象，这样一来，就可以使用cout&lt;&lt;time的形式（运算符左侧操作数是第一个参数）。p393 123456void operator&lt;&lt;(ostream &amp;os, const Time &amp;t)&#123; os&lt;&lt;t.hours&lt;&lt;t.minutes; //os是cout的引用，别名，省去了拷贝副本的时间&#125;cout&lt;&lt;time1; //等价于下式operator&lt;&lt;(cout,time1); &lt;&lt;的第二种重载版本 上面的重载方法有一些问题，那就是无法使用cout&lt;&lt;time1&lt;&lt;time2&lt;&lt;endl;这样的形式，解决方法如下：p394 1234ostream &amp; operator&lt;&lt;(ostream &amp; os, const Time &amp; t)&#123; os&lt;&lt;t.hours&lt;&lt;t.minutes; return os; //返回os的引用，以便实现连续使用&lt;&lt;的操作。&#125; 重载运算符：作为成员函数还是非成员函数 成员函数和非成员函数的实现方法二者均可，但不能都实现，否则会产生二义性错误。p398 再谈重载：一个矢量类 一个应用了运算符重载和友元设计的例子——矢量类。p398 类的自动转换和强制类型转换 只有一个参数的构造函数可以作为转换函数。如果使用关键字explicit限定了这种构造函数，则它只能用于显示转换，否则也可以用于隐式转换。p413 转换函数 转进行从对象到基本类型的转换，必须使用特殊的C++运算符——转换函数operator typeName()。创建转换函数时，需要注意以下几点：p415 转换函数必须是类方法 转换函数不能指定返回类型 转换函数不能有参数12345678operator double() const; //转换为double类型的函数原型。Stonewt::operator double() const&#123; // 转换函数的定义 return pounds;&#125;double d = stonewt; //隐式调用转换函数double d = double(stonewt); //显式调用转换函数 在进行类型转换时，一定要注意是否有二义性，如果有，编译器将产生错误。p418 提供执行自动、隐式的转换函数存在的问题是，在用户不希望进行转换时，转换函数也可能进行转换。消除这种隐患的方式是在转换函数原型前加上关键字explicit。（C++98不能将explicit用于转换函数，C++11可以）。另一种方法是使用功能相同的非转换函数，在进行转换时显式调用该函数即可。p419 总之，C++为类提供了下面的类型转换：p419 只有一个参数的类构造函数用于将类型与该参数相同的值转换为类类型。在构造函数声明中使用explicit可防止隐式转换。 被称为转换函数的特殊类成员运算符函数，用于将类对象转换为其他类型。没有返回类型、没有参数，名为operator typeName()。 将加法等二元运算符定义为友元可以让程序更容易适应自动类型转换。因为这会可以自动将对象类型转换成基本类型，或者将基本类型转换为对象类型进行运算。p420 将double变量与对象相加，由两种选择。一种是借助类型转换，另一种是在重载函数中显式接受double参数而不进行类型转换。 前者定义简单，但需要类型转换，增加了内存和时间开销。后面定义麻烦，需要写更多逻辑，但运行速度快。p421]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-Ubuntu个人必装软件]]></title>
    <url>%2Fz_post%2FLinux-Ubuntu%E4%B8%AA%E4%BA%BA%E5%BF%85%E8%A3%85%E8%BD%AF%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[vimoh-my-zshChromeAtomGuake TerminalEverNoteFoxit ReaderTeamViewerSogouinputVMware WorkstationGpartedVCL]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Plan]]></title>
    <url>%2Fz_post%2F%E5%85%B6%E4%BB%96-Plan%2F</url>
    <content type="text"><![CDATA[论文阅读Speed_accuracy_trade-offs(优先) ResNet ResNeXt Inception Inception-ResNet Non Local FCIS FCN cascade rcnn TDM MobileNet An analysis ofdeep neural network models for practical applications instance-aware semantic segmentation via multi-task network cascade cvpr2016MR CNN iccv2015attention net iccv2015g-cnn cvpr2016 空洞卷积, Deeplab]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《C++ PrimerPlus》 第一章～第八章]]></title>
    <url>%2Fz_post%2FCpp-Book-CppPrimerPlusChapter1_8%2F</url>
    <content type="text"><![CDATA[第一章 预备知识C++简介 C++融合了三种不同的变成方式：1、C语言代表的过程性语言 2、带有类的面向对象语言 3、C++模板支持的泛型编程 C++简史20世纪70年代早期，贝尔实验室的Dennis Ritchie开发了C语言。 20世纪80年代，贝尔实验室的Bjarne Stroustrup开发了C++语言。 可移植性和标准C++98 C++11 程序创建的技巧编译和链接 第二章 开始学习C++进入C++输入输出：C++能在使用printf()、scanf()和其他所有标准的C输入输出函数，只需要包含常规的C语言的stdio.h文件即可 main函数： main函数是被操作系统调用的，他是程序与操作系统之间的接口。p14 int main( void ) 在括号中用void明确指出，函数不接受任何参数，在CPP中，让括号空着与使用void完全等效。但是在C中，让括号空着意味着对于是否接受参数保持沉默。p15 许多程序员喜欢使用下面的函数头，并省略返回语句：void main()。这在逻辑上是可以理解的，大部分系统也适用，但由于它不是当前标准的内容，因此在有些系统上不能工作。新的标准中对这一点作出了让步，如果编译器到达main()函数末尾时没有遇到返回语句，则自动添加return 0语句 （只对main函数有效，对其他函数不会隐含return 0）。p15 （疑问：在测试的时候报错说main函数必须是int返回类型？同时，非main函数也可以不写明return语句，但是返回的值是6295680？？） 有一些非标准函数，他们使用_tmain() 形式，这种情况下，有一个隐藏的main（）调用它，但是常规的独立程序都需要main()。p15 头文件名，名称空间： 新标准的CPP不适用头文件的.h扩展名，而利用命名空间机制。 新的cout，cin为了避免产生函数名冲突，需要使用std::cout，std::cin来使用 如果使用using namespace std； 则表示std名称空间中的所要名称都可用，但这是一个隐患，推荐使用using std::cout的方式（为了方便，大多会使用using namespace std） p33：using namespace std可以放在main中，表示只有main可以访问其命名空间，也可以放在iostream下面，表示文件中的所有函数都能访问 使用cout进行输出： cout是一个预定义的对象，是某个类的特定实例，&lt;&lt;符号表示它将后面的字符串发送给cout：cout&lt;&lt;string 从概念上看，输出是一个流，即从程序流出的一系列字符。cout对象表示这种流，其属性定义在iostream文件中，&lt;&lt;是cout对象的一个属性，它表示将其右侧的信息插入到流中，该符号与按位左移运算符实际上是重载关系 打印的时候，cout会将整数形式的数字自动转换成字符串形式，注意整数25与字符串25有天壤之别 endl确保程序继续运行前刷新输出？控制符：诸如endl等对于cout来说有特殊含义的符号（manipulator）传统Cpp不能把回车放在字符串中间，但是C++11新增的原始字符串可以包含回车 C++语句其他C++语句 cin.get（） 一般需要两条，一条用于接受多余的换行，有一条用于让程序暂停。cin使用&gt;&gt;运算符才输入流中抽取字符。p24 函数 C++函数和C一样，不允许嵌套定义。p30 main不是关键字，因为它不是语言的组成部分，可以做关键字，但最好别这样，会引起其他错误。cout也不是关键字，而是一个对象名，所以可以在不适用cout的程序中，将cout用作变量名。p31 第三章 处理数据简单变量变量名，符号类型： CPP命名规则：以两个下划线或下划线和大写字母打头的名称被保留给实现（编译器及其使用的资源）。p38 CPP的字节位数根据字符集的大小而定，对于基本字符集ASCII和EBCDIC来说，一字节为8为，而对于国际编程Unicode来说，一字节可能为16为或32位，int在老式机器中一般为16位，而在现在多为32位。p39 预编译指令#define是c遗留下来的，cpp中多用const关键字p42。 cpp中有一种c没有的初始化赋值语法：int a（42）。C++11具有新的初始化方式。p42 常数后缀，ul，lu，uL，LU等等都可以，均表示unsigned long常量。在cpp中，对十进制整数采用的长度规则，与16进制和8进制略有不同。p47 通用字符名，以/u开头 Unicode与ISO 10646。p52 char默认情况下既不是无符号，也不是有符号。 wcha_t 与 underlying（底层类型） cin和cout将输入和输出看做是char流，因此不适合用来处理wchar_t类型，以l或L为前缀应用wcin和wcout。cpp11新增的char16_t char32_t分别以u和U为前缀。p53 const限定符 p54：const比#define更好，1.它可以明确指定类型，2.cpp的作用于规则将定义限制在特定的函数或头文件中，3.const可用于复杂类型，如数组和结构体 浮点数C++算术运算符 求模运算符只能用于整形。p59 对于float类型，11.17+50.25=61.419998 具体愿意是float的精度限制所导致的（将操作数转化成二进制即可理解）。p60 数值类型转换，对于精度丢失的情况，最终结果会根据系统的不同而不同。p63 c++11中的{}初始化赋值法不允许narrowing缩窄，即只能小赋给大，不能大赋给小（但是const可以，只要能hold住要赋的值即可）。p64整型提升：c++在计算表达式时自动将bool char unsigned char signed char short转换为int。如果shot比int短，则unsigned short类型将被转化为int，如果长度相同，则unsigned short将被转化为unsigned int，以此确保在对unsigned short进行提升时不会损失数据。wchar_t被提升为下列类型中第一个宽度足够的类型：int，unsigned int，long，unsigned long。更多转化规则可以查看校验表p64 强制类型转化通用格式：（typeName）value；typeName（value）第一种格式来自C语法，第二种是纯粹C++语法。p65 c++11中新增了auto类型声明的用法，让编译器根据初始值的类型推断变量的类型。主要用于复杂类型。p66 第四章 复合类型数组 c++中数组的arraySize只能是常量，const，或常量表达式，不能是变量。p71 字符串 c++11初始化数组时，可以省略等号。c++标准模板库（STL）提供了一种数组替代品模板类vector，c++11新增了模板类array。p74 ‘s’表示83 “s”表示的是某块内存的地址。 cout会默认自动拼接两段字符串，并且可以不在同一行。p75 c++使用空白（空格，制表，换行）来确定字符串的结束位置。为了读取空白可以采用cin的成员函数面向行的输入：cin.getline（）和cin.get（）。二者以换行为结束，前者会舍弃换行符，后者会将其保留在输入队列中（注意是输入队列，这相当于输入缓冲区，下面读取函数有可能会读到这个换行符）。二者的返回值为cin对象，可以继续调用函数。getline（）使用起来更简单方便，但get（）更能检查出错误。另外要注意二者读取空行时的区别。p78 string类 string对象和字符数组之间的主要区别是string对象可以声明为简单变量，类设计让程序能够自动处理string的大小。p83 原始字符串 raw。p87 结构 p89：C++允许在声明结构变量时省略关键字struct。但是C不允许 p92：c++的结构特性比C更多。 位字段，共用体（长度为其最大成员长度） 共用体枚举 对于枚举变量，只有赋值运算符，枚举创建的是符号常量，可以代替const。枚举量的值可以重复。p96 指针：*运算符称为间接值（indirect value）或解除引用（dereferencing）。p101：不管是指向何种类型的指针，其指针变量本身的长度是一定的。p9917.10.19 指针和自由存储空间 C++利用new关键字代替了malloc（）来分配内存：int* p=new int; 用指针和new进行的内存分配是在程序运行时进行的（只有运行时，指针才知道它指向的是哪一块地址）。p102 delete关键字只能释放new的内存，不能用于一般变量，同时，不可以重复释放，否则结果未知。 不能用sizeof运算符确定动态数组包含的字节数。p104 指针、数组和指针算术 指向数组的指针和数组名基本等价，区别是：1，指针值可以变，而数组名的值不能变。2，sizeof用在数组名上返回数组长度，用在指针上放回指针的长度。注意short tell[10]; 中tell与&amp;tell的关系。p109 cout打印字符数组的关键不在于变量是一个数组名，而在于它是一个char的地址！在cout和多数c++表达式中，char数组名，char指针和双引号下的字符串常量都被解释为字符串第一个字符的地址。p109 第五章 循环和关系表达式for循环while循环do while循环基于范围的for循环（C++11) c++11新增了一种基于范围的for循环，它简化了一种常见的循环任务：对数组或容器类的循环for（int x：arr）和for（int &amp;x：arr），前者不可以改变x的值，后者可以。5.5节详解cin.get（）函数。p152 循环和文本输入 cin在获取用户输入的字符时，将忽略空格和换行符，并且，发送给cin的输入会被缓冲，只有在用户按下回车键后，他输入的内容才会被发送给程序，为了读取空格和换行符，可以利用cin.get（char）进行补救，char的函数声明是引用，所以，可以改变char的值。p154 第六章 分支语句和逻辑运算符if语句逻辑表达式字符函数库cctype?:运算符switch语句 p181 ：c++的switch语句中必须是整数表达式，一般为int或char或枚举 break和continue语句读取数字的循环简单文件输入/输出 打开已经存在的文件，接受输出时，默认将它的长度截断为零，文件原来的内容会丢失。p194 函数exit（）的原型是在头文件cstdlib中定义的，在该头文件中，还定义了一个用于操作系统通信的参数值EXIT_FAILURE。p195 windows系统中的文本文件每行都已回车字符和换行符两个字符结尾，在通常情况下，C++在读取文件时将这两个字符转换为换行符，并在写入文件时执行相反的转换。有些文本编辑器不会自动在文件的最后一行加上换行符，因此，需要手动按下回车键再保存文件。p196 第七章 函数——C++的编程模块复习函数的基本知识 在C++中不能将数组作为函数返回值 （但是可以将数组作为结构或这对象的组成部分返回）。p204 函数定义必须提供标识符，而函数原型不要求，有类型列表就足够了：void cheers（int），通常，在原型的参数列表中，可以包括变量名，也可以不包括。原型中的变量名相当于占位符，因此不必与函数定义中的变量名相同。但是，好的变量名可以帮助理解程序功能，所以一般建议加上。p206 C++与接受可变参数的C函数交互时可能用到：void say（…）的形式。p206 通常，函数原型会自动将被传递的参数强制转换为期望的类型。（但函数重载可以导致二义性，因此不允许某些自动强制类型转换） 函数参数和按值传递 C++通常按值传递参数，这会让函数在自身的作用域内保持实参的副本，这种方式在一定程度上可以确保数据的完整性和安全性。 函数和数组 在C++中，当且仅当用于函数头或函数原型中，int *arr和int arr[]的含义才是相同的。在其他的环境下，二者的含义并不同，前者代表指向int类型的指针，后者代表数组名。p213 以下程序说明了数组函数一些有趣的地方，首先，cookies和arr指向同一个地址，但sizeof cookies的值是32，而sizeof arr的值是4。sizeof cookies是整个数组的长度，sizeof arr只是指针变量的长度。这也是必须显示传递数组长度，而不能在函数中使用sizeof arr的原因，因为指针本身并没有指出数组的长度。p215 12int cookies[size]=&#123;1,2,3,4,5,6,7,8&#125;;int *arr = cookies 由为防止函数中无意中修改数组的内容，可以在声明形参的时候使用关键字const，但应注意，这并不是意味着原始数组必须是常量而只意味着不能在函数中修改数组中的值。（对于普通变量来说，由于C++默认按值传递的特性，这种保护会自动实现）p217 使用数组区间（range）的函数 ：对于处理数组的函数，必须将数组的数据种类、起始位置和元素个数传递给它，传统的方法是传递数组名和数组个数n。另一种方法是传递两个指针，分别标识数组的开头和结尾，即数组区间。STL方法使用“超尾”的概念来指定区间，即end指针的是最后一个元素后面的指针。p220 指针和const： 情况1，pt指向一个const int，因此不能使用pt来修改这个值，但是这并不意味着age是一个常量，而只是说对于pt来说这是一个常量，我们依然可以直接通过age来修改age的值，但不能通过pt来修改它。同时，我们可以修改pt的值，即pt可以重新指向另一个地址。 情况2，finger只能指向age，但是允许使用finger来修改age。简而言之，finger和ps都是const，而*finger和ps不是。 情况3，stick只能指向age，并且不能通过stick修改age的值。p2211234int age=30；const int *pt=&amp;age；int *const finger=&amp;age;const int * const stick=&amp;age； 函数和二维数组 数组作参数的函数，必须牢记，数组名被视为地址，因此，相应的形参是一个指针，正确的函数原型如下所示，二者含义完全相同，后者可读性更强。注意，前者的括号是必不可少的，式子3代表的是指针数组，而不是指向二维数组的指针。123int sum (int (*arr)[4])int sum (int arr[][4])int *arr[4] 函数和C-风格字符串 C-风格字符串与常规char数组之间的区别：字符串有内置的结束字符’\0’。p225 空字符’\0’值等于0，因此可以直接用于while（）里的循环判定。p227 函数无法返回一个字符串，但是可以返回字符串的地址。p227 函数和结构 在涉及到函数时，结构变量的行为更接近与基本的单值变量，默认情况下是按值传递的，函数将使用原始结构的副本。当结构非常大时，这会增加内存要求，因此更推荐使用指针来传递结构体。指针传递时使用间接成员运算符’-&gt;’访问，值传递时使用成员运算符’.’访问。p228 当程序在输入循环以后还需要进行输入时，可以使用 cin.clear() 重置输入。p233 函数和string对象 虽然C-风格字符串和string对象的用途几乎相同，但与char数组相比，string对象更像是一个单一变量，可以将string直接复制，也可以直接在函数中传递。 函数和array对象 在C++中，类对象是基于结构的，因此结构变成方面的考虑因素也适用于类，所以可以按值将对象传递给函数。p236 array模板并非只能存储基本类型数据，它还可以存储类对象。p237 递归 C++函数允许自己调用自己（然而，与C语言不同，C++不允许main()调用自己） 函数指针 与数据项类似，函数也有地址，函数名即为函数的地址，它是存储其机器语言代码的内存的开始地址。p241 使用场景：要在当前函数中使用不同的算法来实现灵活的功能，可以将算法的函数地址作为参数进行传递，这就是函数指针。p241 注意以下代码的区别。p242 123int think ();process(think); //传递了函数的地址，process函数能够在其内部调用think函数thought(think()); //传递了函数的返回值 声明函数指针，最简单的方法就是，先写出该函数的原型，然后用(*pf)替换函数名即可，如下所示,pf即为函数指针。注意，括号的优先级比星号高，所以这里括号不可少。p242 1234double pam(int,double);double (*pf)(int,double); //pf是一个指针，指向doubel （int，double）类型的函数double *pf(int,double); //pf是一个函数，返回double *类型的数据pf = pam; //正确声明函数指针后，便可以将相应的函数赋给它 在使用函数指针时，下面两种方法等价！这很神奇！前者的好处是强调当前正在使用函数指针，后者的好处是使用起来很方便。至于为什么会这样，主要是因为有两种流派的声音，C++对这两种流派进行了折衷，认为二者都正确。p243 12345678double pam(int);double (*pf)(int);pf = pam;double y;y = pam(5);//下面两种方法等价y = (*pf)(5);y = pf(5); C++11的自动类型推断功能在函数指针声明并初始化时十分方便，以下两种声明初始化方式等价。p245 123const double *f1(const double ar[], int n);const bouble *(*pf)(const double ar[], int n) = f1;auto pf = f1; 函数指针数组,[]的优先级高级星号，所以先指明了这是一个包含3个元素的数组，声明的其他部分指出了元素的类型。所以pa是一个包含三个指针的数组，每个指针都指向一个函数，该函数返回指向double类型的指针。p245 1234567const double *(*pa[3])(const double *,int) = &#123;f1,f2,f3&#125;;auto pb = &#123;f1,f2,f3&#125; //非法！ auto只能用于单值初始化，不能用于初始化列表。auto pb=pa //但可以利用声明好的pa数组，来声明同样类型的数组。//使用时，想使用数组一样即可const double *px = pa[0](av,3);const double *py = (*pb[0])(av,3); //前面的括号必不可少 下面的声明，表示pd首先是一个指针，它指向一个包含三个元素的数组，数组中的元素是函数指针。这里pd其实就是指向pa的地址，pa是上面声明的函数指针数组的名字，也就是函数指针数组的首地址。p245 123456const double* (*(*pd)[3])(const double*,int) = &amp;pa;//调用方法，用``(*pd)``代替``pa``即可(*pd)[i](av,3); //返回指针(*(*pd)[i])(av,3); //与上面等价， 返回指针*(*pd)[i](av,3); //注意如果不带括号，先返回指针，然和用星号得到指针指向的值*(*(*pd)[i])(av,3) //与上一条等价，先返回指针，然和用星号得到指针指向的值 函数指针的声明有时候会很长，此时可使用auto（C++11）或typedef来对代码进行简化，方便编程。p248 12345678910//下面两条语句等价，前者使用方便，缺点就是无法直观看出pc的类型，后续程序可能会不小心产生类型赋值错误auto pc = &amp;pa;const double* (*(*pd)[3])(const double*,int) = &amp;pa;// 可以用typedef简化声明typedef double real; //正常声明变量，前面加上typedef，即可用后者代替前者typedef const double* (*p_fun)(const double*, int);p_fun pa[3] = &#123;f1,f2,f3&#125;;p_fun (*pa)[3] = &amp;pa; 第八章 函数探幽C++内联函数 常规函数与内敛函数之间的主要区别在于C++编译器如何将它们组合到程序中。 传统函数在被调用后，会立即存储该指令的内存地址，并将函数参数复制到堆栈，跳到函数起点的内存单元，然后执行函数的机器代码，之后再跳回到地址被保存的指令处。 来回跳跃并记录跳跃位置需要一定的开销。p253 内联函数的编译代码与其他程序的代码“内联”起来了，即编译器会使用相应的函数代码来替换函数调用（这就省去了来回跳跃的时间开销和内存开销）。 内联函数无需跳跃时间，因此加快了运行速度，但同时增加了存储内联函数的内存开销，如果程序在10个不同的地方调用同一个内联函数，就需要存储10个副本。 当函数的代码执行时间很短（函数很小），则内联调用可以省去调用时间。但是由于这个过程相当快，因此尽管接伸了该调用过程的大部分时间，但节省的时间绝对值并不大，除非该函数被经常调用。p253 使用内联时，在函数声明或定义前加上关键字inline。通常的做法是省略原型，将整个定义放在原型处，并加上内联关键字。p254 1inline double square(double x) &#123; return x*x&#125; inline工具是C++新增的特性，原始的C语言使用#define来实现内联（文本替换）p255 引用变量 引用变量，是 已定义的变量的别名 ，他的主要作用是用作函数的形参，如此一来，函数将使用原始数据，而不是其副本。 &amp;符号在变量前（右值）是代表“取地址”，在类型附近时（左值）代表“引用” 引用和指针的区别（引用看上去很像伪装的指针 “&amp;rodents=prats”）： 123int rats = 101;int &amp; rodents = rats; //rodents是rats的别名，二者指向同一块内存地址int * prats = &amp;rats; //prats指向rats的内存地址 引用在声明的同时必须进行初始化（做函数参数时，在函数调用时使用实参初始化），而不能像指针那样，先声明，在赋值 。引用更接近const指针，必须在创建时进行初始化，一旦与某个变量关联起来，就将一直效忠于它。p256123456789int rats = 101;int &amp; rodents = rats;int * const pr = &amp;rats; //上式是该式的伪装表示int bunnies = 50;rodents = bunnies; //试图将rodents变成bunnies的别名cout&lt;&lt;rodents&lt;&lt;endl; //输出50,和rodents值一样count&lt;&lt;rats&lt;&lt;endl; //但同时rats的值也变成了50cout&lt;&lt;&amp;rodents&lt;&lt;endl;cout&lt;&lt;&amp;bunnies&lt;&lt;endl; //二者的内存地址并不相同 const double &amp;ra用作函数参数时，在函数内不能修改ra的值（会报错），这在行为上与按值传递类似，但是当ra内存占用比较大时（结构或对象），就会很省内存（按值传递会生成副本，内存消耗大）。p261 对于基本类型，使用按值传递兼容性更好，因为按值传递可以自动强制类型转换，而const引用的限制更严格，因为它是别名，所以不能将表达式赋给引用。p261 12//现代C++中会报错，但早期C++只会警告，会创建一个临时变量，并将其初始化为x+3.0的值double &amp; ra = x + 1.0; 临时变量、引用参数和const： 当前，如果实参与引用参数不匹配，仅当引用参数为const引用时，C++将生成临时变量。创建临时变量的两种情况：p262 实参的类型正确，但不是左值。（字面常量，表达式） 实参的类型不正确，但可以转换为正确的类型。（int转double） 左值：左值参数是可以被引用的数据对象，例如，变量、数组元素、结构成员、引用和接触引用的指针都是左值。 非左值：字面常量（用引号扩起的字符串除外，它们由其地址表示）和包含多项的表达式。 （C语言中，左值最初指的是可出现在赋值语句左边的实体，引入const关键字后，const变量，虽然一般不出现在左边，但是可以通过地址访问它们） 非const引用无法生成临时变量，这是因为如果接受引用参数的函数的意图是修改作为参数传递的变量，临时变量将无法实现修改，所以现在的C++标准禁止创建临时变量（老的编译器只会发出警告”Warning: Temporary used for parameter ‘ra’ in call to refcube(double &amp;)”，遇到这种警告，一定要排除）。p263 将引用参数声明为const引用的理由有三个： p263 使用const可以避免无意中修改数据的变成错误; 使用const使函数能够处理cnost和非const实参，否则只能接受非const数据; 使用const引用能使函数能够正确生成并使用临时变量。 C++11新增了另一种引用—— 右值引用（rvalue reference） 。这种引用可指向右值，是使用&amp;&amp;声明的。新增右值引用的主要目的是，让库设计人员能够提供有些操作的更有效实现，实例见18章。&amp;声明的叫左值引用。：p263 123double &amp;&amp; rref = std::sqrt(36.00); // not allowed for double &amp;double j = 15.0;double &amp;&amp; jref = 2.0*j + 15.6; //not allowed for double &amp; 返回引用与传统返回机制的区别： 传统返回机制是按值传递函数参数类似，计算关键字return后面的表达式，并将结果返回给调用参数。而返回引用是返回return后面的变量的别名，并不会生成新的副本。p267 返回引用需要注意的问题： 最重要的是要避免返回函数终止时不再存在的内存单元的引用。（同样，也应避免返回指向临时变量的指针）。如下面的情况：p267 123456const double &amp; clone(double &amp; dref)&#123; double newguy; newguy = dref; return newguy; //返回newguy的引用，但是newguy在函数结束时会释放内存,会报错 return dref; //返回dref的引用，可行&#125; 前者可以编译，后者不可以。因为前者是指针，指向x，而后者是变量，是独立于x的副本。指针和副本都会在函数结束时释放，但是x并不会释放。p268 1234567891011#include &lt;iostream&gt;using namespace std;const int &amp; clone(int &amp; x)&#123; //可以编译 int *y = &amp;x; return *y;&#125;const int &amp; clone(int &amp; x)&#123; //不可以编译 int y = x; return y;&#125; 将C-风格字符串用作string对象引用参数，形参类型为const string &amp;时，实参类型可以为char*, const char*, string等（“abc”类型为const char*）。原因如下：p270 string类定义了一种char*到string的转换功能，这使得可以使用C-风格字符串来初始化string对象 const引用形参具有创建临时变量的属性。因此，当类型不符合时，会创建临时变量 对象、继承和引用： 除了可以使用父类的方法外，继承的另一个特征是，基类引用可以指向派生类对象，而无需进行强制类型转换。这种特征的实际结果是，可以定义一个接受基类引用作为参数的函数，调用该函数时，可以将基类对象作为实参，也可以将派生类对象作为实参。p271 使用引用参数两个主要原因： p274 程序员能够修改调用函数中的数据对象; 通过传递引用而不是整个数据对象，可以提高程序的运行速度。 指导原则： p274 对于使用传递的值而不作修改的函数 如果数据对象很小，如内置数据类型或小型结构，则按值传递; 如果数据对象是数组，则使用指针，因为这是唯一的选择，并将指针声明为指向const的指针; 如果数据对象是较大的结构，则是用const指针或const引用，以提高程序的效率。这样可以节省复制结构所需的时间和空间; 如果数据对象是类对象，则是用const引用。传递类对象参数的标准方式是按引用传递。 对于修改调用函数中的数据的函数 如果数据对象是内置数据类型，则是用指针; 如果数据对象是数组，则只能使用指针; 如果数据对象是结构，则使用引用或指针; 如果数据对象是类，则使用引用。 默认参数 对于带参数列表的函数，必须从右向左添加默认值。（即带默认值的参数的右边所有参数都要有默认值）。p275 12int harpo(int n, int m=4, int j=5); //validint chico(int n, int m=6, int j); //invalid 实参按从左到右的顺序一次被赋给相应的形参，而不能跳过任何参数。（这点与python不同） p275 123beeps = harpo(2); //same as harpo(2,4,5)beeps = harpo(1,8); //same as harpo(1,8,5)beeps = harpo(3, ,8); /invalid 函数重载 “多态”指的是函数有多种形式，“重载”指的是可以有多个同名的函数。二者指的是一回事 。p276 函数重载的关键是函数的参数列表——函数特征标（function signature）。如果两个函数的参数数目和类型相同，同时参数的排列顺序也相同，则它们的特征标相同，而 参数变量名是无关紧要的 。p277 编译器在检查函数特征标时，将把 类型引用和类型本身视为同一个特征标 。如以下两个看起来不同的特征标是不能共存的(它们都接受同一个参数x，会使得程序具有二义性)：p277 12double cube(double x);double cube(double &amp;x); 函数重载只看特征标是否相同，不关心函数返回类型。p278 当传入参数类型可以被强制转换时，将调用最匹配的版本：p278 123456void staff(double &amp; rs); // matches modifiable lvaluevoid staff(const double &amp; rcs); //matches rvalue, const lvaluevoid stove(double &amp; r1); // matches modifiable lvaluevoid stove(const double &amp; r2); //matches const lvaluevoid stove(double &amp;&amp; r3); //matches rvalue 名称修饰： C++通过名称修饰（name decoration）或名称矫正（name mangling）来区分重载函数，它会根据函数原型中指定的形参类型对每个函数名进行加密。p289 函数模板 函数模板是通用的函数描述，它们使用泛型来定义函数。模板并不创建任何函数，而只是告诉编译器如何定义函数。在标准C++98添加关键字typename之前，C++使用关键字class来创建模板。p281 12345678//如果需要多个将同一种算法用于不同类型的函数，可以使用模板template &lt;typename AnyType&gt; //注意没有分号;void Swap(AnyType &amp;a, AnyType &amp;b)&#123; //可以交换多种类型 AnyType temp; temp = a; a = b; b = temp;&#125; 函数模板不能缩短可执行程序。对于以不同类型多次调用模板的程序来说，最终仍然会生成多个独立的函数定义，就像以手工方式定义一样。 最终的代码不包含任何模板，而只包含了为程序生成的实际函数。p283 重载的模板： 被重载的模板的函数特征标必须不同：p283 1234template &lt;typename T&gt;void Swap(T &amp;a, T &amp;b);template &lt;typename T&gt;void Swap(T *a, T *b, int n); 显式具体化： （具体机制随着C++的演变而不断变化，下面是ISO/ANSI C++标准）p286 对于给定的函数名，可以有非模板函数、模板函数和显式具体化模板函数以及它们的重载版本。 显式具体化的原型和定义应以template&lt;&gt;打头，并通过名称来指出类型。 具体化优先于常规模板，而非模板函数优先于具体化和常规模板。12345struct job&#123;...&#125;;void Swap(job &amp;, job &amp;); //非模板函数template &lt;typename T&gt;void Swap(T &amp;, T &amp;); //模板函数template &lt;&gt; void Swap&lt;job&gt;(job &amp;, job &amp;); //显式具体化 实例化和具体化： 在代码中包含函数模板本身并不会生成函数定义，它只是一个用于生成函数定义的方案。编译器使用模板为特定类型生成函数定义时，得到的是模板实例（instantiation）。也就是说，模板并非函数定义，模板实例才是函数定义。p288 隐式实例化和显式实例化： p288 隐式(implicit)：通过函数调用导致编译器生成模板实例（大多数情况下都是隐式） 显示(explicit)：直接命令编译器创建特定的实例，方法如下：1template void Swap&lt;int&gt;(int, int); //explicit instantiation 显式实例化和显式具体化的区别： p288 显式实例化：使用Swap()模板来生成int类型的函数定义 1template void Swap&lt;int&gt;(int, int); //explicit instantiation 显式具体化(explicit specialization)：不要使用Swap()模板来生成函数定义，而应使用专门为int类型显式定义的函数定义。这些原型必须有自己的函数定义。 12template &lt;&gt; void Swap&lt;int&gt;(int &amp;, int &amp;);template &lt;&gt; void Swap(int &amp;, int &amp;); //这两句声明等价，任选其一 警告： 试图在同一个文件（或转换单元）中使用同一种类型的显式实例化和显式具体化将出错。 隐式实例化、显式实例化和显式具体化统称为具体化（specialization）。 它们的相同之处在于，它们表示的都是使用具体类型的函数定义，而不是通用描述。p289 重载解析（overloading resolution）： 对于函数重载、函数模板和函数模板重载，C++需要（且有）一个定义良好的策略，来决定为函数调用使用哪一个函数定义，尤其是有多个参数时。该策略大概过程如下：p289 第一步：创建候选函数列表。其中包含与被调用函数的名称相同的函数和模板函数。 第二步：使用候选函数列表创建可行函数列表。这些都是参数数目正确的函数，为此有一个隐式转换序列，其中包括实参类型与相应的形参类型完全匹配的情况。 第三步：确定是否有最佳的可行函数。如果有，则使用它，否则该函数调用出错。 匹配顺序： p290 完全匹配，但常规函数优先于模板。 提升转换（如，char和shorts自动转换为int，float自动转换为double）。 标准转换（如，int转换为char，long转换为double）。 用户自定义的转换（如，类声明中定义的转换）。 完全匹配与最佳匹配 完全匹配不等于最佳匹配，通常，有两个函数完全匹配是一种错误，但这一规则有两个例外。即有时候，即使两个函数都完全匹配，仍可完成重载解析。p290 指向非const数据的指针和引用，优先与非const指针和引用参数匹配。 下面两个式子都是完全匹配，但程序会选择前者，而不是报错： 1234567891011void recycle(blot &amp;); //#1void recycle(const blot &amp;); //#2struct blot &#123;int a; char b[10];&#125;;blot ink = &#123;25,&quot;spots&quot;&#125;;recycle(ink); //选择#1，因为ink没有被声明为const//然而，const和非const之间的区别只适用于指针和引用指向的数据//即，如果是如下定义，则将出现二义性错误void recycle(blot);void recycle(const blot); 两个完全匹配的函数，一个是非模板函数，另一个不是。此时，非模板函数将优先于模板函数（包括显式具体化）。如果两个完全匹配的函数都是模板函数，则较具体的模板函数优先。 C++98新增的特性—— 部分排序规则（partial ordering rules） 可以找出最具体的模板。 8.5小节涵盖的知识点很多，并且由于篇幅原因，没有详细展开，需要多看。]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深度学习》]]></title>
    <url>%2Fz_post%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Book-%E8%8A%B1%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[第一章 引言第二章 线性代数2.1 标量, 向量, 矩阵和张量 标量(scalar): 一个单独的数字 向量(vector): 一般默认是列向量, 为一列数字 矩阵(matrix): 多个列向量组成, 可看做是二维数组 张量(tensor): 超过两维的数组 2.2 矩阵和向量相乘2.3 单位矩阵和逆矩阵2.4 线性相关性和生成子空间范数]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉面试总结]]></title>
    <url>%2Fz_post%2F%E9%9D%A2%E8%AF%95-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[说话一定要说的慢一点, 一字一句, 有点节奏的说, 要显得自己胸有成竹, 对一部分的只是很懂, 很了解(切忌着急说一堆) 不会的地方大胆承认, 不要支支吾吾 在Faster RCNN中, 如果两个物体重合度很高, 会怎么样卷积神经网络复杂度分析卷积神经网络复杂度分析 全连接层的作用是什么？https://www.zhihu.com/question/41037974 卷积计算,卷积层特参数个数及征图谱尺寸计算卷积层输入图谱大小为 $D_in \times D_{in} \times depth_{in}$ , 卷积核尺寸为 $F \times F \times depth_{in}$, 步长为 $stride$ ,结合padding,输出的图谱size是多少 D_{out} = \frac{D_{in} - F + 2*Padding}{stride} + 1输出的特征图谱的深度为卷积核的个数: $depth_{out} = Num_{filters}$ 本层的偏置参数数量: $Num_{bias} = Num_{filters}$, 注意只与卷积核的个数有关, 与输入的特征图谱的深度无关 该层的参数个数 = 卷积核参数个数 + 偏置项参数个数: Num_{params} = F \times F \times depth_{in} \times depth_{out} + Num_{bias}L2正则化和L2规范化(归一化)的不同正则化是指正则项, 计算完以后是一个矢量. 归一化是将向量中每个元素进行归一化, 计算完以后还是同size的向量, L2归一化实际上就是对每一个元素除以L2正则项. 从 rcnn 到 fasterhttps://blog.csdn.net/xiaoye5606/article/details/71191429 为什么fast rcnn的roi pooling比spp的 spatial pooling效果好???神经网络参数初始化分类问题为什么用交叉熵典型错误答案1: 如果用交叉熵，能保证神经网络训练时是一个凸优化问题 错误原因: 凸函数的复合并不一定是凸函数 典型错误答案2: 如果当前值与目标值相差很远，则梯度下降法迭代时收敛的更快一些 错误原因: 欧式距离(平方损失)也能起到这个作用, 为什么不用? 正确答案: Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison. Relu和Dropout都具有正则化作用, 它们在正则化方面的区别是什么?Relu是强制正则化(所有神经元的输出值, 只要小于0, 就置为0) Dropout是随机正则化(随机让一些神经元的不起作用) 介绍一下 hard negative mining(难样例挖掘)dropout内部是怎么实现的在 训练阶段 给每个神经元的参数都会乘以 $\frac{1}{\alpha_{dropout}}$, 这样一来, 在训练阶段可以随时更改dropout的参数值, 而对于测试阶段来说, 无需对神经元进行任何额外处理, 所有的神经元都相当于适配了训练过程中dropout对参数带来的影响. 简述一下BN首先标准化就是将数据归一到一个希望的区间内, 一般都是归一化到激活函数敏感区域内, 而BN和传统标准标准化的区别主要有两点: BN是在每一个batch上做标准化的, 并且不仅仅只对输入层数据做标准化, 对网络内部的隐藏层输入也会进行标准话 第二就是BN并不是在标准的减均值初标准差之后, 还会进行一个线性变换,其本质就是改变数据分布的方差和均值. 对应的两个参数是通过学习学出来的. 其主要思想是考虑到数据可能本身就具有一定的不对称性, 并且激活函数也不一定就在面对标准数据时才有最好的表现, 因此 关于BN的详细解析可以看: 各种初始化方式，及公式各个参数对训练的影响目标检测，数据不平衡问题怎么解决对于目标物的不平衡问题, 通过采样方法来缓解. 对于前后景样本数的不平衡问题, 尝试使用FocalLoss来解决 L = -(1-p_t)^\gamma log(p_t)你的zerotensor和TF比性能上有优势吗常用的数据增强技术水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转 有哪些可以避免过拟合的办法数据增强, 正则化, 模型融合(其中dropout是模型融合方法中最高效和常用的技巧) 为了防止过拟合，增加训练样本是一个好的解决方案。此外，还可使用数据增强、L1 正则化、L2 正则化、Dropout、DropConnect 和早停（Early stopping）法等 正则化L1和L2的区别rcnn。。推导svm卷积层的参数个数计算公式是：输入的filers×kernerl size ×输出的filters。如： （3×3×256）×512 括号前面是每一个卷积核的大小，后面的是总共有512个卷积核 梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法详见梯度消失和梯度爆炸问题深入解析 关于各种激活函数的解析与讨论简述ResNet嵌入式开发很底层 一般还是倾向于做一些上层的东西推导SVM比较Boosting和Bagging的异同二者都是集成学习方法, 都是将多个弱学习器组合成强学习器的方法, 它们的区别在于: Boosting: 每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重, 训练得到k个弱分类器, 他们都有各自的权重, 通过加权组合的方式得到最终的分类结果 Bagging: 从原始数据集中每一轮又放回地抽取训练集(抽取的训练集小于原始数据集), 训练得到k个弱学习器, 然后将这k个软学习器的分类结果结合, 得到最终的分类结果. 无监督学习中存在过拟合吗?存在.//TODO 补充 什么情况下会产生无监督的过拟合 什么是K折交叉验证?将原始数据集划分为k个子集, 将其中一个子集作为验证集, 其余k-1个子集作为训练集, 如此训练和验证一轮成为一次交叉验证. 交叉验证重复k此, 每个子集都会做一次验证, 最终得到k个模型, 然后可以对这k个模型的结果加权平均, 以作为评估整体模型的依据 关于k折交叉验证, 需要注意什么?k越大, 不一定效果越好, 而且越大的k会加大训练时间; 在选择k时, 需要考虑最小化数据集之间的方差, 比如对于2分类任务, 如果采用2折交叉验证, 即对原始数据集二分,若此时训练集中都是A类别, 验证集中都是B类别, 则交叉验证效果会非常差 对于一个二分类问题, 我们定义超过阈值t的判定为正例, 否则判定为负例. 现在若将t增大, 则准确率和召回率会如何变化?准确率 = TP / (TP + FP), 召回率 = TP / (TP, FN) 若增大阈值t, 则更多不确定的样本将会被分为负例, 剩余确定样本的所占比例会增大, 那么准确率就会提升(或不变); 同时, 由于那些不确定的样本中还可能包含有正例, 引起, 阈值调大后, 这些正例就会被认为是负例, 所以召回率减小(或不变) 增加网络层数, 是否总能减小训练集错误率?不能, 有时候网络层数过深, 还会因为梯度消失导致模型退化, 使得模型性能降低 在目标检测问题上, 如何做数据增广?softmax怎么跟交叉熵损失函数结合?用梯度下降训练神经网络的参数, 为什么参数有时候会被训练为nan值?输入数据本身存在nan值, 或者考虑是否梯度爆炸了(可以试着降低学习率, 或者利用截断法先知梯度的值) 有没有自己试过更改模型的框架.有时候读paper会遇到一些好的点子或者方法, 自己会去加到现有的网络中去验证一下是不是能够提升模型的性能, 一般情况下, 比较经典且认可度较高的一些算法, 由于在网上都能找到相应的源码, 加上去的时候性能往往会有一点提升, 但是有时候有的方法比较偏, 我加完了以后有时候是没作用, 有时候是性能降低了, 我不知道到底是我实现的和paper有出入, 还是这个东西不适合当前框架 说一下你所有的提高精度的方法, 并且说明它们带来了多少的精度提升!OHEM ~3% SSD已经使用了难样例挖掘的技巧, Focal Loss 相比之下为什么能够提高池化的优点, 优化的缺点优点: 显著减少参数数量, 降低过拟合 池化单元具有平移不变性 缺点:pooling能够增大感受野, 让后续的卷积看到更多的信息, 但是它在降维的过程中丢失了一些信息, 这对segmentation要求的精确location有一定的影响, 所以pooling层跟segmentation有一定的冲突, 但是感受野的增大有可以特征检测实例的准确率, 还可以降低计算量, 增强泛化能力. 所以这个是实例分割问题需要解决的一个关键点之一. 待定https://blog.csdn.net/comway_Li/article/details/82532573 完善bisai待看https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion/64986 https://arxiv.org/abs/1809.00778 https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion 谈谈你参加的比赛对于一个比赛任务, 我会首先进行预处理, 之后, 会根据数据集的数据分布来对参数进行调整, 比如, 先只训练顶层, 然后逐步放开, 最后再训练所有层的参数. 在训练的时候,我一般都会采用bagging的思想, 将训练集随机28分, 分成3份, 然后训练, 最后进行模型融合, 融合的时候我一般都是对训练结果进行融合. 如果是目标检测累任务, 那么就:… 如果是实力分割类任务, 那么就: (对于一个新任务,) 你一般都会使用那些数据预处理方法 训练数据可视化 首先, 不论是什么样的数据集, 我都会先随机挑选 20 到 100张 的训练数据, 然后根据标签, 将图片数据可视化出来, 比如说如果是目标检测的任务, 我就会用opencv 的cv2.rectangle() 函数和 cv2.putText() 函数将标签里面的bbox标签和类别标签画到图片上去, 并且建立一个字典结构, 将不同的class-id对应到不同的颜色, 如果是实力分割任务, 我就会将mask标签反应到图片上去, 一般就是先将单通道的mask扩展成多通道的, 同时根据不同的class-id赋予不同的颜色, 最后利用numpy的where方法和原始图片进行叠加. 一般对于这种几十张的smaple图片, 我都是直接保存, 这样以后想再看的时候也不用重新跑脚本了. 之后, 我就会先简单浏览一下这些数据, 对整个数据集有一个初步的把握, 大概知道哪些物体被标注了, 有时候也能发现很多标注存在问题, 不过这也没有办法, 毕竟标注是一个很费时费力的工作, 错误在所难免. 计算数据分布信息 然后我就会写个脚本对整个数据集和标签进行遍历, 统计一些信息, 通常我会检测这么几个信息: 图像的平均尺寸, 整个数据集的像素平均值, 每张图片平均包含的目标个数, 每个类别的目标个数以及目标的平均大小, 同时, 因为平均值有时候往往反应不出来太多信息, 所以我还会用matplotlib把每种信息的直方图画出来, 然后看一下数据的整体分布是什么样子的, 比如图片size的分布, 目标大小的分布等等, 我主要就是根据这些分布信息来决定我最开始的参数设置. 主要调的参数就是imagesize,anchors相关的参数, 其他的还有就非极大抑制和置信度的阈值, 有时候还会试一下BN的作用(默认是关闭的) 然后一般情况下我都会对数据集做增广 常用的就是裁剪, 反转, 虚化, 颜色变换等等, 增广我不会做太多, 一般就用一些常用的增广方法 比赛中用到的模型融合方法:对于目标检测任务: 我用的融合策略就是先以一个结果文件为基准, 然后用另一个结果文件里面的某张图片的框去跟前一个结果文件对应图片的所有框作比较, 因为之间会对框的面积做排序, 所以只与面积相似的框作比较, 看看框的位置是不是也相似, 如果相似, 就认为检测的是同一个物体, 然后就看他们的类别是否相同, 这里我一般会使用三个结果文件(来自于三个不同模型)进行投票选择. 对于有的框不在另一个文件的, 我就会根据框的置信度来设置一个阈值, 大于阈值的我就直接把框加进去, 如果有票数相同的, 就按置信度来区分. 你的方法与其他人方法的区别是什么? 为什么比别人的方法差?对于faster rcnn 你都调了哪些参数?首先调的是anchor相关参数, 比如anchor size 和 anchor ratio 然后是学习率, 前景后景的样本比例, 非极大抑制的阈值, 候选区域块的生成个数, 图片的缩放尺度等等 BN具体是什么实现的对于平均移动了解吗积分图, 快速求矩阵的核样本不均衡问题怎么解决详细说一下Focal Loss说一下为什么Faster 比YOLO和SSD更准确样本采样的理论化值是anchor的参数设值怎么选的？ 为什么这么设置在调试RPN网络时有没有遇到什么问题？简述一下faster rcnn模型简述一下ResNet模型及它解决的问题]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>知识点梳理</tag>
      </tags>
  </entry>
</search>
